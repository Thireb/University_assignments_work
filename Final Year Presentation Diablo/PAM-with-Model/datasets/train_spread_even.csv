Abstract
"  In this paper, we show synchronization for a group of output passive agents
that communicate with each other according to an underlying communication graph
to achieve a common goal. We propose a distributed event-triggered control
framework that will guarantee synchronization and considerably decrease the
required communication load on the band-limited network. We define a general
Byzantine attack on the event-triggered multi-agent network system and
characterize its negative effects on synchronization. The Byzantine agents are
capable of intelligently falsifying their data and manipulating the underlying
communication graph by altering their respective control feedback weights. We
introduce a decentralized detection framework and analyze its steady-state and
transient performances. We propose a way of identifying individual Byzantine
neighbors and a learning-based method of estimating the attack parameters.
Lastly, we propose learning-based control approaches to mitigate the negative
effects of the adversarial attack.
"
"  We present a method for EMG-driven teleoperation of non-anthropomorphic robot
hands. EMG sensors are appealing as a wearable, inexpensive and unobtrusive way
to gather information about the teleoperator's hand pose. However, mapping from
EMG signals to the pose space of a non-anthropomorphic hand presents multiple
challenges. We present a method that first projects from forearm EMG into a
subspace relevant to teleoperation. To increase robustness, we use a model
which combines continuous and discrete predictors along different dimensions of
this subspace. We then project from the teleoperation subspace into the pose
space of the robot hand. We show that our method is effective and intuitive, as
it enables novice users to teleoperate pick and place tasks faster and more
robustly than state-of-the-art EMG teleoperation methods when applied to a
non-anthropomorphic, multi-DOF robot hand.
"
"  Traditionally social sciences are interested in structuring people in
multiple groups based on their individual preferences. This pa- per suggests an
approach to this problem in the framework of a non- cooperative game theory.
Definition of a suggested finite game includes a family of nested simultaneous
non-cooperative finite games with intra- and inter-coalition externalities. In
this family, games differ by the size of maximum coalition, partitions and by
coalition structure formation rules. A result of every game consists of
partition of players into coalitions and a payoff? profiles for every player.
Every game in the family has an equilibrium in mixed strategies with possibly
more than one coalition. The results of the game differ from those
conventionally discussed in cooperative game theory, e.g. the Shapley value,
strong Nash, coalition-proof equilibrium, core, kernel, nucleolus. We discuss
the following applications of the new game: cooperation as an allocation in one
coalition, Bayesian games, stochastic games and construction of a
non-cooperative criterion of coalition structure stability for studying focal
points.
"
"  Entropic regularization is quickly emerging as a new standard in optimal
transport (OT). It enables to cast the OT computation as a differentiable and
unconstrained convex optimization problem, which can be efficiently solved
using the Sinkhorn algorithm. However, entropy keeps the transportation plan
strictly positive and therefore completely dense, unlike unregularized OT. This
lack of sparsity can be problematic in applications where the transportation
plan itself is of interest. In this paper, we explore regularizing the primal
and dual OT formulations with a strongly convex term, which corresponds to
relaxing the dual and primal constraints with smooth approximations. We show
how to incorporate squared $2$-norm and group lasso regularizations within that
framework, leading to sparse and group-sparse transportation plans. On the
theoretical side, we bound the approximation error introduced by regularizing
the primal and dual formulations. Our results suggest that, for the regularized
primal, the approximation error can often be smaller with squared $2$-norm than
with entropic regularization. We showcase our proposed framework on the task of
color transfer.
"
"  We generalize a support vector machine to a support spinor machine by using
the mathematical structure of wedge product over vector machine in order to
extend field from vector field to spinor field. The separated hyperplane is
extended to Kolmogorov space in time series data which allow us to extend a
structure of support vector machine to a support tensor machine and a support
tensor machine moduli space. Our performance test on support spinor machine is
done over one class classification of end point in physiology state of time
series data after empirical mode analysis and compared with support vector
machine test. We implement algorithm of support spinor machine by using
Holo-Hilbert amplitude modulation for fully nonlinear and nonstationary time
series data analysis.
"
"  Visual tracking is a fundamental problem in computer vision. Recently, some
deep-learning-based tracking algorithms have been achieving record-breaking
performances. However, due to the high complexity of deep learning, most deep
trackers suffer from low tracking speed, and thus are impractical in many
real-world applications. Some new deep trackers with smaller network structure
achieve high efficiency while at the cost of significant decrease on precision.
In this paper, we propose to transfer the feature for image classification to
the visual tracking domain via convolutional channel reductions. The channel
reduction could be simply viewed as an additional convolutional layer with the
specific task. It not only extracts useful information for object tracking but
also significantly increases the tracking speed. To better accommodate the
useful feature of the target in different scales, the adaptation filters are
designed with different sizes. The yielded visual tracker is real-time and also
illustrates the state-of-the-art accuracies in the experiment involving two
well-adopted benchmarks with more than 100 test videos.
"
"  We design a deterministic polynomial time $c^n$ approximation algorithm for
the permanent of positive semidefinite matrices where $c=e^{\gamma+1}\simeq
4.84$. We write a natural convex relaxation and show that its optimum solution
gives a $c^n$ approximation of the permanent. We further show that this factor
is asymptotically tight by constructing a family of positive semidefinite
matrices.
"
"  Recent advances in policy gradient methods and deep learning have
demonstrated their applicability for complex reinforcement learning problems.
However, the variance of the performance gradient estimates obtained from the
simulation is often excessive, leading to poor sample efficiency. In this
paper, we apply the stochastic variance reduced gradient descent (SVRG) to
model-free policy gradient to significantly improve the sample-efficiency. The
SVRG estimation is incorporated into a trust-region Newton conjugate gradient
framework for the policy optimization. On several Mujoco tasks, our method
achieves significantly better performance compared to the state-of-the-art
model-free policy gradient methods in robotic continuous control such as trust
region policy optimization (TRPO)
"
"  We primarily study a special a weighted low-rank approximation of matrices
and then apply it to solve the background modeling problem. We propose two
algorithms for this purpose: one operates in the batch mode on the entire data
and the other one operates in the batch-incremental mode on the data and
naturally captures more background variations and computationally more
effective. Moreover, we propose a robust technique that learns the background
frame indices from the data and does not require any training frames. We
demonstrate through extensive experiments that by inserting a simple weight in
the Frobenius norm, it can be made robust to the outliers similar to the
$\ell_1$ norm. Our methods match or outperform several state-of-the-art online
and batch background modeling methods in virtually all quantitative and
qualitative measures.
"
"  Although compelling assessments have been examined in recent years, more
studies are required to yield a better understanding of the several methods
where assessment techniques significantly affect student learning process. Most
of the educational research in this area does not consider demographics data,
differing methodologies, and notable sample size. To address these drawbacks,
the objective of our study is to analyse student learning outcomes of multiple
assessment formats for a web-facilitated in-class section with an asynchronous
online class of a core data communications course in the Undergraduate IT
program of the Information Sciences and Technology (IST) Department at George
Mason University (GMU). In this study, students were evaluated based on course
assessments such as home and lab assignments, skill-based assessments, and
traditional midterm and final exams across all four sections of the course. All
sections have equivalent content, assessments, and teaching methodologies.
Student demographics such as exam type and location preferences are considered
in our study to determine whether they have any impact on their learning
approach. Large amount of data from the learning management system (LMS),
Blackboard (BB) Learn, had to be examined to compare the results of several
assessment outcomes for all students within their respective section and
amongst students of other sections. To investigate the effect of dissimilar
assessment formats on student performance, we had to correlate individual
question formats with the overall course grade. The results show that
collective assessment formats allow students to be effective in demonstrating
their knowledge.
"
"  Decoding human brain activities via functional magnetic resonance imaging
(fMRI) has gained increasing attention in recent years. While encouraging
results have been reported in brain states classification tasks, reconstructing
the details of human visual experience still remains difficult. Two main
challenges that hinder the development of effective models are the perplexing
fMRI measurement noise and the high dimensionality of limited data instances.
Existing methods generally suffer from one or both of these issues and yield
dissatisfactory results. In this paper, we tackle this problem by casting the
reconstruction of visual stimulus as the Bayesian inference of missing view in
a multiview latent variable model. Sharing a common latent representation, our
joint generative model of external stimulus and brain response is not only
""deep"" in extracting nonlinear features from visual images, but also powerful
in capturing correlations among voxel activities of fMRI recordings. The
nonlinearity and deep structure endow our model with strong representation
ability, while the correlations of voxel activities are critical for
suppressing noise and improving prediction. We devise an efficient variational
Bayesian method to infer the latent variables and the model parameters. To
further improve the reconstruction accuracy, the latent representations of
testing instances are enforced to be close to that of their neighbours from the
training set via posterior regularization. Experiments on three fMRI recording
datasets demonstrate that our approach can more accurately reconstruct visual
stimuli.
"
"  If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to
its $k-$nearest neighbors, then it is well known that there exists a giant
connected component with high probability. We prove that in $[0,1]^d$ it
suffices to connect every point to $ c_{d,1} \log{\log{n}}$ points chosen
randomly among its $ c_{d,2} \log{n}-$nearest neighbors to ensure a giant
component of size $n - o(n)$ with high probability. This construction yields a
much sparser random graph with $\sim n \log\log{n}$ instead of $\sim n \log{n}$
edges that has comparable connectivity properties. This result has nontrivial
implications for problems in data science where an affinity matrix is
constructed: instead of picking the $k-$nearest neighbors, one can often pick
$k' \ll k$ random points out of the $k-$nearest neighbors without sacrificing
efficiency. This can massively simplify and accelerate computation, we
illustrate this with several numerical examples.
"
"  This paper shows that conditional independence reasoning can be applied to
optimize epistemic model checking, in which one verifies that a model for a
number of agents operating with imperfect information satisfies a formula
expressed in a modal multi-agent logic of knowledge. The optimization has been
implemented in the epistemic model checker MCK. The paper reports experimental
results demonstrating that it can yield multiple orders of magnitude
performance improvements.
"
"  Agile localization of anomalous events plays a pivotal role in enhancing the
overall reliability of the grid and avoiding cascading failures. This is
especially of paramount significance in the large-scale grids due to their
geographical expansions and the large volume of data generated. This paper
proposes a stochastic graphical framework, by leveraging which it aims to
localize the anomalies with the minimum amount of data. This framework
capitalizes on the strong correlation structures observed among the
measurements collected from different buses. The proposed approach, at its
core, collects the measurements sequentially and progressively updates its
decision about the location of the anomaly. The process resumes until the
location of the anomaly can be identified with desired reliability. We provide
a general theory for the quickest anomaly localization and also investigate its
application for quickest line outage localization. Simulations in the IEEE
118-bus model are provided to establish the gains of the proposed approach.
"
"  The tensor train decomposition decomposes a tensor into a ""train"" of 3-way
tensors that are interconnected through the summation of auxiliary indices. The
decomposition is stable, has a well-defined notion of rank and enables the user
to perform various linear algebra operations on vectors and matrices of
exponential size in a computationally efficient manner. The tensor ring
decomposition replaces the train by a ring through the introduction of one
additional auxiliary variable. This article discusses a major issue with the
tensor ring decomposition: its inability to compute an exact minimal-rank
decomposition from a decomposition with sub-optimal ranks. Both the contraction
operation and Hadamard product are motivated from applications and it is shown
through simple examples how the tensor ring-rounding procedure fails to
retrieve minimal-rank decompositions with these operations. These observations,
together with the already known issue of not being able to find a best low-rank
tensor ring approximation to a given tensor indicate that the applicability of
tensor rings is severely limited.
"
"  Remote sensing image scene classification plays an important role in a wide
range of applications and hence has been receiving remarkable attention. During
the past years, significant efforts have been made to develop various datasets
or present a variety of approaches for scene classification from remote sensing
images. However, a systematic review of the literature concerning datasets and
methods for scene classification is still lacking. In addition, almost all
existing datasets have a number of limitations, including the small scale of
scene classes and the image numbers, the lack of image variations and
diversity, and the saturation of accuracy. These limitations severely limit the
development of new approaches especially deep learning-based methods. This
paper first provides a comprehensive review of the recent progress. Then, we
propose a large-scale dataset, termed ""NWPU-RESISC45"", which is a publicly
available benchmark for REmote Sensing Image Scene Classification (RESISC),
created by Northwestern Polytechnical University (NWPU). This dataset contains
31,500 images, covering 45 scene classes with 700 images in each class. The
proposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total
image number, (ii) holds big variations in translation, spatial resolution,
viewpoint, object pose, illumination, background, and occlusion, and (iii) has
high within-class diversity and between-class similarity. The creation of this
dataset will enable the community to develop and evaluate various data-driven
algorithms. Finally, several representative methods are evaluated using the
proposed dataset and the results are reported as a useful baseline for future
research.
"
"  An efficient adaptive algorithm for the removal of Salt and Pepper noise from
gray scale and color image is presented in this paper. In this proposed method
first a 3X3 window is taken and the central pixel of the window is considered
as the processing pixel. If the processing pixel is found as uncorrupted, then
it is left unchanged. And if the processing pixel is found corrupted one, then
the window size is increased according to the conditions given in the proposed
algorithm. Finally the processing pixel or the central pixel is replaced by
either the mean, median or trimmed value of the elements in the current window
depending upon different conditions of the algorithm. The proposed algorithm
efficiently removes noise at all densities with better Peak Signal to Noise
Ratio (PSNR) and Image Enhancement Factor (IEF). The proposed algorithm is
compared with different existing algorithms like MF, AMF, MDBUTMF, MDBPTGMF and
AWMF.
"
"  Strong product is an efficient way to construct a larger digraph through some
specific small digraphs. The large digraph constructed by the strong product
method contains the factor digraphs as its subgraphs, and can retain some good
properties of the factor digraphs. The distance of digraphs is one of the most
basic structural parameters in graph theory, and it plays an important role in
analyzing the effectiveness of interconnection networks. In particular, it
provides a basis for measuring the transmission delay of networks. When the
topological structure of an interconnection network is represented by a
digraph, the average distance of the directed graph is a good measure of the
communication performance of the network. In this paper, we mainly investigate
the distance and average distance of strong product digraphs, and give a
formula for the distance of strong product digraphs and an algorithm for
solving the average distance of strong product digraphs.
"
"  Energy has been increasingly generated or collected by different entities on
the power grid (e.g., universities, hospitals and householdes) via solar
panels, wind turbines or local generators in the past decade. With local
energy, such electricity consumers can be considered as ""microgrids"" which can
simulataneously generate and consume energy. Some microgrids may have excessive
energy that can be shared to other power consumers on the grid. To this end,
all the entities have to share their local private information (e.g., their
local demand, local supply and power quality data) to each other or a
third-party to find and implement the optimal energy sharing solution. However,
such process is constrained by privacy concerns raised by the microgrids. In
this paper, we propose a privacy preserving scheme for all the microgrids which
can securely implement their energy sharing against both semi-honest and
colluding adversaries. The proposed approach includes two secure communication
protocols that can ensure quantified privacy leakage and handle collusions.
"
"  Word embeddings use vectors to represent words such that the geometry between
vectors captures semantic relationship between the words. In this paper, we
develop a framework to demonstrate how the temporal dynamics of the embedding
can be leveraged to quantify changes in stereotypes and attitudes toward women
and ethnic minorities in the 20th and 21st centuries in the United States. We
integrate word embeddings trained on 100 years of text data with the U.S.
Census to show that changes in the embedding track closely with demographic and
occupation shifts over time. The embedding captures global social shifts --
e.g., the women's movement in the 1960s and Asian immigration into the U.S --
and also illuminates how specific adjectives and occupations became more
closely associated with certain populations over time. Our framework for
temporal analysis of word embedding opens up a powerful new intersection
between machine learning and quantitative social science.
"
"  We introduce $\Psi$ec, a local spectral exterior calculus that provides a
discretization of Cartan's exterior calculus of differential forms using
wavelet functions. Our construction consists of differential form wavelets with
flexible directional localization, between fully isotropic and curvelet- and
ridgelet-like, that provide tight frames for the spaces of $k$-forms in
$\mathbb{R}^2$ and $\mathbb{R}^3$. By construction, these wavelets satisfy the
de Rahm co-chain complex, the Hodge decomposition, and that the integral of a
$k+1$-form is a $k$-form. They also enforce Stokes' theorem for differential
forms, and we show that with a finite number of wavelet levels it is most
efficiently approximated using anisotropic curvelet- or ridgelet-like forms.
Our construction is based on the intrinsic geometric properties of the exterior
calculus in the Fourier domain. To reveal these, we extend existing results on
the Fourier transform of differential forms to a frequency domain description
of the exterior calculus, including, for example, a Parseval theorem for forms
and a description of the symbols of all important operators.
"
"  Limited-angle computed tomography (CT) is often used in clinical applications
such as C-arm CT for interventional imaging. However, CT images from limited
angles suffers from heavy artifacts due to incomplete projection data. Existing
iterative methods require extensive calculations but can not deliver
satisfactory results. Based on the observation that the artifacts from limited
angles have some directional property and are globally distributed, we propose
a novel multi-scale wavelet domain residual learning architecture, which
compensates for the artifacts. Experiments have shown that the proposed method
effectively eliminates artifacts, thereby preserving edge and global structures
of the image.
"
"  This paper proposes a new algorithm for controlling classification results by
generating a small additive perturbation without changing the classifier
network. Our work is inspired by existing works generating adversarial
perturbation that worsens classification performance. In contrast to the
existing methods, our work aims to generate perturbations that can enhance
overall classification performance. To solve this performance enhancement
problem, we newly propose a perturbation generation network (PGN) influenced by
the adversarial learning strategy. In our problem, the information in a large
external dataset is summarized by a small additive perturbation, which helps to
improve the performance of the classifier trained with the target dataset. In
addition to this performance enhancement problem, we show that the proposed PGN
can be adopted to solve the classical adversarial problem without utilizing the
information on the target classifier. The mentioned characteristics of our
method are verified through extensive experiments on publicly available visual
datasets.
"
"  We analyze the secrecy outage probability in the downlink for wireless
networks with spatially (Poisson) distributed eavesdroppers (EDs) under the
assumption that the base station employs transmit antenna selection (TAS) to
enhance secrecy performance. We compare the cases where the receiving user
equipment (UE) operates in half-duplex (HD) mode and full-duplex (FD) mode. In
the latter case, the UE simultaneously receives the intended downlink message
and transmits a jamming signal to strengthen secrecy. We investigate two models
of (semi)passive eavesdropping: (1) EDs act independently and (2) EDs collude
to intercept the transmitted message. For both of these models, we obtain
expressions for the secrecy outage probability in the downlink for HD and FD UE
operation. The expressions for HD systems have very accurate approximate or
exact forms in terms of elementary and/or special functions for all path loss
exponents. Those related to the FD systems have exact integral forms for
general path loss exponents, while exact closed forms are given for specific
exponents. A closed-form approximation is also derived for the FD case with
colluding EDs. The resulting analysis shows that the reduction in the secrecy
outage probability is logarithmic in the number of antennas used for TAS and
identifies conditions under which HD operation should be used instead of FD
jamming at the UE. These performance trends and exact relations between system
parameters can be used to develop adaptive power allocation and duplex
operation methods in practice. Examples of such techniques are alluded to
herein.
"
"  We introduce a novel method for defining geographic districts in road
networks using stable matching. In this approach, each geographic district is
defined in terms of a center, which identifies a location of interest, such as
a post office or polling place, and all other network vertices must be labeled
with the center to which they are associated. We focus on defining geographic
districts that are equitable, in that every district has the same number of
vertices and the assignment is stable in terms of geographic distance. That is,
there is no unassigned vertex-center pair such that both would prefer each
other over their current assignments. We solve this problem using a version of
the classic stable matching problem, called symmetric stable matching, in which
the preferences of the elements in both sets obey a certain symmetry. In our
case, we study a graph-based version of stable matching in which nodes are
stably matched to a subset of nodes denoted as centers, prioritized by their
shortest-path distances, so that each center is apportioned a certain number of
nodes. We show that, for a planar graph or road network with $n$ nodes and $k$
centers, the problem can be solved in $O(n\sqrt{n}\log n)$ time, which improves
upon the $O(nk)$ runtime of using the classic Gale-Shapley stable matching
algorithm when $k$ is large. Finally, we provide experimental results on road
networks for these algorithms and a heuristic algorithm that performs better
than the Gale-Shapley algorithm for any range of values of $k$.
"
"  Computer aided diagnostic (CAD) system is crucial for modern med-ical
imaging. But almost all CAD systems operate on reconstructed images, which were
optimized for radiologists. Computer vision can capture features that is subtle
to human observers, so it is desirable to design a CAD system op-erating on the
raw data. In this paper, we proposed a deep-neural-network-based detection
system for lung nodule detection in computed tomography (CT). A
primal-dual-type deep reconstruction network was applied first to convert the
raw data to the image space, followed by a 3-dimensional convolutional neural
network (3D-CNN) for the nodule detection. For efficient network training, the
deep reconstruction network and the CNN detector was trained sequentially
first, then followed by one epoch of end-to-end fine tuning. The method was
evaluated on the Lung Image Database Consortium image collection (LIDC-IDRI)
with simulated forward projections. With 144 multi-slice fanbeam pro-jections,
the proposed end-to-end detector could achieve comparable sensitivity with the
reference detector, which was trained and applied on the fully-sampled image
data. It also demonstrated superior detection performance compared to detectors
trained on the reconstructed images. The proposed method is general and could
be expanded to most detection tasks in medical imaging.
"
"  It has recently been shown that if feedback effects of decisions are ignored,
then imposing fairness constraints such as demographic parity or equality of
opportunity can actually exacerbate unfairness. We propose to address this
challenge by modeling feedback effects as the dynamics of a Markov decision
processes (MDPs). First, we define analogs of fairness properties that have
been proposed for supervised learning. Second, we propose algorithms for
learning fair decision-making policies for MDPs. We also explore extensions to
reinforcement learning, where parts of the dynamical system are unknown and
must be learned without violating fairness. Finally, we demonstrate the need to
account for dynamical effects using simulations on a loan applicant MDP.
"
"  It is difficult for humans to efficiently teach robots how to correctly
perform a task. One intuitive solution is for the robot to iteratively learn
the human's preferences from corrections, where the human improves the robot's
current behavior at each iteration. When learning from corrections, we argue
that while the robot should estimate the most likely human preferences, it
should also know what it does not know, and integrate this uncertainty as it
makes decisions. We advance the state-of-the-art by introducing a Kalman filter
for learning from corrections: this approach obtains the uncertainty of the
estimated human preferences. Next, we demonstrate how the estimate uncertainty
can be leveraged for active learning and risk-sensitive deployment. Our results
indicate that obtaining and leveraging uncertainty leads to faster learning
from human corrections.
"
"  To perform tasks specified by natural language instructions, autonomous
agents need to extract semantically meaningful representations of language and
map it to visual elements and actions in the environment. This problem is
called task-oriented language grounding. We propose an end-to-end trainable
neural architecture for task-oriented language grounding in 3D environments
which assumes no prior linguistic or perceptual knowledge and requires only raw
pixels from the environment and the natural language instruction as input. The
proposed model combines the image and text representations using a
Gated-Attention mechanism and learns a policy to execute the natural language
instruction using standard reinforcement and imitation learning methods. We
show the effectiveness of the proposed model on unseen instructions as well as
unseen maps, both quantitatively and qualitatively. We also introduce a novel
environment based on a 3D game engine to simulate the challenges of
task-oriented language grounding over a rich set of instructions and
environment states.
"
"  It is known that individuals in social networks tend to exhibit homophily
(a.k.a. assortative mixing) in their social ties, which implies that they
prefer bonding with others of their own kind. But what are the reasons for this
phenomenon? Is it that such relations are more convenient and easier to
maintain? Or are there also some more tangible benefits to be gained from this
collective behaviour?
The current work takes a game-theoretic perspective on this phenomenon, and
studies the conditions under which different assortative mixing strategies lead
to equilibrium in an evolving social network. We focus on a biased preferential
attachment model where the strategy of each group (e.g., political or social
minority) determines the level of bias of its members toward other group
members and non-members. Our first result is that if the utility function that
the group attempts to maximize is the degree centrality of the group,
interpreted as the sum of degrees of the group members in the network, then the
only strategy achieving Nash equilibrium is a perfect homophily, which implies
that cooperation with other groups is harmful to this utility function. A
second, and perhaps more surprising, result is that if a reward for inter-group
cooperation is added to the utility function (e.g., externally enforced by an
authority as a regulation), then there are only two possible equilibria,
namely, perfect homophily or perfect heterophily, and it is possible to
characterize their feasibility spaces. Interestingly, these results hold
regardless of the minority-majority ratio in the population.
We believe that these results, as well as the game-theoretic perspective
presented herein, may contribute to a better understanding of the forces that
shape the groups and communities of our society.
"
"  In the last decade, deep learning has contributed to advances in a wide range
computer vision tasks including texture analysis. This paper explores a new
approach for texture segmentation using deep convolutional neural networks,
sharing important ideas with classic filter bank based texture segmentation
methods. Several methods are developed to train Fully Convolutional Networks to
segment textures in various applications. We show in particular that these
networks can learn to recognize and segment a type of texture, e.g. wood and
grass from texture recognition datasets (no training segmentation). We
demonstrate that Fully Convolutional Networks can learn from repetitive
patterns to segment a particular texture from a single image or even a part of
an image. We take advantage of these findings to develop a method that is
evaluated on a series of supervised and unsupervised experiments and improve
the state of the art on the Prague texture segmentation datasets.
"
"  This paper presents an automated supervised method for Persian wordnet
construction. Using a Persian corpus and a bi-lingual dictionary, the initial
links between Persian words and Princeton WordNet synsets have been generated.
These links will be discriminated later as correct or incorrect by employing
seven features in a trained classification system. The whole method is just a
classification system, which has been trained on a train set containing FarsNet
as a set of correct instances. State of the art results on the automatically
derived Persian wordnet is achieved. The resulted wordnet with a precision of
91.18% includes more than 16,000 words and 22,000 synsets.
"
"  Visual question answering (or VQA) is a new and exciting problem that
combines natural language processing and computer vision techniques. We present
a survey of the various datasets and models that have been used to tackle this
task. The first part of the survey details the various datasets for VQA and
compares them along some common factors. The second part of this survey details
the different approaches for VQA, classified into four types: non-deep learning
models, deep learning models without attention, deep learning models with
attention, and other models which do not fit into the first three. Finally, we
compare the performances of these approaches and provide some directions for
future work.
"
"  With the rise of end-to-end learning through deep learning, person detectors
and re-identification (ReID) models have recently become very strong.
Multi-camera multi-target (MCMT) tracking has not fully gone through this
transformation yet. We intend to take another step in this direction by
presenting a theoretically principled way of integrating ReID with tracking
formulated as an optimal Bayes filter. This conveniently side-steps the need
for data-association and opens up a direct path from full images to the core of
the tracker. While the results are still sub-par, we believe that this new,
tight integration opens many interesting research opportunities and leads the
way towards full end-to-end tracking from raw pixels.
"
"  From medical charts to national census, healthcare has traditionally operated
under a paper-based paradigm. However, the past decade has marked a long and
arduous transformation bringing healthcare into the digital age. Ranging from
electronic health records, to digitized imaging and laboratory reports, to
public health datasets, today, healthcare now generates an incredible amount of
digital information. Such a wealth of data presents an exciting opportunity for
integrated machine learning solutions to address problems across multiple
facets of healthcare practice and administration. Unfortunately, the ability to
derive accurate and informative insights requires more than the ability to
execute machine learning models. Rather, a deeper understanding of the data on
which the models are run is imperative for their success. While a significant
effort has been undertaken to develop models able to process the volume of data
obtained during the analysis of millions of digitalized patient records, it is
important to remember that volume represents only one aspect of the data. In
fact, drawing on data from an increasingly diverse set of sources, healthcare
data presents an incredibly complex set of attributes that must be accounted
for throughout the machine learning pipeline. This chapter focuses on
highlighting such challenges, and is broken down into three distinct
components, each representing a phase of the pipeline. We begin with attributes
of the data accounted for during preprocessing, then move to considerations
during model building, and end with challenges to the interpretation of model
output. For each component, we present a discussion around data as it relates
to the healthcare domain and offer insight into the challenges each may impose
on the efficiency of machine learning techniques.
"
"  In autonomous racing, vehicles operate close to the limits of handling and a
sensor failure can have critical consequences. To limit the impact of such
failures, this paper presents the redundant perception and state estimation
approaches developed for an autonomous race car. Redundancy in perception is
achieved by estimating the color and position of the track delimiting objects
using two sensor modalities independently. Specifically, learning-based
approaches are used to generate color and pose estimates, from LiDAR and camera
data respectively. The redundant perception inputs are fused by a particle
filter based SLAM algorithm that operates in real-time. Velocity is estimated
using slip dynamics, with reliability being ensured through a probabilistic
failure detection algorithm. The sub-modules are extensively evaluated in
real-world racing conditions using the autonomous race car ""gotthard
driverless"", achieving lateral accelerations up to 1.7G and a top speed of
90km/h.
"
"  Deep neural networks (DNNs) have begun to have a pervasive impact on various
applications of machine learning. However, the problem of finding an optimal
DNN architecture for large applications is challenging. Common approaches go
for deeper and larger DNN architectures but may incur substantial redundancy.
To address these problems, we introduce a network growth algorithm that
complements network pruning to learn both weights and compact DNN architectures
during training. We propose a DNN synthesis tool (NeST) that combines both
methods to automate the generation of compact and accurate DNNs. NeST starts
with a randomly initialized sparse network called the seed architecture. It
iteratively tunes the architecture with gradient-based growth and
magnitude-based pruning of neurons and connections. Our experimental results
show that NeST yields accurate, yet very compact DNNs, with a wide range of
seed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we
reduce network parameters by 70.2x (74.3x) and floating-point operations
(FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce
network parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively.
NeST's grow-and-prune paradigm delivers significant additional parameter and
FLOPs reduction relative to pruning-only methods.
"
"  Automatically determining the optimal size of a neural network for a given
task without prior information currently requires an expensive global search
and training many networks from scratch. In this paper, we address the problem
of automatically finding a good network size during a single training cycle. We
introduce *nonparametric neural networks*, a non-probabilistic framework for
conducting optimization over all possible network sizes and prove its soundness
when network growth is limited via an L_p penalty. We train networks under this
framework by continuously adding new units while eliminating redundant units
via an L_2 penalty. We employ a novel optimization algorithm, which we term
*adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising
results.
"
"  Traffic for internet video streaming has been rapidly increasing and is
further expected to increase with the higher definition videos and IoT
applications, such as 360 degree videos and augmented virtual reality
applications. While efficient management of heterogeneous cloud resources to
optimize the quality of experience is important, existing work in this problem
space often left out important factors. In this paper, we present a model for
describing a today's representative system architecture for video streaming
applications, typically composed of a centralized origin server and several CDN
sites. Our model comprehensively considers the following factors: limited
caching spaces at the CDN sites, allocation of CDN for a video request, choice
of different ports from the CDN, and the central storage and bandwidth
allocation. With the model, we focus on minimizing a performance metric, stall
duration tail probability (SDTP), and present a novel, yet efficient, algorithm
to solve the formulated optimization problem. The theoretical bounds with
respect to the SDTP metric are also analyzed and presented. Our extensive
simulation results demonstrate that the proposed algorithms can significantly
improve the SDTP metric, compared to the baseline strategies. Small-scale video
streaming system implementation in a real cloud environment further validates
our results.
"
"  This paper is concerned with the channel estimation problem in multi-user
millimeter wave (mmWave) wireless systems with large antenna arrays. We develop
a novel simultaneous-estimation with iterative fountain training (SWIFT)
framework, in which multiple users estimate their channels at the same time and
the required number of channel measurements is adapted to various channel
conditions of different users. To achieve this, we represent the beam direction
estimation process by a graph, referred to as the beam-on-graph, and associate
the channel estimation process with a code-on-graph decoding problem.
Specifically, the base station (BS) and each user measure the channel with a
series of random combinations of transmit/receive beamforming vectors until the
channel estimate converges. As the proposed SWIFT does not adapt the BS's beams
to any single user, we are able to estimate all user channels simultaneously.
Simulation results show that SWIFT can significantly outperform the existing
random beamforming-based approaches, which use a predetermined number of
measurements, over a wide range of signal-to-noise ratios and channel coherence
time. Furthermore, by utilizing the users' order in terms of completing their
channel estimation, our SWIFT framework can infer the sequence of users'
channel quality and perform effective user scheduling to achieve superior
performance.
"
"  A policy maker faces a sequence of unknown outcomes. At each stage two
(self-proclaimed) experts provide probabilistic forecasts on the outcome in the
next stage. A comparison test is a protocol for the policy maker to
(eventually) decide which of the two experts is better informed. The protocol
takes as input the sequence of pairs of forecasts and actual realizations and
(weakly) ranks the two experts. We propose two natural properties that such a
comparison test must adhere to and show that these essentially uniquely
determine the comparison test. This test is a function of the derivative of the
induced pair of measures at the realization.
"
"  Transfer learning through fine-tuning a pre-trained neural network with an
extremely large dataset, such as ImageNet, can significantly accelerate
training while the accuracy is frequently bottlenecked by the limited dataset
size of the new target task. To solve the problem, some regularization methods,
constraining the outer layer weights of the target network using the starting
point as references (SPAR), have been studied. In this paper, we propose a
novel regularized transfer learning framework DELTA, namely DEep Learning
Transfer using Feature Map with Attention. Instead of constraining the weights
of neural network, DELTA aims to preserve the outer layer outputs of the target
network. Specifically, in addition to minimizing the empirical loss, DELTA
intends to align the outer layer outputs of two networks, through constraining
a subset of feature maps that are precisely selected by attention that has been
learned in an supervised learning manner. We evaluate DELTA with the
state-of-the-art algorithms, including L2 and L2-SP. The experiment results
show that our proposed method outperforms these baselines with higher accuracy
for new tasks.
"
"  In this work, we present an analysis of the Burst failure effect in the
$H_\infty$ norm. We present a procedure to perform an analysis between
different Markov Chain models and a numerical example. In the numerical example
the results obtained pointed out that the burst failure effect in the
performance does not exceed 6.3%. However, this work is an introduction for a
wider and more extensive analysis in this subject.
"
"  The architecture of Exascale computing facilities, which involves millions of
heterogeneous processing units, will deeply impact on scientific applications.
Future astrophysical HPC applications must be designed to make such computing
systems exploitable. The ExaNeSt H2020 EU-funded project aims to design and
develop an exascale ready prototype based on low-energy-consumption ARM64 cores
and FPGA accelerators. We participate to the design of the platform and to the
validation of the prototype with cosmological N-body and hydrodynamical codes
suited to perform large-scale, high-resolution numerical simulations of cosmic
structures formation and evolution. We discuss our activities on astrophysical
applications to take advantage of the underlying architecture.
"
"  The game of chess is the most widely-studied domain in the history of
artificial intelligence. The strongest programs are based on a combination of
sophisticated search techniques, domain-specific adaptations, and handcrafted
evaluation functions that have been refined by human experts over several
decades. In contrast, the AlphaGo Zero program recently achieved superhuman
performance in the game of Go, by tabula rasa reinforcement learning from games
of self-play. In this paper, we generalise this approach into a single
AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in
many challenging domains. Starting from random play, and given no domain
knowledge except the game rules, AlphaZero achieved within 24 hours a
superhuman level of play in the games of chess and shogi (Japanese chess) as
well as Go, and convincingly defeated a world-champion program in each case.
"
"  We prove that counting copies of any graph $F$ in another graph $G$ can be
achieved using basic matrix operations on the adjacency matrix of $G$.
Moreover, the resulting algorithm is competitive for medium-sized $F$: our
algorithm recovers the best known complexity for rooted 6-clique counting and
improves on the best known for 9-cycle counting. Underpinning our proofs is the
new result that, for a general class of graph operators, matrix operations are
homomorphisms for operations on rooted graphs.
"
"  Notifications provide a unique mechanism for increasing the effectiveness of
real-time information delivery systems. However, notifications that demand
users' attention at inopportune moments are more likely to have adverse effects
and might become a cause of potential disruption rather than proving beneficial
to users. In order to address these challenges a variety of intelligent
notification mechanisms based on monitoring and learning users' behavior have
been proposed. The goal of such mechanisms is maximizing users' receptivity to
the delivered information by automatically inferring the right time and the
right context for sending a certain type of information.
This article provides an overview of the current state of the art in the area
of intelligent notification mechanisms that relies on the awareness of users'
context and preferences. More specifically, we first present a survey of
studies focusing on understanding and modeling users' interruptibility and
receptivity to notifications from desktops and mobile devices. Then, we discuss
the existing challenges and opportunities in developing mechanisms for
intelligent notification systems in a variety of application scenarios.
"
"  A novel matching based heuristic algorithm designed to detect specially
formulated infeasible zero-one IPs is presented. The algorithm input is a set
of nested doubly stochastic subsystems and a set E of instance defining
variables set at zero level. The algorithm deduces additional variables at zero
level until either a constraint is violated (the IP is infeasible), or no more
variables can be deduced zero (the IP is undecided). All feasible IPs, and all
infeasible IPs not detected infeasible are undecided. We successfully apply the
algorithm to a small set of specially formulated infeasible zero-one IP
instances of the Hamilton cycle decision problem. We show how to model both the
graph and subgraph isomorphism decision problems for input to the algorithm.
Increased levels of nested doubly stochastic subsystems can be implemented
dynamically. The algorithm is designed for parallel processing, and for
inclusion of techniques in addition to matching.
"
"  Deep convolution neural networks demonstrate impressive results in the
super-resolution domain. A series of studies concentrate on improving peak
signal noise ratio (PSNR) by using much deeper layers, which are not friendly
to constrained resources. Pursuing a trade-off between the restoration capacity
and the simplicity of models is still non-trivial. Recent contributions are
struggling to manually maximize this balance, while our work achieves the same
goal automatically with neural architecture search. Specifically, we handle
super-resolution with a multi-objective approach. We also propose an elastic
search tactic at both micro and macro level, based on a hybrid controller that
profits from evolutionary computation and reinforcement learning. Quantitative
experiments help us to draw a conclusion that our generated models dominate
most of the state-of-the-art methods with respect to the individual FLOPS.
"
"  A key challenge in complex visuomotor control is learning abstract
representations that are effective for specifying goals, planning, and
generalization. To this end, we introduce universal planning networks (UPN).
UPNs embed differentiable planning within a goal-directed policy. This planning
computation unrolls a forward model in a latent space and infers an optimal
action plan through gradient descent trajectory optimization. The
plan-by-gradient-descent process and its underlying representations are learned
end-to-end to directly optimize a supervised imitation learning objective. We
find that the representations learned are not only effective for goal-directed
visual imitation via gradient-based trajectory optimization, but can also
provide a metric for specifying goals using images. The learned representations
can be leveraged to specify distance-based rewards to reach new target states
for model-free reinforcement learning, resulting in substantially more
effective learning when solving new tasks described via image-based goals. We
were able to achieve successful transfer of visuomotor planning strategies
across robots with significantly different morphologies and actuation
capabilities.
"
"  Exploratory testing is neither black nor white, but rather a continuum of
exploration exists. In this research we propose an approach for decision
support helping practitioners to distribute time between different degrees of
exploratory testing on that continuum. To make the continuum manageable, five
levels have been defined: freestyle testing, high, medium and low degrees of
exploration, and scripted testing. The decision support approach is based on
the repertory grid technique. The approach has been used in one company. The
method for data collection was focus groups. The results showed that the
proposed approach aids practitioners in the reflection of what exploratory
testing levels to use, and aligns their understanding for priorities of
decision criteria and the performance of exploratory testing levels in their
contexts. The findings also showed that the participating company, which is
currently conducting mostly scripted testing, should spend more time on testing
using higher degrees of exploration in comparison to scripted testing.
"
"  In this paper we suggest that in the framework of the Category Theory it is
possible to demonstrate the mathematical and logical \textit{dual equivalence}
between the category of the $q$-deformed Hopf Coalgebras and the category of
the $q$-deformed Hopf Algebras in QFT, interpreted as a thermal field theory.
Each pair algebra-coalgebra characterizes, indeed, a QFT system and its
mirroring thermal bath, respectively, so to model dissipative quantum systems
persistently in far-from-equilibrium conditions, with an evident significance
also for biological sciences. The $q$-deformed Hopf Coalgebras and the
$q$-deformed Hopf Algebras constitute two dual categories because characterized
by the same functor $T$, related with the Bogoliubov transform, and by its
contravariant application $T^{op}$, respectively. The \textit{q}-deformation
parameter, indeed, is related to the Bogoliubov angle, and it is effectively a
thermal parameter. Therefore, the different values of $q$ identify univocally,
and then label, the vacua appearing in the foliation process of the quantum
vacuum. This means that, in the framework of Universal Coalgebra, as general
theory of dynamic and computing systems (""labelled state-transition systems""),
the so labelled infinitely many quantum vacua can be interpreted as the Final
Coalgebra of an ""Infinite State Black-Box Machine"". All this opens the way to
the possibility of designing a new class of universal quantum computing
architectures based on this coalgebraic formulation of QFT, as its ability of
naturally generating a Fibonacci progression demonstrates.
"
"  Curated web archive collections contain focused digital content which is
collected by archiving organizations, groups, and individuals to provide a
representative sample covering specific topics and events to preserve them for
future exploration and analysis. In this paper, we discuss how to best support
collaborative construction and exploration of these collections through the
ArchiveWeb system. ArchiveWeb has been developed using an iterative
evaluation-driven design-based research approach, with considerable user
feedback at all stages. The first part of this paper describes the important
insights we gained from our initial requirements engineering phase during the
first year of the project and the main functionalities of the current
ArchiveWeb system for searching, constructing, exploring, and discussing web
archive collections. The second part summarizes the feedback we received on
this version from archiving organizations and libraries, as well as our
corresponding plans for improving and extending the system for the next
release.
"
"  Learning network representations has a variety of applications, such as
network classification. Most existing work in this area focuses on static
undirected networks and do not account for presence of directed edges or
temporarily changes. Furthermore, most work focuses on node representations
that do poorly on tasks like network classification. In this paper, we propose
a novel, flexible and scalable network embedding methodology, \emph{gl2vec},
for network classification in both static and temporal directed networks.
\emph{gl2vec} constructs vectors for feature representation using static or
temporal network graphlet distributions and a null model for comparing them
against random graphs. We argue that \emph{gl2vec} can be used to classify and
compare networks of varying sizes and time period with high accuracy. We
demonstrate the efficacy and usability of \emph{gl2vec} over existing
state-of-the-art methods on network classification tasks such as network type
classification and subgraph identification in several real-world static and
temporal directed networks. Experimental results further show that
\emph{gl2vec}, concatenated with a wide range of state-of-the-art methods,
improves classification accuracy by up to $10\%$ in real-world applications
such as detecting departments for subgraphs in an email network or identifying
mobile users given their app switching behaviors represented as static or
temporal directed networks.
"
"  Market research is generally performed by surveying a representative sample
of customers with questions that includes contexts such as psycho-graphics,
demographics, attitude and product preferences. Survey responses are used to
segment the customers into various groups that are useful for targeted
marketing and communication. Reducing the number of questions asked to the
customer has utility for businesses to scale the market research to a large
number of customers. In this work, we model this task using Bayesian networks.
We demonstrate the effectiveness of our approach using an example market
segmentation of broadband customers.
"
"  In this paper, we formalize the notion of distributed sensitive social
networks (DSSNs), which encompasses networks like enmity networks, financial
transaction networks, supply chain networks and sexual relationship networks.
Compared to the well studied traditional social networks, DSSNs are often more
challenging to study, given the privacy concerns of the individuals on whom the
network is knit. In the current work, we envision the use of secure multiparty
tools and techniques for performing privacy preserving social network analysis
over DSSNs. As a step towards realizing this, we design efficient
data-oblivious algorithms for computing the K-shell decomposition and the
PageRank centrality measure for a given DSSN. The designed data-oblivious
algorithms can be translated into equivalent secure computation protocols. We
also list a string of challenges that are needed to be addressed, for employing
secure computation protocols as a practical solution for studying DSSNs.
"
"  We propose a new approach to the problem of optimizing autoencoders for lossy
image compression. New media formats, changing hardware technology, as well as
diverse requirements and content types create a need for compression algorithms
which are more flexible than existing codecs. Autoencoders have the potential
to address this need, but are difficult to optimize directly due to the
inherent non-differentiabilty of the compression loss. We here show that
minimal changes to the loss are sufficient to train deep autoencoders
competitive with JPEG 2000 and outperforming recently proposed approaches based
on RNNs. Our network is furthermore computationally efficient thanks to a
sub-pixel architecture, which makes it suitable for high-resolution images.
This is in contrast to previous work on autoencoders for compression using
coarser approximations, shallower architectures, computationally expensive
methods, or focusing on small images.
"
"  We present a slow control system to gather all relevant environment
information necessary to effectively and reliably run an HPC (High Performance
Computing) system at a high value over price ratio. The scalable and reliable
overall concept is presented as well as a newly developed hardware device for
sensor read out. This device incorporates a Raspberry Pi, an Arduino and PoE
(Power over Ethernet) functionality in a compact form factor. The system is in
use at the 2 PFLOPS cluster of the Johannes Gutenberg-University and
Helmholtz-Institute in Mainz.
"
"  We propose a data-driven framework for optimizing privacy-preserving data
release mechanisms toward the information-theoretically optimal tradeoff
between minimizing distortion of useful data and concealing sensitive
information. Our approach employs adversarially-trained neural networks to
implement randomized mechanisms and to perform a variational approximation of
mutual information privacy. We empirically validate our Privacy-Preserving
Adversarial Networks (PPAN) framework with experiments conducted on discrete
and continuous synthetic data, as well as the MNIST handwritten digits dataset.
With the synthetic data, we find that our model-agnostic PPAN approach achieves
tradeoff points very close to the optimal tradeoffs that are
analytically-derived from model knowledge. In experiments with the MNIST data,
we visually demonstrate a learned tradeoff between minimizing the pixel-level
distortion versus concealing the written digit.
"
"  The current prominence and future promises of the Internet of Things (IoT),
Internet of Everything (IoE) and Internet of Nano Things (IoNT) are extensively
reviewed and a summary survey report is presented. The analysis clearly
distinguishes between IoT and IoE which are wrongly considered to be the same
by many people. Upon examining the current advancement in the fields of IoT,
IoE and IoNT, the paper presents scenarios for the possible future expansion of
their applications.
"
"  Adversarial examples are perturbed inputs designed to fool machine learning
models. Adversarial training injects such examples into training data to
increase robustness. To scale this technique to large datasets, perturbations
are crafted using fast single-step methods that maximize a linear approximation
of the model's loss. We show that this form of adversarial training converges
to a degenerate global minimum, wherein small curvature artifacts near the data
points obfuscate a linear approximation of the loss. The model thus learns to
generate weak perturbations, rather than defend against strong ones. As a
result, we find that adversarial training remains vulnerable to black-box
attacks, where we transfer perturbations computed on undefended models, as well
as to a powerful novel single-step attack that escapes the non-smooth vicinity
of the input data via a small random step. We further introduce Ensemble
Adversarial Training, a technique that augments training data with
perturbations transferred from other models. On ImageNet, Ensemble Adversarial
Training yields models with strong robustness to black-box attacks. In
particular, our most robust model won the first round of the NIPS 2017
competition on Defenses against Adversarial Attacks.
"
"  In this thesis we present the novel semi-supervised network-based algorithm
P-Net, which is able to rank and classify patients with respect to a specific
phenotype or clinical outcome under study. The peculiar and innovative
characteristic of this method is that it builds a network of samples/patients,
where the nodes represent the samples and the edges are functional or genetic
relationships between individuals (e.g. similarity of expression profiles), to
predict the phenotype under study. In other words, it constructs the network in
the ""sample space"" and not in the ""biomarker space"" (where nodes represent
biomolecules (e.g. genes, proteins) and edges represent functional or genetic
relationships between nodes), as usual in state-of-the-art methods. To assess
the performances of P-Net, we apply it on three different publicly available
datasets from patients afflicted with a specific type of tumor: pancreatic
cancer, melanoma and ovarian cancer dataset, by using the data and following
the experimental set-up proposed in two recently published papers [Barter et
al., 2014, Winter et al., 2012]. We show that network-based methods in the
""sample space"" can achieve results competitive with classical supervised
inductive systems. Moreover, the graph representation of the samples can be
easily visualized through networks and can be used to gain visual clues about
the relationships between samples, taking into account the phenotype associated
or predicted for each sample. To our knowledge this is one of the first works
that proposes graph-based algorithms working in the ""sample space"" of the
biomolecular profiles of the patients to predict their phenotype or outcome,
thus contributing to a novel research line in the framework of the Network
Medicine.
"
"  We consider the characterization as well as the construction of quantum codes
that allow to transmit both quantum and classical information, which we refer
to as `hybrid codes'. We construct hybrid codes $[\![n,k{: }m,d]\!]_q$ with
length $n$ and distance $d$, that simultaneously transmit $k$ qudits and $m$
symbols from a classical alphabet of size $q$. Many good codes such as
$[\![7,1{: }1,3]\!]_2$, $[\![9,2{: }2,3]\!]_2$, $[\![10,3{: }2,3]\!]_2$,
$[\![11,4{: }2,3]\!]_2$, $[\![11,1{: }2,4]\!]_2$, $[\![13,1{: }4,4]\!]_2$,
$[\![13,1{: }1,5]\!]_2$, $[\![14,1{: }2,5]\!]_2$, $[\![15,1{: }3,5]\!]_2$,
$[\![19,9{: }1,4]\!]_2$, $[\![20,9{: }2,4]\!]_2$, $[\![21,9{: }3,4]\!]_2$,
$[\![22,9{: }4,4]\!]_2$ have been found. All these codes have better parameters
than hybrid codes obtained from the best known stabilizer quantum codes.
"
"  Molecular fingerprints, i.e. feature vectors describing atomistic
neighborhood configurations, is an important abstraction and a key ingredient
for data-driven modeling of potential energy surface and interatomic force. In
this paper, we present the Density-Encoded Canonically Aligned Fingerprint
(DECAF) fingerprint algorithm, which is robust and efficient, for fitting
per-atom scalar and vector quantities. The fingerprint is essentially a
continuous density field formed through the superimposition of smoothing
kernels centered on the atoms. Rotational invariance of the fingerprint is
achieved by aligning, for each fingerprint instance, the neighboring atoms onto
a local canonical coordinate frame computed from a kernel minisum optimization
procedure. We show that this approach is superior over PCA-based methods
especially when the atomistic neighborhood is sparse and/or contains symmetry.
We propose that the `distance' between the density fields be measured using a
volume integral of their pointwise difference. This can be efficiently computed
using optimal quadrature rules, which only require discrete sampling at a small
number of grid points. We also experiment on the choice of weight functions for
constructing the density fields, and characterize their performance for fitting
interatomic potentials. The applicability of the fingerprint is demonstrated
through a set of benchmark problems.
"
"  Log-linear models are arguably the most successful class of graphical models
for large-scale applications because of their simplicity and tractability.
Learning and inference with these models require calculating the partition
function, which is a major bottleneck and intractable for large state spaces.
Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the
condition of having a ""good"" proposal distribution is often not satisfied in
practice.
In this paper, we add a new dimension to efficient estimation via sampling.
We propose a new sampling scheme and an unbiased estimator that estimates the
partition function accurately in sub-linear time. Our samples are generated in
near-constant time using locality sensitive hashing (LSH), and so are
correlated and unnormalized. We demonstrate the effectiveness of our proposed
approach by comparing the accuracy and speed of estimating the partition
function against other state-of-the-art estimation techniques including IS and
the efficient variant of Gumbel-Max sampling. With our efficient sampling
scheme, we accurately train real-world language models using only 1-2% of
computations.
"
"  Discrete particle simulations are widely used to study large-scale
particulate flows in complex geometries where particle-particle and
particle-fluid interactions require an adequate representation but the
computational cost has to be kept low. In this work, we present a novel
coupling approach for such simulations. A lattice Boltzmann formulation of the
generalized Navier-Stokes equations is used to describe the fluid motion. This
promises efficient simulations suitable for high performance computing and,
since volume displacement effects by the solid phase are considered, our
approach is also applicable to non-dilute particulate systems. The discrete
element method is combined with an explicit evaluation of interparticle
lubrication forces to simulate the motion of individual submerged particles.
Drag, pressure and added mass forces determine the momentum transfer by
fluid-particle interactions. A stable coupling algorithm is presented and
discussed in detail. We demonstrate the validity of our approach for dilute as
well as dense systems by predicting the settling velocity of spheres over a
broad range of solid volume fractions in good agreement with semi-empirical
correlations. Additionally, the accuracy of particle-wall interactions in a
viscous fluid is thoroughly tested and established. Our approach can thus be
readily used for various particulate systems and can be extended
straightforward to e.g. non-spherical particles.
"
"  Genealogical networks, also known as family trees or population pedigrees,
are commonly studied by genealogists wanting to know about their ancestry, but
they also provide a valuable resource for disciplines such as digital
demography, genetics, and computational social science. These networks are
typically constructed by hand through a very time-consuming process, which
requires comparing large numbers of historical records manually. We develop
computational methods for automatically inferring large-scale genealogical
networks. A comparison with human-constructed networks attests to the accuracy
of the proposed methods. To demonstrate the applicability of the inferred
large-scale genealogical networks, we present a longitudinal analysis on the
mating patterns observed in a network. This analysis shows a consistent
tendency of people choosing a spouse with a similar socioeconomic status, a
phenomenon known as assortative mating. Interestingly, we do not observe this
tendency to consistently decrease (nor increase) over our study period of 150
years.
"
"  The TTE approach to Computable Analysis is the study of so-called
representations (encodings for continuous objects such as reals, functions, and
sets) with respect to the notions of computability they induce. A rich variety
of such representations had been devised over the past decades, particularly
regarding closed subsets of Euclidean space plus subclasses thereof (like
compact subsets). In addition, they had been compared and classified with
respect to both non-uniform computability of single sets and uniform
computability of operators on sets. In this paper we refine these
investigations from the point of view of computational complexity. Benefiting
from the concept of second-order representations and complexity recently
devised by Kawamura & Cook (2012), we determine parameterized complexity bounds
for operators such as union, intersection, projection, and more generally
function image and inversion. By indicating natural parameters in addition to
the output precision, we get a uniform view on results by Ko (1991-2013),
Braverman (2004/05) and Zhao & Müller (2008), relating these problems to the
P/UP/NP question in discrete complexity theory.
"
"  In this paper we investigate the computational complexity of deciding if a
given finite algebraic structure satisfies a fixed (strong) Maltsev condition
$\Sigma$. Our goal in this paper is to show that $\Sigma$-testing can be
accomplished in polynomial time when the algebras tested are idempotent and the
Maltsev condition $\Sigma$ can be described using paths. Examples of such path
conditions are having a Maltsev term, having a majority operation, and having a
chain of Jónsson (or Gumm) terms of fixed length.
"
"  With the development of neural networks based machine learning and their
usage in mission critical applications, voices are rising against the
\textit{black box} aspect of neural networks as it becomes crucial to
understand their limits and capabilities. With the rise of neuromorphic
hardware, it is even more critical to understand how a neural network, as a
distributed system, tolerates the failures of its computing nodes, neurons, and
its communication channels, synapses. Experimentally assessing the robustness
of neural networks involves the quixotic venture of testing all the possible
failures, on all the possible inputs, which ultimately hits a combinatorial
explosion for the first, and the impossibility to gather all the possible
inputs for the second.
In this paper, we prove an upper bound on the expected error of the output
when a subset of neurons crashes. This bound involves dependencies on the
network parameters that can be seen as being too pessimistic in the average
case. It involves a polynomial dependency on the Lipschitz coefficient of the
neurons activation function, and an exponential dependency on the depth of the
layer where a failure occurs. We back up our theoretical results with
experiments illustrating the extent to which our prediction matches the
dependencies between the network parameters and robustness. Our results show
that the robustness of neural networks to the average crash can be estimated
without the need to neither test the network on all failure configurations, nor
access the training set used to train the network, both of which are
practically impossible requirements.
"
"  We show how Leibnitz.s indiscernibility principle and Gentzen's original work
lead to extensions of the sequent calculus to first order logic with equality
and investigate the cut elimination property. Furthermore we discuss and
improve the nonlengthening property of Lifshitz and Orevkov.
"
"  Training deep neural networks is known to require a large number of training
samples. However, in many applications only few training samples are available.
In this work, we tackle the issue of training neural networks for
classification task when few training samples are available. We attempt to
solve this issue by proposing a new regularization term that constrains the
hidden layers of a network to learn class-wise invariant representations. In
our regularization framework, learning invariant representations is generalized
to the class membership where samples with the same class should have the same
representation. Numerical experiments over MNIST and its variants showed that
our proposal helps improving the generalization of neural network particularly
when trained with few samples. We provide the source code of our framework
this https URL .
"
"  We introduce multi-modal, attention-based neural machine translation (NMT)
models which incorporate visual features into different parts of both the
encoder and the decoder. We utilise global image features extracted using a
pre-trained convolutional neural network and incorporate them (i) as words in
the source sentence, (ii) to initialise the encoder hidden state, and (iii) as
additional data to initialise the decoder hidden state. In our experiments, we
evaluate how these different strategies to incorporate global image features
compare and which ones perform best. We also study the impact that adding
synthetic multi-modal, multilingual data brings and find that the additional
data have a positive impact on multi-modal models. We report new
state-of-the-art results and our best models also significantly improve on a
comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k
data set according to all metrics evaluated. To the best of our knowledge, it
is the first time a purely neural model significantly improves over a PBSMT
model on all metrics evaluated on this data set.
"
"  Nonnegative Matrix Factorization (NMF) is a widely used technique for data
representation. Inspired by the expressive power of deep learning, several NMF
variants equipped with deep architectures have been proposed. However, these
methods mostly use the only nonnegativity while ignoring task-specific features
of data. In this paper, we propose a novel deep approximately orthogonal
nonnegative matrix factorization method where both nonnegativity and
orthogonality are imposed with the aim to perform a hierarchical clustering by
using different level of abstractions of data. Experiment on two face image
datasets showed that the proposed method achieved better clustering performance
than other deep matrix factorization methods and state-of-the-art single layer
NMF variants.
"
"  We consider the problem of recovering the superposition of $R$ distinct
complex exponential functions from compressed non-uniform time-domain samples.
Total Variation (TV) minimization or atomic norm minimization was proposed in
the literature to recover the $R$ frequencies or the missing data. However, it
is known that in order for TV minimization and atomic norm minimization to
recover the missing data or the frequencies, the underlying $R$ frequencies are
required to be well-separated, even when the measurements are noiseless. This
paper shows that the Hankel matrix recovery approach can super-resolve the $R$
complex exponentials and their frequencies from compressed non-uniform
measurements, regardless of how close their frequencies are to each other. We
propose a new concept of orthonormal atomic norm minimization (OANM), and
demonstrate that the success of Hankel matrix recovery in separation-free
super-resolution comes from the fact that the nuclear norm of a Hankel matrix
is an orthonormal atomic norm. More specifically, we show that, in traditional
atomic norm minimization, the underlying parameter values $\textbf{must}$ be
well separated to achieve successful signal recovery, if the atoms are changing
continuously with respect to the continuously-valued parameter. In contrast,
for the OANM, it is possible the OANM is successful even though the original
atoms can be arbitrarily close.
As a byproduct of this research, we provide one matrix-theoretic inequality
of nuclear norm, and give its proof from the theory of compressed sensing.
"
"  With the wide application of machine learning algorithms to the real world,
class imbalance and concept drift have become crucial learning issues. Class
imbalance happens when the data categories are not equally represented, i.e.,
at least one category is minority compared to other categories. It can cause
learning bias towards the majority class and poor generalization. Concept drift
is a change in the underlying distribution of the problem, and is a significant
issue specially when learning from data streams. It requires learners to be
adaptive to dynamic changes.
Class imbalance and concept drift can significantly hinder predictive
performance, and the problem becomes particularly challenging when they occur
simultaneously. This challenge arises from the fact that one problem can affect
the treatment of the other. For example, drift detection algorithms based on
the traditional classification error may be sensitive to the imbalanced degree
and become less effective; and class imbalance techniques need to be adaptive
to changing imbalance rates, otherwise the class receiving the preferential
treatment may not be the correct minority class at the current moment.
Therefore, the mutual effect of class imbalance and concept drift should be
considered during algorithm design.
The aim of this workshop is to bring together researchers from the areas of
class imbalance learning and concept drift in order to encourage discussions
and new collaborations on solving the combined issue of class imbalance and
concept drift. It provides a forum for international researchers and
practitioners to share and discuss their original work on addressing new
challenges and research issues in class imbalance learning, concept drift, and
the combined issues of class imbalance and concept drift. The proceedings
include 8 papers on these topics.
"
"  Synthesis of DNA molecules offers unprecedented advances in storage
technology. Yet, the microscopic world in which these molecules reside induces
error patterns that are fundamentally different from their digital
counterparts. Hence, to maintain reliability in reading and writing, new coding
schemes must be developed. In a reading technique called shotgun sequencing, a
long DNA string is read in a sliding window fashion, and a profile vector is
produced. It was recently suggested by Kiah et al. that such a vector can
represent the permutation which is induced by its entries, and hence a
rank-modulation scheme arises. Although this interpretation suggests high error
tolerance, it is unclear which permutations are feasible, and how to produce a
DNA string whose profile vector induces a given permutation. In this paper, by
observing some necessary conditions, an upper bound for the number of feasible
permutations is given. Further, a technique for deciding the feasibility of a
permutation is devised. By using insights from this technique, an algorithm for
producing a considerable number of feasible permutations is given, which
applies to any alphabet size and any window length.
"
"  Kernel PCA is a widely used nonlinear dimension reduction technique in
machine learning, but storing the kernel matrix is notoriously challenging when
the sample size is large. Inspired by Yi et al. [2016], where the idea of
partial matrix sampling followed by nonconvex optimization is proposed for
matrix completion and robust PCA, we apply a similar approach to
memory-efficient Kernel PCA. In theory, with no assumptions on the kernel
matrix in terms of eigenvalues or eigenvectors, we established a model-free
theory for the low-rank approximation based on any local minimum of the
proposed objective function. As interesting byproducts, when the underlying
positive semidefinite matrix is assumed to be low-rank and highly structured,
corollaries of our main theorem improve the state-of-the-art results of Ge et
al. [2016, 2017] for nonconvex matrix completion with no spurious local minima.
Numerical experiments also show that our approach is competitive in terms of
approximation accuracy compared to the well-known Nyström algorithm for
Kernel PCA.
"
"  Organized crime inflicts human suffering on a genocidal scale: the Mexican
drug cartels have murdered 150,000 people since 2006, upwards of 700,000 people
per year are ""exported"" in a human trafficking industry enslaving an estimated
40 million people. These nefarious industries rely on sophisticated money
laundering schemes to operate. Despite tremendous resources dedicated to
anti-money laundering (AML) only a tiny fraction of illicit activity is
prevented. The research community can help. In this brief paper, we map the
structural and behavioral dynamics driving the technical challenge. We review
AML methods, current and emergent. We provide a first look at scalable graph
convolutional neural networks for forensic analysis of financial data, which is
massive, dense, and dynamic. We report preliminary experimental results using a
large synthetic graph (1M nodes, 9M edges) generated by a data simulator we
created called AMLSim. We consider opportunities for high performance
efficiency, in terms of computation and memory, and we share results from a
simple graph compression experiment. Our results support our working hypothesis
that graph deep learning for AML bears great promise in the fight against
criminal financial activity.
"
"  Shape completion, the problem of estimating the complete geometry of objects
from partial observations, lies at the core of many vision and robotics
applications. In this work, we propose Point Completion Network (PCN), a novel
learning-based approach for shape completion. Unlike existing shape completion
methods, PCN directly operates on raw point clouds without any structural
assumption (e.g. symmetry) or annotation (e.g. semantic class) about the
underlying shape. It features a decoder design that enables the generation of
fine-grained completions while maintaining a small number of parameters. Our
experiments show that PCN produces dense, complete point clouds with realistic
structures in the missing regions on inputs with various levels of
incompleteness and noise, including cars from LiDAR scans in the KITTI dataset.
"
"  In this paper we establish a connection between non-convex optimization
methods for training deep neural networks and nonlinear partial differential
equations (PDEs). Relaxation techniques arising in statistical physics which
have already been used successfully in this context are reinterpreted as
solutions of a viscous Hamilton-Jacobi PDE. Using a stochastic control
interpretation allows we prove that the modified algorithm performs better in
expectation that stochastic gradient descent. Well-known PDE regularity results
allow us to analyze the geometry of the relaxed energy landscape, confirming
empirical evidence. The PDE is derived from a stochastic homogenization
problem, which arises in the implementation of the algorithm. The algorithms
scale well in practice and can effectively tackle the high dimensionality of
modern neural networks.
"
"  Predicting the outcome of sports events is a hard task. We quantify this
difficulty with a coefficient that measures the distance between the observed
final results of sports leagues and idealized perfectly balanced competitions
in terms of skill. This indicates the relative presence of luck and skill. We
collected and analyzed all games from 198 sports leagues comprising 1503
seasons from 84 countries of 4 different sports: basketball, soccer, volleyball
and handball. We measured the competitiveness by countries and sports. We also
identify in each season which teams, if removed from its league, result in a
completely random tournament. Surprisingly, not many of them are needed. As
another contribution of this paper, we propose a probabilistic graphical model
to learn about the teams' skills and to decompose the relative weights of luck
and skill in each game. We break down the skill component into factors
associated with the teams' characteristics. The model also allows to estimate
as 0.36 the probability that an underdog team wins in the NBA league, with a
home advantage adding 0.09 to this probability. As shown in the first part of
the paper, luck is substantially present even in the most competitive
championships, which partially explains why sophisticated and complex
feature-based models hardly beat simple models in the task of forecasting
sports' outcomes.
"
"  We propose a generalization of neural network sequence models. Instead of
predicting one symbol at a time, our multi-scale model makes predictions over
multiple, potentially overlapping multi-symbol tokens. A variation of the
byte-pair encoding (BPE) compression algorithm is used to learn the dictionary
of tokens that the model is trained with. When applied to language modelling,
our model has the flexibility of character-level models while maintaining many
of the performance benefits of word-level models. Our experiments show that
this model performs better than a regular LSTM on language modeling tasks,
especially for smaller models.
"
"  In the last few years, microblogging platforms such as Twitter have given
rise to a deluge of textual data that can be used for the analysis of informal
communication between millions of individuals. In this work, we propose an
information-theoretic approach to geographic language variation using a corpus
based on Twitter. We test our models with tens of concepts and their associated
keywords detected in Spanish tweets geolocated in Spain. We employ
dialectometric measures (cosine similarity and Jensen-Shannon divergence) to
quantify the linguistic distance on the lexical level between cells created in
a uniform grid over the map. This can be done for a single concept or in the
general case taking into account an average of the considered variants. The
latter permits an analysis of the dialects that naturally emerge from the data.
Interestingly, our results reveal the existence of two dialect macrovarieties.
The first group includes a region-specific speech spoken in small towns and
rural areas whereas the second cluster encompasses cities that tend to use a
more uniform variety. Since the results obtained with the two different metrics
qualitatively agree, our work suggests that social media corpora can be
efficiently used for dialectometric analyses.
"
"  There are large amounts of insight and social discovery potential in mining
crowd-sourced comments left on popular news forums like Reddit.com, Tumblr.com,
Facebook.com and Hacker News. Unfortunately, due the overwhelming amount of
participation with its varying quality of commentary, extracting value out of
such data isn't always obvious nor timely. By designing efficient, single-pass
and adaptive natural language filters to quickly prune spam, noise, copy-cats,
marketing diversions, and out-of-context posts, we can remove over a third of
entries and return the comments with a higher probability of relatedness to the
original article in question. The approach presented here uses an adaptive,
two-step filtering process. It first leverages the original article posted in
the thread as a starting corpus to parse comments by matching intersecting
words and term-ratio balance per sentence then grows the corpus by adding new
words harvested from high-matching comments to increase filtering accuracy over
time.
"
"  This paper addresses the challenge of humanoid robot teleoperation in a
natural indoor environment via a Brain-Computer Interface (BCI). We leverage
deep Convolutional Neural Network (CNN) based image and signal understanding to
facilitate both real-time object detection and dry-Electroencephalography (EEG)
based human cortical brain bio-signal decoding. We employ recent advances in
dry-EEG technology to stream and collect the cortical waveforms from subjects
while the subjects fixate on variable Steady-State Visual Evoked Potential
(SSVEP) stimuli generated directly from the environment the robot is
navigating. To these ends, we propose the use of novel variable BCI stimuli by
utilising the real-time video streamed via the on-board robot camera as visual
input for SSVEP where the CNN detected natural scene objects are altered and
flickered with differing frequencies (10Hz, 12Hz and 15Hz). These stimuli are
not akin to traditional stimuli - as both the dimensions of the flicker regions
and their on-screen position changes depending on the scene objects detected in
the scene. On-screen object selection via dry-EEG enabled SSVEP in this way,
facilitates the on-line decoding of human cortical brain signals via a
secondary CNN approach into teleoperation robot commands (approach object, move
in a specific direction: right, left or back). This SSVEP decoding model is
trained via a priori offline experimental data in which very similar visual
input is present for all subjects. The resulting offline classification
demonstrates extremely high performance and with mean accuracies of 96% and 90%
for the real-time robot navigation experiment across multiple test subjects.
"
"  In this paper, the problem of tracking desired longitudinal and lateral
motions for a vehicle is addressed. Let us point out that a ""good"" modeling is
often quite difficult or even impossible to obtain. It is due for example to
parametric uncertainties, for the vehicle mass, inertia or for the interaction
forces between the wheels and the road pavement. To overcome this type of
difficulties, we consider a model-free control approach leading to
""intelligent"" controllers. The longitudinal and the lateral motions, on one
hand, and the driving/braking torques and the steering wheel angle, on the
other hand, are respectively the output and the input variables. An important
part of this work is dedicated to present simulation results with actual data.
Actual data, used in Matlab as reference trajectories, have been previously
recorded with an instrumented Peugeot 406 experimental car. The simulation
results show the efficiency of our approach. Some comparisons with a nonlinear
flatness-based control in one hand, and with a classical PID control in another
hand confirm this analysis. Other virtual data have been generated through the
interconnected platform SiVIC/RTMaps, which is a virtual simulation platform
for prototyping and validation of advanced driving assistance systems.
"
"  Grip control during robotic in-hand manipulation is usually modeled as part
of a monolithic task, relying on complex controllers specialized for specific
situations. Such approaches do not generalize well and are difficult to apply
to novel manipulation tasks. Here, we propose a modular object stabilization
method based on a proposition that explains how humans achieve grasp stability.
In this bio-mimetic approach, independent tactile grip stabilization
controllers ensure that slip does not occur locally at the engaged robot
fingers. Such local slip is predicted from the tactile signals of each
fingertip sensor i.e., BioTac and BioTac SP by Syntouch. We show that stable
grasps emerge without any form of central communication when such independent
controllers are engaged in the control of multi-digit robotic hands. These
grasps are resistant to external perturbations while being capable of
stabilizing a large variety of objects.
"
"  We study fairness within the stochastic, \emph{multi-armed bandit} (MAB)
decision making framework. We adapt the fairness framework of ""treating similar
individuals similarly"" to this setting. Here, an `individual' corresponds to an
arm and two arms are `similar' if they have a similar quality distribution.
First, we adopt a {\em smoothness constraint} that if two arms have a similar
quality distribution then the probability of selecting each arm should be
similar. In addition, we define the {\em fairness regret}, which corresponds to
the degree to which an algorithm is not calibrated, where perfect calibration
requires that the probability of selecting an arm is equal to the probability
with which the arm has the best quality realization. We show that a variation
on Thompson sampling satisfies smooth fairness for total variation distance,
and give an $\tilde{O}((kT)^{2/3})$ bound on fairness regret. This complements
prior work, which protects an on-average better arm from being less favored. We
also explain how to extend our algorithm to the dueling bandit setting.
"
"  Virtual Reality, an immersive technology that replicates an environment via
computer-simulated reality, gets a lot of attention in the entertainment
industry. However, VR has also great potential in other areas, like the medical
domain, Examples are intervention planning, training and simulation. This is
especially of use in medical operations, where an aesthetic outcome is
important, like for facial surgeries. Alas, importing medical data into Virtual
Reality devices is not necessarily trivial, in particular, when a direct
connection to a proprietary application is desired. Moreover, most researcher
do not build their medical applications from scratch, but rather leverage
platforms like MeVisLab, MITK, OsiriX or 3D Slicer. These platforms have in
common that they use libraries like ITK and VTK, and provide a convenient
graphical interface. However, ITK and VTK do not support Virtual Reality
directly. In this study, the usage of a Virtual Reality device for medical data
under the MeVisLab platform is presented. The OpenVR library is integrated into
the MeVisLab platform, allowing a direct and uncomplicated usage of the head
mounted display HTC Vive inside the MeVisLab platform. Medical data coming from
other MeVisLab modules can directly be connected per drag-and-drop to the
Virtual Reality module, rendering the data inside the HTC Vive for immersive
virtual reality inspection.
"
"  We consider the problem of adversarial (non-stochastic) online learning with
partial information feedback, where at each round, a decision maker selects an
action from a finite set of alternatives. We develop a black-box approach for
such problems where the learner observes as feedback only losses of a subset of
the actions that includes the selected action. When losses of actions are
non-negative, under the graph-based feedback model introduced by Mannor and
Shamir, we offer algorithms that attain the so called ""small-loss"" $o(\alpha
L^{\star})$ regret bounds with high probability, where $\alpha$ is the
independence number of the graph, and $L^{\star}$ is the loss of the best
action. Prior to our work, there was no data-dependent guarantee for general
feedback graphs even for pseudo-regret (without dependence on the number of
actions, i.e. utilizing the increased information feedback). Taking advantage
of the black-box nature of our technique, we extend our results to many other
applications such as semi-bandits (including routing in networks), contextual
bandits (even with an infinite comparator class), as well as learning with
slowly changing (shifting) comparators.
In the special case of classical bandit and semi-bandit problems, we provide
optimal small-loss, high-probability guarantees of
$\tilde{O}(\sqrt{dL^{\star}})$ for actual regret, where $d$ is the number of
actions, answering open questions of Neu. Previous bounds for bandits and
semi-bandits were known only for pseudo-regret and only in expectation. We also
offer an optimal $\tilde{O}(\sqrt{\kappa L^{\star}})$ regret guarantee for
fixed feedback graphs with clique-partition number at most $\kappa$.
"
"  The $q$-Coloring problem asks whether the vertices of a graph can be properly
colored with $q$ colors. Lokshtanov et al. [SODA 2011] showed that $q$-Coloring
on graphs with a feedback vertex set of size $k$ cannot be solved in time
$\mathcal{O}^*((q-\varepsilon)^k)$, for any $\varepsilon > 0$, unless the
Strong Exponential-Time Hypothesis (SETH) fails. In this paper we perform a
fine-grained analysis of the complexity of $q$-Coloring with respect to a
hierarchy of parameters. We show that even when parameterized by the vertex
cover number, $q$ must appear in the base of the exponent: Unless ETH fails,
there is no universal constant $\theta$ such that $q$-Coloring parameterized by
vertex cover can be solved in time $\mathcal{O}^*(\theta^k)$ for all fixed $q$.
We apply a method due to Jansen and Kratsch [Inform. & Comput. 2013] to prove
that there are $\mathcal{O}^*((q - \varepsilon)^k)$ time algorithms where $k$
is the vertex deletion distance to several graph classes $\mathcal{F}$ for
which $q$-Coloring is known to be solvable in polynomial time. We generalize
earlier ad-hoc results by showing that if $\mathcal{F}$ is a class of graphs
whose $(q+1)$-colorable members have bounded treedepth, then there exists some
$\varepsilon > 0$ such that $q$-Coloring can be solved in time
$\mathcal{O}^*((q-\varepsilon)^k)$ when parameterized by the size of a given
modulator to $\mathcal{F}$. In contrast, we prove that if $\mathcal{F}$ is the
class of paths - some of the simplest graphs of unbounded treedepth - then no
such algorithm can exist unless SETH fails.
"
"  Planar object tracking is an actively studied problem in vision-based robotic
applications. While several benchmarks have been constructed for evaluating
state-of-the-art algorithms, there is a lack of video sequences captured in the
wild rather than in constrained laboratory environment. In this paper, we
present a carefully designed planar object tracking benchmark containing 210
videos of 30 planar objects sampled in the natural environment. In particular,
for each object, we shoot seven videos involving various challenging factors,
namely scale change, rotation, perspective distortion, motion blur, occlusion,
out-of-view, and unconstrained. The ground truth is carefully annotated
semi-manually to ensure the quality. Moreover, eleven state-of-the-art
algorithms are evaluated on the benchmark using two evaluation metrics, with
detailed analysis provided for the evaluation results. We expect the proposed
benchmark to benefit future studies on planar object tracking.
"
"  Many systems of structured argumentation explicitly require that the facts
and rules that make up the argument for a conclusion be the minimal set
required to derive the conclusion. ASPIC+ does not place such a requirement on
arguments, instead requiring that every rule and fact that are part of an
argument be used in its construction. Thus ASPIC+ arguments are minimal in the
sense that removing any element of the argument would lead to a structure that
is not an argument. In this brief note we discuss these two types of minimality
and show how the first kind of minimality can, if desired, be recovered in
ASPIC+.
"
"  This work analyses surprising elections, and attempts to quantify the notion
of surprise in elections. A voter is surprised if their estimate of the winner
(assumed to be based on a combination of the preferences of their social
connections and popular media predictions) is different from the true winner. A
voter's social connections are assumed to consist of contacts on social media
and geographically proximate people. We propose a simple mathematical model for
combining the global information (traditional media) as well as the local
information (local neighbourhood) of a voter in the case of a two-candidate
election. We show that an unbiased, influential media can nullify the effect of
filter bubbles and result in a less surprised populace. Surprisingly, an
influential media source biased towards the winners of the election also
results in a less surprised populace. Our model shows that elections will be
unsurprising for all of the voters with a high probability under certain
assumptions on the social connection model in the presence of an influential,
unbiased traditional media source. Our experiments with the UK-EU referendum
(popularly known as Brexit) dataset support our theoretical predictions. Since
surprising elections can lead to significant economic movements, it is a
worthwhile endeavour to figure out the causes of surprising elections.
"
"  In this paper we demonstrate how genetic algorithms can be used to reverse
engineer an evaluation function's parameters for computer chess. Our results
show that using an appropriate expert (or mentor), we can evolve a program that
is on par with top tournament-playing chess programs, outperforming a two-time
World Computer Chess Champion. This performance gain is achieved by evolving a
program that mimics the behavior of a superior expert. The resulting evaluation
function of the evolved program consists of a much smaller number of parameters
than the expert's. The extended experimental results provided in this paper
include a report of our successful participation in the 2008 World Computer
Chess Championship. In principle, our expert-driven approach could be used in a
wide range of problems for which appropriate experts are available.
"
"  Social conventions govern countless behaviors all of us engage in every day,
from how we greet each other to the languages we speak. But how can shared
conventions emerge spontaneously in the absence of a central coordinating
authority? The Naming Game model shows that networks of locally interacting
individuals can spontaneously self-organize to produce global coordination.
Here, we provide a gentle introduction to the main features of the model, from
the dynamics observed in homogeneously mixing populations to the role played by
more complex social networks, and to how slight modifications of the basic
interaction rules give origin to a richer phenomenology in which more
conventions can co-exist indefinitely.
"
"  This work is devoted to elaboration on the idea to use block term
decomposition for group data analysis and to raise the possibility of modelling
group activity with (Lr, 1) and Tucker blocks. A new generalization of block
tensor decomposition was considered in application to group data analysis.
Suggested approach was evaluated on multilabel classification task for a set of
images. This contribution also reports results of investigation on clustering
with proposed tensor models in comparison with known matrix models, namely
common orthogonal basis extraction and group independent component analysis.
"
"  This paper reviews the historic of ChaLearn Looking at People (LAP) events.
We started in 2011 (with the release of the first Kinect device) to run
challenges related to human action/activity and gesture recognition. Since then
we have regularly organized events in a series of competitions covering all
aspects of visual analysis of humans. So far we have organized more than 10
international challenges and events in this field. This paper reviews
associated events, and introduces the ChaLearn LAP platform where public
resources (including code, data and preprints of papers) related to the
organized events are available. We also provide a discussion on perspectives of
ChaLearn LAP activities.
"
"  The explosive increase in number of smart devices hosting sophisticated
applications is rapidly affecting the landscape of information communication
technology industry. Mobile subscriptions, expected to reach 8.9 billion by
2022, would drastically increase the demand of extra capacity with aggregate
throughput anticipated to be enhanced by a factor of 1000. In an already
crowded radio spectrum, it becomes increasingly difficult to meet ever growing
application demands of wireless bandwidth. It has been shown that the allocated
spectrum is seldom utilized by the primary users and hence contains spectrum
holes that may be exploited by the unlicensed users for their communication. As
we enter the Internet Of Things (IoT) era in which appliances of common use
will become smart digital devices with rigid performance requirements (such as
low latency, energy efficiency, etc.), current networks face the vexing problem
of how to create sufficient capacity for such applications. The fifth
generation of cellular networks (5G) envisioned to address these challenges are
thus required to incorporate cognition and intelligence to resolve the
aforementioned issues.
"
"  Column-sparse packing problems arise in several contexts in both
deterministic and stochastic discrete optimization. We present two unifying
ideas, (non-uniform) attenuation and multiple-chance algorithms, to obtain
improved approximation algorithms for some well-known families of such
problems. As three main examples, we attain the integrality gap, up to
lower-order terms, for known LP relaxations for k-column sparse packing integer
programs (Bansal et al., Theory of Computing, 2012) and stochastic k-set
packing (Bansal et al., Algorithmica, 2012), and go ""half the remaining
distance"" to optimal for a major integrality-gap conjecture of Furedi, Kahn and
Seymour on hypergraph matching (Combinatorica, 1993).
"
"  We propose a new linear algebraic approach to the computation of Tarskian
semantics in logic. We embed a finite model M in first-order logic with N
entities in N-dimensional Euclidean space R^N by mapping entities of M to N
dimensional one-hot vectors and k-ary relations to order-k adjacency tensors
(multi-way arrays). Second given a logical formula F in prenex normal form, we
compile F into a set Sigma_F of algebraic formulas in multi-linear algebra with
a nonlinear operation. In this compilation, existential quantifiers are
compiled into a specific type of tensors, e.g., identity matrices in the case
of quantifying two occurrences of a variable. It is shown that a systematic
evaluation of Sigma_F in R^N gives the truth value, 1(true) or 0(false), of F
in M. Based on this framework, we also propose an unprecedented way of
computing the least models defined by Datalog programs in linear spaces via
matrix equations and empirically show its effectiveness compared to
state-of-the-art approaches.
"
"  We present a Character-Word Long Short-Term Memory Language Model which both
reduces the perplexity with respect to a baseline word-level language model and
reduces the number of parameters of the model. Character information can reveal
structural (dis)similarities between words and can even be used when a word is
out-of-vocabulary, thus improving the modeling of infrequent and unknown words.
By concatenating word and character embeddings, we achieve up to 2.77% relative
improvement on English compared to a baseline model with a similar amount of
parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level
models with a larger number of parameters.
"
"  Machine learning methods have found many applications in Raman spectroscopy,
especially for the identification of chemical species. However, almost all of
these methods require non-trivial preprocessing such as baseline correction
and/or PCA as an essential step. Here we describe our unified solution for the
identification of chemical species in which a convolutional neural network is
trained to automatically identify substances according to their Raman spectrum
without the need of ad-hoc preprocessing steps. We evaluated our approach using
the RRUFF spectral database, comprising mineral sample data. Superior
classification performance is demonstrated compared with other frequently used
machine learning algorithms including the popular support vector machine.
"
"  Early recognition of abnormal rhythm in ECG signals is crucial for monitoring
or diagnosing patients' cardiac conditions and increasing the success rate of
the treatment. Classifying abnormal rhythms into fine-grained categories is
very challenging due to the the broad taxonomy of rhythms, noises and lack of
real-world data and annotations from large number of patients. This paper
presents a new ECG classification method based on Deep Convolutional Neural
Networks (DCNN) and online decision fusion. Different from previous methods
which utilize hand-crafted features or learn features from the original signal
domain, the proposed DCNN based method learns features and classifiers from the
time-frequency domain in an end-to-end manner. First, the ECG wave signal is
transformed to time-frequency domain by using Short-Time Fourier Transform.
Next, specific DCNN models are trained on ECG samples of specific length.
Finally, an online decision fusion method is proposed to fuse past and current
decisions from different models into a more accurate one. Experimental results
on both synthetic and real-world ECG datasets convince the effectiveness and
efficiency of the proposed method.
"
"  Currently, most speech processing techniques use magnitude spectrograms as
front-end and are therefore by default discarding part of the signal: the
phase. In order to overcome this limitation, we propose an end-to-end learning
method for speech denoising based on Wavenet. The proposed model adaptation
retains Wavenet's powerful acoustic modeling capabilities, while significantly
reducing its time-complexity by eliminating its autoregressive nature.
Specifically, the model makes use of non-causal, dilated convolutions and
predicts target fields instead of a single target sample. The discriminative
adaptation of the model we propose, learns in a supervised fashion via
minimizing a regression loss. These modifications make the model highly
parallelizable during both training and inference. Both computational and
perceptual evaluations indicate that the proposed method is preferred to Wiener
filtering, a common method based on processing the magnitude spectrogram.
"
"  A class of methods based on multichannel linear prediction (MCLP) can achieve
effective blind dereverberation of a source, when the source is observed with a
microphone array. We propose an inventive use of MCLP as a pre-processing step
for blind source separation with a microphone array. We show theoretically
that, under certain assumptions, such pre-processing reduces the original blind
reverberant source separation problem to a non-reverberant one, which in turn
can be effectively tackled using existing methods. We demonstrate our claims
using real recordings obtained with an eight-microphone circular array in
reverberant environments.
"
"  Many successful methods have been proposed for learning low dimensional
representations on large-scale networks, while almost all existing methods are
designed in inseparable processes, learning embeddings for entire networks even
when only a small proportion of nodes are of interest. This leads to great
inconvenience, especially on super-large or dynamic networks, where these
methods become almost impossible to implement. In this paper, we formalize the
problem of separated matrix factorization, based on which we elaborate a novel
objective function that preserves both local and global information. We further
propose SepNE, a simple and flexible network embedding algorithm which
independently learns representations for different subsets of nodes in
separated processes. By implementing separability, our algorithm reduces the
redundant efforts to embed irrelevant nodes, yielding scalability to
super-large networks, automatic implementation in distributed learning and
further adaptations. We demonstrate the effectiveness of this approach on
several real-world networks with different scales and subjects. With comparable
accuracy, our approach significantly outperforms state-of-the-art baselines in
running times on large networks.
"
"  We present a user of model interaction based on the physics of kinetic
exchange, and extend it to individuals placed in a grid with local interaction.
We show with numerical analysis and partial analytical results that the
critical symmetry breaking transitions and percolation effects typical of the
full interaction model do not take place if the range of interaction is
limited, allowing for the co-existence of majorty and minority opinions in the
same community.
We then introduce a peer recommender system in the model, showing that, even
with very local iteraction and a small probability of appeal to the
recommender, its presence is sufficient to make both symmetry breaking and
percolation reappear. This seems to indicate that one effect of a
recommendation system is to uniform the opinions of a community, reducing
minority opinions or making them disappear. Although the recommender system
does uniform the community opinion, it doesn't constrain it, in the sense that
all opinions have the same probability of becoming the dominating one. We do a
partial study, however, that suggests that a ""mischievous"" recommender might be
able to bias a community so that one opinion will emerge over the opposite with
overwhelming probability.
"
"  Software engineering considers performance evaluation to be one of the key
portions of software quality assurance. Unfortunately, there seems to be a lack
of standard methodologies for performance evaluation even in the scope of
experimental computer science. Inspired by the concept of ""instantiation"" in
object-oriented programming, we distinguish the generic performance evaluation
logic from the distributed and ad-hoc relevant studies, and develop an abstract
evaluation methodology (by analogy of ""class"") we name Domain Knowledge-driven
Methodology (DoKnowMe). By replacing five predefined domain-specific knowledge
artefacts, DoKnowMe could be instantiated into specific methodologies (by
analogy of ""object"") to guide evaluators in performance evaluation of different
software and even computing systems. We also propose a generic validation
framework with four indicators (i.e.~usefulness, feasibility, effectiveness and
repeatability), and use it to validate DoKnowMe in the Cloud services
evaluation domain. Given the positive and promising validation result, we plan
to integrate more common evaluation strategies to improve DoKnowMe and further
focus on the performance evaluation of Cloud autoscaler systems.
"
"  Neural networks with low-precision weights and activations offer compelling
efficiency advantages over their full-precision equivalents. The two most
frequently discussed benefits of quantization are reduced memory consumption,
and a faster forward pass when implemented with efficient bitwise operations.
We propose a third benefit of very low-precision neural networks: improved
robustness against some adversarial attacks, and in the worst case, performance
that is on par with full-precision models. We focus on the very low-precision
case where weights and activations are both quantized to $\pm$1, and note that
stochastically quantizing weights in just one layer can sharply reduce the
impact of iterative attacks. We observe that non-scaled binary neural networks
exhibit a similar effect to the original defensive distillation procedure that
led to gradient masking, and a false notion of security. We address this by
conducting both black-box and white-box experiments with binary models that do
not artificially mask gradients.
"
"  Stable Marriage is a fundamental problem to both computer science and
economics. Four well-known NP-hard optimization versions of this problem are
the Sex-Equal Stable Marriage (SESM), Balanced Stable Marriage (BSM),
max-Stable Marriage with Ties (max-SMT) and min-Stable Marriage with Ties
(min-SMT) problems. In this paper, we analyze these problems from the viewpoint
of Parameterized Complexity. We conduct the first study of these problems with
respect to the parameter treewidth. First, we study the treewidth $\mathtt{tw}$
of the primal graph. We establish that all four problems are W[1]-hard. In
particular, while it is easy to show that all four problems admit algorithms
that run in time $n^{O(\mathtt{tw})}$, we prove that all of these algorithms
are likely to be essentially optimal. Next, we study the treewidth
$\mathtt{tw}$ of the rotation digraph. In this context, the max-SMT and min-SMT
are not defined. For both SESM and BSM, we design (non-trivial) algorithms that
run in time $2^{\mathtt{tw}}n^{O(1)}$. Then, for both SESM and BSM, we also
prove that unless SETH is false, algorithms that run in time
$(2-\epsilon)^{\mathtt{tw}}n^{O(1)}$ do not exist for any fixed $\epsilon>0$.
We thus present a comprehensive, complete picture of the behavior of central
optimization versions of Stable Marriage with respect to treewidth.
"
"  Electricity market price predictions enable energy market participants to
shape their consumption or supply while meeting their economic and
environmental objectives. By utilizing the basic properties of the
supply-demand matching process performed by grid operators, we develop a method
to recover energy market's structure and predict the resulting nodal prices as
a function of generation mix and system load on the grid. Our methodology uses
the latest advancements in compressed sensing and statistics to cope with the
high-dimensional and sparse power grid topologies, underlying physical laws, as
well as scarce, public market data. Rigorous validations using Southwest Power
Pool (SPP) market data demonstrate significantly higher accuracy of the
proposed approach when compared to the state-of-the-art industry benchmark.
"
"  Community detection or clustering is a fundamental task in the analysis of
network data. Many real networks have a bipartite structure which makes
community detection challenging. In this paper, we consider a model which
allows for matched communities in the bipartite setting, in addition to node
covariates with information about the matching. We derive a simple fast
algorithm for fitting the model based on variational inference ideas and show
its effectiveness on both simulated and real data. A variation of the model to
allow for degree-correction is also considered, in addition to a novel approach
to fitting such degree-corrected models.
"
"  The topological interference management (TIM) problem studies
partially-connected interference networks with no channel state information
except for the network topology (i.e., connectivity graph) at the transmitters.
In this paper, we consider a similar problem in the uplink cellular networks,
while message passing is enabled at the receivers (e.g., base stations), so
that the decoded messages can be routed to other receivers via backhaul links
to help further improve network performance. For this TIM problem with decoded
message passing (TIM-MP), we model the interference pattern by conflict
digraphs, connect orthogonal access to the acyclic set coloring on conflict
digraphs, and show that one-to-one interference alignment boils down to
orthogonal access because of message passing. With the aid of polyhedral
combinatorics, we identify the structural properties of certain classes of
network topologies where orthogonal access achieves the optimal
degrees-of-freedom (DoF) region in the information-theoretic sense. The
relation to the conventional index coding with simultaneous decoding is also
investigated by formulating a generalized index coding problem with successive
decoding as a result of decoded message passing. The properties of reducibility
and criticality are also studied, by which we are able to prove the linear
optimality of orthogonal access in terms of symmetric DoF for the networks up
to four users with all possible network topologies (218 instances). Practical
issues of the tradeoff between the overhead of message passing and the
achievable symmetric DoF are also discussed, in the hope of facilitating
efficient backhaul utilization.
"
"  Knowledge base completion (KBC) aims to predict missing information in a
knowledge base.In this paper, we address the out-of-knowledge-base (OOKB)
entity problem in KBC:how to answer queries concerning test entities not
observed at training time. Existing embedding-based KBC models assume that all
test entities are available at training time, making it unclear how to obtain
embeddings for new entities without costly retraining. To solve the OOKB entity
problem without retraining, we use graph neural networks (Graph-NNs) to compute
the embeddings of OOKB entities, exploiting the limited auxiliary knowledge
provided at test time.The experimental results show the effectiveness of our
proposed model in the OOKB setting.Additionally, in the standard KBC setting in
which OOKB entities are not involved, our model achieves state-of-the-art
performance on the WordNet dataset. The code and dataset are available at
this https URL
"
"  Feature engineering is a crucial step in the process of predictive modeling.
It involves the transformation of given feature space, typically using
mathematical functions, with the objective of reducing the modeling error for a
given target. However, there is no well-defined basis for performing effective
feature engineering. It involves domain knowledge, intuition, and most of all,
a lengthy process of trial and error. The human attention involved in
overseeing this process significantly influences the cost of model generation.
We present a new framework to automate feature engineering. It is based on
performance driven exploration of a transformation graph, which systematically
and compactly enumerates the space of given options. A highly efficient
exploration strategy is derived through reinforcement learning on past
examples.
"
"  Frequent Pattern Mining is a one field of the most significant topics in data
mining. In recent years, many algorithms have been proposed for mining frequent
itemsets. A new algorithm has been presented for mining frequent itemsets based
on N-list data structure called Prepost algorithm. The Prepost algorithm is
enhanced by implementing compact PPC-tree with the general tree. Prepost
algorithm can only find a frequent itemsets with required (pre-order and
post-order) for each node. In this chapter, we improved prepost algorithm based
on Hadoop platform (HPrepost), proposed using the Mapreduce programming model.
The main goals of proposed method are efficient mining frequent itemsets
requiring less running time and memory usage. We have conduct experiments for
the proposed scheme to compare with another algorithms. With dense datasets,
which have a large average length of transactions, HPrepost is more effective
than frequent itemsets algorithms in terms of execution time and memory usage
for all min-sup. Generally, our algorithm outperforms algorithms in terms of
runtime and memory usage with small thresholds and large datasets.
"
"  We exhibit an $O((\log k)^6)$-competitive randomized algorithm for the
$k$-server problem on any metric space. It is shown that a potential-based
algorithm for the fractional $k$-server problem on hierarchically separated
trees (HSTs) with competitive ratio $f(k)$ can be used to obtain a randomized
algorithm for any metric space with competitive ratio $f(k)^2 O((\log k)^2)$.
Employing the $O((\log k)^2)$-competitive algorithm for HSTs from our joint
work with Bubeck, Cohen, Lee, and Mądry (2017) yields the claimed bound.
The best previous result independent of the geometry of the underlying metric
space is the $2k-1$ competitive ratio established for the deterministic work
function algorithm by Koutsoupias and Papadimitriou (1995). Even for the
special case when the underlying metric space is the real line, the best known
competitive ratio was $k$. Since deterministic algorithms can do no better than
$k$ on any metric space with at least $k+1$ points, this establishes that for
every metric space on which the problem is non-trivial, randomized algorithms
give an exponential improvement over deterministic algorithms.
"
"  This paper proposes an approach to detect emotion from human speech employing
majority voting technique over several machine learning techniques. The
contribution of this work is in two folds: firstly it selects those features of
speech which is most promising for classification and secondly it uses the
majority voting technique that selects the exact class of emotion. Here,
majority voting technique has been applied over Neural Network (NN), Decision
Tree (DT), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN). Input
vector of NN, DT, SVM and KNN consists of various acoustic and prosodic
features like Pitch, Mel-Frequency Cepstral coefficients etc. From speech
signal many feature have been extracted and only promising features have been
selected. To consider a feature as promising, Fast Correlation based feature
selection (FCBF) and Fisher score algorithms have been used and only those
features are selected which are highly ranked by both of them. The proposed
approach has been tested on Berlin dataset of emotional speech [3] and
Electromagnetic Articulography (EMA) dataset [4]. The experimental result shows
that majority voting technique attains better accuracy over individual machine
learning techniques. The employment of the proposed approach can effectively
recognize the emotion of human beings in case of social robot, intelligent chat
client, call-center of a company etc.
"
"  In article the basic principles put in a basis of algorithmicallysoftware of
hypercomplex number calculations, structure of a software, structure of
functional subsystems are considered. The most important procedures included in
subsystems are considered, program listings and examples of their application
are given.
"
"  Reactive power compensation is an important challenge in current and future
smart power systems. However, in the context of reactive power compensation,
most existing studies assume that customers can assess their compensation
value, i.e., Var unit, objectively. In this paper, customers are assumed to
make decisions that pertain to reactive power coordination. In consequence, the
way in which those customers evaluate the compensation value resulting from
their individual decisions will impact the overall grid performance. In
particular, a behavioral framework, based on the framing effect of prospect
theory (PT), is developed to study the effect of both objective value and
subjective evaluation in a reactive power compensation game. For example, such
effect allows customers to optimize a subjective value of their utility which
essentially frames the objective utility with respect to a reference point.
This game enables customers to coordinate the use of their electrical devices
to compensate reactive power. For the proposed game, both the objective case
using expected utility theory (EUT) and the PT consideration are solved via a
learning algorithm that converges to a mixed-strategy Nash equilibrium. In
addition, several key properties of this game are derived analytically.
Simulation results show that, under PT, customers are likely to make decisions
that differ from those predicted by classical models. For instance, using an
illustrative two-customer case, we show that a PT customer will increase the
conservative strategy (achieving a high power factor) by 29% compared to a
conventional customer. Similar insights are also observed for a case with three
customers.
"
"  Online trust systems are playing an important role in to-days world and face
various challenges in building them. Billions of dollars of products and
services are traded through electronic commerce, files are shared among large
peer-to-peer networks and smart contracts can potentially replace paper
contracts with digital contracts. These systems rely on trust mechanisms in
peer-to-peer networks like reputation systems or a trustless public ledger. In
most cases, reputation systems are build to determine the trustworthiness of
users and to provide incentives for users to make a fair contribution to the
peer-to-peer network. The main challenges are how to set up a good trust
system, how to deal with security issues and how to deal with strategic users
trying to cheat on the system. The Sybil attack, the most important attack on
reputation systems is discussed. At last match making in two sided markets and
the strategy proofness of these markets are discussed.
"
"  The Constrained Application Protocol (CoAP) is a specialized Web transfer
protocol for resource-oriented applications intended to run on constrained
devices, typically part of the Internet of Things. In this paper we leverage
Information-Centric Networking (ICN), deployed within the domain of a network
provider that interconnects, in addition to other terminals, CoAP endpoints in
order to provide enhanced CoAP services. We present various CoAP-specific
communication scenarios and discuss how ICN can provide benefits to both
network providers and CoAP applications, even though the latter are not aware
of the existence of ICN. In particular, the use of ICN results in smaller state
management complexity at CoAP endpoints, simpler implementation at CoAP
endpoints, and less communication overhead in the network.
"
"  In many domains, a latent competition among different conventions determines
which one will come to dominate. One sees such effects in the success of
community jargon, of competing frames in political rhetoric, or of terminology
in technical contexts. These effects have become widespread in the online
domain, where the data offers the potential to study competition among
conventions at a fine-grained level.
In analyzing the dynamics of conventions over time, however, even with
detailed on-line data, one encounters two significant challenges. First, as
conventions evolve, the underlying substance of their meaning tends to change
as well; and such substantive changes confound investigations of social
effects. Second, the selection of a convention takes place through the complex
interactions of individuals within a community, and contention between the
users of competing conventions plays a key role in the convention's evolution.
Any analysis must take place in the presence of these two issues.
In this work we study a setting in which we can cleanly track the competition
among conventions. Our analysis is based on the spread of low-level authoring
conventions in the eprint arXiv over 24 years: by tracking the spread of macros
and other author-defined conventions, we are able to study conventions that
vary even as the underlying meaning remains constant. We find that the
interaction among co-authors over time plays a crucial role in the selection of
them; the distinction between more and less experienced members of the
community, and the distinction between conventions with visible versus
invisible effects, are both central to the underlying processes. Through our
analysis we make predictions at the population level about the ultimate success
of different synonymous conventions over time--and at the individual level
about the outcome of ""fights"" between people over convention choices.
"
"  Analog-to-digital converters (ADCs) are a major contributor to the power
consumption of multiple-input multiple-output (MIMO) communication systems with
large number of antennas. Use of low resolution ADCs has been proposed as a
means to decrease power consumption in MIMO receivers. However, reducing the
ADC resolution leads to performance loss in terms of achievable transmission
rates. In order to mitigate the rate-loss, the receiver can perform analog
processing of the received signals before quantization. Prior works consider
one-shot analog processing where at each channel-use, analog linear
combinations of the received signals are fed to a set of one-bit threshold
ADCs. In this paper, a receiver architecture is proposed which uses a sequence
of delay elements to allow for blockwise linear combining of the received
analog signals. In the high signal to noise ratio regime, it is shown that the
proposed architecture achieves the maximum achievable transmission rate given a
fixed number of one-bit ADCs. Furthermore, a tradeoff between transmission rate
and the number of delay elements is identified which quantifies the increase in
maximum achievable rate as the number of delay elements is increased.
"
"  We design controllers from formal specifications for positive discrete-time
monotone systems that are subject to bounded disturbances. Such systems are
widely used to model the dynamics of transportation and biological networks.
The specifications are described using signal temporal logic (STL), which can
express a broad range of temporal properties. We formulate the problem as a
mixed-integer linear program (MILP) and show that under the assumptions made in
this paper, which are not restrictive for traffic applications, the existence
of open-loop control policies is sufficient and almost necessary to ensure the
satisfaction of STL formulas. We establish a relation between satisfaction of
STL formulas in infinite time and set-invariance theories and provide an
efficient method to compute robust control invariant sets in high dimensions.
We also develop a robust model predictive framework to plan controls optimally
while ensuring the satisfaction of the specification. Illustrative examples and
a traffic management case study are included.
"
"  Progress in deep learning is slowed by the days or weeks it takes to train
large models. The natural solution of using more hardware is limited by
diminishing returns, and leads to inefficient use of additional resources. In
this paper, we present a large batch, stochastic optimization algorithm that is
both faster than widely used algorithms for fixed amounts of computation, and
also scales up substantially better as more computational resources become
available. Our algorithm implicitly computes the inverse Hessian of each
mini-batch to produce descent directions; we do so without either an explicit
approximation to the Hessian or Hessian-vector products. We demonstrate the
effectiveness of our algorithm by successfully training large ImageNet models
(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch
sizes of up to 32000 with no loss in validation error relative to current
baselines, and no increase in the total number of steps. At smaller mini-batch
sizes, our optimizer improves the validation error in these models by 0.8-0.9%.
Alternatively, we can trade off this accuracy to reduce the number of training
steps needed by roughly 10-30%. Our work is practical and easily usable by
others -- only one hyperparameter (learning rate) needs tuning, and
furthermore, the algorithm is as computationally cheap as the commonly used
Adam optimizer.
"
"  This paper proposes a new scheme to secure the transmissions in an untrusted
decode-and-forward (DF) relaying network. A legitimate source node, Alice,
sends her data to a legitimate destination node, Bob, with the aid of an
untrusted DF relay node, Charlie. To secure the transmissions from Charlie
during relaying time slots, each data codeword is secured using a secret-key
codeword that has been previously shared between Alice and Bob during the
perfectly secured time slots (i.e., when the channel secrecy rate is positive).
The secret-key bits exchanged between Alice and Bob are stored in a
finite-length buffer and are used to secure data transmission whenever needed.
We model the secret-key buffer as a queueing system and analyze its Markov
chain. Our numerical results show the gains of our proposed scheme relative to
benchmarks. Moreover, the proposed scheme achieves an upper bound on the secure
throughput.
"
"  This paper explores an incremental training strategy for the skip-gram model
with negative sampling (SGNS) from both empirical and theoretical perspectives.
Existing methods of neural word embeddings, including SGNS, are multi-pass
algorithms and thus cannot perform incremental model update. To address this
problem, we present a simple incremental extension of SGNS and provide a
thorough theoretical analysis to demonstrate its validity. Empirical
experiments demonstrated the correctness of the theoretical analysis as well as
the practical usefulness of the incremental algorithm.
"
"  Treating optimization methods as dynamical systems can be traced back
centuries ago in order to comprehend the notions and behaviors of optimization
methods. Lately, this mind set has become the driving force to design new
optimization methods. Inspired by the recent dynamical system viewpoint of
Nesterov's fast method, we propose two classes of fast methods, formulated as
hybrid control systems, to obtain pre-specified exponential convergence rate.
Alternative to the existing fast methods which are parametric-in-time second
order differential equations, we dynamically synthesize feedback controls in a
state-dependent manner. Namely, in the first class the damping term is viewed
as the control input, while in the second class the amplitude with which the
gradient of the objective function impacts the dynamics serves as the
controller. The objective function requires to satisfy a certain sharpness
criterion, the so-called Polyak--{\L}ojasiewicz inequality. Moreover, we
establish that both hybrid structures possess Zeno-free solution trajectories.
We finally provide a mechanism to determine the discretization step size to
attain an exponential convergence rate.
"
"  Processing sequential data of variable length is a major challenge in a wide
range of applications, such as speech recognition, language modeling,
generative image modeling and machine translation. Here, we address this
challenge by proposing a novel recurrent neural network (RNN) architecture, the
Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both
multiscale RNNs and deep transition RNNs as it processes sequential data on
different timescales and learns complex transition functions from one time step
to the next. We evaluate the FS-RNN on two character level language modeling
data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of
the art results to $1.19$ and $1.25$ bits-per-character (BPC), respectively. In
addition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize
Wikipedia outperforming the best known compression algorithm with respect to
the BPC measure. We also present an empirical investigation of the learning and
network dynamics of the FS-RNN, which explains the improved performance
compared to other RNN architectures. Our approach is general as any kind of RNN
cell is a possible building block for the FS-RNN architecture, and thus can be
flexibly applied to different tasks.
"
"  OR multi-access channel is a simple model where the channel output is the
Boolean OR among the Boolean channel inputs. We revisit this model, showing
that employing Bloom filter, a randomized data structure, as channel inputs
achieves its capacity region with joint decoding and the symmetric sum rate of
$\ln 2$ bits per channel use without joint decoding. We then proceed to the
""many-access"" regime where the number of potential users grows without bound,
treating both activity recognition and message transmission problems,
establishing scaling laws which are optimal within a constant factor, based on
Bloom filter channel inputs.
"
"  Social media platforms contain a great wealth of information which provides
opportunities for us to explore hidden patterns or unknown correlations, and
understand people's satisfaction with what they are discussing. As one
showcase, in this paper, we present a system, TwiInsight which explores the
insight of Twitter data. Different from other Twitter analysis systems,
TwiInsight automatically extracts the popular topics under different categories
(e.g., healthcare, food, technology, sports and transport) discussed in Twitter
via topic modeling and also identifies the correlated topics across different
categories. Additionally, it also discovers the people's opinions on the tweets
and topics via the sentiment analysis. The system also employs an intuitive and
informative visualization to show the uncovered insight. Furthermore, we also
develop and compare six most popular algorithms - three for sentiment analysis
and three for topic modeling.
"
"  Transparency, user trust, and human comprehension are popular ethical
motivations for interpretable machine learning. In support of these goals,
researchers evaluate model explanation performance using humans and real world
applications. This alone presents a challenge in many areas of artificial
intelligence. In this position paper, we propose a distinction between
descriptive and persuasive explanations. We discuss reasoning suggesting that
functional interpretability may be correlated with cognitive function and user
preferences. If this is indeed the case, evaluation and optimization using
functional metrics could perpetuate implicit cognitive bias in explanations
that threaten transparency. Finally, we propose two potential research
directions to disambiguate cognitive function and explanation models, retaining
control over the tradeoff between accuracy and interpretability.
"
"  In this paper, we reconsider the unfolding-based technique that we have
introduced previously for detecting loops in standard term rewriting. We
improve it by guiding the unfolding process, using distinguished positions in
the rewrite rules. This results in a depth-first computation of the unfoldings,
whereas the original technique was breadth-first. We have implemented this new
approach in our tool NTI and compared it to the previous one on a bunch of
rewrite systems. The results we get are promising (better times, more
successful proofs).
"
"  Random Fourier features is one of the most popular techniques for scaling up
kernel methods, such as kernel ridge regression. However, despite impressive
empirical results, the statistical properties of random Fourier features are
still not well understood. In this paper we take steps toward filling this gap.
Specifically, we approach random Fourier features from a spectral matrix
approximation point of view, give tight bounds on the number of Fourier
features required to achieve a spectral approximation, and show how spectral
matrix approximation bounds imply statistical guarantees for kernel ridge
regression.
Qualitatively, our results are twofold: on the one hand, we show that random
Fourier feature approximation can provably speed up kernel ridge regression
under reasonable assumptions. At the same time, we show that the method is
suboptimal, and sampling from a modified distribution in Fourier space, given
by the leverage function of the kernel, yields provably better performance. We
study this optimal sampling distribution for the Gaussian kernel, achieving a
nearly complete characterization for the case of low-dimensional bounded
datasets. Based on this characterization, we propose an efficient sampling
scheme with guarantees superior to random Fourier features in this regime.
"
"  The present paper considers testing an Erdos--Renyi random graph model
against a stochastic block model in the asymptotic regime where the average
degree of the graph grows with the graph size n. Our primary interest lies in
those cases in which the signal-to-noise ratio is at a constant level. Focusing
on symmetric two block alternatives, we first derive joint central limit
theorems for linear spectral statistics of power functions for properly
rescaled graph adjacency matrices under both the null and local alternative
hypotheses. The powers in the linear spectral statistics are allowed to grow to
infinity together with the graph size. In addition, we show that linear
spectral statistics of Chebyshev polynomials are closely connected to signed
cycles of growing lengths that determine the asymptotic likelihood ratio test
for the hypothesis testing problem of interest. This enables us to construct a
sequence of test statistics that achieves the exact optimal asymptotic power
within $O(n^3 \log n)$ time complexity in the contiguous regime when $n^2
p_{n,av}^3 \to\infty$ where $p_{n,av}$ is the average connection probability.
We further propose a class of adaptive tests that are computationally tractable
and completely data-driven. They achieve nontrivial powers in the contiguous
regime and consistency in the singular regime whenever $n p_{n,av} \to\infty$.
These tests remain powerful when the alternative becomes a more general
stochastic block model with more than two blocks.
"
"  In this paper, we propose a new loss function called generalized end-to-end
(GE2E) loss, which makes the training of speaker verification models more
efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike
TE2E, the GE2E loss function updates the network in a way that emphasizes
examples that are difficult to verify at each step of the training process.
Additionally, the GE2E loss does not require an initial stage of example
selection. With these properties, our model with the new loss function
decreases speaker verification EER by more than 10%, while reducing the
training time by 60% at the same time. We also introduce the MultiReader
technique, which allows us to do domain adaptation - training a more accurate
model that supports multiple keywords (i.e. ""OK Google"" and ""Hey Google"") as
well as multiple dialects.
"
"  Learning interpretable features from complex multilayer networks is a
challenging and important problem. The need for such representations is
particularly evident in multilayer networks of the brain, where nodal
characteristics may help model and differentiate regions of the brain according
to individual, cognitive task, or disease. Motivated by this problem, we
introduce the multi-node2vec algorithm, an efficient and scalable feature
engineering method that automatically learns continuous node feature
representations from multilayer networks. Multi-node2vec relies upon a
second-order random walk sampling procedure that efficiently explores the
inner- and intra- layer ties of the observed multilayer network is utilized to
identify multilayer neighborhoods. Maximum likelihood estimators of the nodal
features are identified through the use of the Skip-gram neural network model
on the collection of sampled neighborhoods. We investigate the conditions under
which multi-node2vec is an approximation of a closed-form matrix factorization
problem. We demonstrate the efficacy of multi-node2vec on a multilayer
functional brain network from resting state fMRI scans over a group of 74
healthy individuals. We find that multi-node2vec outperforms contemporary
methods on complex networks, and that multi-node2vec identifies nodal
characteristics that closely associate with the functional organization of the
brain.
"
"  Recent years have witnessed the growing demands for resolving numerous bug
reports in software maintenance. Aiming to reduce the time testers/developers
take in perusing bug reports, the task of bug report summarization has
attracted a lot of research efforts in the literature. However, no systematic
analysis has been conducted on attribute construction which heavily impacts the
performance of supervised algorithms for bug report summarization. In this
study, we first conduct a survey to reveal the existing methods for attribute
construction in mining software repositories. Then, we propose a new method
named Crowd-Attribute to infer new effective attributes from the crowdgenerated
data in crowdsourcing and develop a new tool named Crowdsourcing Software
Engineering Platform to facilitate this method. With Crowd-Attribute, we
successfully construct 11 new attributes and propose a new supervised algorithm
named Logistic Regression with Crowdsourced Attributes (LRCA). To evaluate the
effectiveness of LRCA, we build a series of large scale data sets with 105,177
bug reports. Experiments over both the public data set SDS with 36 manually
annotated bug reports and new large-scale data sets demonstrate that LRCA can
consistently outperform the state-of-the-art algorithms for bug report
summarization.
"
"  In this paper, we propose an efficient transfer leaning methods for training
a personalized language model using a recurrent neural network with long
short-term memory architecture. With our proposed fast transfer learning
schemes, a general language model is updated to a personalized language model
with a small amount of user data and a limited computing resource. These
methods are especially useful for a mobile device environment while the data is
prevented from transferring out of the device for privacy purposes. Through
experiments on dialogue data in a drama, it is verified that our transfer
learning methods have successfully generated the personalized language model,
whose output is more similar to the personal language style in both qualitative
and quantitative aspects.
"
"  This paper is concerned with the convergence and long-term stability analysis
of the feedback particle filter (FPF) algorithm. The FPF is an interacting
system of $N$ particles where the interaction is designed such that the
empirical distribution of the particles approximates the posterior
distribution. It is known that in the mean-field limit ($N=\infty$), the
distribution of the particles is equal to the posterior distribution. However
little is known about the convergence to the mean-field limit. In this paper,
we consider the FPF algorithm for the linear Gaussian setting. In this setting,
the algorithm is similar to the ensemble Kalman-Bucy filter algorithm. Although
these algorithms have been numerically evaluated and widely used in
applications, their convergence and long-term stability analysis remains an
active area of research. In this paper, we show that, (i) the mean-field limit
is well-defined with a unique strong solution; (ii) the mean-field process is
stable with respect to the initial condition; (iii) we provide conditions such
that the finite-$N$ system is long term stable and we obtain some mean-squared
error estimates that are uniform in time.
"
"  Temporary earth retaining structures (TERS) help prevent collapse during
construction excavation. To ensure that these structures are operating within
design specifications, load forces on supports must be monitored. Current
monitoring approaches are expensive, sparse, off-line, and thus difficult to
integrate into predictive models. This work aims to show that wirelessly
connected battery powered sensors are feasible, practical, and have similar
accuracy to existing sensor systems. We present the design and validation of
ReStructure, an end-to-end prototype wireless sensor network for collection,
communication, and aggregation of strain data. ReStructure was validated
through a six months deployment on a real-life excavation site with all but one
node producing valid and accurate strain measurements at higher frequency than
existing ones. These results and the lessons learnt provide the basis for
future widespread wireless TERS monitoring that increase measurement density
and integrate closely with predictive models to provide timely alerts of damage
or potential failure.
"
"  We consider the problem of optimizing the placement of stubborn agents in a
social network in order to maximally impact population opinions.
We assume individuals in a directed social network each have a latent opinion
that evolves over time in response to social media posts by their neighbors.
The individuals randomly communicate noisy versions of their latent opinion to
their neighbors. Each individual updates his opinion using a time-varying
update rule that has him become more stubborn with time and be less affected by
new posts. The dynamic update rule is a novel component of our model and
reflects realistic behaviors observed in many psychological studies.
We show that in the presence of stubborn agents with immutable opinions and
under fairly general conditions on the stubbornness rate of the individuals,
the opinions converge to an equilibrium determined by a linear system. We give
an interesting electrical network interpretation of the equilibrium. We also
use this equilibrium to present a simple closed form expression for harmonic
influence centrality, which is a function that quantifies how much a node can
affect the mean opinion in a network. We develop a discrete optimization
formulation for the problem of maximally shifting opinions in a network by
targeting nodes with stubborn agents. We show that this is an optimization
problem with a monotone and submodular objective, allowing us to utilize a
greedy algorithm. Finally, we show that a small number of stubborn agents can
non-trivially influence a large population using simulated networks.
"
"  This work fits in the context of community microgrids, where members of a
community can exchange energy and services among themselves, without going
through the usual channels of the public electricity grid. We introduce and
analyze a framework to operate a community microgrid, and to share the
resulting revenues and costs among its members. A market-oriented pricing of
energy exchanges within the community is obtained by implementing an internal
local market based on the marginal pricing scheme. The market aims at
maximizing the social welfare of the community, thanks to the more efficient
allocation of resources, the reduction of the peak power to be paid, and the
increased amount of reserve, achieved at an aggregate level. A community
microgrid operator, acting as a benevolent planner, redistributes revenues and
costs among the members, in such a way that the solution achieved by each
member within the community is not worse than the solution it would achieve by
acting individually. In this way, each member is incentivized to participate in
the community on a voluntary basis. The overall framework is formulated in the
form of a bilevel model, where the lower level problem clears the market, while
the upper level problem plays the role of the community microgrid operator.
Numerical results obtained on a real test case implemented in Belgium show
significant cost savings on a yearly scale for the community members, as
compared to the case when they act individually.
"
"  In unsupervised domain mapping, the learner is given two unmatched datasets
$A$ and $B$. The goal is to learn a mapping $G_{AB}$ that translates a sample
in $A$ to the analog sample in $B$. Recent approaches have shown that when
learning simultaneously both $G_{AB}$ and the inverse mapping $G_{BA}$,
convincing mappings are obtained. In this work, we present a method of learning
$G_{AB}$ without learning $G_{BA}$. This is done by learning a mapping that
maintains the distance between a pair of samples. Moreover, good mappings are
obtained, even by maintaining the distance between different parts of the same
sample before and after mapping. We present experimental results that the new
method not only allows for one sided mapping learning, but also leads to
preferable numerical results over the existing circularity-based constraint.
Our entire code is made publicly available at
this https URL .
"
"  We present an approach to adaptively utilize deep neural networks in order to
reduce the evaluation time on new examples without loss of accuracy. Rather
than attempting to redesign or approximate existing networks, we propose two
schemes that adaptively utilize networks. We first pose an adaptive network
evaluation scheme, where we learn a system to adaptively choose the components
of a deep network to be evaluated for each example. By allowing examples
correctly classified using early layers of the system to exit, we avoid the
computational time associated with full evaluation of the network. We extend
this to learn a network selection system that adaptively selects the network to
be evaluated for each example. We show that computational time can be
dramatically reduced by exploiting the fact that many examples can be correctly
classified using relatively efficient networks and that complex,
computationally costly networks are only necessary for a small fraction of
examples. We pose a global objective for learning an adaptive early exit or
network selection policy and solve it by reducing the policy learning problem
to a layer-by-layer weighted binary classification problem. Empirically, these
approaches yield dramatic reductions in computational cost, with up to a 2.8x
speedup on state-of-the-art networks from the ImageNet image recognition
challenge with minimal (<1%) loss of top5 accuracy.
"
"  In acoustic scene classification researches, audio segment is usually split
into multiple samples. Majority voting is then utilized to ensemble the results
of the samples. In this paper, we propose a punishment voting algorithm based
on the super categories construction method for acoustic scene classification.
Specifically, we propose a DenseNet-like model as the base classifier. The base
classifier is trained by the CQT spectrograms generated from the raw audio
segments. Taking advantage of the results of the base classifier, we propose a
super categories construction method using the spectral clustering. Super
classifiers corresponding to the constructed super categories are further
trained. Finally, the super classifiers are utilized to enhance the majority
voting of the base classifier by punishment voting. Experiments show that the
punishment voting obviously improves the performances on both the DCASE2017
Development dataset and the LITIS Rouen dataset.
"
"  Deep convolutional networks have achieved great success for image
recognition. However, for action recognition in videos, their advantage over
traditional methods is not so evident. We present a general and flexible
video-level framework for learning action models in videos. This method, called
temporal segment network (TSN), aims to model long-range temporal structures
with a new segment-based sampling and aggregation module. This unique design
enables our TSN to efficiently learn action models by using the whole action
videos. The learned models could be easily adapted for action recognition in
both trimmed and untrimmed videos with simple average pooling and multi-scale
temporal window integration, respectively. We also study a series of good
practices for the instantiation of TSN framework given limited training
samples. Our approach obtains the state-the-of-art performance on four
challenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%),
THUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB
difference for motion models, our method can still achieve competitive accuracy
on UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the temporal
segment networks, we won the video classification track at the ActivityNet
challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and
the proposed good practices.
"
"  The current study applies deep learning to herbalism. Toward the goal, we
acquired the de-identified health insurance reimbursements that were claimed in
a 10-year period from 2004 to 2013 in the National Health Insurance Database of
Taiwan, the total number of reimbursement records equaling 340 millions. Two
artificial intelligence techniques were applied to the dataset: residual
convolutional neural network multitask classifier and attention-based recurrent
neural network. The former works to translate from herbal prescriptions to
diseases; and the latter from diseases to herbal prescriptions. Analysis of the
classification results indicates that herbal prescriptions are specific to:
anatomy, pathophysiology, sex and age of the patient, and season and year of
the prescription. Further analysis identifies temperature and gross domestic
product as the meteorological and socioeconomic factors that are associated
with herbal prescriptions. Analysis of the neural machine transitional result
indicates that the recurrent neural network learnt not only syntax but also
semantics of diseases and herbal prescriptions.
"
"  In privacy amplification, two mutually trusted parties aim to amplify the
secrecy of an initial shared secret $X$ in order to establish a shared private
key $K$ by exchanging messages over an insecure communication channel. If the
channel is authenticated the task can be solved in a single round of
communication using a strong randomness extractor; choosing a quantum-proof
extractor allows one to establish security against quantum adversaries.
In the case that the channel is not authenticated, Dodis and Wichs (STOC'09)
showed that the problem can be solved in two rounds of communication using a
non-malleable extractor, a stronger pseudo-random construction than a strong
extractor.
We give the first construction of a non-malleable extractor that is secure
against quantum adversaries. The extractor is based on a construction by Li
(FOCS'12), and is able to extract from source of min-entropy rates larger than
$1/2$. Combining this construction with a quantum-proof variant of the
reduction of Dodis and Wichs, shown by Cohen and Vidick (unpublished), we
obtain the first privacy amplification protocol secure against active quantum
adversaries.
"
"  In a reversible language, any forward computation can be undone by a finite
sequence of backward steps. Reversible computing has been studied in the
context of different programming languages and formalisms, where it has been
used for testing and verification, among others. In this paper, we consider a
subset of Erlang, a functional and concurrent programming language based on the
actor model. We present a formal semantics for reversible computation in this
language and prove its main properties, including its causal consistency. We
also build on top of it a rollback operator that can be used to undo the
actions of a process up to a given checkpoint.
"
"  In this paper, we investigate the convergence and consistency properties of
an Invariant-Extended Kalman Filter (RI-EKF) based Simultaneous Localization
and Mapping (SLAM) algorithm. Basic convergence properties of this algorithm
are proven. These proofs do not require the restrictive assumption that the
Jacobians of the motion and observation models need to be evaluated at the
ground truth. It is also shown that the output of RI-EKF is invariant under any
stochastic rigid body transformation in contrast to $\mathbb{SO}(3)$ based EKF
SLAM algorithm ($\mathbb{SO}(3)$-EKF) that is only invariant under
deterministic rigid body transformation. Implications of these invariance
properties on the consistency of the estimator are also discussed. Monte Carlo
simulation results demonstrate that RI-EKF outperforms $\mathbb{SO}(3)$-EKF,
Robocentric-EKF and the ""First Estimates Jacobian"" EKF, for 3D point feature
based SLAM.
"
"  Finding a maximum cut is a fundamental task in many computational settings.
Surprisingly, it has been insufficiently studied in the classic distributed
settings, where vertices communicate by synchronously sending messages to their
neighbors according to the underlying graph, known as the $\mathcal{LOCAL}$ or
$\mathcal{CONGEST}$ models. We amend this by obtaining almost optimal
algorithms for Max-Cut on a wide class of graphs in these models. In
particular, for any $\epsilon > 0$, we develop randomized approximation
algorithms achieving a ratio of $(1-\epsilon)$ to the optimum for Max-Cut on
bipartite graphs in the $\mathcal{CONGEST}$ model, and on general graphs in the
$\mathcal{LOCAL}$ model.
We further present efficient deterministic algorithms, including a
$1/3$-approximation for Max-Dicut in our models, thus improving the best known
(randomized) ratio of $1/4$. Our algorithms make non-trivial use of the greedy
approach of Buchbinder et al. (SIAM Journal on Computing, 2015) for maximizing
an unconstrained (non-monotone) submodular function, which may be of
independent interest.
"
"  The precise modeling of subatomic particle interactions and propagation
through matter is paramount for the advancement of nuclear and particle physics
searches and precision measurements. The most computationally expensive step in
the simulation pipeline of a typical experiment at the Large Hadron Collider
(LHC) is the detailed modeling of the full complexity of physics processes that
govern the motion and evolution of particle showers inside calorimeters. We
introduce \textsc{CaloGAN}, a new fast simulation technique based on generative
adversarial networks (GANs). We apply these neural networks to the modeling of
electromagnetic showers in a longitudinally segmented calorimeter, and achieve
speedup factors comparable to or better than existing full simulation
techniques on CPU ($100\times$-$1000\times$) and even faster on GPU (up to
$\sim10^5\times$). There are still challenges for achieving precision across
the entire phase space, but our solution can reproduce a variety of geometric
shower shape properties of photons, positrons and charged pions. This
represents a significant stepping stone toward a full neural network-based
detector simulation that could save significant computing time and enable many
analyses now and in the future.
"
"  In this paper we present several values for the next-to-minimal weights of
projective Reed-Muller codes. We work over $\mathbb{F}_q$ with $q \geq 3$ since
in IEEE-IT 62(11) p. 6300-6303 (2016) we have determined the complete values
for the next-to-minimal weights of binary projective Reed-Muller codes. As in
loc. cit. here we also find examples of codewords with next-to-minimal weight
whose set of zeros is not in a hyperplane arrangement.
"
"  The AliEn (ALICE Environment) file catalogue is a global unique namespace
providing mapping between a UNIX-like logical name structure and the
corresponding physical files distributed over 80 storage elements worldwide.
Powerful search tools and hierarchical metadata information are integral parts
of the system and are used by the Grid jobs as well as local users to store and
access all files on the Grid storage elements. The catalogue has been in
production since 2005 and over the past 11 years has grown to more than 2
billion logical file names. The backend is a set of distributed relational
databases, ensuring smooth growth and fast access. Due to the anticipated fast
future growth, we are looking for ways to enhance the performance and
scalability by simplifying the catalogue schema while keeping the functionality
intact. We investigated different backend solutions, such as distributed key
value stores, as replacement for the relational database. This contribution
covers the architectural changes in the system, together with the technology
evaluation, benchmark results and conclusions.
"
"  In variable or graph selection problems, finding a right-sized model or
controlling the number of false positives is notoriously difficult. Recently, a
meta-algorithm called Stability Selection was proposed that can provide
reliable finite-sample control of the number of false positives. Its benefits
were demonstrated when used in conjunction with the lasso and orthogonal
matching pursuit algorithms.
In this paper, we investigate the applicability of stability selection to
structured selection algorithms: the group lasso and the structured
input-output lasso. We find that using stability selection often increases the
power of both algorithms, but that the presence of complex structure reduces
the reliability of error control under stability selection. We give strategies
for setting tuning parameters to obtain a good model size under stability
selection, and highlight its strengths and weaknesses compared to competing
methods screen and clean and cross-validation. We give guidelines about when to
use which error control method.
"
"  The aggregation of many independent estimates can outperform the most
accurate individual judgment. This centenarian finding, popularly known as the
wisdom of crowds, has been applied to problems ranging from the diagnosis of
cancer to financial forecasting. It is widely believed that social influence
undermines collective wisdom by reducing the diversity of opinions within the
crowd. Here, we show that if a large crowd is structured in small independent
groups, deliberation and social influence within groups improve the crowd's
collective accuracy. We asked a live crowd (N=5180) to respond to
general-knowledge questions (e.g., what is the height of the Eiffel Tower?).
Participants first answered individually, then deliberated and made consensus
decisions in groups of five, and finally provided revised individual estimates.
We found that averaging consensus decisions was substantially more accurate
than aggregating the initial independent opinions. Remarkably, combining as few
as four consensus choices outperformed the wisdom of thousands of individuals.
"
"  We address the problem of verifying the satisfiability of Constrained Horn
Clauses (CHCs) based on theories of inductively defined data structures, such
as lists and trees. We propose a transformation technique whose objective is
the removal of these data structures from CHCs, hence reducing their
satisfiability to a satisfiability problem for CHCs on integers and booleans.
We propose a transformation algorithm and identify a class of clauses where it
always succeeds. We also consider an extension of that algorithm, which
combines clause transformation with reasoning on integer constraints. Via an
experimental evaluation we show that our technique greatly improves the
effectiveness of applying the Z3 solver to CHCs. We also show that our
verification technique based on CHC transformation followed by CHC solving, is
competitive with respect to CHC solvers extended with induction. This paper is
under consideration for acceptance in TPLP.
"
"  We investigate the power of non-determinism in purely functional programming
languages with higher-order types. Specifically, we consider cons-free programs
of varying data orders, equipped with explicit non-deterministic choice.
Cons-freeness roughly means that data constructors cannot occur in function
bodies and all manipulation of storage space thus has to happen indirectly
using the call stack.
While cons-free programs have previously been used by several authors to
characterise complexity classes, the work on non-deterministic programs has
almost exclusively considered programs of data order 0. Previous work has shown
that adding explicit non-determinism to cons-free programs taking data of order
0 does not increase expressivity; we prove that this - dramatically - is not
the case for higher data orders: adding non-determinism to programs with data
order at least 1 allows for a characterisation of the entire class of
elementary-time decidable sets.
Finally we show how, even with non-deterministic choice, the original
hierarchy of characterisations is restored by imposing different restrictions.
"
"  The paper presents a topology optimization approach that designs an optimal
structure, called a self-supporting structure, which is ready to be fabricated
via additive manufacturing without the usage of additional support structures.
Such supports in general have to be created during the fabricating process so
that the primary object can be manufactured layer by layer without collapse,
which is very time-consuming and waste of material.
The proposed approach resolves this problem by formulating the
self-supporting requirements as a novel explicit quadratic continuous
constraint in the topology optimization problem, or specifically, requiring the
number of unsupported elements (in terms of the sum of squares of their
densities) to be zero. Benefiting form such novel formulations, computing
sensitivity of the self-supporting constraint with respect to the design
density is straightforward, which otherwise would require lots of research
efforts in general topology optimization studies. The derived sensitivity for
each element is only linearly dependent on its sole density, which, different
from previous layer-based sensitivities, consequently allows for a parallel
implementation and possible higher convergence rate. In addition, a discrete
convolution operator is also designed to detect the unsupported elements as
involved in each step of optimization iteration, and improves the detection
process 100 times as compared with simply enumerating these elements. The
approach works for cases of general overhang angle, or general domain, and
produces an optimized structures, and their associated optimal compliance, very
close to that of the reference structure obtained without considering the
self-supporting constraint, as demonstrated by extensive 2D and 3D benchmark
examples.
"
"  Reward shaping is one of the most effective methods to tackle the crucial yet
challenging problem of credit assignment in Reinforcement Learning (RL).
However, designing shaping functions usually requires much expert knowledge and
hand-engineering, and the difficulties are further exacerbated given multiple
similar tasks to solve. In this paper, we consider reward shaping on a
distribution of tasks, and propose a general meta-learning framework to
automatically learn the efficient reward shaping on newly sampled tasks,
assuming only shared state space but not necessarily action space. We first
derive the theoretically optimal reward shaping in terms of credit assignment
in model-free RL. We then propose a value-based meta-learning algorithm to
extract an effective prior over the optimal reward shaping. The prior can be
applied directly to new tasks, or provably adapted to the task-posterior while
solving the task within few gradient updates. We demonstrate the effectiveness
of our shaping through significantly improved learning efficiency and
interpretable visualizations across various settings, including notably a
successful transfer from DQN to DDPG.
"
"  Nowadays data compressors are applied to many problems of text analysis, but
many such applications are developed outside of the framework of mathematical
statistics. In this paper we overcome this obstacle and show how several
methods of classical mathematical statistics can be developed based on
applications of the data compressors.
"
"  Two-dimensional embeddings remain the dominant approach to visualize high
dimensional data. The choice of embeddings ranges from highly non-linear ones,
which can capture complex relationships but are difficult to interpret
quantitatively, to axis-aligned projections, which are easy to interpret but
are limited to bivariate relationships. Linear project can be considered as a
compromise between complexity and interpretability, as they allow explicit axes
labels, yet provide significantly more degrees of freedom compared to
axis-aligned projections. Nevertheless, interpreting the axes directions, which
are linear combinations often with many non-trivial components, remains
difficult. To address this problem we introduce a structure aware decomposition
of (multiple) linear projections into sparse sets of axis aligned projections,
which jointly capture all information of the original linear ones. In
particular, we use tools from Dempster-Shafer theory to formally define how
relevant a given axis aligned project is to explain the neighborhood relations
displayed in some linear projection. Furthermore, we introduce a new approach
to discover a diverse set of high quality linear projections and show that in
practice the information of $k$ linear projections is often jointly encoded in
$\sim k$ axis aligned plots. We have integrated these ideas into an interactive
visualization system that allows users to jointly browse both linear
projections and their axis aligned representatives. Using a number of case
studies we show how the resulting plots lead to more intuitive visualizations
and new insight.
"
"  Extensive cooperation among unrelated individuals is unique to humans, who
often sacrifice personal benefits for the common good and work together to
achieve what they are unable to execute alone. The evolutionary success of our
species is indeed due, to a large degree, to our unparalleled other-regarding
abilities. Yet, a comprehensive understanding of human cooperation remains a
formidable challenge. Recent research in social science indicates that it is
important to focus on the collective behavior that emerges as the result of the
interactions among individuals, groups, and even societies. Non-equilibrium
statistical physics, in particular Monte Carlo methods and the theory of
collective behavior of interacting particles near phase transition points, has
proven to be very valuable for understanding counterintuitive evolutionary
outcomes. By studying models of human cooperation as classical spin models, a
physicist can draw on familiar settings from statistical physics. However,
unlike pairwise interactions among particles that typically govern solid-state
physics systems, interactions among humans often involve group interactions,
and they also involve a larger number of possible states even for the most
simplified description of reality. The complexity of solutions therefore often
surpasses that observed in physical systems. Here we review experimental and
theoretical research that advances our understanding of human cooperation,
focusing on spatial pattern formation, on the spatiotemporal dynamics of
observed solutions, and on self-organization that may either promote or hinder
socially favorable states.
"
"  We study Bayesian hypernetworks: a framework for approximate Bayesian
inference in neural networks. A Bayesian hypernetwork $\h$ is a neural network
which learns to transform a simple noise distribution, $p(\vec\epsilon) =
\N(\vec 0,\mat I)$, to a distribution $q(\pp) := q(h(\vec\epsilon))$ over the
parameters $\pp$ of another neural network (the ""primary network"")\@. We train
$q$ with variational inference, using an invertible $\h$ to enable efficient
estimation of the variational lower bound on the posterior $p(\pp | \D)$ via
sampling. In contrast to most methods for Bayesian deep learning, Bayesian
hypernets can represent a complex multimodal approximate posterior with
correlations between parameters, while enabling cheap iid sampling of~$q(\pp)$.
In practice, Bayesian hypernets can provide a better defense against
adversarial examples than dropout, and also exhibit competitive performance on
a suite of tasks which evaluate model uncertainty, including regularization,
active learning, and anomaly detection.
"
"  One of the most challenging tasks when adopting Bayesian Networks (BNs) is
the one of learning their structure from data. This task is complicated by the
huge search space of possible solutions, and by the fact that the problem is
NP-hard. Hence, full enumeration of all the possible solutions is not always
feasible and approximations are often required. However, to the best of our
knowledge, a quantitative analysis of the performance and characteristics of
the different heuristics to solve this problem has never been done before.
For this reason, in this work, we provide a detailed comparison of many
different state-of-the-arts methods for structural learning on simulated data
considering both BNs with discrete and continuous variables, and with different
rates of noise in the data. In particular, we investigate the performance of
different widespread scores and algorithmic approaches proposed for the
inference and the statistical pitfalls within them.
"
"  Forwarding data by name has been assumed to be a necessary aspect of an
information-centric redesign of the current Internet architecture that makes
content access, dissemination, and storage more efficient. The Named Data
Networking (NDN) and Content-Centric Networking (CCNx) architectures are the
leading examples of such an approach. However, forwarding data by name incurs
storage and communication complexities that are orders of magnitude larger than
solutions based on forwarding data using addresses. Furthermore, the specific
algorithms used in NDN and CCNx have been shown to have a number of
limitations. The Addressable Data Networking (ADN) architecture is introduced
as an alternative to NDN and CCNx. ADN is particularly attractive for
large-scale deployments of the Internet of Things (IoT), because it requires
far less storage and processing in relaying nodes than NDN. ADN allows things
and data to be denoted by names, just like NDN and CCNx do. However, instead of
replacing the waist of the Internet with named-data forwarding, ADN uses an
address-based forwarding plane and introduces an information plane that
seamlessly maps names to addresses without the involvement of end-user
applications. Simulation results illustrate the order of magnitude savings in
complexity that can be attained with ADN compared to NDN.
"
"  Delay Tolerant Networking (DTN) is an approach to networking which handles
network disruptions and high delays that may occur in many kinds of
communication networks. The major reasons for high delay include partial
connectivity of networks as can be seen in many types of ad hoc wireless
networks with frequent network partitions, long propagation time as experienced
in inter-planetary and deep space networks, and frequent link disruptions due
to the mobility of nodes as observed in terrestrial wireless network
environments. Experimenting network architectures, protocols, and mobility
models in such real-world scenarios is difficult due to the complexities
involved in the network environment. Therefore, in this document, we present
the documentation of an Urban Delay Tolerant Network Simulator (UDTNSim)
version 0.1, capable of simulating urban road network environments with DTN
characteristics including mobility models and routing protocols. The mobility
models included in this version of UDTNSim are (i) Stationary Movement, (ii)
Simple Random Movement, (iii) Path Type Based Movememt, (iv) Path Memory Based
Movement, (v) Path Type with Restricted Movement, and (vi) Path Type with Wait
Movement. In addition to mobility models, we also provide three routing and
data hand-off protocols: (i) Epidemic Routing, (ii) Superior Only Handoff, and
(iii) Superior Peer Handoff. UDTNSim v0.1 is designed using object-oriented
programming approach in order to provide flexibility in addition of new
features to the DTN environment. UDTNSim v0.1 is distributed as an open source
simulator for the use of the research community.
"
"  In multi-object tracking applications, model parameter tuning is a
prerequisite for reliable performance. In particular, it is difficult to know
statistics of false measurements due to various sensing conditions and changes
in the field of views. In this paper we are interested in designing a
multi-object tracking algorithm that handles unknown false measurement rate.
Recently proposed robust multi-Bernoulli filter is employed for clutter
estimation while generalized labeled multi-Bernoulli filter is considered for
target tracking. Performance evaluation with real videos demonstrates the
effectiveness of the tracking algorithm for real-world scenarios.
"
"  Distributed network optimization has been studied for well over a decade.
However, we still do not have a good idea of how to design schemes that can
simultaneously provide good performance across the dimensions of utility
optimality, convergence speed, and delay. To address these challenges, in this
paper, we propose a new algorithmic framework with all these metrics
approaching optimality. The salient features of our new algorithm are
three-fold: (i) fast convergence: it converges with only $O(\log(1/\epsilon))$
iterations that is the fastest speed among all the existing algorithms; (ii)
low delay: it guarantees optimal utility with finite queue length; (iii) simple
implementation: the control variables of this algorithm are based on virtual
queues that do not require maintaining per-flow information. The new technique
builds on a kind of inexact Uzawa method in the Alternating Directional Method
of Multiplier, and provides a new theoretical path to prove global and linear
convergence rate of such a method without requiring the full rank assumption of
the constraint matrix.
"
"  To obtain uncertainty estimates with real-world Bayesian deep learning
models, practical inference approximations are needed. Dropout variational
inference (VI) for example has been used for machine vision and medical
applications, but VI can severely underestimates model uncertainty.
Alpha-divergences are alternative divergences to VI's KL objective, which are
able to avoid VI's uncertainty underestimation. But these are hard to use in
practice: existing techniques can only use Gaussian approximating
distributions, and require existing models to be changed radically, thus are of
limited use for practitioners. We propose a re-parametrisation of the
alpha-divergence objectives, deriving a simple inference technique which,
together with dropout, can be easily implemented with existing models by simply
changing the loss of the model. We demonstrate improved uncertainty estimates
and accuracy compared to VI in dropout networks. We study our model's epistemic
uncertainty far away from the data using adversarial images, showing that these
can be distinguished from non-adversarial images by examining our model's
uncertainty.
"
"  This paper proposes a novel representation of decomposable graphs based on
semi-latent tree-dependent bipartite graphs. The novel representation has two
main benefits. First, it enables a form of sub-clustering within maximal
cliques of the graph, adding informational richness to the general use of
decomposable graphs that could be harnessed in applications with behavioural
type of data. Second, it allows for a new node-driven Markov chain Monte Carlo
sampler of decomposable graphs that can easily parallelize and scale. The
proposed sampler also benefits from the computational efficiency of
junction-tree-based samplers of decomposable graphs.
"
"  Robotic systems, working together as a team, are becoming valuable players in
different real-world applications, from disaster response to warehouse
fulfillment services. Centralized solutions for coordinating multi-robot teams
often suffer from poor scalability and vulnerability to communication
disruptions. This paper develops a decentralized multi-agent task allocation
(Dec-MATA) algorithm for multi-robot applications. The task planning problem is
posed as a maximum-weighted matching of a bipartite graph, the solution of
which using the blossom algorithm allows each robot to autonomously identify
the optimal sequence of tasks it should undertake. The graph weights are
determined based on a soft clustering process, which also plays a problem
decomposition role seeking to reduce the complexity of the individual-agents'
task assignment problems. To evaluate the new Dec-MATA algorithm, a series of
case studies (of varying complexity) are performed, with tasks being
distributed randomly over an observable 2D environment. A centralized approach,
based on a state-of-the-art MILP formulation of the multi-Traveling Salesman
problem is used for comparative analysis. While getting within 7-28% of the
optimal cost obtained by the centralized algorithm, the Dec-MATA algorithm is
found to be 1-3 orders of magnitude faster and minimally sensitive to
task-to-robot ratios, unlike the centralized algorithm.
"
"  Model precision in a classification task is highly dependent on the feature
space that is used to train the model. Moreover, whether the features are
sequential or static will dictate which classification method can be applied as
most of the machine learning algorithms are designed to deal with either one or
another type of data. In real-life scenarios, however, it is often the case
that both static and dynamic features are present, or can be extracted from the
data. In this work, we demonstrate how generative models such as Hidden Markov
Models (HMM) and Long Short-Term Memory (LSTM) artificial neural networks can
be used to extract temporal information from the dynamic data. We explore how
the extracted information can be combined with the static features in order to
improve the classification performance. We evaluate the existing techniques and
suggest a hybrid approach, which outperforms other methods on several public
datasets.
"
"  Our goal is to improve variance reducing stochastic methods through better
control variates. We first propose a modification of SVRG which uses the
Hessian to track gradients over time, rather than to recondition, increasing
the correlation of the control variates and leading to faster theoretical
convergence close to the optimum. We then propose accurate and computationally
efficient approximations to the Hessian, both using a diagonal and a low-rank
matrix. Finally, we demonstrate the effectiveness of our method on a wide range
of problems.
"
"  We introduce a hybridizable discontinuous Galerkin method for the
incompressible Navier--Stokes equations for which the approximate velocity
field is pointwise divergence-free. The method builds on the method presented
by Labeur and Wells [SIAM J. Sci. Comput., vol. 34 (2012), pp. A889--A913]. We
show that with modifications of the function spaces in the method of Labeur and
Wells it is possible to formulate a simple method with pointwise
divergence-free velocity fields which is momentum conserving, energy stable,
and pressure-robust. Theoretical results are supported by two- and
three-dimensional numerical examples and for different orders of polynomial
approximation.
"
"  Artificial neural networks (ANNs) have gained a well-deserved popularity
among machine learning tools upon their recent successful applications in
image- and sound processing and classification problems. ANNs have also been
applied for predicting the family or function of a protein, knowing its residue
sequence. Here we present two new ANNs with multi-label classification ability,
showing impressive accuracy when classifying protein sequences into 698 UniProt
families (AUC=99.99%) and 983 Gene Ontology classes (AUC=99.45%).
"
"  Connectionist Temporal Classification has recently attracted a lot of
interest as it offers an elegant approach to building acoustic models (AMs) for
speech recognition. The CTC loss function maps an input sequence of observable
feature vectors to an output sequence of symbols. Output symbols are
conditionally independent of each other under CTC loss, so a language model
(LM) can be incorporated conveniently during decoding, retaining the
traditional separation of acoustic and linguistic components in ASR. For fixed
vocabularies, Weighted Finite State Transducers provide a strong baseline for
efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs
provide a straight forward solution for open vocabulary speech recognition and
all-neural models, and can be decoded with beam search. Finally,
sequence-to-sequence models can be used to translate a sequence of individual
sounds into a word string. We compare the performance of these three
approaches, and analyze their error patterns, which provides insightful
guidance for future research and development in this important area.
"
"  This paper presents a novel method to describe battery degradation. We use
the concept of degradation maps to model the incremental charge capacity loss
as a function of discrete battery control actions and state of charge. The maps
can be scaled to represent any battery system in size and power. Their convex
piece-wise affine representations allow for tractable optimal control
formulations and can be used in power system simulations to incorporate battery
degradation. The map parameters for different battery technologies are
published making them an useful basis to benchmark different battery
technologies in case studies.
"
"  In this note we propose a method based on artificial neural network to study
the transition between states governed by stochastic processes. In particular,
we aim for numerical schemes for the committor function, the central object of
transition path theory, which satisfies a high-dimensional Fokker-Planck
equation. By working with the variational formulation of such partial
differential equation and parameterizing the committor function in terms of a
neural network, approximations can be obtained via optimizing the neural
network weights using stochastic algorithms. The numerical examples show that
moderate accuracy can be achieved for high-dimensional problems.
"
"  Let $G$ be a finite simple graph. For $X \subset V(G)$, the difference of
$X$, $d(X) := |X| - |N (X)|$ where $N(X)$ is the neighborhood of $X$ and $\max
\, \{d(X):X\subset V(G)\}$ is called the critical difference of $G$. $X$ is
called a critical set if $d(X)$ equals the critical difference and ker$(G)$ is
the intersection of all critical sets. It is known that ker$(G)$ is an
independent (vertex) set of $G$. diadem$(G)$ is the union of all critical
independent sets. An independent set $S$ is an inclusion minimal set with $d(S)
> 0$ if no proper subset of $S$ has positive difference.
A graph $G$ is called König-Egerváry if the sum of its independence
number ($\alpha (G)$) and matching number ($\mu (G)$) equals $|V(G)|$. It is
known that bipartite graphs are König-Egerváry.
In this paper, we study independent sets with positive difference for which
every proper subset has a smaller difference and prove a result conjectured by
Levit and Mandrescu in 2013. The conjecture states that for any graph, the
number of inclusion minimal sets $S$ with $d(S) > 0$ is at least the critical
difference of the graph. We also give a short proof of the inequality
$|$ker$(G)| + |$diadem$(G)| \le 2\alpha (G)$ (proved by Short in 2016).
A characterization of unicyclic non-König-Egerváry graphs is also
presented and a conjecture which states that for such a graph $G$, the critical
difference equals $\alpha (G) - \mu (G)$, is proved.
We also make an observation about ker$G)$ using Edmonds-Gallai Structure
Theorem as a concluding remark.
"
"  In the ICS, WUT a platform for simulation of cooperation of physical and
virtual mobile agents is under development. The paper describes the motivation
of the research, an organization of the platform, a model of agent, and the
principles of design of the platform. Several experimental simulations are
briefly described.
"
"  Class labels have been empirically shown useful in improving the sample
quality of generative adversarial nets (GANs). In this paper, we mathematically
study the properties of the current variants of GANs that make use of class
label information. With class aware gradient and cross-entropy decomposition,
we reveal how class labels and associated losses influence GAN's training.
Based on that, we propose Activation Maximization Generative Adversarial
Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been
conducted to validate our analysis and evaluate the effectiveness of our
solution, where AM-GAN outperforms other strong baselines and achieves
state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we
demonstrate that, with the Inception ImageNet classifier, Inception Score
mainly tracks the diversity of the generator, and there is, however, no
reliable evidence that it can reflect the true sample quality. We thus propose
a new metric, called AM Score, to provide a more accurate estimation of the
sample quality. Our proposed model also outperforms the baseline methods in the
new metric.
"
"  Structural and topological information play a key role in modeling flow and
transport through fractured rock in the subsurface. Discrete fracture network
(DFN) computational suites such as dfnWorks are designed to simulate flow and
transport in such porous media. Flow and transport calculations reveal that a
small backbone of fractures exists, where most flow and transport occurs.
Restricting the flowing fracture network to this backbone provides a
significant reduction in the network's effective size. However, the particle
tracking simulations needed to determine the reduction are computationally
intensive. Such methods may be impractical for large systems or for robust
uncertainty quantification of fracture networks, where thousands of forward
simulations are needed to bound system behavior.
In this paper, we develop an alternative network reduction approach to
characterizing transport in DFNs, by combining graph theoretical and machine
learning methods. We consider a graph representation where nodes signify
fractures and edges denote their intersections. Using random forest and support
vector machines, we rapidly identify a subnetwork that captures the flow
patterns of the full DFN, based primarily on node centrality features in the
graph. Our supervised learning techniques train on particle-tracking backbone
paths found by dfnWorks, but run in negligible time compared to those
simulations. We find that our predictions can reduce the network to
approximately 20% of its original size, while still generating breakthrough
curves consistent with those of the original network.
"
"  Taking an image and question as the input of our method, it can output the
text-based answer of the query question about the given image, so called Visual
Question Answering (VQA). There are two main modules in our algorithm. Given a
natural language question about an image, the first module takes the question
as input and then outputs the basic questions of the main given question. The
second module takes the main question, image and these basic questions as input
and then outputs the text-based answer of the main question. We formulate the
basic questions generation problem as a LASSO optimization problem, and also
propose a criterion about how to exploit these basic questions to help answer
main question. Our method is evaluated on the challenging VQA dataset and
yields state-of-the-art accuracy, 60.34% in open-ended task.
"
"  In this paper, we examine the physical layer security for cooperative
wireless networks with multiple intermediate nodes, where the
decode-and-forward (DF) protocol is considered. We propose a new joint relay
and jammer selection (JRJS) scheme for protecting wireless communications
against eavesdropping, where an intermediate node is selected as the relay for
the sake of forwarding the source signal to the destination and meanwhile, the
remaining intermediate nodes are employed to act as friendly jammers which
broadcast the artificial noise for disturbing the eavesdropper. We further
investigate the power allocation among the source, relay and friendly jammers
for maximizing the secrecy rate of proposed JRJS scheme and derive a
closed-form sub-optimal solution. Specificially, all the intermediate nodes
which successfully decode the source signal are considered as relay candidates.
For each candidate, we derive the sub-optimal closed-form power allocation
solution and obtain the secrecy rate result of the corresponding JRJS scheme.
Then, the candidate which is capable of achieving the highest secrecy rate is
selected as the relay. Two assumptions about the channel state information
(CSI), namely the full CSI (FCSI) and partial CSI (PCSI), are considered.
Simulation results show that the proposed JRJS scheme outperforms the
conventional pure relay selection, pure jamming and GSVD based beamforming
schemes in terms of secrecy rate. Additionally, the proposed FCSI based power
allocation (FCSI-PA) and PCSI based power allocation (PCSI-PA) schemes both
achieve higher secrecy rates than the equal power allocation (EPA) scheme.
"
"  Traditionally, Blind Speech Separation techniques are computationally
expensive as they update the demixing matrix at every time frame index, making
them impractical to use in many Real-Time applications. In this paper, a robust
data-driven two-microphone sound source localization method is used as a
criterion to reduce the computational complexity of the Independent Vector
Analysis (IVA) Blind Speech Separation (BSS) method. IVA is used to separate
convolutedly mixed speech and noise sources. The practical feasibility of the
proposed method is proved by implementing it on a smartphone device to separate
speech and noise in Real-World scenarios for Hearing-Aid applications. The
experimental results with objective and subjective tests reveal the practical
usability of the developed method in many real-world applications.
"
"  Most of the existing characterizations of the integral input-to-state
stability (iISS) property are not valid for time-varying or switched systems in
cases where converse Lyapunov theorems for stability are not available. This
note provides a characterization that is valid for switched and time-varying
systems, and shows that natural extensions of some of the existing
characterizations result in only sufficient but not necessary conditions. The
results provided also pinpoint suitable iISS gains and relate these to supply
functions and bounds on the function defining the system dynamics.
"
"  This work addresses the problem of segmentation in time series data with
respect to a statistical parameter of interest in Bayesian models. It is common
to assume that the parameters are distinct within each segment. As such, many
Bayesian change point detection models do not exploit the segment parameter
patterns, which can improve performance. This work proposes a Bayesian
mean-shift change point detection algorithm that makes use of repetition in
segment parameters, by introducing segment class labels that utilise a
Dirichlet process prior. The performance of the proposed approach was assessed
on both synthetic and real world data, highlighting the enhanced performance
when using parameter labelling.
"
"  We propose in this article a M/G/c/c state dependent queuing model for road
traffic flow. The model is based on finite capacity queuing theory which
captures the stationary density-flow relationships. It is also inspired from
the deterministic Godunov scheme for the road traffic simulation. We first
present a reformulation of the existing linear case of M/G/c/c state dependent
model, in order to use flow rather than speed variables. We then extend this
model in order to consider upstream traffic demand and downstream traffic
supply. After that, we propose the model for two road sections in tandem where
both sections influence each other. In order to deal with this mutual
dependence, we solve an implicit system given by an algebraic equation.
Finally, we derive some performance measures (throughput and expected travel
time). A comparison with results predicted by the M/G/c/c state dependent
queuing networks shows that the model we propose here captures really the
dynamics of the road traffic.
"
"  A GelSight sensor uses an elastomeric slab covered with a reflective membrane
to measure tactile signals. It measures the 3D geometry and contact force
information with high spacial resolution, and successfully helped many
challenging robot tasks. A previous sensor, based on a semi-specular membrane,
produces high resolution but with limited geometry accuracy. In this paper, we
describe a new design of GelSight for robot gripper, using a Lambertian
membrane and new illumination system, which gives greatly improved geometric
accuracy while retaining the compact size. We demonstrate its use in measuring
surface normals and reconstructing height maps using photometric stereo. We
also use it for the task of slip detection, using a combination of information
about relative motions on the membrane surface and the shear distortions. Using
a robotic arm and a set of 37 everyday objects with varied properties, we find
that the sensor can detect translational and rotational slip in general cases,
and can be used to improve the stability of the grasp.
"
"  In this paper, we build upon previous work on designing informative and
efficient Exploratory Landscape Analysis features for characterizing problems'
landscapes and show their effectiveness in automatically constructing algorithm
selection models in continuous black-box optimization problems. Focussing on
algorithm performance results of the COCO platform of several years, we
construct a representative set of high-performing complementary solvers and
present an algorithm selection model that - compared to the portfolio's single
best solver - on average requires less than half of the resources for solving a
given problem. Therefore, there is a huge gain in efficiency compared to
classical ensemble methods combined with an increased insight into problem
characteristics and algorithm properties by using informative features. Acting
on the assumption that the function set of the Black-Box Optimization Benchmark
is representative enough for practical applications the model allows for
selecting the best suited optimization algorithm within the considered set for
unseen problems prior to the optimization itself based on a small sample of
function evaluations. Note that such a sample can even be reused for the
initial population of an evolutionary (optimization) algorithm so that even the
feature costs become negligible.
"
"  Vehicle-to-vehicle communications can change the driving behavior of drivers
significantly by providing them rich information on downstream traffic flow
conditions. This study seeks to model the varying car-following behaviors
involving connected vehicles and human-driving vehicles in mixed traffic flow.
A revised car-following model is developed using an intelligent driver model
(IDM) to capture drivers' perceptions of their preceding traffic conditions
through vehicle-to-vehicle communications. Stability analysis of the mixed
traffic flow is conducted for a specific case. Numerical results show that the
stable region is apparently enlarged compared with the IDM.
"
"  Advances in virtual reality have generated substantial interest in accurately
reproducing and storing spatial audio in the higher order ambisonics (HOA)
representation, given its rendering flexibility. Recent standardization for HOA
compression adopted a framework wherein HOA data are decomposed into principal
components that are then encoded by standard audio coding, i.e., frequency
domain quantization and entropy coding to exploit psychoacoustic redundancy. A
noted shortcoming of this approach is the occasional mismatch in principal
components across blocks, and the resulting suboptimal transitions in the data
fed to the audio coder. Instead, we propose a framework where singular value
decomposition (SVD) is performed after transformation to the frequency domain
via the modified discrete cosine transform (MDCT). This framework not only
ensures smooth transition across blocks, but also enables frequency dependent
SVD for better energy compaction. Moreover, we introduce a novel noise
substitution technique to compensate for suppressed ambient energy in discarded
higher order ambisonics channels, which significantly enhances the perceptual
quality of the reconstructed HOA signal. Objective and subjective evaluation
results provide evidence for the effectiveness of the proposed framework in
terms of both higher compression gains and better perceptual quality, compared
to existing methods.
"
"  Cloud Computing is a new era of remote computing / Internet based computing
where one can access their personal resources easily from any computer through
Internet. Cloud delivers computing as a utility as it is available to the cloud
consumers on demand. It is a simple pay-per-use consumer-provider service
model. It contains large number of shared resources. So Resource Management is
always a major issue in cloud computing like any other computing paradigm. Due
to the availability of finite resources it is very challenging for cloud
providers to provide all the requested resources. From the cloud providers
perspective cloud resources must be allocated in a fair and efficient manner.
Research Survey is not available from the perspective of resource management as
a process in cloud computing. So this research paper provides a detailed
sequential view / steps on resource management in cloud computing. Firstly this
research paper classifies various resources in cloud computing. It also gives
taxonomy on resource management in cloud computing through which one can do
further research. Lastly comparisons on various resource management algorithms
has been presented.
"
"  The astrophysics community uses different tools for computational tasks such
as complex systems simulations, radiative transfer calculations or big data.
Programming languages like Fortran, C or C++ are commonly present in these
tools and, generally, the language choice was made based on the need for
performance. However, this comes at a cost: safety. For instance, a common
source of error is the access to invalid memory regions, which produces random
execution behaviors and affects the scientific interpretation of the results.
In 2015, Mozilla Research released the first stable version of a new
programming language named Rust. Many features make this new language
attractive for the scientific community, it is open source and it guarantees
memory safety while offering zero-cost abstraction.
We explore the advantages and drawbacks of Rust for astrophysics by
re-implementing the fundamental parts of Mercury-T, a Fortran code that
simulates the dynamical and tidal evolution of multi-planet systems.
"
"  This volume of EPTCS contains the proceedings of the Fifth Workshop on Proof
Exchange for Theorem Proving (PxTP 2017), held on September 23-24, 2017 as part
of the Tableaux, FroCoS and ITP conferences in Brasilia, Brazil. The PxTP
workshop series brings together researchers working on various aspects of
communication, integration, and cooperation between reasoning systems and
formalisms, with a special focus on proofs. The progress in computer-aided
reasoning, both automated and interactive, during the past decades, made it
possible to build deduction tools that are increasingly more applicable to a
wider range of problems and are able to tackle larger problems progressively
faster. In recent years, cooperation between such tools in larger systems has
demonstrated the potential to reduce the amount of manual intervention.
Cooperation between reasoning systems relies on availability of theoretical
formalisms and practical tools to exchange problems, proofs, and models. The
PxTP workshop series strives to encourage such cooperation by inviting
contributions on all aspects of cooperation between reasoning tools, whether
automatic or interactive.
"
"  Software as a Service cloud computing model favorites the Multi-Tenancy as a
key factor to exploit economies of scale. However Multi-Tenancy present several
disadvantages. Therein, our approach comes to assign instances to multi-tenants
with an optimal solution while ensuring more economies of scale and avoiding
tenants hesitation to share resources. The present paper present the
architecture of our user-aware multi-tenancy SaaS approach based on the use of
rich-variant components. The proposed approach seek to model services
functional customization as well as automation of computing the optimal
distribution of instances by tenants. The proposed model takes into
consideration tenants functional requirements and tenants deployment
requirements to deduce an optimal distribution using essentially a specific
variability engine and a graph-based execution framework.
"
"  Efficiently exploiting GPUs is increasingly essential in scientific
computing, as many current and upcoming supercomputers are built using them. To
facilitate this, there are a number of programming approaches, such as CUDA,
OpenACC and OpenMP 4, supporting different programming languages (mainly C/C++
and Fortran). There are also several compiler suites (clang, nvcc, PGI, XL)
each supporting different combinations of languages. In this study, we take a
detailed look at some of the currently available options, and carry out a
comprehensive analysis and comparison using computational loops and
applications from the domain of unstructured mesh computations. Beyond runtimes
and performance metrics (GB/s), we explore factors that influence performance
such as register counts, occupancy, usage of different memory types,
instruction counts, and algorithmic differences. Results of this work show how
clang's CUDA compiler frequently outperform NVIDIA's nvcc, performance issues
with directive-based approaches on complex kernels, and OpenMP 4 support
maturing in clang and XL; currently around 10% slower than CUDA.
"
"  We study the height of a spanning tree $T$ of a graph $G$ obtained by
starting with a single vertex of $G$ and repeatedly selecting, uniformly at
random, an edge of $G$ with exactly one endpoint in $T$ and adding this edge to
$T$.
"
"  Motion planning is the core problem to solve for developing any application
involving an autonomous mobile robot. The fundamental motion planning problem
involves generating a trajectory for a robot for point-to-point navigation
while avoiding obstacles. Heuristic-based search algorithms like A* have been
shown to be extremely efficient in solving such planning problems. Recently,
there has been an increased interest in specifying complex motion plans using
temporal logic. In the state-of-the-art algorithm, the temporal logic motion
planning problem is reduced to a graph search problem and Dijkstra's shortest
path algorithm is used to compute the optimal trajectory satisfying the
specification.
The A* algorithm when used with a proper heuristic for the distance from the
destination can generate an optimal path in a graph efficiently. The primary
challenge for using A* algorithm in temporal logic path planning is that there
is no notion of a single destination state for the robot. In this thesis, we
present a novel motion planning algorithm T* that uses the A* search procedure
in temporal logic path planning \emph{opportunistically} to generate an optimal
trajectory satisfying a temporal logic query. Our experimental results
demonstrate that T* achieves an order of magnitude improvement over the
state-of-the-art algorithm to solve many temporal logic motion planning
problems.
"
"  Learning-based approaches for robotic grasping using visual sensors typically
require collecting a large size dataset, either manually labeled or by many
trial and errors of a robotic manipulator in the real or simulated world. We
propose a simpler learning-from-demonstration approach that is able to detect
the object to grasp from merely a single demonstration using a convolutional
neural network we call GraspNet. In order to increase robustness and decrease
the training time even further, we leverage data from previous demonstrations
to quickly fine-tune a GrapNet for each new demonstration. We present some
preliminary results on a grasping experiment with the Franka Panda cobot for
which we can train a GraspNet with only hundreds of train iterations.
"
"  List decoding of insertions and deletions in the Levenshtein metric is
considered. The Levenshtein distance between two sequences is the minimum
number of insertions and deletions needed to turn one of the sequences into the
other. In this paper, a Johnson-like upper bound on the maximum list size when
list decoding in the Levenshtein metric is derived. This bound depends only on
the length and minimum Levenshtein distance of the code, the length of the
received word, and the alphabet size. It shows that polynomial-time list
decoding beyond half the Levenshtein distance is possible for many parameters.
Further, we also prove a lower bound on list decoding of deletions with with
the well-known binary Varshamov-Tenengolts (VT) codes which shows that the
maximum list size grows exponentially with the number of deletions. Finally, an
efficient list decoding algorithm for two insertions/deletions with VT codes is
given. This decoder can be modified to a polynomial-time list decoder of any
constant number of insertions/deletions.
"
"  Boltzmann exploration is widely used in reinforcement learning to provide a
trade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et
al., 2017) it has been shown that pure Boltzmann exploration does not perform
well from a regret perspective, even in the simplest setting of stochastic
multi-armed bandit (MAB) problems. In this paper, we show that a simple
modification to Boltzmann exploration, motivated by a variation of the standard
doubling trick, achieves $O(K\log^{1+\alpha} T)$ regret for a stochastic MAB
problem with $K$ arms, where $\alpha>0$ is a parameter of the algorithm. This
improves on the result in (Cesa-Bianchi et al., 2017), where an algorithm
inspired by the Gumbel-softmax trick achieves $O(K\log^2 T)$ regret. We also
show that our algorithm achieves $O(\beta(G) \log^{1+\alpha} T)$ regret in
stochastic MAB problems with graph-structured feedback, without knowledge of
the graph structure, where $\beta(G)$ is the independence number of the
feedback graph. Additionally, we present extensive experimental results on real
datasets and applications for multi-armed bandits with both traditional bandit
feedback and graph-structured feedback. In all cases, our algorithm performs as
well or better than the state-of-the-art.
"
"  Both GPS and WiFi based localization have been exploited in recent years, yet
most researches focus on localizing at home without environment context.
Besides, the near home or workplace area is complex and has little attention in
smart home or IOT. Therefore, after exploring the realistic route in and out of
building, we conducted a time localization system (TLS) based on off-the-shelf
smart phones with WiFi identification. TLS can identify the received signal
strength indication (RSSI) of home and construct radio map of users' time route
without site survey. As designed to service the smart devices in home, TLS
applies the time interval as the distance of positions and as the variables of
WiFi environment to mark time points. Experimental results with real users show
that TLS as a service system for timeline localization achieves a median
accuracy of 70 seconds and is more robust compared with nearest neighbor
localization approach.
"
"  Since the advent of online real estate database companies like Zillow, Trulia
and Redfin, the problem of automatic estimation of market values for houses has
received considerable attention. Several real estate websites provide such
estimates using a proprietary formula. Although these estimates are often close
to the actual sale prices, in some cases they are highly inaccurate. One of the
key factors that affects the value of a house is its interior and exterior
appearance, which is not considered in calculating automatic value estimates.
In this paper, we evaluate the impact of visual characteristics of a house on
its market value. Using deep convolutional neural networks on a large dataset
of photos of home interiors and exteriors, we develop a method for estimating
the luxury level of real estate photos. We also develop a novel framework for
automated value assessment using the above photos in addition to home
characteristics including size, offered price and number of bedrooms. Finally,
by applying our proposed method for price estimation to a new dataset of real
estate photos and metadata, we show that it outperforms Zillow's estimates.
"
"  Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
"
"  In the Internet of Things (IoT) community, Wireless Sensor Network (WSN) is a
key technique to enable ubiquitous sensing of environments and provide reliable
services to applications. WSN programs, typically interrupt-driven, implement
the functionalities via the collaboration of Interrupt Procedure Instances
(IPIs, namely executions of interrupt processing logic). However, due to the
complicated concurrency model of WSN programs, the IPIs are interleaved
intricately and the program behaviours are hard to predicate from the source
codes. Thus, to improve the software quality of WSN programs, it is significant
to disentangle the interleaved executions and develop various IPI-based program
analysis techniques, including offline and online ones. As the common
foundation of those techniques, a generic efficient and real-time algorithm to
identify IPIs is urgently desired. However, the existing
instance-identification approach cannot satisfy the desires. In this paper, we
first formally define the concept of IPI. Next, we propose a generic
IPI-identification algorithm, and prove its correctness, real-time and
efficiency. We also conduct comparison experiments to illustrate that our
algorithm is more efficient than the existing one in terms of both time and
space. As the theoretical analyses and empirical studies exhibit, our algorithm
provides the groundwork for IPI-based analyses of WSN programs in IoT
environment.
"
"  We have implemented an optimization that specializes type-generic array
accesses after inlining of polymorphic functions in the native-code OCaml
compiler. Polymorphic array operations (read and write) in OCaml require
runtime type dispatch because of ad hoc memory representations of integer and
float arrays. It cannot be removed even after being monomorphized by inlining
because the intermediate language is mostly untyped. We therefore extended it
with explicit type application like System F (while keeping implicit type
abstraction by means of unique identifiers for type variables). Our
optimization has achieved up to 21% speed-up of numerical programs.
"
"  We obtain the first polynomial-time algorithm for exact tensor completion
that improves over the bound implied by reduction to matrix completion. The
algorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal
components in $\mathbb R^n$ from $r\cdot \tilde O(n^{1.5})$ randomly observed
entries of the tensor. This bound improves over the previous best one of
$r\cdot \tilde O(n^{2})$ by reduction to exact matrix completion. Our bound
also matches the best known results for the easier problem of approximate
tensor completion (Barak & Moitra, 2015).
Our algorithm and analysis extends seminal results for exact matrix
completion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares
method. The main technical challenge is to show that a small number of randomly
chosen monomials are enough to construct a degree-3 polynomial with precisely
planted orthogonal global optima over the sphere and that this fact can be
certified within the sum-of-squares proof system.
"
"  Outlier detection and cluster number estimation is an important issue for
clustering real data. This paper focuses on spectral clustering, a time-tested
clustering method, and reveals its important properties related to outliers.
The highlights of this paper are the following two mathematical observations:
first, spectral clustering's intrinsic property of an outlier cluster
formation, and second, the singularity of an outlier cluster with a valid
cluster number. Based on these observations, we designed a function that
evaluates clustering and outlier detection results. In experiments, we prepared
two scenarios, face clustering in photo album and person re-identification in a
camera network. We confirmed that the proposed method detects outliers and
estimates the number of clusters properly in both problems. Our method
outperforms state-of-the-art methods in both the 128-dimensional sparse space
for face clustering and the 4,096-dimensional non-sparse space for person
re-identification.
"
"  The inference of network topologies from relational data is an important
problem in data analysis. Exemplary applications include the reconstruction of
social ties from data on human interactions, the inference of gene
co-expression networks from DNA microarray data, or the learning of semantic
relationships based on co-occurrences of words in documents. Solving these
problems requires techniques to infer significant links in noisy relational
data. In this short paper, we propose a new statistical modeling framework to
address this challenge. It builds on generalized hypergeometric ensembles, a
class of generative stochastic models that give rise to analytically tractable
probability spaces of directed, multi-edge graphs. We show how this framework
can be used to assess the significance of links in noisy relational data. We
illustrate our method in two data sets capturing spatio-temporal proximity
relations between actors in a social system. The results show that our
analytical framework provides a new approach to infer significant links from
relational data, with interesting perspectives for the mining of data on social
systems.
"
"  An important class of real-world networks have directed edges, and in
addition, some rank ordering on the nodes, for instance the ""popularity"" of
users in online social networks. Yet, nearly all research related to explosive
percolation has been restricted to undirected networks. Furthermore,
information on such rank ordered networks typically flows from higher ranked to
lower ranked individuals, such as follower relations, replies and retweets on
Twitter.
Here we introduce a simple percolation process on an ordered, directed
network where edges are added monotonically with respect to the rank ordering.
We show with a numerical approach that the emergence of a dominant strongly
connected component appears to be discontinuous. Large scale connectivity
occurs at very high density compared with most percolation processes, and this
holds not just for the strongly connected component structure but for the
weakly connected component structure as well. We present analysis with
branching processes which explains this unusual behavior and gives basic
intuition for the underlying mechanisms. We also show that before the emergence
of a dominant strongly connected component, multiple giant strongly connected
components may exist simultaneously. By adding a competitive percolation rule
with a small bias to link uses of similar rank, we show this leads to formation
of two distinct components, one of high ranked users, and one of low ranked
users, with little flow between the two components.
"
"  This paper presents results of topic modeling and network models of topics
using the International Conference on Computational Science corpus, which
contains domain-specific (computational science) papers over sixteen years (a
total of 5695 papers). We discuss topical structures of International
Conference on Computational Science, how these topics evolve over time in
response to the topicality of various problems, technologies and methods, and
how all these topics relate to one another. This analysis illustrates
multidisciplinary research and collaborations among scientific communities, by
constructing static and dynamic networks from the topic modeling results and
the keywords of authors. The results of this study give insights about the past
and future trends of core discussion topics in computational science. We used
the Non-negative Matrix Factorization topic modeling algorithm to discover
topics and labeled and grouped results hierarchically.
"
"  Merging mobile edge computing with the dense deployment of small cell base
stations promises enormous benefits such as a real proximity, ultra-low latency
access to cloud functionalities. However, the envisioned integration creates
many new challenges and one of the most significant is mobility management,
which is becoming a key bottleneck to the overall system performance. Simply
applying existing solutions leads to poor performance due to the highly
overlapped coverage areas of multiple base stations in the proximity of the
user and the co-provisioning of radio access and computing services. In this
paper, we develop a novel user-centric mobility management scheme, leveraging
Lyapunov optimization and multi-armed bandits theories, in order to maximize
the edge computation performance for the user while keeping the user's
communication energy consumption below a constraint. The proposed scheme
effectively handles the uncertainties present at multiple levels in the system
and provides both short-term and long-term performance guarantee. Simulation
results show that our proposed scheme can significantly improve the computation
performance (compared to state of the art) while satisfying the communication
energy constraint.
"
"  Experimental Particle Physics has been at the forefront of analyzing the
worlds largest datasets for decades. The HEP community was the first to develop
suitable software and computing tools for this task. In recent times, new
toolkits and systems collectively called Big Data technologies have emerged to
support the analysis of Petabyte and Exabyte datasets in industry. While the
principles of data analysis in HEP have not changed (filtering and transforming
experiment-specific data formats), these new technologies use different
approaches and promise a fresh look at analysis of very large datasets and
could potentially reduce the time-to-physics with increased interactivity. In
this talk, we present an active LHC Run 2 analysis, searching for dark matter
with the CMS detector, as a testbed for Big Data technologies. We directly
compare the traditional NTuple-based analysis with an equivalent analysis using
Apache Spark on the Hadoop ecosystem and beyond. In both cases, we start the
analysis with the official experiment data formats and produce publication
physics plots. We will discuss advantages and disadvantages of each approach
and give an outlook on further studies needed.
"
"  Guided by critical systems found in nature we develop a novel mechanism
consisting of inhomogeneous polynomial regularisation via which we can induce
scale invariance in deep learning systems. Technically, we map our deep
learning (DL) setup to a genuine field theory, on which we act with the
Renormalisation Group (RG) in momentum space and produce the flow equations of
the couplings; those are translated to constraints and consequently interpreted
as ""critical regularisation"" conditions in the optimiser; the resulting
equations hence prove to be sufficient conditions for - and serve as an elegant
and simple mechanism to induce scale invariance in any deep learning setup.
"
"  We introduce and analyze an extension to the matching problem on a weighted
bipartite graph: Assignment with Type Constraints. The two parts of the graph
are partitioned into subsets called types and blocks; we seek a matching with
the largest sum of weights under the constraint that there is a pre-specified
cap on the number of vertices matched in every type-block pair. Our primary
motivation stems from the public housing program of Singapore, accounting for
over 70% of its residential real estate. To promote ethnic diversity within its
housing projects, Singapore imposes ethnicity quotas: each new housing
development comprises blocks of flats and each ethnicity-based group in the
population must not own more than a certain percentage of flats in a block.
Other domains using similar hard capacity constraints include matching
prospective students to schools or medical residents to hospitals. Limiting
agents' choices for ensuring diversity in this manner naturally entails some
welfare loss. One of our goals is to study the trade-off between diversity and
social welfare in such settings. We first show that, while the classic
assignment program is polynomial-time computable, adding diversity constraints
makes it computationally intractable; however, we identify a
$\tfrac{1}{2}$-approximation algorithm, as well as reasonable assumptions on
the weights that permit poly-time algorithms. Next, we provide two upper bounds
on the price of diversity -- a measure of the loss in welfare incurred by
imposing diversity constraints -- as functions of natural problem parameters.
We conclude the paper with simulations based on publicly available data from
two diversity-constrained allocation problems -- Singapore Public Housing and
Chicago School Choice -- which shed light on how the constrained maximization
as well as lottery-based variants perform in practice.
"
"  Noncritical soft-faults and model deviations are a challenge for Fault
Detection and Diagnosis (FDD) of resident Autonomous Underwater Vehicles
(AUVs). Such systems may have a faster performance degradation due to the
permanent exposure to the marine environment, and constant monitoring of
component conditions is required to ensure their reliability. This works
presents an evaluation of Recurrent Neural Networks (RNNs) for a data-driven
fault detection and diagnosis scheme for underwater thrusters with empirical
data. The nominal behavior of the thruster was modeled using the measured
control input, voltage, rotational speed and current signals. We evaluated the
performance of fault classification using all the measured signals compared to
using the computed residuals from the nominal model as features.
"
"  We study SIS epidemic spreading processes unfolding on a recent
generalisation of the activity-driven modelling framework. In this model of
time-varying networks each node is described by two variables: activity and
attractiveness. The first, describes the propensity to form connections. The
second, defines the propensity to attract them. We derive analytically the
epidemic threshold considering the timescale driving the evolution of contacts
and the contagion as comparable. The solutions are general and hold for any
joint distribution of activity and attractiveness. The theoretical picture is
confirmed via large-scale numerical simulations performed considering
heterogeneous distributions and different correlations between the two
variables. We find that heterogeneous distributions of attractiveness alter the
contagion process. In particular, in case of uncorrelated and positive
correlations between the two variables, heterogeneous attractiveness
facilitates the spreading. On the contrary, negative correlations between
activity and attractiveness hamper the spreading. The results presented
contribute to the understanding of the dynamical properties of time-varying
networks and their effects on contagion phenomena unfolding on their fabric.
"
"  In this paper we describe and evaluate a mixed reality system that aims to
augment users in task guidance applications by combining automated and
unsupervised information collection with minimally invasive video guides. The
result is a self-contained system that we call GlaciAR (Glass-enabled
Contextual Interactions for Augmented Reality), that operates by extracting
contextual interactions from observing users performing actions. GlaciAR is
able to i) automatically determine moments of relevance based on a head motion
attention model, ii) automatically produce video guidance information, iii)
trigger these video guides based on an object detection method, iv) learn
without supervision from observing multiple users and v) operate fully on-board
a current eyewear computer (Google Glass). We describe the components of
GlaciAR together with evaluations on how users are able to use the system to
achieve three tasks. We see this work as a first step toward the development of
systems that aim to scale up the notoriously difficult authoring problem in
guidance systems and where people's natural abilities are enhanced via
minimally invasive visual guidance.
"
"  Benefited from the widely deployed infrastructure, the LTE network has
recently been considered as a promising candidate to support the
vehicle-to-everything (V2X) services. However, with a massive number of devices
accessing the V2X network in the future, the conventional OFDM-based LTE
network faces the congestion issues due to its low efficiency of orthogonal
access, resulting in significant access delay and posing a great challenge
especially to safety-critical applications. The non-orthogonal multiple access
(NOMA) technique has been well recognized as an effective solution for the
future 5G cellular networks to provide broadband communications and massive
connectivity. In this article, we investigate the applicability of NOMA in
supporting cellular V2X services to achieve low latency and high reliability.
Starting with a basic V2X unicast system, a novel NOMA-based scheme is proposed
to tackle the technical hurdles in designing high spectral efficient scheduling
and resource allocation schemes in the ultra dense topology. We then extend it
to a more general V2X broadcasting system. Other NOMA-based extended V2X
applications and some open issues are also discussed.
"
"  Disagreement-based approaches generate multiple classifiers and exploit the
disagreement among them with unlabeled data to improve learning performance.
Co-training is a representative paradigm of them, which trains two classifiers
separately on two sufficient and redundant views; while for the applications
where there is only one view, several successful variants of co-training with
two different classifiers on single-view data instead of two views have been
proposed. For these disagreement-based approaches, there are several important
issues which still are unsolved, in this article we present theoretical
analyses to address these issues, which provides a theoretical foundation of
co-training and disagreement-based approaches.
"
"  What if someone built a ""box"" that applies quantum superposition not just to
quantum bits in the microscopic but also to macroscopic everyday ""objects"",
such as Schrödinger's cat or a human being? If that were possible, and if the
different ""copies"" of a man could exploit quantum interference to synchronize
and collapse into their preferred state, then one (or they?) could in a sense
choose their future, win the lottery, break codes and other security devices,
and become king of the world, or actually of the many-worlds. We set up the
plot-line of a new episode of Black Mirror to reflect on what might await us if
one were able to build such a technology.
"
"  Unmanned aircraft have decreased the cost required to collect remote sensing
imagery, which has enabled researchers to collect high-spatial resolution data
from multiple sensor modalities more frequently and easily. The increase in
data will push the need for semantic segmentation frameworks that are able to
classify non-RGB imagery, but this type of algorithmic development requires an
increase in publicly available benchmark datasets with class labels. In this
paper, we introduce a high-resolution multispectral dataset with image labels.
This new benchmark dataset has been pre-split into training/testing folds in
order to standardize evaluation and continue to push state-of-the-art
classification frameworks for non-RGB imagery.
"
"  In this paper, we present an approach to select a subset of requirement
elicitation technique for an optimum result in the requirement elicitation
process. Our approach consists of three steps. First, we identify various
attribute in three important dimensions namely project, people and the process
of software development that can influence the outcome of an elicitation
process. Second, we construct three p matrix (3PM) separately for each
dimension, that shows a relation between the elicitation techniques and three
dimensions of a software. Third, we provide a mapping criteria and use them in
the selection of a subset of elicitation techniques. We demonstrate the
applicability of the proposed approach using case studies to evaluate and
provide the contextual knowledge of selecting requirement elicitation
technique.
"
"  Transfer learning is a popular practice in deep neural networks, but
fine-tuning of large number of parameters is a hard task due to the complex
wiring of neurons between splitting layers and imbalance distributions of data
in pretrained and transferred domains. The reconstruction of the original
wiring for the target domain is a heavy burden due to the size of
interconnections across neurons. We propose a distributed scheme that tunes the
convolutional filters individually while backpropagates them jointly by means
of basic probability assignment. Some of the most recent advances in evidence
theory show that in a vast variety of the imbalanced regimes, optimizing of
some proper objective functions derived from contingency matrices prevents
biases towards high-prior class distributions. Therefore, the original filters
get gradually transferred based on individual contributions to overall
performance of the target domain. This largely reduces the expected complexity
of transfer learning whilst highly improves precision. Our experiments on
standard benchmarks and scenarios confirm the consistent improvement of our
distributed deep transfer learning strategy.
"
"  Enabling robots to autonomously navigate complex environments is essential
for real-world deployment. Prior methods approach this problem by having the
robot maintain an internal map of the world, and then use a localization and
planning method to navigate through the internal map. However, these approaches
often include a variety of assumptions, are computationally intensive, and do
not learn from failures. In contrast, learning-based methods improve as the
robot acts in the environment, but are difficult to deploy in the real-world
due to their high sample complexity. To address the need to learn complex
policies with few samples, we propose a generalized computation graph that
subsumes value-based model-free methods and model-based methods, with specific
instantiations interpolating between model-free and model-based. We then
instantiate this graph to form a navigation model that learns from raw images
and is sample efficient. Our simulated car experiments explore the design
decisions of our navigation model, and show our approach outperforms
single-step and $N$-step double Q-learning. We also evaluate our approach on a
real-world RC car and show it can learn to navigate through a complex indoor
environment with a few hours of fully autonomous, self-supervised training.
Videos of the experiments and code can be found at github.com/gkahn13/gcg
"
"  Deep Neural Networks are built to generalize outside of training set in mind
by using techniques such as regularization, early stopping and dropout. But
considerations to make them more resilient to adversarial examples are rarely
taken. As deep neural networks become more prevalent in mission-critical and
real-time systems, miscreants start to attack them by intentionally making deep
neural networks to misclassify an object of one type to be seen as another
type. This can be catastrophic in some scenarios where the classification of a
deep neural network can lead to a fatal decision by a machine. In this work, we
used GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method
and Jacobian Saliency Method, used those crafted adversarial samples to attack
another Deep Convolutional Neural Network and built the attacked network to be
more resilient against adversarial attacks by making it more robust by
Defensive Distillation and Adversarial Training
"
"  In the Simply Typed $\lambda$-calculus Statman investigates the reducibility
relation $\leq_{\beta\eta}$ between types: for $A,B \in \mathbb{T}^0$, types
freely generated using $\rightarrow$ and a single ground type $0$, define $A
\leq_{\beta\eta} B$ if there exists a $\lambda$-definable injection from the
closed terms of type $A$ into those of type $B$. Unexpectedly, the induced
partial order is the (linear) well-ordering (of order type) $\omega + 4$.
In the proof a finer relation $\leq_{h}$ is used, where the above injection
is required to be a Böhm transformation, and an (a posteriori) coarser
relation $\leq_{h^+}$, requiring a finite family of Böhm transformations that
is jointly injective.
We present this result in a self-contained, syntactic, constructive and
simplified manner. En route similar results for $\leq_h$ (order type $\omega +
5$) and $\leq_{h^+}$ (order type $8$) are obtained. Five of the equivalence
classes of $\leq_{h^+}$ correspond to canonical term models of Statman, one to
the trivial term model collapsing all elements of the same type, and one does
not even form a model by the lack of closed terms of many types.
"
"  This paper shows a detailed modeling of three-link robotic finger that is
actuated by nylon artificial muscles and a simulink model that can be used for
numerical study of a robotic finger. The robotic hand prototype was recently
demonstrated in recent publication Wu, L., Jung de Andrade, M., Saharan,
L.,Rome, R., Baughman, R., and Tadesse, Y., 2017, Compact and Low-cost Humanoid
Hand Powered by Nylon Artificial Muscles, Bioinspiration & Biomimetics, 12 (2).
The robotic hand is a 3D printed, lightweight and compact hand actuated by
silver-coated nylon muscles, often called Twisted and coiled Polymer (TCP)
muscles. TCP muscles are thermal actuators that contract when they are heated
and they are getting attention for application in robotics. The purpose of this
paper is to demonstrate the modeling equations that were derived based on Euler
Lagrangian approach that is suitable for implementation in simulink model.
"
"  A key task in Bayesian statistics is sampling from distributions that are
only specified up to a partition function (i.e., constant of proportionality).
However, without any assumptions, sampling (even approximately) can be #P-hard,
and few works have provided ""beyond worst-case"" guarantees for such settings.
For log-concave distributions, classical results going back to Bakry and
Émery (1985) show that natural continuous-time Markov chains called Langevin
diffusions mix in polynomial time. The most salient feature of log-concavity
violated in practice is uni-modality: commonly, the distributions we wish to
sample from are multi-modal. In the presence of multiple deep and
well-separated modes, Langevin diffusion suffers from torpid mixing.
We address this problem by combining Langevin diffusion with simulated
tempering. The result is a Markov chain that mixes more rapidly by
transitioning between different temperatures of the distribution. We analyze
this Markov chain for the canonical multi-modal distribution: a mixture of
gaussians (of equal variance). The algorithm based on our Markov chain provably
samples from distributions that are close to mixtures of gaussians, given
access to the gradient of the log-pdf. For the analysis, we use a spectral
decomposition theorem for graphs (Gharan and Trevisan, 2014) and a Markov chain
decomposition technique (Madras and Randall, 2002).
"
"  We provide a compositional coalgebraic semantics for strategic games. In our
framework, like in the semantics of functional programming languages,
coalgebras represent the observable behaviour of systems derived from the
behaviour of the parts over an unobservable state space. We use coalgebras to
describe and program stage games, finitely and potentially infinitely repeated
hierarchical or parallel games with imperfect and incomplete information based
on deterministic, non-deterministic or probabilistic decisions of learning
agents in possibly endogenous networks. Our framework is compositional in that
arbitrarily complex network of games can be composed. The coalgebraic approach
allows to represent self-referential or reflexive structures like institutional
dynamics, strategic network formation from within the network, belief
formation, learning agents or other self-referential phenomena that
characterise complex social systems of cognitive agents. And finally our games
represent directly runnable code in functional programming languages that can
also be analysed by sophisticated verification and logical tools of software
engineering.
"
"  Anytime almost-surely asymptotically optimal planners, such as RRT*,
incrementally find paths to every state in the search domain. This is
inefficient once an initial solution is found as then only states that can
provide a better solution need to be considered. Exact knowledge of these
states requires solving the problem but can be approximated with heuristics.
This paper formally defines these sets of states and demonstrates how they
can be used to analyze arbitrary planning problems. It uses the well-known
$L^2$ norm (i.e., Euclidean distance) to analyze minimum-path-length problems
and shows that existing approaches decrease in effectiveness factorially (i.e.,
faster than exponentially) with state dimension. It presents a method to
address this curse of dimensionality by directly sampling the prolate
hyperspheroids (i.e., symmetric $n$-dimensional ellipses) that define the $L^2$
informed set.
The importance of this direct informed sampling technique is demonstrated
with Informed RRT*. This extension of RRT* has less theoretical dependence on
state dimension and problem size than existing techniques and allows for linear
convergence on some problems. It is shown experimentally to find better
solutions faster than existing techniques on both abstract planning problems
and HERB, a two-arm manipulation robot.
"
"  We propose a novel approach to 3D human pose estimation from a single depth
map. Recently, convolutional neural network (CNN) has become a powerful
paradigm in computer vision. Many of computer vision tasks have benefited from
CNNs, however, the conventional approach to directly regress 3D body joint
locations from an image does not yield a noticeably improved performance. In
contrast, we formulate the problem as estimating per-voxel likelihood of key
body joints from a 3D occupancy grid. We argue that learning a mapping from
volumetric input to volumetric output with 3D convolution consistently improves
the accuracy when compared to learning a regression from depth map to 3D joint
coordinates. We propose a two-stage approach to reduce the computational
overhead caused by volumetric representation and 3D convolution: Holistic 2D
prediction and Local 3D prediction. In the first stage, Planimetric Network
(P-Net) estimates per-pixel likelihood for each body joint in the holistic 2D
space. In the second stage, Volumetric Network (V-Net) estimates the per-voxel
likelihood of each body joints in the local 3D space around the 2D estimations
of the first stage, effectively reducing the computational cost. Our model
outperforms existing methods by a large margin in publicly available datasets.
"
"  A very important problem in combinatorial optimization is partitioning a
network into communities of densely connected nodes; where the connectivity
between nodes inside a particular community is large compared to the
connectivity between nodes belonging to different ones. This problem is known
as community detection, and has become very important in various fields of
science including chemistry, biology and social sciences. The problem of
community detection is a twofold problem that consists of determining the
number of communities and, at the same time, finding those communities. This
drastically increases the solution space for heuristics to work on, compared to
traditional graph partitioning problems. In many of the scientific domains in
which graphs are used, there is the need to have the ability to partition a
graph into communities with the ``highest quality'' possible since the presence
of even small isolated communities can become crucial to explain a particular
phenomenon. We have explored community detection using the power of quantum
annealers, and in particular the D-Wave 2X and 2000Q machines. It turns out
that the problem of detecting at most two communities naturally fits into the
architecture of a quantum annealer with almost no need of reformulation. This
paper addresses a systematic study of detecting two or more communities in a
network using a quantum annealer.
"
"  For large-scale industrial processes under closed-loop control, process
dynamics directly resulting from control action are typical characteristics and
may show different behaviors between real faults and normal changes of
operating conditions. However, conventional distributed monitoring approaches
do not consider the closed-loop control mechanism and only explore static
characteristics, which thus are incapable of distinguishing between real
process faults and nominal changes of operating conditions, leading to
unnecessary alarms. In this regard, this paper proposes a distributed
monitoring method for closed-loop industrial processes by concurrently
exploring static and dynamic characteristics. First, the large-scale
closed-loop process is decomposed into several subsystems by developing a
sparse slow feature analysis (SSFA) algorithm which capture changes of both
static and dynamic information. Second, distributed models are developed to
separately capture static and dynamic characteristics from the local and global
aspects. Based on the distributed monitoring system, a two-level monitoring
strategy is proposed to check different influences on process characteristics
resulting from changes of the operating conditions and control action, and thus
the two changes can be well distinguished from each other. Case studies are
conducted based on both benchmark data and real industrial process data to
illustrate the effectiveness of the proposed method.
"
"  The problem of high-dimensional and large-scale representation of visual data
is addressed from an unsupervised learning perspective. The emphasis is put on
discrete representations, where the description length can be measured in bits
and hence the model capacity can be controlled. The algorithmic infrastructure
is developed based on the synthesis and analysis prior models whose
rate-distortion properties, as well as capacity vs. sample complexity
trade-offs are carefully optimized. These models are then extended to
multi-layers, namely the RRQ and the ML-STC frameworks, where the latter is
further evolved as a powerful deep neural network architecture with fast and
sample-efficient training and discrete representations. For the developed
algorithms, three important applications are developed. First, the problem of
large-scale similarity search in retrieval systems is addressed, where a
double-stage solution is proposed leading to faster query times and shorter
database storage. Second, the problem of learned image compression is targeted,
where the proposed models can capture more redundancies from the training
images than the conventional compression codecs. Finally, the proposed
algorithms are used to solve ill-posed inverse problems. In particular, the
problems of image denoising and compressive sensing are addressed with
promising results.
"
"  Recently, Czumaj et.al. (arXiv 2017) presented a parallel (almost)
$2$-approximation algorithm for the maximum matching problem in only
$O({(\log\log{n})^2})$ rounds of the massive parallel computation (MPC)
framework, when the memory per machine is $O(n)$. The main approach in their
work is a way of compressing $O(\log{n})$ rounds of a distributed algorithm for
maximum matching into only $O({(\log\log{n})^2})$ MPC rounds.
In this note, we present a similar algorithm for the closely related problem
of approximating the minimum vertex cover in the MPC framework. We show that
one can achieve an $O(\log{n})$ approximation to minimum vertex cover in only
$O(\log\log{n})$ MPC rounds when the memory per machine is $O(n)$. Our
algorithm for vertex cover is similar to the maximum matching algorithm of
Czumaj et.al. but avoids many of the intricacies in their approach and as a
result admits a considerably simpler analysis (at a cost of a worse
approximation guarantee). We obtain this result by modifying a previous
parallel algorithm by Khanna and the author (SPAA 2017) for vertex cover that
allowed for compressing $O(\log{n})$ rounds of a distributed algorithm into
constant MPC rounds when the memory allowed per machine is $O(n\sqrt{n})$.
"
"  We propose a general framework for interactively learning models, such as
(binary or non-binary) classifiers, orderings/rankings of items, or clusterings
of data points. Our framework is based on a generalization of Angluin's
equivalence query model and Littlestone's online learning model: in each
iteration, the algorithm proposes a model, and the user either accepts it or
reveals a specific mistake in the proposal. The feedback is correct only with
probability $p > 1/2$ (and adversarially incorrect with probability $1 - p$),
i.e., the algorithm must be able to learn in the presence of arbitrary noise.
The algorithm's goal is to learn the ground truth model using few iterations.
Our general framework is based on a graph representation of the models and
user feedback. To be able to learn efficiently, it is sufficient that there be
a graph $G$ whose nodes are the models and (weighted) edges capture the user
feedback, with the property that if $s, s^*$ are the proposed and target
models, respectively, then any (correct) user feedback $s'$ must lie on a
shortest $s$-$s^*$ path in $G$. Under this one assumption, there is a natural
algorithm reminiscent of the Multiplicative Weights Update algorithm, which
will efficiently learn $s^*$ even in the presence of noise in the user's
feedback.
From this general result, we rederive with barely any extra effort classic
results on learning of classifiers and a recent result on interactive
clustering; in addition, we easily obtain new interactive learning algorithms
for ordering/ranking.
"
"  Machine learning (ML) is increasingly deployed in real world contexts,
supplying actionable insights and forming the basis of automated
decision-making systems. While issues resulting from biases pre-existing in
training data have been at the center of the fairness debate, these systems are
also affected by technical and emergent biases, which often arise as
context-specific artifacts of implementation. This position paper interprets
technical bias as an epistemological problem and emergent bias as a dynamical
feedback phenomenon. In order to stimulate debate on how to change machine
learning practice to effectively address these issues, we explore this broader
view on bias, stress the need to reflect on epistemology, and point to
value-sensitive design methodologies to revisit the design and implementation
process of automated decision-making systems.
"
"  This article expands on research that has been done to develop a recurrent
neural network (RNN) capable of predicting aircraft engine vibrations using
long short-term memory (LSTM) neurons. LSTM RNNs can provide a more
generalizable and robust method for prediction over analytical calculations of
engine vibration, as analytical calculations must be solved iteratively based
on specific empirical engine parameters, making this approach ungeneralizable
across multiple engines. In initial work, multiple LSTM RNN architectures were
proposed, evaluated and compared. This research improves the performance of the
most effective LSTM network design proposed in the previous work by using a
promising neuroevolution method based on ant colony optimization (ACO) to
develop and enhance the LSTM cell structure of the network. A parallelized
version of the ACO neuroevolution algorithm has been developed and the evolved
LSTM RNNs were compared to the previously used fixed topology. The evolved
networks were trained on a large database of flight data records obtained from
an airline containing flights that suffered from excessive vibration. Results
were obtained using MPI (Message Passing Interface) on a high performance
computing (HPC) cluster, evolving 1000 different LSTM cell structures using 168
cores over 4 days. The new evolved LSTM cells showed an improvement of 1.35%,
reducing prediction error from 5.51% to 4.17% when predicting excessive engine
vibrations 10 seconds in the future, while at the same time dramatically
reducing the number of weights from 21,170 to 11,810.
"
"  We present PhyShare, a new haptic user interface based on actuated robots.
Virtual reality has recently been gaining wide adoption, and an effective
haptic feedback in these scenarios can strongly support user's sensory in
bridging virtual and physical world. Since participants do not directly observe
these robotic proxies, we investigate the multiple mappings between physical
robots and virtual proxies that can utilize the resources needed to provide a
well rounded VR experience. PhyShare bots can act either as directly touchable
objects or invisible carriers of physical objects, depending on different
scenarios. They also support distributed collaboration, allowing remotely
located VR collaborators to share the same physical feedback.
"
"  Graph drawings are useful tools for exploring the structure and dynamics of
data that can be represented by pair-wise relationships among a set of objects.
Typical real-world social, biological or technological networks exhibit high
complexity resulting from a large number and broad heterogeneity of objects and
relationships. Thus, mapping these networks into a low-dimensional space to
visualize the dynamics of network-driven processes is a challenging task. Often
we want to analyze how a single node is influenced by or is influencing its
local network as the source of a spreading process. Here I present a network
layout algorithm for graphs with millions of nodes that visualizes spreading
phenomena from the perspective of a single node. The algorithm consists of
three stages to allow for an interactive graph exploration: First, a global
solution for the network layout is found in spherical space that minimizes
distance errors between all nodes. Second, a focal node is interactively
selected, and distances to this node are further optimized. Third, node
coordinates are mapped to a circular representation and drawn with additional
features to represent the network-driven phenomenon. The effectiveness and
scalability of this method are shown for a large collaboration network of
scientists, where we are interested in the citation dynamics around a focal
author.
"
"  Epilepsy is common neurological diseases, affecting about 0.6-0.8 % of world
population. Epileptic patients suffer from chronic unprovoked seizures, which
can result in broad spectrum of debilitating medical and social consequences.
Since seizures, in general, occur infrequently and are unpredictable, automated
seizure detection systems are recommended to screen for seizures during
long-term electroencephalogram (EEG) recordings. In addition, systems for early
seizure detection can lead to the development of new types of intervention
systems that are designed to control or shorten the duration of seizure events.
In this article, we investigate the utility of recurrent neural networks (RNNs)
in designing seizure detection and early seizure detection systems. We propose
a deep learning framework via the use of Gated Recurrent Unit (GRU) RNNs for
seizure detection. We use publicly available data in order to evaluate our
method and demonstrate very promising evaluation results with overall accuracy
close to 100 %. We also systematically investigate the application of our
method for early seizure warning systems. Our method can detect about 98% of
seizure events within the first 5 seconds of the overall epileptic seizure
duration.
"
"  During maintenance, software developers deal with numerous change requests
made by the users of a software system. Studies show that the developers find
it challenging to select appropriate search terms from a change request during
concept location. In this paper, we propose a novel technique--QUICKAR--that
automatically suggests helpful reformulations for a given query by leveraging
the crowdsourced knowledge from Stack Overflow. It determines semantic
similarity or relevance between any two terms by analyzing their adjacent word
lists from the programming questions of Stack Overflow, and then suggests
semantically relevant queries for concept location. Experiments using 510
queries from two software systems suggest that our technique can improve or
preserve the quality of 76% of the initial queries on average which is
promising. Comparison with one baseline technique validates our preliminary
findings, and also demonstrates the potential of our technique.
"
"  We consider the weighted belief-propagation (WBP) decoder recently proposed
by Nachmani et al. where different weights are introduced for each Tanner graph
edge and optimized using machine learning techniques. Our focus is on
simple-scaling models that use the same weights across certain edges to reduce
the storage and computational burden. The main contribution is to show that
simple scaling with few parameters often achieves the same gain as the full
parameterization. Moreover, several training improvements for WBP are proposed.
For example, it is shown that minimizing average binary cross-entropy is
suboptimal in general in terms of bit error rate (BER) and a new ""soft-BER""
loss is proposed which can lead to better performance. We also investigate
parameter adapter networks (PANs) that learn the relation between the
signal-to-noise ratio and the WBP parameters. As an example, for the (32,16)
Reed-Muller code with a highly redundant parity-check matrix, training a PAN
with soft-BER loss gives near-maximum-likelihood performance assuming simple
scaling with only three parameters.
"
"  Technological developments alongside VLSI achievements enable mobile devices
to be equipped with multiple radio interfaces which is known as multihoming. On
the other hand, the combination of various wireless access technologies, known
as Next Generation Wireless Networks (NGWNs) has been introduced to provide
continuous connection to mobile devices in any time and location. Cognitive
radio networks as a part of NGWNs aroused to overcome spectrum inefficiency and
spectrum scarcity issues. In order to provide seamless and ubiquitous
connection across heterogeneous wireless access networks in the context of
cognitive radio networks, utilizing Mobile IPv6 is beneficial. In this paper, a
mobile device equipped with two radio interfaces is considered in order to
evaluate performance of spectrum handover in terms of handover latency. The
analytical results show that the proposed model can achieve better performance
compared to other related mobility management protocols mainly in terms of
handover latency.
"
"  A cyclic proof system gives us another way of representing inductive
definitions and efficient proof search. In 2011 Brotherston and Simpson
conjectured the equivalence between the provability of the classical cyclic
proof system and that of the classical system of Martin-Lof's inductive
definitions.
This paper studies the conjecture for intuitionistic logic.
This paper first points out that the countermodel of FOSSACS 2017 paper by
the same authors shows the conjecture for intuitionistic logic is false in
general. Then this paper shows the conjecture for intuitionistic logic is true
under arithmetic, namely, the provability of the intuitionistic cyclic proof
system is the same as that of the intuitionistic system of Martin-Lof's
inductive definitions when both systems contain Heyting arithmetic HA.
For this purpose, this paper also shows that HA proves Podelski-Rybalchenko
theorem for induction and Kleene-Brouwer theorem for induction. These results
immediately give another proof to the conjecture under arithmetic for classical
logic shown in LICS 2017 paper by the same authors.
"
"  Daily operation of a large-scale experiment is a challenging task,
particularly from perspectives of routine monitoring of quality for data being
taken. We describe an approach that uses Machine Learning for the automated
system to monitor data quality, which is based on partial use of data qualified
manually by detector experts. The system automatically classifies marginal
cases: both of good an bad data, and use human expert decision to classify
remaining ""grey area"" cases.
This study uses collision data collected by the CMS experiment at LHC in
2010. We demonstrate that proposed workflow is able to automatically process at
least 20\% of samples without noticeable degradation of the result.
"
"  It is well known that the ""store language"" of every pushdown automaton -- the
set of store configurations (state and stack contents) that can appear as an
intermediate step in accepting computations -- is a regular language. Here many
models of language acceptors with various data structures are examined, along
with a study of their store languages. For each model, an attempt is made to
find the simplest model that accepts their store languages. Some connections
between store languages of one-way and two-way machines generally are
demonstrated, as with connections between nondeterministic and deterministic
machines. A nice application of these store language results is also presented,
showing a general technique for proving families accepted by many deterministic
models are closed under right quotient with regular languages, resolving some
open questions (and significantly simplifying proofs for others that are known)
in the literature. Lower bounds on the space complexity for recognizing store
languages for the languages to be non-regular are obtained.
"
"  The session search task aims at best serving the user's information need
given her previous search behavior during the session. We propose an extended
relevance model that captures the user's dynamic information need in the
session. Our relevance modelling approach is directly driven by the user's
query reformulation (change) decisions and the estimate of how much the user's
search behavior affects such decisions. Overall, we demonstrate that, the
proposed approach significantly boosts session search performance.
"
"  Recently, graph neural networks (GNNs) have revolutionized the field of graph
representation learning through effectively learned node embeddings, and
achieved state-of-the-art results in tasks such as node classification and link
prediction. However, current GNN methods are inherently flat and do not learn
hierarchical representations of graphs---a limitation that is especially
problematic for the task of graph classification, where the goal is to predict
the label associated with an entire graph. Here we propose DiffPool, a
differentiable graph pooling module that can generate hierarchical
representations of graphs and can be combined with various graph neural network
architectures in an end-to-end fashion. DiffPool learns a differentiable soft
cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a
set of clusters, which then form the coarsened input for the next GNN layer.
Our experimental results show that combining existing GNN methods with DiffPool
yields an average improvement of 5-10% accuracy on graph classification
benchmarks, compared to all existing pooling approaches, achieving a new
state-of-the-art on four out of five benchmark data sets.
"
"  People participate and activate in online social networks and thus tremendous
amount of network data is generated; data regarding their interactions,
interests and activities. Some people search for specific questions through
online social platforms such as forums and they may receive a suitable response
via experts. To categorize people as experts and to evaluate their willingness
to cooperate, one can use ranking and cooperation problems from complex
networks. In this paper, we investigate classical ranking algorithms besides
the prisoner dilemma game to simulate cooperation and defection of agents. We
compute the correlation among the node rank and node cooperativity via three
strategies. The first strategy is involved in node level; however, other
strategies are calculated regarding neighborhood of nodes. We find out
correlations among specific ranking algorithms and cooperativtiy of nodes. Our
observations may be applied to estimate the propensity of people (experts) to
cooperate in future based on their ranking values.
"
"  The Met Office's weather and climate simulation code the Unified Model is
used for both operational Numerical Weather Prediction and Climate modelling.
The computational performance of the model running on parallel supercomputers
is a key consideration. A Krylov sub-space solver is employed to solve the
equations of the dynamical core of the model, known as ENDGame. These describe
the evolution of the Earth's atmosphere. Typically, 64-bit precision is used
throughout weather and climate applications. This work presents a
mixed-precision implementation of the solver, the beneficial effect on run-time
and the impact on solver convergence. The complex interplay of errors arising
from accumulated round-off in floating-point arithmetic and other numerical
effects is discussed. A careful analysis is required, however, the
mixed-precision solver is now employed in the operational forecast to satisfy
run-time constraints without compromising the accuracy of the solution.
"
"  Self Organizing Networks (SONs) are considered as vital deployments towards
upcoming dense cellular networks. From a mobile carrier point of view,
continuous coverage optimization is critical for better user perceptions. The
majority of SON contributions introduce novel algorithms that optimize specific
performance metrics. However, they require extensive processing delays and
advanced knowledge of network statistics that may not be available. In this
work, a progressive Autonomous Coverage Optimization (ACO) method combined with
adaptive cell dimensioning is proposed. The proposed method emphasizes the fact
that the effective cell coverage is a variant on actual user distributions. ACO
algorithm builds a generic Space-Time virtual coverage map per cell to detect
coverage holes in addition to limited or extended coverage conditions.
Progressive levels of optimization are followed to timely resolve coverage
issues with maintaining optimization stability. Proposed ACO is verified under
both simulations and practical deployment in a pilot cluster for a worldwide
mobile carrier. Key Performance Indicators show that proposed ACO method
significantly enhances system coverage and performance.
"
"  A many-valued modal logic is introduced that combines the usual Kripke frame
semantics of the modal logic K with connectives interpreted locally at worlds
by lattice and group operations over the real numbers. A labelled tableau
system is provided and a coNEXPTIME upper bound obtained for checking validity
in the logic. Focussing on the modal-multiplicative fragment, the labelled
tableau system is then used to establish completeness for a sequent calculus
that admits cut-elimination and an axiom system that extends the multiplicative
fragment of Abelian logic.
"
"  During routine state space circuit analysis of an arbitrarily connected set
of nodes representing a lossless LC network, a matrix was formed that was
observed to implicitly capture connectivity of the nodes in a graph similar to
the conventional incidence matrix, but in a slightly different manner. This
matrix has only 0, 1 or -1 as its elements. A sense of direction (of the graph
formed by the nodes) is inherently encoded in the matrix because of the
presence of -1. It differs from the incidence matrix because of leaving out the
datum node from the matrix. Calling this matrix as forward adjacency matrix, it
was found that its inverse also displays useful and interesting physical
properties when a specific style of node-indexing is adopted for the nodes in
the graph. The graph considered is connected but does not have any closed
loop/cycle (corresponding to closed loop of inductors in a circuit) as with its
presence the matrix is not invertible. Incidentally, by definition the graph
being considered is a tree. The properties of the forward adjacency matrix and
its inverse, along with rigorous proof, are presented.
"
"  Given a graph $ G $ with $ n $ vertices and a set $ S $ of $ n $ points in
the plane, a point-set embedding of $ G $ on $ S $ is a planar drawing such
that each vertex of $ G $ is mapped to a distinct point of $ S $. A
straight-line point-set embedding is a point-set embedding with no edge bends
or curves. The point-set embeddability problem is NP-complete, even when $ G $
is $ 2 $-connected and $ 2 $-outerplanar. It has been solved polynomially only
for a few classes of planar graphs. Suppose that $ S $ is the set of vertices
of a simple polygon. A straight-line polygon embedding of a graph is a
straight-line point-set embedding of the graph onto the vertices of the polygon
with no crossing between edges of graph and the edges of polygon. In this
paper, we present $ O(n) $-time algorithms for polygon embedding of path and
cycle graphs in simple convex polygon and same time algorithms for polygon
embedding of path and cycle graphs in a large type of simple polygons where $n$
is the number of vertices of the polygon.
"
"  In this paper, we investigate the parametric knapsack problem, in which the
item profits are affine functions depending on a real-valued parameter. The aim
is to provide a solution for all values of the parameter. It is well-known that
any exact algorithm for the problem may need to output an exponential number of
knapsack solutions. We present a fully polynomial-time approximation scheme
(FPTAS) for the problem that, for any desired precision $\varepsilon \in
(0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the
parameter. This is the first FPTAS for the parametric knapsack problem that
does not require the slopes and intercepts of the affine functions to be
non-negative but works for arbitrary integral values. Our FPTAS outputs
$\mathcal{O}(\frac{n^2}{\varepsilon})$ knapsack solutions and runs in strongly
polynomial-time $\mathcal{O}(\frac{n^4}{\varepsilon^2})$. Even for the special
case of positive input data, this is the first FPTAS with a strongly polynomial
running time. We also show that this time bound can be further improved to
$\mathcal{O}(\frac{n^2}{\varepsilon} \cdot A(n,\varepsilon))$, where
$A(n,\varepsilon)$ denotes the running time of any FPTAS for the traditional
(non-parametric) knapsack problem.
"
"  Dozens of new models on fixation prediction are published every year and
compared on open benchmarks such as MIT300 and LSUN. However, progress in the
field can be difficult to judge because models are compared using a variety of
inconsistent metrics. Here we show that no single saliency map can perform well
under all metrics. Instead, we propose a principled approach to solve the
benchmarking problem by separating the notions of saliency models, maps and
metrics. Inspired by Bayesian decision theory, we define a saliency model to be
a probabilistic model of fixation density prediction and a saliency map to be a
metric-specific prediction derived from the model density which maximizes the
expected performance on that metric given the model density. We derive these
optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,
NSS, CC, SIM, KL-Div) and show that they can be computed analytically or
approximated with high precision. We show that this leads to consistent
rankings in all metrics and avoids the penalties of using one saliency map for
all metrics. Our method allows researchers to have their model compete on many
different metrics with state-of-the-art in those metrics: ""good"" models will
perform well in all metrics.
"
"  This study focusses on self-balancing microgrids to smartly utilize and
prevent overdrawing of available power capacity of the grid. A distributed
framework for automated distribution of optimal power demand is proposed, where
all building in a microgrid dynamically and simultaneously adjusts their own
power consumption to reach their individual optimal power demands while
cooperatively striving to maintain the overall grid stable. Emphasis has been
given to aspects of algorithm that yields lower time of convergence and is
demonstrated through quantitative and qualitative analysis of simulation
results.
"
"  Internet or things (IoT) is changing our daily life rapidly. Although new
technologies are emerging everyday and expanding their influence in this
rapidly growing area, many classic theories can still find their places. In
this paper, we study the important applications of the classic network coding
theory in two important components of Internet of things, including the IoT
core network, where data is sensed and transmitted, and the distributed cloud
storage, where the data generated by the IoT core network is stored. First we
propose an adaptive network coding (ANC) scheme in the IoT core network to
improve the transmission efficiency. We demonstrate the efficacy of the scheme
and the performance advantage over existing schemes through simulations. %Next
we study the application of network coding in the distributed cloud storage.
Next we introduce the optimal storage allocation problem in the network coding
based distributed cloud storage, which aims at searching for the most reliable
allocation that distributes the $n$ data components into $N$ data centers,
given the failure probability $p$ of each data center. Then we propose a
polynomial-time optimal storage allocation (OSA) scheme to solve the problem.
Both the theoretical analysis and the simulation results show that the storage
reliability could be greatly improved by the OSA scheme.
"
"  In the Steiner Forest problem, we are given a graph and a collection of
source-sink pairs, and the goal is to find a subgraph of minimum total length
such that all pairs are connected. The problem is APX-Hard and can be
2-approximated by, e.g., the elegant primal-dual algorithm of Agrawal, Klein,
and Ravi from 1995.
We give a local-search-based constant-factor approximation for the problem.
Local search brings in new techniques to an area that has for long not seen any
improvements and might be a step towards a combinatorial algorithm for the more
general survivable network design problem. Moreover, local search was an
essential tool to tackle the dynamic MST/Steiner Tree problem, whereas dynamic
Steiner Forest is still wide open.
It is easy to see that any constant factor local search algorithm requires
steps that add/drop many edges together. We propose natural local moves which,
at each step, either (a) add a shortest path in the current graph and then drop
a bunch of inessential edges, or (b) add a set of edges to the current
solution. This second type of moves is motivated by the potential function we
use to measure progress, combining the cost of the solution with a penalty for
each connected component. Our carefully-chosen local moves and potential
function work in tandem to eliminate bad local minima that arise when using
more traditional local moves.
"
"  In recent years, several powerful techniques have been developed to design
{\em randomized} polynomial-space parameterized algorithms. In this paper, we
introduce an enhancement of color coding to design deterministic
polynomial-space parameterized algorithms. Our approach aims at reducing the
number of random choices by exploiting the special structure of a solution.
Using our approach, we derive the following deterministic algorithms (see
Introduction for problem definitions).
1. Polynomial-space $O^*(3.86^k)$-time (exponential-space $O^*(3.41^k)$-time)
algorithm for {\sc $k$-Internal Out-Branching}, improving upon the previously
fastest {\em exponential-space} $O^*(5.14^k)$-time algorithm for this problem.
2. Polynomial-space $O^*((2e)^{k+o(k)})$-time (exponential-space
$O^*(4.32^k)$-time) algorithm for {\sc $k$-Colorful Out-Branching} on
arc-colored digraphs and {\sc $k$-Colorful Perfect Matching} on planar
edge-colored graphs.
To obtain our polynomial space algorithms, we show that $(n,k,\alpha
k)$-splitters ($\alpha\ge 1$) and in particular $(n,k)$-perfect hash families
can be enumerated one by one with polynomial delay.
"
"  Let $S$ be a string of length $n$. In this paper we introduce the notion of
\emph{string attractor}: a subset of the string's positions $[1,n]$ such that
every distinct substring of $S$ has an occurrence crossing one of the
attractor's elements. We first show that the minimum attractor's size yields
upper-bounds to the string's repetitiveness as measured by its linguistic
complexity and by the length of its longest repeated substring. We then prove
that all known compressors for repetitive strings induce a string attractor
whose size is bounded by their associated repetitiveness measure, and can
therefore be considered as approximations of the smallest one. Using further
reductions, we derive the approximation ratios of these compressors with
respect to the smallest attractor and solve several open problems related to
the asymptotic relations between repetitiveness measures (in particular,
between the the sizes of the Lempel-Ziv factorization, the run-length
Burrows-Wheeler transform, the smallest grammar, and the smallest macro
scheme). These reductions directly provide approximation algorithms for the
smallest string attractor. We then apply string attractors to solve efficiently
a fundamental problem in the field of compressed computation: we present a
universal compressed data structure for text extraction that improves existing
strategies simultaneously for \emph{all} known dictionary compressors and that,
by recent lower bounds, almost matches the optimal running time within the
resulting space. To conclude, we consider generalizations of string attractors
to labeled graphs, show that the attractor problem is NP-complete on trees, and
provide a logarithmic approximation computable in polynomial time.
"
"  Purpose of review: This paper presents a review of the current state of the
art in remote sensing based monitoring of forest disturbances and forest
degradation from optical Earth Observation data. Part one comprises an overview
of currently available optical remote sensing sensors, which can be used for
forest disturbance and degradation mapping. Part two reviews the two main
categories of existing approaches: classical image-to-image change detection
and time series analysis. Recent findings: With the launch of the Sentinel-2a
satellite and available Landsat imagery, time series analysis has become the
most promising but also most demanding category of degradation mapping
approaches. Four time series classification methods are distinguished. The
methods are explained and their benefits and drawbacks are discussed. A
separate chapter presents a number of recent forest degradation mapping studies
for two different ecosystems: temperate forests with a geographical focus on
Europe and tropical forests with a geographical focus on Africa. Summary: The
review revealed that a wide variety of methods for the detection of forest
degradation is already available. Today, the main challenge is to transfer
these approaches to high resolution time series data from multiple sensors.
Future research should also focus on the classification of disturbance types
and the development of robust up-scalable methods to enable near real time
disturbance mapping in support of operational reactive measures.
"
"  Differential testing to solve the oracle problem has been applied in many
scenarios where multiple supposedly equivalent implementations exist, such as
multiple implementations of a C compiler. If the multiple systems disagree on
the output for a given test input, we have likely discovered a bug without
every having to specify what the expected output is. Research on variational
analyses (or variability-aware or family-based analyses) can benefit from
similar ideas. The goal of most variational analyses is to perform an analysis,
such as type checking or model checking, over a large number of configurations
much faster than an existing traditional analysis could by analyzing each
configuration separately. Variational analyses are very suitable for
differential testing, since the existence nonvariational analysis can provide
the oracle for test cases that would otherwise be tedious or difficult to
write. In this experience paper, I report how differential testing has helped
in developing KConfigReader, a tool for translating the Linux kernel's kconfig
model into a propositional formula. Differential testing allows us to quickly
build a large test base and incorporate external tests that avoided many
regressions during development and made KConfigReader likely the most precise
kconfig extraction tool available.
"
"  Blind Source Separation (BSS) is a challenging matrix factorization problem
that plays a central role in multichannel imaging science. In a large number of
applications, such as astrophysics, current unmixing methods are limited since
real-world mixtures are generally affected by extra instrumental effects like
blurring. Therefore, BSS has to be solved jointly with a deconvolution problem,
which requires tackling a new inverse problem: deconvolution BSS (DBSS). In
this article, we introduce an innovative DBSS approach, called DecGMCA, based
on sparse signal modeling and an efficient alternative projected least square
algorithm. Numerical results demonstrate that the DecGMCA algorithm performs
very well on simulations. It further highlights the importance of jointly
solving BSS and deconvolution instead of considering these two problems
independently. Furthermore, the performance of the proposed DecGMCA algorithm
is demonstrated on simulated radio-interferometric data.
"
"  Computing the medoid of a large number of points in high-dimensional space is
an increasingly common operation in many data science problems. We present an
algorithm Med-dit which uses O(n log n) distance evaluations to compute the
medoid with high probability. Med-dit is based on a connection with the
multi-armed bandit problem. We evaluate the performance of Med-dit empirically
on the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds
of thousands of points living in tens of thousands of dimensions, and observe a
5-10x improvement in performance over the current state of the art. Med-dit is
available at this https URL
"
"  Sampling logconcave functions arising in statistics and machine learning has
been a subject of intensive study. Recent developments include analyses for
Langevin dynamics and Hamiltonian Monte Carlo (HMC). While both approaches have
dimension-independent bounds for the underlying $\mathit{continuous}$ processes
under sufficiently strong smoothness conditions, the resulting discrete
algorithms have complexity and number of function evaluations growing with the
dimension. Motivated by this problem, in this paper, we give a general
algorithm for solving multivariate ordinary differential equations whose
solution is close to the span of a known basis of functions (e.g., polynomials
or piecewise polynomials). The resulting algorithm has polylogarithmic depth
and essentially tight runtime - it is nearly linear in the size of the
representation of the solution.
We apply this to the sampling problem to obtain a nearly linear
implementation of HMC for a broad class of smooth, strongly logconcave
densities, with the number of iterations (parallel depth) and gradient
evaluations being $\mathit{polylogarithmic}$ in the dimension (rather than
polynomial as in previous work). This class includes the widely-used loss
function for logistic regression with incoherent weight matrices and has been
subject of much study recently. We also give a faster algorithm with $
\mathit{polylogarithmic~depth}$ for the more general and standard class of
strongly convex functions with Lipschitz gradient. These results are based on
(1) an improved contraction bound for the exact HMC process and (2) logarithmic
bounds on the degree of polynomials that approximate solutions of the
differential equations arising in implementing HMC.
"
"  The celebrated integer relation finding algorithm PSLQ has been successfully
used in many applications. PSLQ was only analyzed theoretically for exact input
data, however, when the input data are irrational numbers, they must be
approximate ones due to the finite precision of the computer. When the
algorithm takes empirical data (inexact data with error bounded) instead of
exact real numbers as its input, how do we theoretically ensure the output of
the algorithm to be an exact integer relation?
In this paper, we investigate the PSLQ algorithm for empirical data as its
input. Firstly, we give a termination condition for this case. Secondly, we
analyze a perturbation on the hyperplane matrix constructed from the input data
and hence disclose a relationship between the accuracy of the input data and
the output quality (an upper bound on the absolute value of the inner product
of the exact data and the computed integer relation), which naturally leads to
an error control strategy for PSLQ. Further, we analyze the complexity bound of
the PSLQ algorithm for empirical data. Examples on transcendental numbers and
algebraic numbers show the meaningfulness of our error control strategy.
"
"  Tensor completion is a problem of filling the missing or unobserved entries
of partially observed tensors. Due to the multidimensional character of tensors
in describing complex datasets, tensor completion algorithms and their
applications have received wide attention and achievement in areas like data
mining, computer vision, signal processing, and neuroscience. In this survey,
we provide a modern overview of recent advances in tensor completion algorithms
from the perspective of big data analytics characterized by diverse variety,
large volume, and high velocity. We characterize these advances from four
perspectives: general tensor completion algorithms, tensor completion with
auxiliary information (variety), scalable tensor completion algorithms
(volume), and dynamic tensor completion algorithms (velocity). Further, we
identify several tensor completion applications on real-world data-driven
problems and present some common experimental frameworks popularized in the
literature. Our goal is to summarize these popular methods and introduce them
to researchers and practitioners for promoting future research and
applications. We conclude with a discussion of key challenges and promising
research directions in this community for future exploration.
"
"  A path (resp. cycle) decomposition of a graph $G$ is a set of edge-disjoint
paths (resp. cycles) of $G$ that covers the edge set of $G$. Gallai (1966)
conjectured that every graph on $n$ vertices admits a path decomposition of
size at most $\lfloor (n+1)/2\rfloor$, and Hajós (1968) conjectured that
every Eulerian graph on $n$ vertices admits a cycle decomposition of size at
most $\lfloor (n-1)/2\rfloor$. Gallai's Conjecture was verified for many
classes of graphs. In particular, Lovász (1968) verified this conjecture for
graphs with at most one vertex of even degree, and Pyber (1996) verified it for
graphs in which every cycle contains a vertex of odd degree. Hajós'
Conjecture, on the other hand, was verified only for graphs with maximum degree
$4$ and for planar graphs. In this paper, we verify Gallai's and Hajós'
Conjectures for graphs with treewidth at most $3$. Moreover, we show that the
only graphs with treewidth at most $3$ that do not admit a path decomposition
of size at most $\lfloor n/2\rfloor$ are isomorphic to $K_3$ or $K_5-e$.
Finally, we use the technique developed in this paper to present new proofs for
Gallai's and Hajós' Conjectures for graphs with maximum degree at most $4$,
and for planar graphs with girth at least $6$.
"
"  The concept of emergence is a powerful concept to explain very complex
behaviour by simple underling rules. Existing approaches of producing emergent
collective behaviour have many limitations making them unable to account for
the complexity we see in the real world. In this paper we propose a new
dynamic, non-local, and time independent approach that uses a network like
structure to implement the laws or the rules, where the mathematical equations
representing the rules are converted to a series of switching decisions carried
out by the network on the particles moving in the network. The proposed
approach is used to generate patterns with different types of symmetry.
"
"  Motion planning classically concerns the problem of accomplishing a goal
configuration while avoiding obstacles. However, the need for more
sophisticated motion planning methodologies, taking temporal aspects into
account, has emerged. To address this issue, temporal logics have recently been
used to formulate such advanced specifications. This paper will consider Signal
Temporal Logic in combination with Model Predictive Control. A robustness
metric, called Discrete Average Space Robustness, is introduced and used to
maximize the satisfaction of specifications which results in a natural
robustness against noise. The comprised optimization problem is convex and
formulated as a Linear Program.
"
"  Deep Neural Networks (DNNs) are universal function approximators providing
state-of- the-art solutions on wide range of applications. Common perceptual
tasks such as speech recognition, image classification, and object tracking are
now commonly tackled via DNNs. Some fundamental problems remain: (1) the lack
of a mathematical framework providing an explicit and interpretable
input-output formula for any topology, (2) quantification of DNNs stability
regarding adversarial examples (i.e. modified inputs fooling DNN predictions
whilst undetectable to humans), (3) absence of generalization guarantees and
controllable behaviors for ambiguous patterns, (4) leverage unlabeled data to
apply DNNs to domains where expert labeling is scarce as in the medical field.
Answering those points would provide theoretical perspectives for further
developments based on a common ground. Furthermore, DNNs are now deployed in
tremendous societal applications, pushing the need to fill this theoretical gap
to ensure control, reliability, and interpretability.
"
"  We describe a new cardinality estimation algorithm that is extremely
space-efficient. It applies one of three novel estimators to the compressed
state of the Flajolet-Martin-85 coupon collection process. In an
apples-to-apples empirical comparison against compressed HyperLogLog sketches,
the new algorithm simultaneously wins on all three dimensions of the
time/space/accuracy tradeoff. Our prototype uses the zstd compression library,
and produces sketches that are smaller than the entropy of HLL, so no possible
implementation of compressed HLL can match its space efficiency. The paper's
technical contributions include analyses and simulations of the three new
estimators, accurate values for the entropies of FM85 and HLL, and a
non-trivial method for estimating a double asymptotic limit via simulation.
"
"  Object tracking systems play important roles in tracking moving objects and
overcoming problems such as safety, security and other location-related
applications. Problems arise from the difficulties in creating a well-defined
and understandable description of tracking systems. Nowadays, describing such
processes results in fragmental representation that most of the time leads to
difficulties creating documentation. Additionally, once learned by assigned
personnel, repeated tasks result in them continuing on autopilot in a way that
often degrades their effectiveness. This paper proposes the modeling of
tracking systems in terms of a new diagrammatic methodology to produce
engineering-like schemata. The resultant diagrams can be used in documentation,
explanation, communication, education and control.
"
"  Wheeled ground robots are limited from exploring extreme environments such as
caves, lava tubes and skylights. Small robots that utilize unconventional
mobility through hopping, flying and rolling can overcome many roughness
limitations and thus extend exploration sites of interest on Moon and Mars. In
this paper we introduce a network of 3 kg, 0.30 m diameter ball robots
(pit-bots) that can fly, hop and roll using an onboard miniature propulsion
system. These pit-bots can be deployed from a lander or large rover. Each robot
is equipped with a smartphone sized computer, stereo camera and laser
rangefinder to per-form navigation and mapping. The ball robot can carry a
payload of 1 kg or perform sample return. Our studies show a range of 5 km and
0.7 hours flight time on the Moon.
"
"  Currently, lower limb robotic rehabilitation is widely developed, However,
the devices used so far seem to not have a uniform criteria for their design,
because, on the contrary, each developed mechanism is often presented as if it
does not take into account the criteria used in previous designs. On the other
hand, the diagnosis of lower limb from robotic devices has been little studied.
This chapter presents a guide for the design of robotic devices in diagnosis of
lower limbs, taking into account the mobility of the human leg and the
techniques used by physiotherapists in the execution of exercises and the
rehabilitation of rehabilitation and diagnosis tests, as well as the
recommendations made by various authors, among other aspects. The proposed
guide is illustrated through a case study based on a parallel robot RPU+3UPS
able to make movements that are applied during the processes of rehabilitation
and diagnosis. The proposal presents advantages over some existing devices such
as its load capacity that can support, and also allows you to restrict the
movement in directions required by the rehabilitation and the diagnosis
movements.
"
"  Dynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict
time-series data. Gaussian DyBM is a DyBM that assumes the predicted data is
generated by a Gaussian distribution whose first-order moment (mean)
dynamically changes over time but its second-order moment (variance) is fixed.
However, in many financial applications, the assumption is quite limiting in
two aspects. First, even when the data follows a Gaussian distribution, its
variance may change over time. Such variance is also related to important
temporal economic indicators such as the market volatility. Second, financial
time-series data often requires learning datasets generated by the generalized
Gaussian distribution with an additional shape parameter that is important to
approximate heavy-tailed distributions. Addressing those aspects, we show how
to extend DyBM that results in significant performance improvement in
predicting financial time-series data.
"
"  In this paper, we focus on applications in machine learning, optimization,
and control that call for the resilient selection of a few elements, e.g.
features, sensors, or leaders, against a number of adversarial
denial-of-service attacks or failures. In general, such resilient optimization
problems are hard, and cannot be solved exactly in polynomial time, even though
they often involve objective functions that are monotone and submodular.
Notwithstanding, in this paper we provide the first scalable,
curvature-dependent algorithm for their approximate solution, that is valid for
any number of attacks or failures, and which, for functions with low curvature,
guarantees superior approximation performance. Notably, the curvature has been
known to tighten approximations for several non-resilient maximization
problems, yet its effect on resilient maximization had hitherto been unknown.
We complement our theoretical analyses with supporting empirical evaluations.
"
"  Automatically detecting sound units of humpback whales in complex
time-varying background noises is a current challenge for scientists. In this
paper, we explore the applicability of Convolution Neural Network (CNN) method
for this task. In the evaluation stage, we present 6 bi-class classification
experimentations of whale sound detection against different background noise
types (e.g., rain, wind). In comparison to classical FFT-based representation
like spectrograms, we showed that the use of image-based pretrained CNN
features brought higher performance to classify whale sounds and background
noise.
"
"  Given an input string s and a specific Lindenmayer system (the so-called
Fibonacci grammar), we define an automaton which is capable of (i) determining
whether s belongs to the set of strings that the Fibonacci grammar can generate
(in other words, if s corresponds to a generation of the grammar) and, if so,
(ii) reconstructing the previous generation.
"
"  Geo-tags from micro-blog posts have been shown to be useful in many data
mining applications. This work seeks to find out if the location type derived
from these geo-tags can benefit input methods, which attempts to predict the
next word a user will input during typing. If a correlation between different
location types and a change in word distribution can be found, the location
type information can be used to make the input method more accurate. This work
queried micro-blog posts from Twitter API and location type of these posts from
Google Place API, forming a dataset of around 500k samples. A statistical study
on the word distribution found weak support for the assumption. An LSTM based
prediction experiment found a 2% edge in the accuracy from language models
leveraging location type information when compared to a baseline without that
information.
"
"  We study the classical complexity of the exact Boson Sampling problem where
the objective is to produce provably correct random samples from a particular
quantum mechanical distribution. The computational framework was proposed by
Aaronson and Arkhipov in 2011 as an attainable demonstration of `quantum
supremacy', that is a practical quantum computing experiment able to produce
output at a speed beyond the reach of classical (that is non-quantum) computer
hardware. Since its introduction Boson Sampling has been the subject of intense
international research in the world of quantum computing. On the face of it,
the problem is challenging for classical computation. Aaronson and Arkhipov
show that exact Boson Sampling is not efficiently solvable by a classical
computer unless $P^{\#P} = BPP^{NP}$ and the polynomial hierarchy collapses to
the third level.
The fastest known exact classical algorithm for the standard Boson Sampling
problem takes $O({m + n -1 \choose n} n 2^n )$ time to produce samples for a
system with input size $n$ and $m$ output modes, making it infeasible for
anything but the smallest values of $n$ and $m$. We give an algorithm that is
much faster, running in $O(n 2^n + \operatorname{poly}(m,n))$ time and $O(m)$
additional space. The algorithm is simple to implement and has low constant
factor overheads. As a consequence our classical algorithm is able to solve the
exact Boson Sampling problem for system sizes far beyond current photonic
quantum computing experimentation, thereby significantly reducing the
likelihood of achieving near-term quantum supremacy in the context of Boson
Sampling.
"
"  The purpose of this note is to attract attention to the following conjecture
(metastable $r$-fold Whitney trick) by clarifying its status as not having a
complete proof, in the sense described in the paper.
Assume that $D=D_1\sqcup\ldots\sqcup D_r$ is disjoint union of $r$ disks of
dimension $s$, $f:D\to B^d$ a proper PL map such that $f\partial
D_1\cap\ldots\cap f\partial D_r=\emptyset$, $rd\ge (r+1)s+3$ and $d\ge s+3$. If
the map $$f^r:\partial(D_1\times\ldots\times D_r)\to
(B^d)^r-\{(x,x,\ldots,x)\in(B^d)^r\ |\ x\in B^d\}$$ extends to
$D_1\times\ldots\times D_r$, then there is a PL map $\overline f:D\to B^d$ such
that $$\overline f=f \quad\text{on}\quad D_r\cup\partial D\quad\text{and}\quad
\overline fD_1\cap\ldots\cap \overline fD_r=\emptyset.$$
"
"  Which studies, theories, and ideas have influenced Eugene Garfield's
scientific work? Recently, the method reference publication year spectroscopy
(RPYS) has been introduced, which can be used to answer this and related
questions. Since then, several studies have been published dealing with the
historical roots of research fields and scientists. The program CRExplorer
(this http URL) was specifically developed for RPYS. In this study,
we use this program to investigate the historical roots of Eugene Garfield's
oeuvre.
"
"  In this paper, we focus on online representation learning in non-stationary
environments which may require continuous adaptation of model architecture. We
propose a novel online dictionary-learning (sparse-coding) framework which
incorporates the addition and deletion of hidden units (dictionary elements),
and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of
the hippocampus, known to be associated with improved cognitive function and
adaptation to new environments. In the online learning setting, where new input
instances arrive sequentially in batches, the neuronal-birth is implemented by
adding new units with random initial weights (random dictionary elements); the
number of new units is determined by the current performance (representation
error) of the dictionary, higher error causing an increase in the birth rate.
Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity)
on the dictionary within the block-coordinate descent optimization at each
iteration of our online alternating minimization scheme, which iterates between
the code and dictionary updates. Finally, hidden unit connectivity adaptation
is facilitated by introducing sparsity in dictionary elements. Our empirical
evaluation on several real-life datasets (images and language) as well as on
synthetic data demonstrates that the proposed approach can considerably
outperform the state-of-art fixed-size (nonadaptive) online sparse coding of
Mairal et al. (2009) in the presence of nonstationary data. Moreover, we
identify certain properties of the data (e.g., sparse inputs with nearly
non-overlapping supports) and of the model (e.g., dictionary sparsity)
associated with such improvements.
"
"  With the advent of modern communications systems, much attention has been put
on developing methods for securely transferring information between
constituents of wireless sensor networks. To this effect, we introduce a
mathematical programming formulation for the key management problem, which
broadly serves as a mechanism for encrypting communications. In particular, an
integer programming model of the q-Composite scheme is proposed and utilized to
distribute keys among nodes of a network whose topology is known. Numerical
experiments demonstrating the effectiveness of the proposed model are conducted
using using a well-known optimization solver package. An illustrative example
depicting an optimal encryption for a small-scale network is also presented.
"
"  Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to
synthesize a new image that retains the high-level structure of a content
image, rendered in the low-level texture of a style image. This is achieved by
constraining the new image to have high-level CNN features similar to the
content image, and lower-level CNN features similar to the style image. However
in the traditional optimization objective, low-level features of the content
image are absent, and the low-level features of the style image dominate the
low-level detail structures of the new image. Hence in the synthesized image,
many details of the content image are lost, and a lot of inconsistent and
unpleasing artifacts appear. As a remedy, we propose to steer image synthesis
with a novel loss function: the Laplacian loss. The Laplacian matrix
(""Laplacian"" in short), produced by a Laplacian operator, is widely used in
computer vision to detect edges and contours. The Laplacian loss measures the
difference of the Laplacians, and correspondingly the difference of the detail
structures, between the content image and a new image. It is flexible and
compatible with the traditional style transfer constraints. By incorporating
the Laplacian loss, we obtain a new optimization objective for neural style
transfer named Lapstyle. Minimizing this objective will produce a stylized
image that better preserves the detail structures of the content image and
eliminates the artifacts. Experiments show that Lapstyle produces more
appealing stylized images with less artifacts, without compromising their
""stylishness"".
"
"  Reinforcement learning has emerged as a promising methodology for training
robot controllers. However, most results have been limited to simulation due to
the need for a large number of samples and the lack of automated-yet-safe data
collection methods. Model-based reinforcement learning methods provide an
avenue to circumvent these challenges, but the traditional concern has been the
mismatch between the simulator and the real world. Here, we show that control
policies learned in simulation can successfully transfer to a physical system,
composed of three Phantom robots pushing an object to various desired target
positions. We use a modified form of the natural policy gradient algorithm for
learning, applied to a carefully identified simulation model. The resulting
policies, trained entirely in simulation, work well on the physical system
without additional training. In addition, we show that training with an
ensemble of models makes the learned policies more robust to modeling errors,
thus compensating for difficulties in system identification.
"
"  In both H.264 and HEVC, context-adaptive binary arithmetic coding (CABAC) is
adopted as the entropy coding method. CABAC relies on manually designed
binarization processes as well as handcrafted context models, which may
restrict the compression efficiency. In this paper, we propose an arithmetic
coding strategy by training neural networks, and make preliminary studies on
coding of the intra prediction modes in HEVC. Instead of binarization, we
propose to directly estimate the probability distribution of the 35 intra
prediction modes with the adoption of a multi-level arithmetic codec. Instead
of handcrafted context models, we utilize convolutional neural network (CNN) to
perform the probability estimation. Simulation results show that our proposed
arithmetic coding leads to as high as 9.9% bits saving compared with CABAC.
"
"  Information-Centric Networking is a promising networking paradigm that
overcomes many of the limitations of current networking architectures. Various
research efforts investigate solutions for securing ICN. Nevertheless, most of
these solutions relax security requirements in favor of network performance. In
particular, they weaken end-user privacy and the architecture's tolerance to
security breaches in order to support middleboxes that offer services such as
caching and content replication. In this paper, we adapt TLS, a widely used
security standard, to an ICN context. We design solutions that allow session
reuse and migration among multiple stakeholders and we propose an extension
that allows authorized middleboxes to lawfully and transparently intercept
secured communications.
"
"  This paper develops meshless methods for probabilistically describing
discretisation error in the numerical solution of partial differential
equations. This construction enables the solution of Bayesian inverse problems
while accounting for the impact of the discretisation of the forward problem.
In particular, this drives statistical inferences to be more conservative in
the presence of significant solver error. Theoretical results are presented
describing rates of convergence for the posteriors in both the forward and
inverse problems. This method is tested on a challenging inverse problem with a
nonlinear forward model.
"
"  Changes in the structure of observed social and complex networks' structure
can indicate a significant underlying change in an organization, or reflect the
response of the network to an external event. Automatic detection of change
points in evolving networks is rudimentary to the research and the
understanding of the effect of such events on networks. Here we present an
easy-to-implement and fast framework for change point detection in temporal
evolving networks. Unlike previous approaches, our method is size agnostic, and
does not require either prior knowledge about the network's size and structure,
nor does it require obtaining historical information or nodal identities over
time. We use both synthetic data derived from dynamic models and two real
datasets: Enron email exchange and Ask-Ubuntu forum. Our framework succeeds
with both precision and recall and outperforms previous solutions
"
"  Deep reinforcement learning (RL) has proven a powerful technique in many
sequential decision making domains. However, Robotics poses many challenges for
RL, most notably training on a physical system can be expensive and dangerous,
which has sparked significant interest in learning control policies using a
physics simulator. While several recent works have shown promising results in
transferring policies trained in simulation to the real world, they often do
not fully utilize the advantage of working with a simulator. In this work, we
exploit the full state observability in the simulator to train better policies
which take as input only partial observations (RGBD images). We do this by
employing an actor-critic training algorithm in which the critic is trained on
full states while the actor (or policy) gets rendered images as input. We show
experimentally on a range of simulated tasks that using these asymmetric inputs
significantly improves performance. Finally, we combine this method with domain
randomization and show real robot experiments for several tasks like picking,
pushing, and moving a block. We achieve this simulation to real world transfer
without training on any real world data.
"
"  We consider a spatial stochastic model of wireless cellular networks, where
the base stations (BSs) are deployed according to a simple and stationary point
process on $\mathbb{R}^d$, $d\ge2$. In this model, we investigate tail
asymptotics of the distribution of signal-to-interference ratio (SIR), which is
a key quantity in wireless communications. In the case where the path-loss
function representing signal attenuation is unbounded at the origin, we derive
the exact tail asymptotics of the SIR distribution under an appropriate
sufficient condition. While we show that widely-used models based on a Poisson
point process and on a determinantal point process meet the sufficient
condition, we also give a counterexample violating it. In the case of bounded
path-loss functions, we derive a logarithmically asymptotic upper bound on the
SIR tail distribution for the Poisson-based and $\alpha$-Ginibre-based models.
A logarithmically asymptotic lower bound with the same order as the upper bound
is also obtained for the Poisson-based model.
"
"  We show the problem of counting homomorphisms from the fundamental group of a
homology $3$-sphere $M$ to a finite, non-abelian simple group $G$ is
#P-complete, in the case that $G$ is fixed and $M$ is the computational input.
Similarly, deciding if there is a non-trivial homomorphism is NP-complete. In
both reductions, we can guarantee that every non-trivial homomorphism is a
surjection. As a corollary, for any fixed integer $m \ge 5$, it is NP-complete
to decide whether $M$ admits a connected $m$-sheeted covering.
Our construction is inspired by universality results in topological quantum
computation. Given a classical reversible circuit $C$, we construct $M$ so that
evaluations of $C$ with certain initialization and finalization conditions
correspond to homomorphisms $\pi_1(M) \to G$. An intermediate state of $C$
likewise corresponds to a homomorphism $\pi_1(\Sigma_g) \to G$, where
$\Sigma_g$ is a pointed Heegaard surface of $M$ of genus $g$. We analyze the
action on these homomorphisms by the pointed mapping class group
$\text{MCG}_*(\Sigma_g)$ and its Torelli subgroup $\text{Tor}_*(\Sigma_g)$. By
results of Dunfield-Thurston, the action of $\text{MCG}_*(\Sigma_g)$ is as
large as possible when $g$ is sufficiently large; we can pass to the Torelli
group using the congruence subgroup property of $\text{Sp}(2g,\mathbb{Z})$. Our
results can be interpreted as a sharp classical universality property of an
associated combinatorial $(2+1)$-dimensional TQFT.
"
"  The use of semi-autonomous and autonomous robotic assistants to aid in care
of the elderly is expected to ease the burden on human caretakers, with
small-stage testing already occurring in a variety of countries. Yet, it is
likely that these robots will need to request human assistance via
teleoperation when domain expertise is needed for a specific task. As
deployment of robotic assistants moves to scale, mapping these requests for
human aid to the teleoperators themselves will be a difficult online
optimization problem. In this paper, we design a system that allocates requests
to a limited number of teleoperators, each with different specialities, in an
online fashion. We generalize a recent model of online job scheduling with a
worst-case competitive-ratio bound to our setting. Next, we design a scalable
machine-learning-based teleoperator-aware task scheduling algorithm and show,
experimentally, that it performs well when compared to an omniscient optimal
scheduling algorithm.
"
"  We start by asking an interesting yet challenging question, ""If an eyewitness
can only recall the eye features of the suspect, such that the forensic artist
can only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig.
1), can advanced computer vision techniques help generate the whole face
image?"" A more generalized question is that if a large proportion (e.g., more
than 50%) of the face/sketch is missing, can a realistic whole face
sketch/image still be estimated. Existing face completion and generation
methods either do not conduct domain transfer learning or can not handle large
missing area. For example, the inpainting approach tends to blur the generated
region when the missing area is large (i.e., more than 50%). In this paper, we
exploit the potential of deep learning networks in filling large missing region
(e.g., as high as 95% missing) and generating realistic faces with
high-fidelity in cross domains. We propose the recursive generation by
bidirectional transformation networks (r-BTN) that recursively generates a
whole face/sketch from a small sketch/face patch. The large missing area and
the cross domain challenge make it difficult to generate satisfactory results
using a unidirectional cross-domain learning structure. On the other hand, a
forward and backward bidirectional learning between the face and sketch domains
would enable recursive estimation of the missing region in an incremental
manner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial
constraint to encourage the generation of realistic faces/sketches. Extensive
experiments have been conducted to demonstrate the superior performance from
r-BTN as compared to existing potential solutions.
"
"  Sleep condition is closely related to an individual's health. Poor sleep
conditions such as sleep disorder and sleep deprivation affect one's daily
performance, and may also cause many chronic diseases. Many efforts have been
devoted to monitoring people's sleep conditions. However, traditional
methodologies require sophisticated equipment and consume a significant amount
of time. In this paper, we attempt to develop a novel way to predict
individual's sleep condition via scrutinizing facial cues as doctors would.
Rather than measuring the sleep condition directly, we measure the
sleep-deprived fatigue which indirectly reflects the sleep condition. Our
method can predict a sleep-deprived fatigue rate based on a selfie provided by
a subject. This rate is used to indicate the sleep condition. To gain deeper
insights of human sleep conditions, we collected around 100,000 faces from
selfies posted on Twitter and Instagram, and identified their age, gender, and
race using automatic algorithms. Next, we investigated the sleep condition
distributions with respect to age, gender, and race. Our study suggests among
the age groups, fatigue percentage of the 0-20 youth and adolescent group is
the highest, implying that poor sleep condition is more prevalent in this age
group. For gender, the fatigue percentage of females is higher than that of
males, implying that more females are suffering from sleep issues than males.
Among ethnic groups, the fatigue percentage in Caucasian is the highest
followed by Asian and African American.
"
"  In the context of music production, distortion effects are mainly used for
aesthetic reasons and are usually applied to electric musical instruments. Most
existing methods for nonlinear modeling are often either simplified or
optimized to a very specific circuit. In this work, we investigate deep
learning architectures for audio processing and we aim to find a general
purpose end-to-end deep neural network to perform modeling of nonlinear audio
effects. We show the network modeling various nonlinearities and we discuss the
generalization capabilities among different instruments.
"
"  In this paper, we consider adaptive decision-making problems for stochastic
state estimation with partial observations. First, we introduce the concept of
weak adaptive submodularity, a generalization of adaptive submodularity, which
has found great success in solving challenging adaptive state estimation
problems. Then, for the problem of active diagnosis, i.e., discrete state
estimation via active sensing, we show that an adaptive greedy policy has a
near-optimal performance guarantee when the reward function possesses this
property. We further show that the reward function for group-based active
diagnosis, which arises in applications such as medical diagnosis and state
estimation with persistent sensor faults, is also weakly adaptive submodular.
Finally, in experiments of state estimation for an aircraft electrical system
with persistent sensor faults, we observe that an adaptive greedy policy
performs equally well as an exhaustive search.
"
"  Many machine learning problems can be formulated as consensus optimization
problems which can be solved efficiently via a cooperative multi-agent system.
However, the agents in the system can be unreliable due to a variety of
reasons: noise, faults and attacks. Providing erroneous updates leads the
optimization process in a wrong direction, and degrades the performance of
distributed machine learning algorithms. This paper considers the problem of
decentralized learning using ADMM in the presence of unreliable agents. First,
we rigorously analyze the effect of erroneous updates (in ADMM learning
iterations) on the convergence behavior of multi-agent system. We show that the
algorithm linearly converges to a neighborhood of the optimal solution under
certain conditions and characterize the neighborhood size analytically. Next,
we provide guidelines for network design to achieve a faster convergence. We
also provide conditions on the erroneous updates for exact convergence to the
optimal solution. Finally, to mitigate the influence of unreliable agents, we
propose \textsf{ROAD}, a robust variant of ADMM, and show its resilience to
unreliable agents with an exact convergence to the optimum.
"
"  Estimating the Domain of Attraction (DA) of non-polynomial systems is a
challenging problem. Taylor expansion is widely adopted for transforming a
nonlinear analytic function into a polynomial function, but the performance of
Taylor expansion is not always satisfactory. This paper provides solvable ways
for estimating the DA via Chebyshev approximation. Firstly, for Chebyshev
approximation without the remainder, higher order derivatives of Lyapunov
functions are used for estimating the DA, and the largest estimate is obtained
by solving a generalized eigenvalue problem. Moreover, for Chebyshev
approximation with the remainder, an uncertain polynomial system is
reformulated, and a condition is proposed for ensuring the convergence to the
largest estimate with a selected Lyapunov function. Numerical examples
demonstrate that both accuracy and efficiency are improved compared to Taylor
approximation.
"
"  Sparse additive modeling is a class of effective methods for performing
high-dimensional nonparametric regression. In this work we show how shape
constraints such as convexity/concavity and their extensions, can be integrated
into additive models. The proposed sparse difference of convex additive models
(SDCAM) can estimate most continuous functions without any a priori smoothness
assumption. Motivated by a characterization of difference of convex functions,
our method incorporates a natural regularization functional to avoid
overfitting and to reduce model complexity. Computationally, we develop an
efficient backfitting algorithm with linear per-iteration complexity.
Experiments on both synthetic and real data verify that our method is
competitive against state-of-the-art sparse additive models, with improved
performance in most scenarios.
"
"  We present a performance analysis appropriate for comparing algorithms using
different numerical discretizations. By taking into account the total
time-to-solution, numerical accuracy with respect to an error norm, and the
computation rate, a cost-benefit analysis can be performed to determine which
algorithm and discretization are particularly suited for an application. This
work extends the performance spectrum model in Chang et. al. 2017 for
interpretation of hardware and algorithmic tradeoffs in numerical PDE
simulation. As a proof-of-concept, popular finite element software packages are
used to illustrate this analysis for Poisson's equation.
"
"  Recent advances have enabled 3d object reconstruction approaches using a
single off-the-shelf RGB-D camera. Although these approaches are successful for
a wide range of object classes, they rely on stable and distinctive geometric
or texture features. Many objects like mechanical parts, toys, household or
decorative articles, however, are textureless and characterized by minimalistic
shapes that are simple and symmetric. Existing in-hand scanning systems and 3d
reconstruction techniques fail for such symmetric objects in the absence of
highly distinctive features. In this work, we show that extracting 3d hand
motion for in-hand scanning effectively facilitates the reconstruction of even
featureless and highly symmetric objects and we present an approach that fuses
the rich additional information of hands into a 3d reconstruction pipeline,
significantly contributing to the state-of-the-art of in-hand scanning.
"
"  There has been a long standing interest in understanding `Social Influence'
both in Social Sciences and in Computational Linguistics. In this paper, we
present a novel approach to study and measure interpersonal influence in daily
interactions. Motivated by the basic principles of influence, we attempt to
identify indicative linguistic features of the posts in an online knitting
community. We present the scheme used to operationalize and label the posts
with indicator features. Experiments with the identified features show an
improvement in the classification accuracy of influence by 3.15%. Our results
illustrate the important correlation between the characteristics of the
language and its potential to influence others.
"
"  With a core-periphery structure of networks, core nodes are densely
interconnected, peripheral nodes are connected to core nodes to different
extents, and peripheral nodes are sparsely interconnected. Core-periphery
structure composed of a single core and periphery has been identified for
various networks. However, analogous to the observation that many empirical
networks are composed of densely interconnected groups of nodes, i.e.,
communities, a network may be better regarded as a collection of multiple cores
and peripheries. We propose a scalable algorithm to detect multiple
non-overlapping groups of core-periphery structure in a network. We illustrate
our algorithm using synthesised and empirical networks. For example, we find
distinct core-periphery pairs with different political leanings in a network of
political blogs and separation between international and domestic subnetworks
of airports in some single countries in a world-wide airport network.
"
"  In visual exploration and analysis of data, determining how to select and
transform the data for visualization is a challenge for data-unfamiliar or
inexperienced users. Our main hypothesis is that for many data sets and common
analysis tasks, there are relatively few ""data slices"" that result in effective
visualizations. By focusing human users on appropriate and suitably transformed
parts of the underlying data sets, these data slices can help the users carry
their task to correct completion.
To verify this hypothesis, we develop a framework that permits us to capture
exemplary data slices for a user task, and to explore and parse
visual-exploration sequences into a format that makes them distinct and easy to
compare. We develop a recommendation system, DataSlicer, that matches a
""currently viewed"" data slice with the most promising ""next effective"" data
slices for the given exploration task. We report the results of controlled
experiments with an implementation of the DataSlicer system, using four common
analytical task types. The experiments demonstrate statistically significant
improvements in accuracy and exploration speed versus users without access to
our system.
"
"  The construction of permutation trinomials over finite fields attracts
people's interest recently due to their simple form and some additional
properties. Motivated by some results on the construction of permutation
trinomials with Niho exponents, by constructing some new fractional polynomials
that permute the set of the $(q+1)$-th roots of unity in $\mathbb F_{q^2}$, we
present several classes of permutation trinomials with Niho exponents over
$\mathbb F_{q^2}$, where $q=5^k$.
"
"  Image semantic segmentation is more and more being of interest for computer
vision and machine learning researchers. Many applications on the rise need
accurate and efficient segmentation mechanisms: autonomous driving, indoor
navigation, and even virtual or augmented reality systems to name a few. This
demand coincides with the rise of deep learning approaches in almost every
field or application target related to computer vision, including semantic
segmentation or scene understanding. This paper provides a review on deep
learning methods for semantic segmentation applied to various application
areas. Firstly, we describe the terminology of this field as well as mandatory
background concepts. Next, the main datasets and challenges are exposed to help
researchers decide which are the ones that best suit their needs and their
targets. Then, existing methods are reviewed, highlighting their contributions
and their significance in the field. Finally, quantitative results are given
for the described methods and the datasets in which they were evaluated,
following up with a discussion of the results. At last, we point out a set of
promising future works and draw our own conclusions about the state of the art
of semantic segmentation using deep learning techniques.
"
"  While all kinds of mixed data -from personal data, over panel and scientific
data, to public and commercial data- are collected and stored, building
probabilistic graphical models for these hybrid domains becomes more difficult.
Users spend significant amounts of time in identifying the parametric form of
the random variables (Gaussian, Poisson, Logit, etc.) involved and learning the
mixed models. To make this difficult task easier, we propose the first
trainable probabilistic deep architecture for hybrid domains that features
tractable queries. It is based on Sum-Product Networks (SPNs) with piecewise
polynomial leave distributions together with novel nonparametric decomposition
and conditioning steps using the Hirschfeld-Gebelein-Rényi Maximum
Correlation Coefficient. This relieves the user from deciding a-priori the
parametric form of the random variables but is still expressive enough to
effectively approximate any continuous distribution and permits efficient
learning and inference. Our empirical evidence shows that the architecture,
called Mixed SPNs, can indeed capture complex distributions across a wide range
of hybrid domains.
"
"  Incremental improvements in accuracy of Convolutional Neural Networks are
usually achieved through use of deeper and more complex models trained on
larger datasets. However, enlarging dataset and models increases the
computation and storage costs and cannot be done indefinitely. In this work, we
seek to improve the identification and verification accuracy of a
text-independent speaker recognition system without use of extra data or deeper
and more complex models by augmenting the training and testing data, finding
the optimal dimensionality of embedding space and use of more discriminative
loss functions. Results of experiments on VoxCeleb dataset suggest that: (i)
Simple repetition and random time-reversion of utterances can reduce prediction
errors by up to 18%. (ii) Lower dimensional embeddings are more suitable for
verification. (iii) Use of proposed logistic margin loss function leads to
unified embeddings with state-of-the-art identification and competitive
verification accuracies.
"
"  This paper describes a method for learning low-dimensional approximations of
nonlinear dynamical systems, based on neural-network approximations of the
underlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD)
provides a useful data-driven approximation of the Koopman operator for
analyzing dynamical systems. This paper addresses a fundamental problem
associated with EDMD: a trade-off between representational capacity of the
dictionary and over-fitting due to insufficient data. A new neural network
architecture combining an autoencoder with linear recurrent dynamics in the
encoded state is used to learn a low-dimensional and highly informative
Koopman-invariant subspace of observables. A method is also presented for
balanced model reduction of over-specified EDMD systems in feature space.
Nonlinear reconstruction using partially linear multi-kernel regression aims to
improve reconstruction accuracy from the low-dimensional state when the data
has complex but intrinsically low-dimensional structure. The techniques
demonstrate the ability to identify Koopman eigenfunctions of the unforced
Duffing equation, create accurate low-dimensional models of an unstable
cylinder wake flow, and make short-time predictions of the chaotic
Kuramoto-Sivashinsky equation.
"
"  In the first part of this paper we present a formalization in Agda of the
James construction in homotopy type theory. We include several fragments of
code to show what the Agda code looks like, and we explain several techniques
that we used in the formalization. In the second part, we use the James
construction to give a constructive proof that $\pi_4(\mathbb{S}^3)$ is of the
form $\mathbb{Z}/n\mathbb{Z}$ (but we do not compute the $n$ here).
"
"  We study the problem of finding the maximum of a function defined on the
nodes of a connected graph. The goal is to identify a node where the function
obtains its maximum. We focus on local iterative algorithms, which traverse the
nodes of the graph along a path, and the next iterate is chosen from the
neighbors of the current iterate with probability distribution determined by
the function values at the current iterate and its neighbors. We study two
algorithms corresponding to a Metropolis-Hastings random walk with different
transition kernels: (i) The first algorithm is an exponentially weighted random
walk governed by a parameter $\gamma$. (ii) The second algorithm is defined
with respect to the graph Laplacian and a smoothness parameter $k$. We derive
convergence rates for the two algorithms in terms of total variation distance
and hitting times. We also provide simulations showing the relative convergence
rates of our algorithms in comparison to an unbiased random walk, as a function
of the smoothness of the graph function. Our algorithms may be categorized as a
new class of ""descent-based"" methods for function maximization on the nodes of
a graph.
"
"  The models of collective decision-making considered in this paper are
nonlinear interconnected cooperative systems with saturating interactions.
These systems encode the possible outcomes of a decision process into different
steady states of the dynamics. In particular, they are characterized by two
main attractors in the positive and negative orthant, representing two choices
of agreement among the agents, associated to the Perron-Frobenius eigenvector
of the system. In this paper we give conditions for the appearance of other
equilibria of mixed sign. The conditions are inspired by Perron-Frobenius
theory and are related to the algebraic connectivity of the network. We also
show how all these equilibria must be contained in a solid disk of radius given
by the norm of the equilibrium point which is located in the positive orthant.
"
"  This paper focuses on the recognition of Activities of Daily Living (ADL)
applying pattern recognition techniques to the data acquired by the
accelerometer available in the mobile devices. The recognition of ADL is
composed by several stages, including data acquisition, data processing, and
artificial intelligence methods. The artificial intelligence methods used are
related to pattern recognition, and this study focuses on the use of Artificial
Neural Networks (ANN). The data processing includes data cleaning, and the
feature extraction techniques to define the inputs for the ANN. Due to the low
processing power and memory of the mobile devices, they should be mainly used
to acquire the data, applying an ANN previously trained for the identification
of the ADL. The main purpose of this paper is to present a new method
implemented with ANN for the identification of a defined set of ADL with a
reliable accuracy. This paper also presents a comparison of different types of
ANN in order to choose the type for the implementation of the final method.
Results of this research probes that the best accuracies are achieved with Deep
Learning techniques with an accuracy higher than 80%.
"
"  In this work, we aim to explore connections between dynamical systems
techniques and combinatorial optimization problems. In particular, we construct
heuristic approaches for the traveling salesman problem (TSP) based on
embedding the relaxed discrete optimization problem into appropriate manifolds.
We explore multiple embedding techniques -- namely, the construction of new
dynamical systems on the manifold of orthogonal matrices and associated
Procrustes approximations of the TSP cost function. Using these dynamical
systems, we analyze the local neighborhood around the optimal TSP solutions
(which are equilibria) using computations to approximate the associated
\emph{stable manifolds}. We find that these flows frequently converge to
undesirable equilibria. However, the solutions of the dynamical systems and the
associated Procrustes approximation provide an interesting biasing approach for
the popular Lin--Kernighan heuristic which yields fast convergence. The
Lin--Kernighan heuristic is typically based on the computation of edges that
have a `high probability' of being in the shortest tour, thereby effectively
pruning the search space. Our new approach, instead, relies on a natural
relaxation of the combinatorial optimization problem to the manifold of
orthogonal matrices and the subsequent use of this solution to bias the
Lin--Kernighan heuristic. Although the initial cost of computing these edges
using the Procrustes solution is higher than existing methods, we find that the
Procrustes solution, when coupled with a homotopy computation, contains
valuable information regarding the optimal edges. We explore the Procrustes
based approach on several TSP instances and find that our approach often
requires fewer $k$-opt moves than existing approaches. Broadly, we hope that
this work initiates more work in the intersection of dynamical systems theory
and combinatorial optimization.
"
"  Two fundamental approaches to information averaging are based on linear and
logarithmic combination, yielding the arithmetic average (AA) and geometric
average (GA) of the fusing initials, respectively. In the context of target
tracking, the two most common formats of data to be fused are random variables
and probability density functions, namely $v$-fusion and $f$-fusion,
respectively. In this work, we analyze and compare the second order statistics
(including variance and mean square error) of AA and GA in terms of both
$v$-fusion and $f$-fusion. The case of weighted Gaussian mixtures representing
multitarget densities in the presence of false alarms and misdetection (whose
weight sums are not necessarily unit) is also considered, the result of which
appears significantly different from that for a single target. In addition to
exact derivation, exemplifying analysis and illustrations are provided.
"
"  The pigeonhole principle states that if n items are contained in m boxes,
then at least one box has no more than n/m items. It is utilized to solve many
data management problems, especially for thresholded similarity searches.
Despite many pigeonhole principle-based solutions proposed in the last few
decades, the condition stated by the principle is weak. It only constrains the
number of items in a single box. By organizing the boxes in a ring, we propose
a new principle, called the pigeonring principle, which constrains the number
of items in multiple boxes and yields stronger conditions.
To utilize the new principle, we focus on problems defined in the form of
identifying data objects whose similarities or distances to the query is
constrained by a threshold. Many solutions to these problems utilize the
pigeonhole principle to find candidates that satisfy a filtering condition. By
the new principle, stronger filtering conditions can be established. We show
that the pigeonhole principle is a special case of the new principle. This
suggests that all the pigeonhole principle-based solutions are possible to be
accelerated by the new principle. A universal filtering framework is introduced
to encompass the solutions to these problems based on the new principle.
Besides, we discuss how to quickly find candidates specified by the new
principle. The implementation requires only minor modifications on top of
existing pigeonhole principle-based algorithms. Experimental results on real
datasets demonstrate the applicability of the new principle as well as the
superior performance of the algorithms based on the new principle.
"
"  We present a memristive device based R$ ^3 $PUF construction achieving highly
desired PUF properties, which are not offered by most current PUF designs: (1)
High reliability, almost 100\% that is crucial for PUF-based cryptographic key
generations, significantly reducing, or even eliminating the expensive overhead
of on-chip error correction logic and the associated helper on-chip data
storage or off-chip storage and transfer. (2) Reconfigurability, while current
PUF designs rarely exhibit such an attractive property. We validate our R$ ^3
$PUF via extensive Monte-Carlo simulations in Cadence based on parameters of
real devices. The R$ ^3 $PUF is simple, cost-effective and easy to manage
compared to other PUF constructions exhibiting high reliability or
reconfigurability. None of previous PUF constructions is able to provide both
desired high reliability and reconfigurability concurrently.
"
"  In this paper, we demonstrate the application of Fuzzy Markup Language (FML)
to construct an FML-based Dynamic Assessment Agent (FDAA), and we present an
FML-based Human-Machine Cooperative System (FHMCS) for the game of Go. The
proposed FDAA comprises an intelligent decision-making and learning mechanism,
an intelligent game bot, a proximal development agent, and an intelligent
agent. The intelligent game bot is based on the open-source code of Facebook
Darkforest, and it features a representational state transfer application
programming interface mechanism. The proximal development agent contains a
dynamic assessment mechanism, a GoSocket mechanism, and an FML engine with a
fuzzy knowledge base and rule base. The intelligent agent contains a GoSocket
engine and a summarization agent that is based on the estimated win rate,
real-time simulation number, and matching degree of predicted moves.
Additionally, the FML for player performance evaluation and linguistic
descriptions for game results commentary are presented. We experimentally
verify and validate the performance of the FDAA and variants of the FHMCS by
testing five games in 2016 and 60 games of Google Master Go, a new version of
the AlphaGo program, in January 2017. The experimental results demonstrate that
the proposed FDAA can work effectively for Go applications.
"
"  Finding central nodes is a fundamental problem in network analysis.
Betweenness centrality is a well-known measure which quantifies the importance
of a node based on the fraction of shortest paths going though it. Due to the
dynamic nature of many today's networks, algorithms that quickly update
centrality scores have become a necessity. For betweenness, several dynamic
algorithms have been proposed over the years, targeting different update types
(incremental- and decremental-only, fully-dynamic). In this paper we introduce
a new dynamic algorithm for updating betweenness centrality after an edge
insertion or an edge weight decrease. Our method is a combination of two
independent contributions: a faster algorithm for updating pairwise distances
as well as number of shortest paths, and a faster algorithm for updating
dependencies. Whereas the worst-case running time of our algorithm is the same
as recomputation, our techniques considerably reduce the number of operations
performed by existing dynamic betweenness algorithms.
"
"  Reinforcement learning (RL) makes it possible to train agents capable of
achiev- ing sophisticated goals in complex and uncertain environments. A key
difficulty in reinforcement learning is specifying a reward function for the
agent to optimize. Traditionally, imitation learning in RL has been used to
overcome this problem. Unfortunately, hitherto imitation learning methods tend
to require that demonstra- tions are supplied in the first-person: the agent is
provided with a sequence of states and a specification of the actions that it
should have taken. While powerful, this kind of imitation learning is limited
by the relatively hard problem of collect- ing first-person demonstrations.
Humans address this problem by learning from third-person demonstrations: they
observe other humans perform tasks, infer the task, and accomplish the same
task themselves.
In this paper, we present a method for unsupervised third-person imitation
learn- ing. Here third-person refers to training an agent to correctly achieve
a simple goal in a simple environment when it is provided a demonstration of a
teacher achieving the same goal but from a different viewpoint; and
unsupervised refers to the fact that the agent receives only these third-person
demonstrations, and is not provided a correspondence between teacher states and
student states. Our methods primary insight is that recent advances from domain
confusion can be utilized to yield domain agnostic features which are crucial
during the training process. To validate our approach, we report successful
experiments on learning from third-person demonstrations in a pointmass domain,
a reacher domain, and inverted pendulum.
"
"  Drafting strong players is crucial for the team success. We describe a new
data-driven interpretable approach for assessing draft prospects in the
National Hockey League. Successful previous approaches have built a predictive
model based on player features, or derived performance predictions from the
observed performance of comparable players in a cohort. This paper develops
model tree learning, which incorporates strengths of both model-based and
cohort-based approaches. A model tree partitions the feature space according to
the values of discrete features, or learned thresholds for continuous features.
Each leaf node in the tree defines a group of players, easily described to
hockey experts, with its own group regression model. Compared to a single
model, the model tree forms an ensemble that increases predictive power.
Compared to cohort-based approaches, the groups of comparables are discovered
from the data, without requiring a similarity metric. The performance
predictions of the model tree are competitive with the state-of-the-art
methods, which validates our model empirically. We show in case studies that
the model tree player ranking can be used to highlight strong and weak points
of players.
"
"  A key part of implementing high-level languages is providing built-in and
default data structures. Yet selecting good defaults is hard. A mutable data
structure's workload is not known in advance, and it may shift over its
lifetime - e.g., between read-heavy and write-heavy, or from heavy contention
by multiple threads to single-threaded or low-frequency use. One idea is to
switch implementations adaptively, but it is nontrivial to switch the
implementation of a concurrent data structure at runtime. Performing the
transition requires a concurrent snapshot of data structure contents, which
normally demands special engineering in the data structure's design. However,
in this paper we identify and formalize an relevant property of lock-free
algorithms. Namely, lock-freedom is sufficient to guarantee that freezing
memory locations in an arbitrary order will result in a valid snapshot. Several
functional languages have data structures that freeze and thaw, transitioning
between mutable and immutable, such as Haskell vectors and Clojure transients,
but these enable only single-threaded writers. We generalize this approach to
augment an arbitrary lock-free data structure with the ability to gradually
freeze and optionally transition to a new representation. This augmentation
doesn't require changing the algorithm or code for the data structure, only
replacing its datatype for mutable references with a freezable variant. In this
paper, we present an algorithm for lifting plain to adaptive data and prove
that the resulting hybrid data structure is itself lock-free, linearizable, and
simulates the original. We also perform an empirical case study in the context
of heating up and cooling down concurrent maps.
"
"  For the past 5 years, the ILSVRC competition and the ImageNet dataset have
attracted a lot of interest from the Computer Vision community, allowing for
state-of-the-art accuracy to grow tremendously. This should be credited to the
use of deep artificial neural network designs. As these became more complex,
the storage, bandwidth, and compute requirements increased. This means that
with a non-distributed approach, even when using the most high-density server
available, the training process may take weeks, making it prohibitive.
Furthermore, as datasets grow, the representation learning potential of deep
networks grows as well by using more complex models. This synchronicity
triggers a sharp increase in the computational requirements and motivates us to
explore the scaling behaviour on petaflop scale supercomputers. In this paper
we will describe the challenges and novel solutions needed in order to train
ResNet-50 in this large scale environment. We demonstrate above 90\% scaling
efficiency and a training time of 28 minutes using up to 104K x86 cores. This
is supported by software tools from Intel's ecosystem. Moreover, we show that
with regular 90 - 120 epoch train runs we can achieve a top-1 accuracy as high
as 77\% for the unmodified ResNet-50 topology. We also introduce the novel
Collapsed Ensemble (CE) technique that allows us to obtain a 77.5\% top-1
accuracy, similar to that of a ResNet-152, while training a unmodified
ResNet-50 topology for the same fixed training budget. All ResNet-50 models as
well as the scripts needed to replicate them will be posted shortly.
"
"  Suffix trees have recently become very successful data structures in handling
large data sequences such as DNA or Protein sequences. Consequently parallel
architectures have become ubiquitous. We present a novel alphabet-dependent
parallel algorithm which attempts to take advantage of the perverseness of the
multicore architecture. Microsatellites are important for their biological
relevance hence our algorithm is based on time efficient construction for
identification of such. We experimentally achieved up to 15x speedup over the
sequential algorithm on different input sizes of biological sequences.
"
"  Understanding the interaction between the valves and walls of the heart is
important in assessing and subsequently treating heart dysfunction. With
advancements in cardiac imaging, nonlinear mechanics and computational
techniques, it is now possible to explore the mechanics of valve-heart
interactions using anatomically and physiologically realistic models. This
study presents an integrated model of the mitral valve (MV) coupled to the left
ventricle (LV), with the geometry derived from in vivo clinical magnetic
resonance images. Numerical simulations using this coupled MV-LV model are
developed using an immersed boundary/finite element method. The model
incorporates detailed valvular features, left ventricular contraction,
nonlinear soft tissue mechanics, and fluid-mediated interactions between the MV
and LV wall. We use the model to simulate the cardiac function from diastole to
systole, and investigate how myocardial active relaxation function affects the
LV pump function. The results of the new model agree with in vivo measurements,
and demonstrate that the diastolic filling pressure increases significantly
with impaired myocardial active relaxation to maintain the normal cardiac
output. The coupled model has the potential to advance fundamental knowledge of
mechanisms underlying MV-LV interaction, and help in risk stratification and
optimization of therapies for heart diseases.
"
"  In deep learning, \textit{depth}, as well as \textit{nonlinearity}, create
non-convex loss surfaces. Then, does depth alone create bad local minima? In
this paper, we prove that without nonlinearity, depth alone does not create bad
local minima, although it induces non-convex loss surface. Using this insight,
we greatly simplify a recently proposed proof to show that all of the local
minima of feedforward deep linear neural networks are global minima. Our
theoretical results generalize previous results with fewer assumptions, and
this analysis provides a method to show similar results beyond square loss in
deep linear models.
"
"  In recent years, bullying and aggression against users on social media have
grown significantly, causing serious consequences to victims of all
demographics. In particular, cyberbullying affects more than half of young
social media users worldwide, and has also led to teenage suicides, prompted by
prolonged and/or coordinated digital harassment. Nonetheless, tools and
technologies for understanding and mitigating it are scarce and mostly
ineffective. In this paper, we present a principled and scalable approach to
detect bullying and aggressive behavior on Twitter. We propose a robust
methodology for extracting text, user, and network-based attributes, studying
the properties of cyberbullies and aggressors, and what features distinguish
them from regular users. We find that bully users post less, participate in
fewer online communities, and are less popular than normal users, while
aggressors are quite popular and tend to include more negativity in their
posts. We evaluate our methodology using a corpus of 1.6M tweets posted over 3
months, and show that machine learning classification algorithms can accurately
detect users exhibiting bullying and aggressive behavior, achieving over 90%
AUC.
"
"  Robots are typically not created with security as a main concern. Contrasting
to typical IT systems, cyberphysical systems rely on security to handle safety
aspects. In light of the former, classic scoring methods such as the Common
Vulnerability Scoring System (CVSS) are not able to accurately capture the
severity of robot vulnerabilities. The present research work focuses upon
creating an open and free to access Robot Vulnerability Scoring System (RVSS)
that considers major relevant issues in robotics including a) robot safety
aspects, b) assessment of downstream implications of a given vulnerability, c)
library and third-party scoring assessments and d) environmental variables,
such as time since vulnerability disclosure or exposure on the web. Finally, an
experimental evaluation of RVSS with contrast to CVSS is provided and discussed
with focus on the robotics security landscape.
"
"  This paper considers a general data-fitting problem over a networked system,
in which many computing nodes are connected by an undirected graph. This kind
of problem can find many real-world applications and has been studied
extensively in the literature. However, existing solutions either need a
central controller for information sharing or requires slot synchronization
among different nodes, which increases the difficulty of practical
implementations, especially for a very large and heterogeneous system.
As a contrast, in this paper, we treat the data-fitting problem over the
network as a stochastic programming problem with many constraints. By adapting
the results in a recent paper, we design a fully distributed and asynchronized
stochastic gradient descent (SGD) algorithm. We show that our algorithm can
achieve global optimality and consensus asymptotically by only local
computations and communications. Additionally, we provide a sharp lower bound
for the convergence speed in the regular graph case. This result fits the
intuition and provides guidance to design a `good' network topology to speed up
the convergence. Also, the merit of our design is validated by experiments on
both synthetic and real-world datasets.
"
"  We develop differentially private hypothesis testing methods for the small
sample regime. Given a sample $\cal D$ from a categorical distribution $p$ over
some domain $\Sigma$, an explicitly described distribution $q$ over $\Sigma$,
some privacy parameter $\varepsilon$, accuracy parameter $\alpha$, and
requirements $\beta_{\rm I}$ and $\beta_{\rm II}$ for the type I and type II
errors of our test, the goal is to distinguish between $p=q$ and
$d_{\rm{TV}}(p,q) \geq \alpha$.
We provide theoretical bounds for the sample size $|{\cal D}|$ so that our
method both satisfies $(\varepsilon,0)$-differential privacy, and guarantees
$\beta_{\rm I}$ and $\beta_{\rm II}$ type I and type II errors. We show that
differential privacy may come for free in some regimes of parameters, and we
always beat the sample complexity resulting from running the $\chi^2$-test with
noisy counts, or standard approaches such as repetition for endowing
non-private $\chi^2$-style statistics with differential privacy guarantees. We
experimentally compare the sample complexity of our method to that of recently
proposed methods for private hypothesis testing.
"
"  We provide, to the best of our knowledge, the first computational study of
extensive-form adversarial team games. These games are sequential, zero-sum
games in which a team of players, sharing the same utility function, faces an
adversary. We define three different scenarios according to the communication
capabilities of the team. In the first, the teammates can communicate and
correlate their actions both before and during the play. In the second, they
can only communicate before the play. In the third, no communication is
possible at all. We define the most suitable solution concepts, and we study
the inefficiency caused by partial or null communication, showing that the
inefficiency can be arbitrarily large in the size of the game tree.
Furthermore, we study the computational complexity of the equilibrium-finding
problem in the three scenarios mentioned above, and we provide, for each of the
three scenarios, an exact algorithm. Finally, we empirically evaluate the
scalability of the algorithms in random games and the inefficiency caused by
partial or null communication.
"
"  When designing control strategies for differential-drive mobile robots, one
standard tool is the consideration of a point at a fixed distance along a line
orthogonal to the wheel axis instead of the full pose of the vehicle. This
abstraction supports replacing the non-holonomic, three-state unicycle model
with a much simpler two-state single-integrator model (i.e., a
velocity-controlled point). Yet this transformation comes at a performance
cost, through the robot's precision and maneuverability. This work contains
derivations for expressions of these precision and maneuverability costs in
terms of the transformation's parameters. Furthermore, these costs show that
only selecting the parameter once over the course of an application may cause
an undue loss of precision. Model Predictive Control (MPC) represents one such
method to ameliorate this condition. However, MPC typically realizes a control
signal, rather than a parameter, so this work also proposes a Parametric Model
Predictive Control (PMPC) method for parameter and sampling horizon
optimization. Experimental results are presented that demonstrate the effects
of the parameterization on the deployment of algorithms developed for the
single-integrator model on actual differential-drive mobile robots.
"
"  Although various norms for reciprocity-based cooperation have been suggested
that are evolutionarily stable against invasion from free riders, the process
of alternation of norms and the role of diversified norms remain unclear in the
evolution of cooperation. We clarify the co-evolutionary dynamics of norms and
cooperation in indirect reciprocity and also identify the indispensable norms
for the evolution of cooperation. Inspired by the gene knockout method, a
genetic engineering technique, we developed the norm knockout method and
clarified the norms necessary for the establishment of cooperation. The results
of numerical investigations revealed that the majority of norms gradually
transitioned to tolerant norms after defectors are eliminated by strict norms.
Furthermore, no cooperation emerges when specific norms that are intolerant to
defectors are knocked out.
"
"  In the online multiple testing problem, p-values corresponding to different
null hypotheses are observed one by one, and the decision of whether or not to
reject the current hypothesis must be made immediately, after which the next
p-value is observed. Alpha-investing algorithms to control the false discovery
rate (FDR), formulated by Foster and Stine, have been generalized and applied
to many settings, including quality-preserving databases in science and
multiple A/B or multi-armed bandit tests for internet commerce. This paper
improves the class of generalized alpha-investing algorithms (GAI) in four
ways: (a) we show how to uniformly improve the power of the entire class of
monotone GAI procedures by awarding more alpha-wealth for each rejection,
giving a win-win resolution to a recent dilemma raised by Javanmard and
Montanari, (b) we demonstrate how to incorporate prior weights to indicate
domain knowledge of which hypotheses are likely to be non-null, (c) we allow
for differing penalties for false discoveries to indicate that some hypotheses
may be more important than others, (d) we define a new quantity called the
decaying memory false discovery rate (mem-FDR) that may be more meaningful for
truly temporal applications, and which alleviates problems that we describe and
refer to as ""piggybacking"" and ""alpha-death"". Our GAI++ algorithms incorporate
all four generalizations simultaneously, and reduce to more powerful variants
of earlier algorithms when the weights and decay are all set to unity. Finally,
we also describe a simple method to derive new online FDR rules based on an
estimated false discovery proportion.
"
"  Most of the current game-theoretic demand-side management methods focus
primarily on the scheduling of home appliances, and the related numerical
experiments are analyzed under various scenarios to achieve the corresponding
Nash-equilibrium (NE) and optimal results. However, not much work is conducted
for academic or commercial buildings. The methods for optimizing
academic-buildings are distinct from the optimal methods for home appliances.
In my study, we address a novel methodology to control the operation of
heating, ventilation, and air conditioning system (HVAC). With the development
of Artificial Intelligence and computer technologies, reinforcement learning
(RL) can be implemented in multiple realistic scenarios and help people to
solve thousands of real-world problems. Reinforcement Learning, which is
considered as the art of future AI, builds the bridge between agents and
environments through Markov Decision Chain or Neural Network and has seldom
been used in power system. The art of RL is that once the simulator for a
specific environment is built, the algorithm can keep learning from the
environment. Therefore, RL is capable of dealing with constantly changing
simulator inputs such as power demand, the condition of power system and
outdoor temperature, etc. Compared with the existing distribution power system
planning mechanisms and the related game theoretical methodologies, our
proposed algorithm can plan and optimize the hourly energy usage, and have the
ability to corporate with even shorter time window if needed.
"
"  This paper explores an interesting new dimension to the challenging problem
of predicting long-term scientific impact (LTSI) usually measured by the number
of citations accumulated by a paper in the long-term. It is well known that
early citations (within 1-2 years after publication) acquired by a paper
positively affects its LTSI. However, there is no work that investigates if the
set of authors who bring in these early citations to a paper also affect its
LTSI. In this paper, we demonstrate for the first time, the impact of these
authors whom we call early citers (EC) on the LTSI of a paper. Note that this
study of the complex dynamics of EC introduces a brand new paradigm in citation
behavior analysis. Using a massive computer science bibliographic dataset we
identify two distinct categories of EC - we call those authors who have high
overall publication/citation count in the dataset as influential and the rest
of the authors as non-influential. We investigate three characteristic
properties of EC and present an extensive analysis of how each category
correlates with LTSI in terms of these properties. In contrast to popular
perception, we find that influential EC negatively affects LTSI possibly owing
to attention stealing. To motivate this, we present several representative
examples from the dataset. A closer inspection of the collaboration network
reveals that this stealing effect is more profound if an EC is nearer to the
authors of the paper being investigated. As an intuitive use case, we show that
incorporating EC properties in the state-of-the-art supervised citation
prediction models leads to high performance margins. At the closing, we present
an online portal to visualize EC statistics along with the prediction results
for a given query paper.
"
"  The segmentation of animals from camera-trap images is a difficult task. To
illustrate, there are various challenges due to environmental conditions and
hardware limitation in these images. We proposed a multi-layer robust principal
component analysis (multi-layer RPCA) approach for background subtraction. Our
method computes sparse and low-rank images from a weighted sum of descriptors,
using color and texture features as case of study for camera-trap images
segmentation. The segmentation algorithm is composed of histogram equalization
or Gaussian filtering as pre-processing, and morphological filters with active
contour as post-processing. The parameters of our multi-layer RPCA were
optimized with an exhaustive search. The database consists of camera-trap
images from the Colombian forest taken by the Instituto de Investigación de
Recursos Biológicos Alexander von Humboldt. We analyzed the performance of
our method in inherent and therefore challenging situations of camera-trap
images. Furthermore, we compared our method with some state-of-the-art
algorithms of background subtraction, where our multi-layer RPCA outperformed
these other methods. Our multi-layer RPCA reached 76.17 and 69.97% of average
fine-grained F-measure for color and infrared sequences, respectively. To our
best knowledge, this paper is the first work proposing multi-layer RPCA and
using it for camera-trap images segmentation.
"
"  We show how any party can encrypt data for an e-passport holder such that
only with physical possession of the e-passport decryption is possible. The
same is possible for electronic identity cards and driver licenses. We also
indicate possible applications. Dutch passports allow for 160 bit security,
theoretically giving sufficient security beyond the year 2079, exceeding
current good practice of 128 bit security. We also introduce the notion of RDE
Extraction PIN which effectively provides the same security as a regular PIN.
Our results ironically suggest that carrying a passport when traveling abroad
might violate export or import laws on strong cryptography.
"
"  Given an input sound signal and a target virtual sound source, sound
spatialisation algorithms manipulate the signal so that a listener perceives it
as though it were emitted from the target source. There exist several
established spatialisation approaches that deliver satisfactory results when
loudspeakers are used to playback the manipulated signal. As headphones have a
number of desirable characteristics over loudspeakers, such as portability,
isolation from the surrounding environment, cost and ease of use, it is
interesting to explore how a sense of acoustic space can be conveyed through
them. This article first surveys traditional spatialisation approaches intended
for loudspeakers, and then reviews them with regard to their adaptability to
headphones.
"
"  We calculate 3-loop master integrals for heavy quark correlators and the
3-loop QCD corrections to the $\rho$-parameter. They obey non-factorizing
differential equations of second order with more than three singularities,
which cannot be factorized in Mellin-$N$ space either. The solution of the
homogeneous equations is possible in terms of convergent close integer power
series as $_2F_1$ Gau\ss{} hypergeometric functions at rational argument. In
some cases, integrals of this type can be mapped to complete elliptic integrals
at rational argument. This class of functions appears to be the next one
arising in the calculation of more complicated Feynman integrals following the
harmonic polylogarithms, generalized polylogarithms, cyclotomic harmonic
polylogarithms, square-root valued iterated integrals, and combinations
thereof, which appear in simpler cases. The inhomogeneous solution of the
corresponding differential equations can be given in terms of iterative
integrals, where the new innermost letter itself is not an iterative integral.
A new class of iterative integrals is introduced containing letters in which
(multiple) definite integrals appear as factors. For the elliptic case, we also
derive the solution in terms of integrals over modular functions and also
modular forms, using $q$-product and series representations implied by Jacobi's
$\vartheta_i$ functions and Dedekind's $\eta$-function. The corresponding
representations can be traced back to polynomials out of Lambert--Eisenstein
series, having representations also as elliptic polylogarithms, a $q$-factorial
$1/\eta^k(\tau)$, logarithms and polylogarithms of $q$ and their $q$-integrals.
Due to the specific form of the physical variable $x(q)$ for different
processes, different representations do usually appear. Numerical results are
also presented.
"
"  We propose a novel distributed inference algorithm for continuous graphical
models, by extending Stein variational gradient descent (SVGD) to leverage the
Markov dependency structure of the distribution of interest. Our approach
combines SVGD with a set of structured local kernel functions defined on the
Markov blanket of each node, which alleviates the curse of high dimensionality
and simultaneously yields a distributed algorithm for decentralized inference
tasks. We justify our method with theoretical analysis and show that the use of
local kernels can be viewed as a new type of localized approximation that
matches the target distribution on the conditional distributions of each node
over its Markov blanket. Our empirical results show that our method outperforms
a variety of baselines including standard MCMC and particle message passing
methods.
"
"  Modal description logics feature modalities that capture dependence of
knowledge on parameters such as time, place, or the information state of
agents. E.g., the logic S5-ALC combines the standard description logic ALC with
an S5-modality that can be understood as an epistemic operator or as
representing (undirected) change. This logic embeds into a corresponding modal
first-order logic S5-FOL. We prove a modal characterization theorem for this
embedding, in analogy to results by van Benthem and Rosen relating ALC to
standard first-order logic: We show that S5-ALC with only local roles is, both
over finite and over unrestricted models, precisely the bisimulation invariant
fragment of S5-FOL, thus giving an exact description of the expressive power of
S5-ALC with only local roles.
"
"  Pairwise ranking methods are the basis of many widely used discriminative
training approaches for structure prediction problems in natural language
processing(NLP). Decomposing the problem of ranking hypotheses into pairwise
comparisons enables simple and efficient solutions. However, neglecting the
global ordering of the hypothesis list may hinder learning. We propose a
listwise learning framework for structure prediction problems such as machine
translation. Our framework directly models the entire translation list's
ordering to learn parameters which may better fit the given listwise samples.
Furthermore, we propose top-rank enhanced loss functions, which are more
sensitive to ranking errors at higher positions. Experiments on a large-scale
Chinese-English translation task show that both our listwise learning framework
and top-rank enhanced listwise losses lead to significant improvements in
translation quality.
"
"  The analysis in Part I revealed interesting properties for subgradient
learning algorithms in the context of stochastic optimization when gradient
noise is present. These algorithms are used when the risk functions are
non-smooth and involve non-differentiable components. They have been long
recognized as being slow converging methods. However, it was revealed in Part I
that the rate of convergence becomes linear for stochastic optimization
problems, with the error iterate converging at an exponential rate $\alpha^i$
to within an $O(\mu)-$neighborhood of the optimizer, for some $\alpha \in
(0,1)$ and small step-size $\mu$. The conclusion was established under weaker
assumptions than the prior literature and, moreover, several important problems
(such as LASSO, SVM, and Total Variation) were shown to satisfy these weaker
assumptions automatically (but not the previously used conditions from the
literature). These results revealed that sub-gradient learning methods have
more favorable behavior than originally thought when used to enable continuous
adaptation and learning. The results of Part I were exclusive to single-agent
adaptation. The purpose of the current Part II is to examine the implications
of these discoveries when a collection of networked agents employs subgradient
learning as their cooperative mechanism. The analysis will show that, despite
the coupled dynamics that arises in a networked scenario, the agents are still
able to attain linear convergence in the stochastic case; they are also able to
reach agreement within $O(\mu)$ of the optimizer.
"
"  In this paper, we explore how we should aggregate the degrees of belief of of
a group of agents to give a single coherent set of degrees of belief, when at
least some of those agents might be probabilistically incoherent. There are a
number of way of aggregating degrees of belief, and there are a number of ways
of fixing incoherent degrees of belief. When we have picked one of each, should
we aggregate first and then fix, or fix first and then aggregate? Or should we
try to do both at once? And when do these different procedures agree with one
another? In this paper, we focus particularly on the final question.
"
"  We consider the problems of robust PAC learning from distributed and
streaming data, which may contain malicious errors and outliers, and analyze
their fundamental complexity questions. In particular, we establish lower
bounds on the communication complexity for distributed robust learning
performed on multiple machines, and on the space complexity for robust learning
from streaming data on a single machine. These results demonstrate that gaining
robustness of learning algorithms is usually at the expense of increased
complexities. As far as we know, this work gives the first complexity results
for distributed and online robust PAC learning.
"
"  In this paper, we present a real-time robust multi-view pedestrian detection
and tracking system for video surveillance using neural networks which can be
used in dynamic environments. The proposed system consists of two phases:
multi-view pedestrian detection and tracking. First, pedestrian detection
utilizes background subtraction to segment the foreground blob. An adaptive
background subtraction method where each of the pixel of input image models as
a mixture of Gaussians and uses an on-line approximation to update the model
applies to extract the foreground region. The Gaussian distributions are then
evaluated to determine which are most likely to result from a background
process. This method produces a steady, real-time tracker in outdoor
environment that consistently deals with changes of lighting condition, and
long-term scene change. Second, the Tracking is performed at two phases:
pedestrian classification and tracking the individual subject. A sliding window
is applied on foreground binary image to select an input window which is used
for selecting the input image patches from actually input frame. The neural
networks is used for classification with PHOG features. Finally, a Kalman
filter is applied to calculate the subsequent step for tracking that aims at
finding the exact position of pedestrians in an input image. The experimental
result shows that the proposed approach yields promising performance on
multi-view pedestrian detection and tracking on different benchmark datasets.
"
"  We present a method for computing stable models of normal logic programs,
i.e., logic programs extended with negation, in the presence of predicates with
arbitrary terms. Such programs need not have a finite grounding, so traditional
methods do not apply. Our method relies on the use of a non-Herbrand universe,
as well as coinduction, constructive negation and a number of other novel
techniques. Using our method, a normal logic program with predicates can be
executed directly under the stable model semantics without requiring it to be
grounded either before or during execution and without requiring that its
variables range over a finite domain. As a result, our method is quite general
and supports the use of terms as arguments, including lists and complex data
structures. A prototype implementation and non-trivial applications have been
developed to demonstrate the feasibility of our method.
"
"  This paper presents a comprehensive survey of existing authentication and
privacy-preserving schemes for 4G and 5G cellular networks. We start by
providing an overview of existing surveys that deal with 4G and 5G
communications, applications, standardization, and security. Then, we give a
classification of threat models in 4G and 5G cellular networks in four
categories, including, attacks against privacy, attacks against integrity,
attacks against availability, and attacks against authentication. We also
provide a classification of countermeasures into three types of categories,
including, cryptography methods, humans factors, and intrusion detection
methods. The countermeasures and informal and formal security analysis
techniques used by the authentication and privacy preserving schemes are
summarized in form of tables. Based on the categorization of the authentication
and privacy models, we classify these schemes in seven types, including,
handover authentication with privacy, mutual authentication with privacy, RFID
authentication with privacy, deniable authentication with privacy,
authentication with mutual anonymity, authentication and key agreement with
privacy, and three-factor authentication with privacy. In addition, we provide
a taxonomy and comparison of authentication and privacy-preserving schemes for
4G and 5G cellular networks in form of tables. Based on the current survey,
several recommendations for further research are discussed at the end of this
paper.
"
"  Analysis of an organization's computer network activity is a key component of
early detection and mitigation of insider threat, a growing concern for many
organizations. Raw system logs are a prototypical example of streaming data
that can quickly scale beyond the cognitive power of a human analyst. As a
prospective filter for the human analyst, we present an online unsupervised
deep learning approach to detect anomalous network activity from system logs in
real time. Our models decompose anomaly scores into the contributions of
individual user behavior features for increased interpretability to aid
analysts reviewing potential cases of insider threat. Using the CERT Insider
Threat Dataset v6.2 and threat detection recall as our performance metric, our
novel deep and recurrent neural network models outperform Principal Component
Analysis, Support Vector Machine and Isolation Forest based anomaly detection
baselines. For our best model, the events labeled as insider threat activity in
our dataset had an average anomaly score in the 95.53 percentile, demonstrating
our approach's potential to greatly reduce analyst workloads.
"
"  We introduce a new sample complexity measure, which we refer to as
split-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size
$m$, the split-sample growth rate $\hat{\tau}_H(m)$ counts how many different
hypotheses can empirical risk minimization output on any sub-sample of $S$ of
size $m/2$. We show that the expected generalization error is upper bounded by
$O\left(\sqrt{\frac{\log(\hat{\tau}_H(2m))}{m}}\right)$. Our result is enabled
by a strengthening of the Rademacher complexity analysis of the expected
generalization error. We show that this sample complexity measure, greatly
simplifies the analysis of the sample complexity of optimal auction design, for
many auction classes studied in the literature. Their sample complexity can be
derived solely by noticing that in these auction classes, ERM on any sample or
sub-sample will pick parameters that are equal to one of the points in the
sample.
"
"  The inability to interpret the model prediction in semantically and visually
meaningful ways is a well-known shortcoming of most existing computer-aided
diagnosis methods. In this paper, we propose MDNet to establish a direct
multimodal mapping between medical images and diagnostic reports that can read
images, generate diagnostic reports, retrieve images by symptom descriptions,
and visualize attention, to provide justifications of the network diagnosis
process. MDNet includes an image model and a language model. The image model is
proposed to enhance multi-scale feature ensembles and utilization efficiency.
The language model, integrated with our improved attention mechanism, aims to
read and explore discriminative image feature descriptions from reports to
learn a direct mapping from sentence words to image pixels. The overall network
is trained end-to-end by using our developed optimization strategy. Based on a
pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we
conduct sufficient experiments to demonstrate that MDNet outperforms
comparative baselines. The proposed image model obtains state-of-the-art
performance on two CIFAR datasets as well.
"
"  Given the potential X-ray radiation risk to the patient, low-dose CT has
attracted a considerable interest in the medical imaging field. The current
main stream low-dose CT methods include vendor-specific sinogram domain
filtration and iterative reconstruction, but they need to access original raw
data whose formats are not transparent to most users. Due to the difficulty of
modeling the statistical characteristics in the image domain, the existing
methods for directly processing reconstructed images cannot eliminate image
noise very well while keeping structural details. Inspired by the idea of deep
learning, here we combine the autoencoder, the deconvolution network, and
shortcut connections into the residual encoder-decoder convolutional neural
network (RED-CNN) for low-dose CT imaging. After patch-based training, the
proposed RED-CNN achieves a competitive performance relative to
the-state-of-art methods in both simulated and clinical cases. Especially, our
method has been favorably evaluated in terms of noise suppression, structural
preservation and lesion detection.
"
"  This paper focuses on a new task, i.e., transplanting a
category-and-task-specific neural network to a generic, modular network without
strong supervision. We design a functionally interpretable structure for the
generic network. Like building LEGO blocks, we teach the generic network a new
category by directly transplanting the module corresponding to the category
from a pre-trained network with a few or even without sample annotations. Our
method incrementally adds new categories to the generic network but does not
affect representations of existing categories. In this way, our method breaks
the typical bottleneck of learning a net for massive tasks and categories,
i.e., the requirement of collecting samples for all tasks and categories at the
same time before the learning begins. Thus, we use a new distillation
algorithm, namely back-distillation, to overcome specific challenges of network
transplanting. Our method without training samples even outperformed the
baseline with 100 training samples.
"
"  Build systems are an essential part of modern software engineering projects.
As software projects change continuously, it is crucial to understand how the
build system changes because neglecting its maintenance can lead to expensive
build breakage. Recent studies have investigated the (co-)evolution of build
configurations and reasons for build breakage, but they did this only on a
coarse grained level. In this paper, we present BUILDDIFF, an approach to
extract detailed build changes from MAVEN build files and classify them into 95
change types. In a manual evaluation of 400 build changing commits, we show
that BUILDDIFF can extract and classify build changes with an average precision
and recall of 0.96 and 0.98, respectively. We then present two studies using
the build changes extracted from 30 open source Java projects to study the
frequency and time of build changes. The results show that the top 10 most
frequent change types account for 73% of the build changes. Among them, changes
to version numbers and changes to dependencies of the projects occur most
frequently. Furthermore, our results show that build changes occur frequently
around releases. With these results, we provide the basis for further research,
such as for analyzing the (co-)evolution of build files with other artifacts or
improving effort estimation approaches. Furthermore, our detailed change
information enables improvements of refactoring approaches for build
configurations and improvements of models to identify error-prone build files.
"
"  The Japanese comic format known as Manga is popular all over the world. It is
traditionally produced in black and white, and colorization is time consuming
and costly. Automatic colorization methods generally rely on greyscale values,
which are not present in manga. Furthermore, due to copyright protection,
colorized manga available for training is scarce. We propose a manga
colorization method based on conditional Generative Adversarial Networks
(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of
training images, our method requires only a single colorized reference image
for training, avoiding the need of a large dataset. Colorizing manga using
cGANs can produce blurry results with artifacts, and the resolution is limited.
We therefore also propose a method of segmentation and color-correction to
mitigate these issues. The final results are sharp, clear, and in high
resolution, and stay true to the character's original color scheme.
"
"  We prove that for any choice of parameters $k,t,\lambda$ the class of all
finite ordered designs with parameters $k,t,\lambda$ is a Ramsey class.
"
"  We consider a system of linear hyperbolic PDEs where the state at one of the
boundary points is controlled using the measurements of another boundary point.
Because of the disturbances in the measurement, the problem of designing
dynamic controllers is considered so that the closed-loop system is robust with
respect to measurement errors. Assuming that the disturbance is a locally
essentially bounded measurable function of time, we derive a
disturbance-to-state estimate which provides an upper bound on the maximum norm
of the state (with respect to the spatial variable) at each time in terms of
$\mathcal{L}^\infty$-norm of the disturbance up to that time. The analysis is
based on constructing a Lyapunov function for the closed-loop system, which
leads to controller synthesis and the conditions on system dynamics required
for stability. As an application of this stability notion, the problem of
quantized control for hyperbolic PDEs is considered where the measurements sent
to the controller are communicated using a quantizer of finite length. The
presence of quantizer yields practical stability only, and the ultimate bounds
on the norm of the state trajectory are also derived.
"
"  Denial of service attacks are especially pertinent to the internet of things
as devices have less computing power, memory and security mechanisms to defend
against them. The task of mitigating these attacks must therefore be redirected
from the device onto a network monitor. Network intrusion detection systems can
be used as an effective and efficient technique in internet of things systems
to offload computation from the devices and detect denial of service attacks
before they can cause harm. However the solution of implementing a network
intrusion detection system for internet of things networks is not without
challenges due to the variability of these systems and specifically the
difficulty in collecting data. We propose a model-hybrid approach to model the
scale of the internet of things system and effectively train network intrusion
detection systems. Through bespoke datasets generated by the model, the IDS is
able to predict a wide spectrum of real-world attacks, and as demonstrated by
an experiment construct more predictive datasets at a fraction of the time of
other more standard techniques.
"
"  This paper reports on a data-driven, interaction-aware motion prediction
approach for pedestrians in environments cluttered with static obstacles. When
navigating in such workspaces shared with humans, robots need accurate motion
predictions of the surrounding pedestrians. Human navigation behavior is mostly
influenced by their surrounding pedestrians and by the static obstacles in
their vicinity. In this paper we introduce a new model based on Long-Short Term
Memory (LSTM) neural networks, which is able to learn human motion behavior
from demonstrated data. To the best of our knowledge, this is the first
approach using LSTMs, that incorporates both static obstacles and surrounding
pedestrians for trajectory forecasting. As part of the model, we introduce a
new way of encoding surrounding pedestrians based on a 1d-grid in polar angle
space. We evaluate the benefit of interaction-aware motion prediction and the
added value of incorporating static obstacles on both simulation and real-world
datasets by comparing with state-of-the-art approaches. The results show, that
our new approach outperforms the other approaches while being very
computationally efficient and that taking into account static obstacles for
motion predictions significantly improves the prediction accuracy, especially
in cluttered environments.
"
"  Geodesic distance matrices can reveal shape properties that are largely
invariant to non-rigid deformations, and thus are often used to analyze and
represent 3-D shapes. However, these matrices grow quadratically with the
number of points. Thus for large point sets it is common to use a low-rank
approximation to the distance matrix, which fits in memory and can be
efficiently analyzed using methods such as multidimensional scaling (MDS). In
this paper we present a novel sparse method for efficiently representing
geodesic distance matrices using biharmonic interpolation. This method exploits
knowledge of the data manifold to learn a sparse interpolation operator that
approximates distances using a subset of points. We show that our method is 2x
faster and uses 20x less memory than current leading methods for solving MDS on
large point sets, with similar quality. This enables analyses of large point
sets that were previously infeasible.
"
"  Small-cell deployment in licensed and unlicensed spectrum is considered to be
one of the key approaches to cope with the ongoing wireless data demand
explosion. Compared to traditional cellular base stations with large
transmission power, small-cells typically have relatively low transmission
power, which makes them attractive for some spectrum bands that have strict
power regulations, for example, the 3.5GHz band [1]. In this paper we consider
a heterogeneous wireless network consisting of one or more service providers
(SPs). Each SP operates in both macro-cells and small-cells, and provides
service to two types of users: mobile and fixed. Mobile users can only
associate with macro-cells whereas fixed users can connect to either macro- or
small-cells. The SP charges a price per unit rate for each type of service.
Each SP is given a fixed amount of bandwidth and splits it between macro- and
small-cells. Motivated by bandwidth regulations, such as those for the 3.5Gz
band, we assume a minimum amount of bandwidth has to be set aside for
small-cells. We study the optimal pricing and bandwidth allocation strategies
in both monopoly and competitive scenarios. In the monopoly scenario the
strategy is unique. In the competitive scenario there exists a unique Nash
equilibrium, which depends on the regulatory constraints. We also analyze the
social welfare achieved, and compare it to that without the small-cell
bandwidth constraints. Finally, we discuss implications of our results on the
effectiveness of the minimum bandwidth constraint on influencing small-cell
deployments.
"
"  We study the effect of adaptive mesh refinement on a parallel domain
decomposition solver of a linear system of algebraic equations. These concepts
need to be combined within a parallel adaptive finite element software. A
prototype implementation is presented for this purpose. It uses adaptive mesh
refinement with one level of hanging nodes. Two and three-level versions of the
Balancing Domain Decomposition based on Constraints (BDDC) method are used to
solve the arising system of algebraic equations. The basic concepts are
recalled and components necessary for the combination are studied in detail. Of
particular interest is the effect of disconnected subdomains, a typical output
of the employed mesh partitioning based on space-filling curves, on the
convergence and solution time of the BDDC method. It is demonstrated using a
large set of experiments that while both refined meshes and disconnected
subdomains have a negative effect on the convergence of BDDC, the number of
iterations remains acceptable. In addition, scalability of the three-level BDDC
solver remains good on up to a few thousands of processor cores. The largest
presented problem using adaptive mesh refinement has over 10^9 unknowns and is
solved on 2048 cores.
"
"  Today's artificial assistants are typically prompted to perform tasks through
direct, imperative commands such as \emph{Set a timer} or \emph{Pick up the
box}. However, to progress toward more natural exchanges between humans and
these assistants, it is important to understand the way non-imperative
utterances can indirectly elicit action of an addressee. In this paper, we
investigate command types in the setting of a grounded, collaborative game. We
focus on a less understood family of utterances for eliciting agent action,
locatives like \emph{The chair is in the other room}, and demonstrate how these
utterances indirectly command in specific game state contexts. Our work shows
that models with domain-specific grounding can effectively realize the
pragmatic reasoning that is necessary for more robust natural language
interaction.
"
"  Generic text embeddings are successfully used in a variety of tasks. However,
they are often learnt by capturing the co-occurrence structure from pure text
corpora, resulting in limitations of their ability to generalize. In this
paper, we explore models that incorporate visual information into the text
representation. Based on comprehensive ablation studies, we propose a
conceptually simple, yet well performing architecture. It outperforms previous
multimodal approaches on a set of well established benchmarks. We also improve
the state-of-the-art results for image-related text datasets, using orders of
magnitude less data.
"
"  In complex, high dimensional and unstructured data it is often difficult to
extract meaningful patterns. This is especially the case when dealing with
textual data. Recent studies in machine learning, information theory and
network science have developed several novel instruments to extract the
semantics of unstructured data, and harness it to build a network of relations.
Such approaches serve as an efficient tool for dimensionality reduction and
pattern detection. This paper applies semantic network science to extract
ideological proximity in the international arena, by focusing on the data from
General Debates in the UN General Assembly on the topics of high salience to
international community. UN General Debate corpus (UNGDC) covers all high-level
debates in the UN General Assembly from 1970 to 2014, covering all UN member
states. The research proceeds in three main steps. First, Latent Dirichlet
Allocation (LDA) is used to extract the topics of the UN speeches, and
therefore semantic information. Each country is then assigned a vector
specifying the exposure to each of the topics identified. This intermediate
output is then used in to construct a network of countries based on information
theoretical metrics where the links capture similar vectorial patterns in the
topic distributions. Topology of the networks is then analyzed through network
properties like density, path length and clustering. Finally, we identify
specific topological features of our networks using the map equation framework
to detect communities in our networks of countries.
"
"  Design of adaptive algorithms for simultaneous regulation and estimation of
MIMO linear dynamical systems is a canonical reinforcement learning problem.
Efficient policies whose regret (i.e. increase in the cost due to uncertainty)
scales at a square-root rate of time have been studied extensively in the
recent literature. Nevertheless, existing strategies are computationally
intractable and require a priori knowledge of key system parameters. The only
exception is a randomized Greedy regulator, for which asymptotic regret bounds
have been recently established. However, randomized Greedy leads to probable
fluctuations in the trajectory of the system, which renders its finite time
regret suboptimal.
This work addresses the above issues by designing policies that utilize input
signals perturbations. We show that perturbed Greedy guarantees non-asymptotic
regret bounds of (nearly) square-root magnitude w.r.t. time. More generally, we
establish high probability bounds on both the regret and the learning accuracy
under arbitrary input perturbations. The settings where Greedy attains the
information theoretic lower bound of logarithmic regret are also discussed. To
obtain the results, state-of-the-art tools from martingale theory together with
the recently introduced method of policy decomposition are leveraged. Beside
adaptive regulators, analysis of input perturbations captures key applications
including remote sensing and distributed control.
"
"  In this paper, we derive upper and lower bounds as well as a simple
closed-form approximation for the capacity of the continuous-time, bandlimited,
additive white Gaussian noise channel in a three-dimensional free-space
electromagnetic propagation environment subject to constraints on the total
effective antenna aperture area of the link and a total transmitter power
constraint. We assume that the communication range is much larger than the
radius of the sphere containing the antennas at both ends of the link, and we
show that, in general, the capacity can only be achieved by transmitting
multiple spatially-multiplexed data streams simultaneously over the channel.
Furthermore, the lower bound on capacity can be approached asymptotically by
transmitting the data streams between a pair of physically-realizable
distributed antenna arrays at either end of the link. A consequence of this
result is that, in general, communication at close to the maximum achievable
data rate on a deep-space communication link can be achieved in practice if and
only if the communication system utilizes spatial multiplexing over a
distributed MIMO antenna array. Such an approach to deep-space communication
does not appear to be envisioned currently by any of the international space
agencies or any commercial space companies. A second consequence is that the
capacity of a long-range free-space communication link, if properly utilized,
grows asymptotically as a function of the square root of the received SNR
rather than only logarithmically in the received SNR.
"
"  Formal verification techniques are widely used for detecting design flaws in
software systems. Formal verification can be done by transforming an already
implemented source code to a formal model and attempting to prove certain
properties of the model (e.g. that no erroneous state can occur during
execution). Unfortunately, transformations from source code to a formal model
often yield large and complex models, making the verification process
inefficient and costly. In order to reduce the size of the resulting model,
optimization transformations can be used. Such optimizations include common
algorithms known from compiler design and different program slicing techniques.
Our paper describes a framework for transforming C programs to a formal model,
enhanced by various optimizations for size reduction. We evaluate and compare
several optimization algorithms regarding their effect on the size of the model
and the efficiency of the verification. Results show that different
optimizations are more suitable for certain models, justifying the need for a
framework that includes several algorithms.
"
"  Many real world tasks such as reasoning and physical interaction require
identification and manipulation of conceptual entities. A first step towards
solving these tasks is the automated discovery of distributed symbol-like
representations. In this paper, we explicitly formalize this problem as
inference in a spatial mixture model where each component is parametrized by a
neural network. Based on the Expectation Maximization framework we then derive
a differentiable clustering method that simultaneously learns how to group and
represent individual entities. We evaluate our method on the (sequential)
perceptual grouping task and find that it is able to accurately recover the
constituent objects. We demonstrate that the learned representations are useful
for next-step prediction.
"
"  Usage of online textual media is steadily increasing. Daily, more and more
news stories, blog posts and scientific articles are added to the online
volumes. These are all freely accessible and have been employed extensively in
multiple research areas, e.g. automatic text summarization, information
retrieval, information extraction, etc. Meanwhile, online debate forums have
recently become popular, but have remained largely unexplored. For this reason,
there are no sufficient resources of annotated debate data available for
conducting research in this genre. In this paper, we collected and annotated
debate data for an automatic summarization task. Similar to extractive gold
standard summary generation our data contains sentences worthy to include into
a summary. Five human annotators performed this task. Inter-annotator
agreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for
Krippendorff's alpha. Moreover, we also implement an extractive summarization
system for online debates and discuss prominent features for the task of
summarizing online debate data automatically.
"
"  We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning
algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior)
classifier, i.e., a randomized classifier obtained by a risk-sensitive
perturbation of the weights of a learned classifier. Entropy-SGD works by
optimizing the bound's prior, violating the hypothesis of the PAC-Bayes theorem
that the prior is chosen independently of the data. Indeed, available
implementations of Entropy-SGD rapidly obtain zero training error on random
labels and the same holds of the Gibbs posterior. In order to obtain a valid
generalization bound, we rely on a result showing that data-dependent priors
obtained by stochastic gradient Langevin dynamics (SGLD) yield valid PAC-Bayes
bounds provided the target distribution of SGLD is $\epsilon$-differentially
private. We observe that test error on MNIST and CIFAR10 falls within the
(empirically nonvacuous) risk bounds computed under the assumption that SGLD
reaches stationarity. In particular, Entropy-SGLD can be configured to yield
relatively tight generalization bounds and still fit real labels, although
these same settings do not obtain state-of-the-art performance.
"
"  Learning sophisticated feature interactions behind user behaviors is critical
in maximizing CTR for recommender systems. Despite great progress, existing
methods seem to have a strong bias towards low- or high-order interactions, or
require expertise feature engineering. In this paper, we show that it is
possible to derive an end-to-end learning model that emphasizes both low- and
high-order feature interactions. The proposed model, DeepFM, combines the power
of factorization machines for recommendation and deep learning for feature
learning in a new neural network architecture. Compared to the latest Wide \&
Deep model from Google, DeepFM has a shared input to its ""wide"" and ""deep""
parts, with no need of feature engineering besides raw features. Comprehensive
experiments are conducted to demonstrate the effectiveness and efficiency of
DeepFM over the existing models for CTR prediction, on both benchmark data and
commercial data.
"
"  When a measurement falls outside the quantization or measurable range, it
becomes saturated and cannot be used in classical reconstruction methods. For
example, in C-arm angiography systems, which provide projection radiography,
fluoroscopy, digital subtraction angiography, and are widely used for medical
diagnoses and interventions, the limited dynamic range of C-arm flat detectors
leads to overexposure in some projections during an acquisition, such as
imaging relatively thin body parts (e.g., the knee). Aiming at overexposure
correction for computed tomography (CT) reconstruction, we in this paper
propose a mixed one-bit compressive sensing (M1bit-CS) to acquire information
from both regular and saturated measurements. This method is inspired by the
recent progress on one-bit compressive sensing, which deals with only sign
observations. Its successful applications imply that information carried by
saturated measurements is useful to improve recovery quality. For the proposed
M1bit-CS model, alternating direction methods of multipliers is developed and
an iterative saturation detection scheme is established. Then we evaluate
M1bit-CS on one-dimensional signal recovery tasks. In some experiments, the
performance of the proposed algorithms on mixed measurements is almost the same
as recovery on unsaturated ones with the same amount of measurements. Finally,
we apply the proposed method to overexposure correction for CT reconstruction
on a phantom and a simulated clinical image. The results are promising, as the
typical streaking artifacts and capping artifacts introduced by saturated
projection data are effectively reduced, yielding significant error reduction
compared with existing algorithms based on extrapolation.
"
"  There is an increased interest in building data analytics frameworks with
advanced algebraic capabilities both in industry and academia. Many of these
frameworks, e.g., TensorFlow and BIDMach, implement their compute-intensive
primitives in two flavors---as multi-thread routines for multi-core CPUs and as
highly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is
the most popular optimization method for model training implemented extensively
on modern data analytics platforms. While the data-intensive properties of SGD
are well-known, there is an intense debate on which of the many SGD variants is
better in practice. In this paper, we perform a comprehensive study of parallel
SGD for training generalized linear models. We consider the impact of three
factors -- computing architecture (multi-core CPU or GPU), synchronous or
asynchronous model updates, and data sparsity -- on three measures---hardware
efficiency, statistical efficiency, and time to convergence. In the process, we
design an optimized asynchronous SGD algorithm for GPU that leverages warp
shuffling and cache coalescing for data and model access. We draw several
interesting findings from our extensive experiments with logistic regression
(LR) and support vector machines (SVM) on five real datasets. For synchronous
SGD, GPU always outperforms parallel CPU---they both outperform a sequential
CPU solution by more than 400X. For asynchronous SGD, parallel CPU is the
safest choice while GPU with data replication is better in certain situations.
The choice between synchronous GPU and asynchronous CPU depends on the task and
the characteristics of the data. As a reference, our best implementation
outperforms TensorFlow and BIDMach consistently. We hope that our insights
provide a useful guide for applying parallel SGD to generalized linear models.
"
"  Deep neural networks have enabled progress in a wide variety of applications.
Growing the size of the neural network typically results in improved accuracy.
As model sizes grow, the memory and compute requirements for training these
models also increases. We introduce a technique to train deep neural networks
using half precision floating point numbers. In our technique, weights,
activations and gradients are stored in IEEE half-precision format.
Half-precision floating numbers have limited numerical range compared to
single-precision numbers. We propose two techniques to handle this loss of
information. Firstly, we recommend maintaining a single-precision copy of the
weights that accumulates the gradients after each optimizer step. This
single-precision copy is rounded to half-precision format during training.
Secondly, we propose scaling the loss appropriately to handle the loss of
information with half-precision gradients. We demonstrate that this approach
works for a wide variety of models including convolution neural networks,
recurrent neural networks and generative adversarial networks. This technique
works for large scale models with more than 100 million parameters trained on
large datasets. Using this approach, we can reduce the memory consumption of
deep learning models by nearly 2x. In future processors, we can also expect a
significant computation speedup using half-precision hardware units.
"
"  We consider large scale empirical risk minimization (ERM) problems, where
both the problem dimension and variable size is large. In these cases, most
second order methods are infeasible due to the high cost in both computing the
Hessian over all samples and computing its inverse in high dimensions. In this
paper, we propose a novel adaptive sample size second-order method, which
reduces the cost of computing the Hessian by solving a sequence of ERM problems
corresponding to a subset of samples and lowers the cost of computing the
Hessian inverse using a truncated eigenvalue decomposition. We show that while
we geometrically increase the size of the training set at each stage, a single
iteration of the truncated Newton method is sufficient to solve the new ERM
within its statistical accuracy. Moreover, for a large number of samples we are
allowed to double the size of the training set at each stage, and the proposed
method subsequently reaches the statistical accuracy of the full training set
approximately after two effective passes. In addition to this theoretical
result, we show empirically on a number of well known data sets that the
proposed truncated adaptive sample size algorithm outperforms stochastic
alternatives for solving ERM problems.
"
"  This paper investigates the role of tutor feedback in language learning using
computational models. We compare two dominant paradigms in language learning:
interactive learning and cross-situational learning - which differ primarily in
the role of social feedback such as gaze or pointing. We analyze the
relationship between these two paradigms and propose a new mixed paradigm that
combines the two paradigms and allows to test algorithms in experiments that
combine no feedback and social feedback. To deal with mixed feedback
experiments, we develop new algorithms and show how they perform with respect
to traditional knn and prototype approaches.
"
"  This paper analyzes the coexistence performance of Wi-Fi and cellular
networks conditioned on non-saturated traffic in the unlicensed spectrum. Under
the condition, the time-domain behavior of a cellular small-cell base station
(SCBS) with a listen-before-talk (LBT) procedure is modeled as a Markov chain,
and it is combined with a Markov chain which describes the time-domain behavior
of a Wi-Fi access point. Using the proposed model, this study finds the optimal
contention window size of cellular SCBSs in which total throughput of both
networks is maximized while satisfying the required throughput of each network,
under the given traffic densities of both networks. This will serve as a
guideline for cellular operators with respect to performing LBT at cellular
SCBSs according to the changes of traffic volumes of both networks over time.
"
"  It is known that the set of all correlated equilibria of an n-player
non-cooperative game is a convex polytope and includes all the Nash equilibria.
Further, the Nash equilibria all lie on the boundary of this polytope. We study
the geometry of both these equilibrium notions when the players have cumulative
prospect theoretic (CPT) preferences. The set of CPT correlated equilibria
includes all the CPT Nash equilibria but it need not be a convex polytope. We
show that it can, in fact, be disconnected. However, all the CPT Nash
equilibria continue to lie on its boundary. We also characterize the sets of
CPT correlated equilibria and CPT Nash equilibria for all 2x2 games.
"
"  While both the data volume and heterogeneity of the digital music content is
huge, it has become increasingly important and convenient to build a
recommendation or search system to facilitate surfacing these content to the
user or consumer community. Most of the recommendation models fall into two
primary species, collaborative filtering based and content based approaches.
Variants of instantiations of collaborative filtering approach suffer from the
common issues of so called ""cold start"" and ""long tail"" problems where there is
not much user interaction data to reveal user opinions or affinities on the
content and also the distortion towards the popular content. Content-based
approaches are sometimes limited by the richness of the available content data
resulting in a heavily biased and coarse recommendation result. In recent
years, the deep neural network has enjoyed a great success in large-scale image
and video recognitions. In this paper, we propose and experiment using deep
convolutional neural network to imitate how human brain processes hierarchical
structures in the auditory signals, such as music, speech, etc., at various
timescales. This approach can be used to discover the latent factor models of
the music based upon acoustic hyper-images that are extracted from the raw
audio waves of music. These latent embeddings can be used either as features to
feed to subsequent models, such as collaborative filtering, or to build
similarity metrics between songs, or to classify music based on the labels for
training such as genre, mood, sentiment, etc.
"
"  We demonstrate the usefulness of adding delay to infinite games with
quantitative winning conditions. In a delay game, one of the players may delay
her moves to obtain a lookahead on her opponent's moves. We show that
determining the winner of delay games with winning conditions given by parity
automata with costs is EXPTIME-complete and that exponential bounded lookahead
is both sufficient and in general necessary. Thus, although the parity
condition with costs is a quantitative extension of the parity condition, our
results show that adding costs does not increase the complexity of delay games
with parity conditions.
Furthermore, we study a new phenomenon that appears in quantitative delay
games: lookahead can be traded for the quality of winning strategies and vice
versa. We determine the extent of this tradeoff. In particular, even the
smallest lookahead allows to improve the quality of an optimal strategy from
the worst possible value to almost the smallest possible one. Thus, the benefit
of introducing lookahead is twofold: not only does it allow the delaying player
to win games she would lose without, but lookahead also allows her to improve
the quality of her winning strategies in games she wins even without lookahead.
"
"  Reachability analysis for hybrid systems is an active area of development and
has resulted in many promising prototype tools. Most of these tools allow users
to express hybrid system as automata with a set of ordinary differential
equations (ODEs) associated with each state, as well as rules for transitions
between states. Significant effort goes into developing and verifying and
correctly implementing those tools. As such, it is desirable to expand the
scope of applicability tools of such as far as possible. With this goal, we
show how compile-time transformations can be used to extend the basic hybrid
ODE formalism traditionally supported in hybrid reachability tools such as
SpaceEx or Flow*. The extension supports certain types of partial derivatives
and equational constraints. These extensions allow users to express, among
other things, the Euler-Lagrangian equation, and to capture practically
relevant constraints that arise naturally in mechanical systems. Achieving this
level of expressiveness requires using a binding time-analysis (BTA), program
differentiation, symbolic Gaussian elimination, and abstract interpretation
using interval analysis. Except for BTA, the other components are either
readily available or can be easily added to most reachability tools. The paper
therefore focuses on presenting both the declarative and algorithmic
specifications for the BTA phase, and establishes the soundness of the
algorithmic specifications with respect to the declarative one.
"
"  The interval subset sum problem (ISSP) is a generalization of the well-known
subset sum problem. Given a set of intervals
$\left\{[a_{i,1},a_{i,2}]\right\}_{i=1}^n$ and a target integer $T,$ the ISSP
is to find a set of integers, at most one from each interval, such that their
sum best approximates the target $T$ but cannot exceed it. In this paper, we
first study the computational complexity of the ISSP. We show that the ISSP is
relatively easy to solve compared to the 0-1 Knapsack problem (KP). We also
identify several subclasses of the ISSP which are polynomial time solvable
(with high probability), albeit the problem is generally NP-hard. Then, we
propose a new fully polynomial time approximation scheme (FPTAS) for solving
the general ISSP problem. The time and space complexities of the proposed
scheme are ${\cal O}\left(n \max\left\{1 / \epsilon,\log n\right\}\right)$ and
${\cal O}\left(n+1/\epsilon\right),$ respectively, where $\epsilon$ is the
relative approximation error. To the best of our knowledge, the proposed scheme
has almost the same time complexity but a significantly lower space complexity
compared to the best known scheme. Both the correctness and efficiency of the
proposed scheme are validated by numerical simulations. In particular, the
proposed scheme successfully solves ISSP instances with $n=100,000$ and
$\epsilon=0.1\%$ within one second.
"
"  Kernel methods are powerful and flexible approach to solve many problems in
machine learning. Due to the pairwise evaluations in kernel methods, the
complexity of kernel computation grows as the data size increases; thus the
applicability of kernel methods is limited for large scale datasets. Random
Fourier Features (RFF) has been proposed to scale the kernel method for solving
large scale datasets by approximating kernel function using randomized Fourier
features. While this method proved very popular, still it exists shortcomings
to be effectively used. As RFF samples the randomized features from a
distribution independent of training data, it requires sufficient large number
of feature expansions to have similar performances to kernelized classifiers,
and this is proportional to the number samples in the dataset. Thus, reducing
the number of feature dimensions is necessary to effectively scale to large
datasets. In this paper, we propose a kernel approximation method in a data
dependent way, coined as Pseudo Random Fourier Features (PRFF) for reducing the
number of feature dimensions and also to improve the prediction performance.
The proposed approach is evaluated on classification and regression problems
and compared with the RFF, orthogonal random features and Nystr{ö}m approach
"
"  This paper focuses on the problem of estimating historical traffic volumes
between sparsely-located traffic sensors, which transportation agencies need to
accurately compute statewide performance measures. To this end, the paper
examines applications of vehicle probe data, automatic traffic recorder counts,
and neural network models to estimate hourly volumes in the Maryland highway
network, and proposes a novel approach that combines neural networks with an
existing profiling method. On average, the proposed approach yields 24% more
accurate estimates than volume profiles, which are currently used by
transportation agencies across the US to compute statewide performance
measures. The paper also quantifies the value of using vehicle probe data in
estimating hourly traffic volumes, which provides important managerial insights
to transportation agencies interested in acquiring this type of data. For
example, results show that volumes can be estimated with a mean absolute
percent error of about 21% at locations where average number of observed probes
is between 30 and 47 vehicles/hr, which provides a useful guideline for
assessing the value of probe vehicle data from different vendors.
"
"  State-of-the-art knowledge compilers generate deterministic subsets of DNNF,
which have been recently shown to be exponentially less succinct than DNNF. In
this paper, we propose a new method to compile DNNFs without enforcing
determinism necessarily. Our approach is based on compiling deterministic DNNFs
with the addition of auxiliary variables to the input formula. These variables
are then existentially quantified from the deterministic structure in linear
time, which would lead to a DNNF that is equivalent to the input formula and
not necessarily deterministic. On the theoretical side, we show that the new
method could generate exponentially smaller DNNFs than deterministic ones, even
by adding a single auxiliary variable. Further, we show that various existing
techniques that introduce auxiliary variables to the input formulas can be
employed in our framework. On the practical side, we empirically demonstrate
that our new method can significantly advance DNNF compilation on certain
benchmarks.
"
"  Information and communications technology can continue to change our world.
These advances will partially depend upon designs that synergistically combine
software with specialized hardware. Today open-source software incubates rapid
software-only innovation. The government can unleash software-hardware
innovation with programs to develop open hardware components, tools, and design
flows that simplify and reduce the cost of hardware design. Such programs will
speed development for startup companies, established industry leaders,
education, scientific research, and for government intelligence and defense
platforms.
"
"  In the present paper, we study the match test for extended regular
expressions. We approach this NP-complete problem by introducing a novel
variant of two-way multihead automata, which reveals that the complexity of the
match test is determined by a hidden combinatorial property of extended regular
expressions, and it shows that a restriction of the corresponding parameter
leads to rich classes with a polynomial time match test. For presentational
reasons, we use the concept of pattern languages in order to specify extended
regular expressions. While this decision, formally, slightly narrows the scope
of our results, an extension of our concepts and results to more general
notions of extended regular expressions is straightforward.
"
"  In this paper, we address the problem of detection, classification and
quantification of emotions of text in any form. We consider English text
collected from social media like Twitter, which can provide information having
utility in a variety of ways, especially opinion mining. Social media like
Twitter and Facebook is full of emotions, feelings and opinions of people all
over the world. However, analyzing and classifying text on the basis of
emotions is a big challenge and can be considered as an advanced form of
Sentiment Analysis. This paper proposes a method to classify text into six
different Emotion-Categories: Happiness, Sadness, Fear, Anger, Surprise and
Disgust. In our model, we use two different approaches and combine them to
effectively extract these emotions from text. The first approach is based on
Natural Language Processing, and uses several textual features like emoticons,
degree words and negations, Parts Of Speech and other grammatical analysis. The
second approach is based on Machine Learning classification algorithms. We have
also successfully devised a method to automate the creation of the training-set
itself, so as to eliminate the need of manual annotation of large datasets.
Moreover, we have managed to create a large bag of emotional words, along with
their emotion-intensities. On testing, it is shown that our model provides
significant accuracy in classifying tweets taken from Twitter.
"
"  We propose an efficient method to generate white-box adversarial examples to
trick a character-level neural classifier. We find that only a few
manipulations are needed to greatly decrease the accuracy. Our method relies on
an atomic flip operation, which swaps one token for another, based on the
gradients of the one-hot input vectors. Due to efficiency of our method, we can
perform adversarial training which makes the model more robust to attacks at
test time. With the use of a few semantics-preserving constraints, we
demonstrate that HotFlip can be adapted to attack a word-level classifier as
well.
"
"  Model instability and poor prediction of long-term behavior are common
problems when modeling dynamical systems using nonlinear ""black-box""
techniques. Direct optimization of the long-term predictions, often called
simulation error minimization, leads to optimization problems that are
generally non-convex in the model parameters and suffer from multiple local
minima. In this work we present methods which address these problems through
convex optimization, based on Lagrangian relaxation, dissipation inequalities,
contraction theory, and semidefinite programming. We demonstrate the proposed
methods with a model order reduction task for electronic circuit design and the
identification of a pneumatic actuator from experiment.
"
"  Workers participating in a crowdsourcing platform can have a wide range of
abilities and interests. An important problem in crowdsourcing is the task
recommendation problem, in which tasks that best match a particular worker's
preferences and reliabilities are recommended to that worker. A task
recommendation scheme that assigns tasks more likely to be accepted by a worker
who is more likely to complete it reliably results in better performance for
the task requester. Without prior information about a worker, his preferences
and reliabilities need to be learned over time. In this paper, we propose a
multi-armed bandit (MAB) framework to learn a worker's preferences and his
reliabilities for different categories of tasks. However, unlike the classical
MAB problem, the reward from the worker's completion of a task is unobservable.
We therefore include the use of gold tasks (i.e., tasks whose solutions are
known \emph{a priori} and which do not produce any rewards) in our task
recommendation procedure. Our model could be viewed as a new variant of MAB, in
which the random rewards can only be observed at those time steps where gold
tasks are used, and the accuracy of estimating the expected reward of
recommending a task to a worker depends on the number of gold tasks used. We
show that the optimal regret is $O(\sqrt{n})$, where $n$ is the number of tasks
recommended to the worker. We develop three task recommendation strategies to
determine the number of gold tasks for different task categories, and show that
they are order optimal. Simulations verify the efficiency of our approaches.
"
"  In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN)
architecture has been proposed for speaker verification in the text-independent
setting. One of the main challenges is the creation of the speaker models. Most
of the previously-reported approaches create speaker models based on averaging
the extracted features from utterances of the speaker, which is known as the
d-vector system. In our paper, we propose an adaptive feature learning by
utilizing the 3D-CNNs for direct speaker model creation in which, for both
development and enrollment phases, an identical number of spoken utterances per
speaker is fed to the network for representing the speakers' utterances and
creation of the speaker model. This leads to simultaneously capturing the
speaker-related information and building a more robust system to cope with
within-speaker variation. We demonstrate that the proposed method significantly
outperforms the traditional d-vector verification system. Moreover, the
proposed system can also be an alternative to the traditional d-vector system
which is a one-shot speaker modeling system by utilizing 3D-CNNs.
"
"  Low-textured image stitching remains a challenging problem. It is difficult
to achieve good alignment and it is easy to break image structures due to
insufficient and unreliable point correspondences. Moreover, because of the
viewpoint variations between multiple images, the stitched images suffer from
projective distortions. To solve these problems, this paper presents a
line-guided local warping method with a global similarity constraint for image
stitching. Line features which serve well for geometric descriptions and scene
constraints, are employed to guide image stitching accurately. On one hand, the
line features are integrated into a local warping model through a designed
weight function. On the other hand, line features are adopted to impose strong
geometric constraints, including line correspondence and line colinearity, to
improve the stitching performance through mesh optimization. To mitigate
projective distortions, we adopt a global similarity constraint, which is
integrated with the projective warps via a designed weight strategy. This
constraint causes the final warp to slowly change from a projective to a
similarity transformation across the image. Finally, the images undergo a
two-stage alignment scheme that provides accurate alignment and reduces
projective distortion. We evaluate our method on a series of images and compare
it with several other methods. The experimental results demonstrate that the
proposed method provides a convincing stitching performance and that it
outperforms other state-of-the-art methods.
"
"  We study a mini-batch diversification scheme for stochastic gradient descent
(SGD). While classical SGD relies on uniformly sampling data points to form a
mini-batch, we propose a non-uniform sampling scheme based on the Determinantal
Point Process (DPP). The DPP relies on a similarity measure between data points
and gives low probabilities to mini-batches which contain redundant data, and
higher probabilities to mini-batches with more diverse data. This
simultaneously balances the data and leads to stochastic gradients with lower
variance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show
that regular SGD and a biased version of stratified sampling emerge as special
cases. Furthermore, DM-SGD generalizes stratified sampling to cases where no
discrete features exist to bin the data into groups. We show experimentally
that our method results more interpretable and diverse features in unsupervised
setups, and in better classification accuracies in supervised setups.
"
"  Data noising is an effective technique for regularizing neural network
models. While noising is widely adopted in application domains such as vision
and speech, commonly used noising primitives have not been developed for
discrete sequence-level settings such as language modeling. In this paper, we
derive a connection between input noising in neural network language models and
smoothing in $n$-gram models. Using this connection, we draw upon ideas from
smoothing to develop effective noising schemes. We demonstrate performance
gains when applying the proposed schemes to language modeling and machine
translation. Finally, we provide empirical analysis validating the relationship
between noising and smoothing.
"
"  We present a generative method to estimate 3D human motion and body shape
from monocular video. Under the assumption that starting from an initial pose
optical flow constrains subsequent human motion, we exploit flow to find
temporally coherent human poses of a motion sequence. We estimate human motion
by minimizing the difference between computed flow fields and the output of an
artificial flow renderer. A single initialization step is required to estimate
motion over multiple frames. Several regularization functions enhance
robustness over time. Our test scenarios demonstrate that optical flow
effectively regularizes the under-constrained problem of human shape and motion
estimation from monocular video.
"
"  In this paper, we study the controllability and stabilizability properties of
the Kolmogorov forward equation of a continuous time Markov chain (CTMC)
evolving on a finite state space, using the transition rates as the control
parameters. Firstly, we prove small-time local and global controllability from
and to strictly positive equilibrium configurations when the underlying graph
is strongly connected. Secondly, we show that there always exists a locally
exponentially stabilizing decentralized linear (density-)feedback law that
takes zero valu at equilibrium and respects the graph structure, provided that
the transition rates are allowed to be negative and the desired target density
lies in the interior of the set of probability densities. For bidirected
graphs, that is, graphs where a directed edge in one direction implies an edge
in the opposite direction, we show that this linear control law can be realized
using a decentralized rational feedback law of the form k(x) = a(x) +
b(x)f(x)/g(x) that also respects the graph structure and control constraints
(positivity and zero at equilibrium). This enables the possibility of using
Linear Matrix Inequality (LMI) based tools to algorithmically construct
decentralized density feedback controllers for stabilization of a robotic swarm
to a target task distribution with no task-switching at equilibrium, as we
demonstrate with several numerical examples.
"
"  We study bipartite community detection in networks, or more generally the
network biclustering problem. We present a fast two-stage procedure based on
spectral initialization followed by the application of a pseudo-likelihood
classifier twice. Under mild regularity conditions, we establish the weak
consistency of the procedure (i.e., the convergence of the misclassification
rate to zero) under a general bipartite stochastic block model. We show that
the procedure is optimal in the sense that it achieves the optimal convergence
rate that is achievable by a biclustering oracle, adaptively over the whole
class, up to constants. This is further formalized by deriving a minimax lower
bound over a class of biclustering problems. The optimal rate we obtain
sharpens some of the existing results and generalizes others to a wide regime
of average degree growth, from sparse networks with average degrees growing
arbitrarily slowly to fairly dense networks with average degrees of order
$\sqrt{n}$. As a special case, we recover the known exact recovery threshold in
the $\log n$ regime of sparsity. To obtain the consistency result, as part of
the provable version of the algorithm, we introduce a sub-block partitioning
scheme that is also computationally attractive, allowing for distributed
implementation of the algorithm without sacrificing optimality. The provable
algorithm is derived from a general class of pseudo-likelihood biclustering
algorithms that employ simple EM type updates. We show the effectiveness of
this general class by numerical simulations.
"
"  This paper demonstrates end-to-end neural network architectures for
Vietnamese named entity recognition. Our best model is a combination of
bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network
(CNN), Conditional Random Field (CRF), using pre-trained word embeddings as
input, which achieves an F1 score of 88.59% on a standard test set. Our system
is able to achieve a comparable performance to the first-rank system of the
VLSP campaign without using any syntactic or hand-crafted features. We also
give an extensive empirical study on using common deep learning models for
Vietnamese NER, at both word and character level.
"
"  We consider the problem of estimating mutual information between dependent
data, an important problem in many science and engineering applications. We
propose a data-driven, non-parametric estimator of mutual information in this
paper. The main novelty of our solution lies in transforming the data to
frequency domain to make the problem tractable. We define a novel
metric--mutual information in frequency--to detect and quantify the dependence
between two random processes across frequency using Cramér's spectral
representation. Our solution calculates mutual information as a function of
frequency to estimate the mutual information between the dependent data over
time. We validate its performance on linear and nonlinear models. In addition,
mutual information in frequency estimated as a part of our solution can also be
used to infer cross-frequency coupling in the data.
"
"  Intracranial carotid artery calcification (ICAC) is a major risk factor for
stroke, and might contribute to dementia and cognitive decline. Reliance on
time-consuming manual annotation of ICAC hampers much demanded further research
into the relationship between ICAC and neurological diseases. Automation of
ICAC segmentation is therefore highly desirable, but difficult due to the
proximity of the lesions to bony structures with a similar attenuation
coefficient. In this paper, we propose a method for automatic segmentation of
ICAC; the first to our knowledge. Our method is based on a 3D fully
convolutional neural network that we extend with two regularization techniques.
Firstly, we use deep supervision (hidden layers supervision) to encourage
discriminative features in the hidden layers. Secondly, we augment the network
with skip connections, as in the recently developed ResNet, and dropout layers,
inserted in a way that skip connections circumvent them. We investigate the
effect of skip connections and dropout. In addition, we propose a simple
problem-specific modification of the network objective function that restricts
the focus to the most important image regions and simplifies the optimization.
We train and validate our model using 882 CT scans and test on 1,000. Our
regularization techniques and objective improve the average Dice score by 7.1%,
yielding an average Dice of 76.2% and 97.7% correlation between predicted ICAC
volumes and manual annotations.
"
"  The field of structural bioinformatics has seen significant advances with the
use of Molecular Dynamics (MD) simulations of biological systems. The MD
methodology has allowed to explain and discover molecular mechanisms in a wide
range of natural processes. There is an impending need to readily share the
ever-increasing amount of MD data, which has been hindered by the lack of
specialized tools in the past. To solve this problem, we present HTMoL, a
state-of-the-art plug-in-free hardware-accelerated web application specially
designed to efficiently transfer and visualize raw MD trajectory files on a web
browser. Now, individual research labs can publish MD data on the Internet, or
use HTMoL to profoundly improve scientific reports by including supplemental MD
data in a journal publication. HTMoL can also be used as a visualization
interface to access MD trajectories generated on a high-performance computer
center directly.
Availability: HTMoL is available free of charge for academic use. All major
browsers are supported. A complete online documentation including instructions
for download, installation, configuration, and examples is available at the
HTMoL website this http URL. Supplementary data are available
online. Corresponding author: mauricio.carrillo@cinvestav.mx
"
"  The auction method developed by Bertsekas in the late 1970s is a relaxation
technique for solving integer-valued assignment problems. It resembles a
competitive bidding process, where unsatisfied persons (bidders) attempt to
claim the objects (lots) offering the best value. By transforming
integer-valued transport problems into assignment problems, the auction method
can be extended to compute optimal transport solutions. We propose a more
general auction method that can be applied directly to real-valued transport
problems. We prove termination and provide a priori error bounds for the
general auction method. Our numerical results indicate that the complexity of
the general auction is roughly comparable to that of the original auction
method, when the latter is applicable.
"
"  The purpose of the present paper is to show that: Eilenberg-type
correspondences = Birkhoff's theorem for (finite) algebras + duality. We
consider algebras for a monad T on a category D and we study (pseudo)varieties
of T-algebras. Pseudovarieties of algebras are also known in the literature as
varieties of finite algebras. Two well-known theorems that characterize
varieties and pseudovarieties of algebras play an important role here:
Birkhoff's theorem and Birkhoff's theorem for finite algebras, the latter also
known as Reiterman's theorem. We prove, under mild assumptions, a categorical
version of Birkhoff's theorem for (finite) algebras to establish a one-to-one
correspondence between (pseudo)varieties of T-algebras and (pseudo)equational
T-theories. Now, if C is a category that is dual to D and B is the comonad on C
that is the dual of T, we get a one-to-one correspondence between
(pseudo)equational T-theories and their dual, (pseudo)coequational B-theories.
Particular instances of (pseudo)coequational B-theories have been already
studied in language theory under the name of ""varieties of languages"" to
establish Eilenberg-type correspondences. All in all, we get a one-to-one
correspondence between (pseudo)varieties of T-algebras and (pseudo)coequational
B-theories, which will be shown to be exactly the nature of Eilenberg-type
correspondences.
"
"  Scientific legacy code in MATLAB/Octave not compatible with modernization of
research workflows is vastly abundant throughout academic community.
Performance of non-vectorized code written in MATLAB/Octave represents a major
burden. A new programming language for technical computing Julia, promises to
address these issues. Although Julia syntax is similar to MATLAB/Octave,
porting code to Julia may be cumbersome for researchers. Here we present
MatlabCompat.jl - a library aimed at simplifying the conversion of your
MATLAB/Octave code to Julia. We show using a simplistic image analysis use case
that MATLAB/Octave code can be easily ported to high performant Julia using
MatlabCompat.jl.
"
"  Robots such as autonomous underwater vehicles (AUVs) and autonomous surface
vehicles (ASVs) have been used for sensing and monitoring aquatic environments
such as oceans and lakes. Environmental sampling is a challenging task because
the environmental attributes to be observed can vary both spatially and
temporally, and the target environment is usually a large and continuous domain
whereas the sampling data is typically sparse and limited. The challenges
require that the sampling method must be informative and efficient enough to
catch up with the environmental dynamics. In this paper we present a planning
and learning method that enables a sampling robot to perform persistent
monitoring tasks by learning and refining a dynamic ""data map"" that models a
spatiotemporal environment attribute such as ocean salinity content. Our
environmental sampling framework consists of two components: to maximize the
information collected, we propose an informative planning component that
efficiently generates sampling waypoints that contain the maximal information;
To alleviate the computational bottleneck caused by large-scale data
accumulated, we develop a component based on a sparse Gaussian Process whose
hyperparameters are learned online by taking advantage of only a subset of data
that provides the greatest contribution. We validate our method with both
simulations running on real ocean data and field trials with an ASV in a lake
environment. Our experiments show that the proposed framework is both accurate
in learning the environmental data map and efficient in catching up with the
dynamic environmental changes.
"
"  In this paper, we present a spectral graph wavelet approach for shape
analysis of carpal bones of human wrist. We apply a metric called global
spectral graph wavelet signature for representation of cortical surface of the
carpal bone based on eigensystem of Laplace-Beltrami operator. Furthermore, we
propose a heuristic and efficient way of aggregating local descriptors of a
carpal bone surface to global descriptor. The resultant global descriptor is
not only isometric invariant, but also much more efficient and requires less
memory storage. We perform experiments on shape of the carpal bones of ten
women and ten men from a publicly-available database. Experimental results show
the excellency of the proposed GSGW compared to recent proposed GPS embedding
approach for comparing shapes of the carpal bones across populations.
"
"  Adversarial training has been shown to regularize deep neural networks in
addition to increasing their robustness to adversarial examples. However, its
impact on very deep state of the art networks has not been fully investigated.
In this paper, we present an efficient approach to perform adversarial training
by perturbing intermediate layer activations and study the use of such
perturbations as a regularizer during training. We use these perturbations to
train very deep models such as ResNets and show improvement in performance both
on adversarial and original test data. Our experiments highlight the benefits
of perturbing intermediate layer activations compared to perturbing only the
inputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the
proposed adversarial training approach. Additional results on WideResNets show
that our approach provides significant improvement in classification accuracy
for a given base model, outperforming dropout and other base models of larger
size.
"
"  The Android OS has become the most popular mobile operating system leading to
a significant increase in the spread of Android malware. Consequently, several
static and dynamic analysis systems have been developed to detect Android
malware. With dynamic analysis, efficient test input generation is needed in
order to trigger the potential run-time malicious behaviours. Most existing
dynamic analysis systems employ random-based input generation methods usually
built using the Android Monkey tool. Random-based input generation has several
shortcomings including limited code coverage, which motivates us to explore
combining it with a state-based method in order to improve efficiency. Hence,
in this paper, we present a novel hybrid test input generation approach
designed to improve dynamic analysis on real devices. We implemented the hybrid
system by integrating a random based tool (Monkey) with a state based tool
(DroidBot) in order to improve code coverage and potentially uncover more
malicious behaviours. The system is evaluated using 2,444 Android apps
containing 1222 benign and 1222 malware samples from the Android malware genome
project. Three scenarios, random only, state-based only, and our proposed
hybrid approach were investigated to comparatively evaluate their performances.
Our study shows that the hybrid approach significantly improved the amount of
dynamic features extracted from both benign and malware samples over the
state-based and commonly used random test input generation method.
"
"  Counting objects in digital images is a process that should be replaced by
machines. This tedious task is time consuming and prone to errors due to
fatigue of human annotators. The goal is to have a system that takes as input
an image and returns a count of the objects inside and justification for the
prediction in the form of object localization. We repose a problem, originally
posed by Lempitsky and Zisserman, to instead predict a count map which contains
redundant counts based on the receptive field of a smaller regression network.
The regression network predicts a count of the objects that exist inside this
frame. By processing the image in a fully convolutional way each pixel is going
to be accounted for some number of times, the number of windows which include
it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true
count we take the average over the redundant predictions. Our contribution is
redundant counting instead of predicting a density map in order to average over
errors. We also propose a novel deep neural network architecture adapted from
the Inception family of networks called the Count-ception network. Together our
approach results in a 20% relative improvement (2.9 to 2.3 MAE) over the state
of the art method by Xie, Noble, and Zisserman in 2016.
"
"  We define a variety of abstract termination principles which form
generalisations of simplification orders, and investigate their computational
content. Simplification orders, which include the well-known multiset and
lexicographic path orderings, are important techniques for proving that
computer programs terminate. Moreover, an analysis of the proofs that these
orders are wellfounded can yield additional quantitative information: namely an
upper bound on the complexity of programs reducing under these orders. In this
paper we focus on extracting computational content from the typically
non-constructive wellfoundedness proofs of termination orders, with an eye
towards the establishment of general metatheorems which characterise bounds on
the derivational complexity induced by these orders. However, ultimately we
have a much broader goal, which is to explore a number of deep mathematical
concepts which underlie termination orders, including minimal-bad-sequence
constructions, modified realizability and bar recursion. We aim to describe how
these concepts all come together to form a particularly elegant illustration of
the bridge between proofs and programs.
"
"  Constraint answer set programming is a promising research direction that
integrates answer set programming with constraint processing. It is often
informally related to the field of satisfiability modulo theories. Yet, the
exact formal link is obscured as the terminology and concepts used in these two
research areas differ. In this paper, we connect these two research areas by
uncovering the precise formal relation between them. We believe that this work
will booster the cross-fertilization of the theoretical foundations and the
existing solving methods in both areas. As a step in this direction we provide
a translation from constraint answer set programs with integer linear
constraints to satisfiability modulo linear integer arithmetic that paves the
way to utilizing modern satisfiability modulo theories solvers for computing
answer sets of constraint answer set programs.
"
"  Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.
"
"  Power plant is a complex and nonstationary system for which the traditional
machine learning modeling approaches fall short of expectations. The
ensemble-based online learning methods provide an effective way to continuously
learn from the dynamic environment and autonomously update models to respond to
environmental changes. This paper proposes such an online ensemble regression
approach to model power plant performance, which is critically important for
operation optimization. The experimental results on both simulated and real
data show that the proposed method can achieve performance with less than 1%
mean average percentage error, which meets the general expectations in field
operations.
"
"  Studies of affect labeling, i.e. putting your feelings into words, indicate
that it can attenuate positive and negative emotions. Here we track the
evolution of individual emotions for tens of thousands of Twitter users by
analyzing the emotional content of their tweets before and after they
explicitly report having a strong emotion. Our results reveal how emotions and
their expression evolve at the temporal resolution of one minute. While the
expression of positive emotions is preceded by a short but steep increase in
positive valence and followed by short decay to normal levels, negative
emotions build up more slowly, followed by a sharp reversal to previous levels,
matching earlier findings of the attenuating effects of affect labeling. We
estimate that positive and negative emotions last approximately 1.25 and 1.5
hours from onset to evanescence. A separate analysis for male and female
subjects is suggestive of possible gender-specific differences in emotional
dynamics.
"
"  This paper positively solves an open problem if it is possible to provide a
Hilbert system to Epistemic Logic of Friendship (EFL) by Seligman, Girard and
Liu. To find a Hilbert system, we first introduce a sound, complete and
cut-free tree (or nested) sequent calculus for EFL, which is an integrated
combination of Seligman's sequent calculus for basic hybrid logic and a tree
sequent calculus for modal logic. Then we translate a tree sequent into an
ordinary formula to specify a Hilbert system of EFL and finally show that our
Hilbert system is sound and complete for the intended two-dimensional
semantics.
"
"  Complex systems in a wide variety of areas such as biological modeling, image
processing, and language recognition can be modeled using networks of very
simple machines called finite automata. Connecting subsystems modeled using
finite automata into a network allows for more computational power. One such
network, called a cellular automaton, consists of an n-dimensional array for n
> 1 with a single finite automaton located at each point of the array. One of
the oldest problems associated with cellular automata is the firing
synchronization problem, originally proposed by John Myhill in 1957. As with
any long-standing problem, there are a large number of solutions to the firing
synchronization problem. Our goal, and the contribution of this work, is to
summarize recent solutions to the problem. We focus primarily on solutions to
the original problem, that is, the problem where the network is a
one-dimensional array and there is a single initiator located at one of the
ends. We summarize both minimal-time and non-minimal-time solutions, with an
emphasis on solutions that were published after 1998. We also focus on
solutions that minimize the number of states required by the finite automata.
In the process we also identify open problems that remain in terms of finding
minimal-state solutions to the firing synchronization problem.
"
"  The practical success of Boolean Satisfiability (SAT) solvers stems from the
CDCL (Conflict-Driven Clause Learning) approach to SAT solving. However, from a
propositional proof complexity perspective, CDCL is no more powerful than the
resolution proof system, for which many hard examples exist. This paper
proposes a new problem transformation, which enables reducing the decision
problem for formulas in conjunctive normal form (CNF) to the problem of solving
maximum satisfiability over Horn formulas. Given the new transformation, the
paper proves a polynomial bound on the number of MaxSAT resolution steps for
pigeonhole formulas. This result is in clear contrast with earlier results on
the length of proofs of MaxSAT resolution for pigeonhole formulas. The paper
also establishes the same polynomial bound in the case of modern core-guided
MaxSAT solvers. Experimental results, obtained on CNF formulas known to be hard
for CDCL SAT solvers, show that these can be efficiently solved with modern
MaxSAT solvers.
"
"  Underwater machine vision has attracted significant attention, but its low
quality has prevented it from a wide range of applications. Although many
different algorithms have been developed to solve this problem, real-time
adaptive methods are frequently deficient. In this paper, based on filtering
and the use of generative adversarial networks (GANs), two approaches are
proposed for the aforementioned issue, i.e., a filtering-based restoration
scheme (FRS) and a GAN-based restoration scheme (GAN-RS). Distinct from
previous methods, FRS restores underwater images in the Fourier domain, which
is composed of a parameter search, filtering, and enhancement. Aiming to
further improve the image quality, GAN-RS can adaptively restore underwater
machine vision in real time without the need for pretreatment. In particular,
information in the Lab color space and the dark channel is developed as loss
functions, namely, underwater index loss and dark channel prior loss,
respectively. More specifically, learning from the underwater index, the
discriminator is equipped with a carefully crafted underwater branch to predict
the underwater probability of an image. A multi-stage loss strategy is then
developed to guarantee the effective training of GANs. Through extensive
comparisons on the image quality and applications, the superiority of the
proposed approaches is confirmed. Consequently, the GAN-RS is considerably
faster and achieves a state-of-the-art performance in terms of the color
correction, contrast stretch, dehazing, and feature restoration of various
underwater scenes. The source code will be made available.
"
"  Recent studies have shown that sketches and diagrams play an important role
in the daily work of software developers. If these visual artifacts are
archived, they are often detached from the source code they document, because
there is no adequate tool support to assist developers in capturing, archiving,
and retrieving sketches related to certain source code artifacts. This paper
presents SketchLink, a tool that aims at increasing the value of sketches and
diagrams created during software development by supporting developers in these
tasks. Our prototype implementation provides a web application that employs the
camera of smartphones and tablets to capture analog sketches, but can also be
used on desktop computers to upload, for instance, computer-generated diagrams.
We also implemented a plugin for a Java IDE that embeds the links in Javadoc
comments and visualizes them in situ in the source code editor as graphical
icons.
"
"  Recently, two influential PNAS papers have shown how our preferences for
'Hello Kitty' and 'Harley Davidson', obtained through Facebook likes, can
accurately predict details about our personality, religiosity, political
attitude and sexual orientation (Konsinski et al. 2013; Youyou et al 2015). In
this paper, we make the claim that though the wide variety of Facebook likes
might predict such personal traits, even more accurate and generalizable
results can be reached through applying a contexts-specific, parsimonious data
strategy. We built this claim by predicting present day voter intention based
solely on likes directed toward posts from political actors. Combining the
online and offline, we join a subsample of surveyed respondents to their public
Facebook activity and apply machine learning classifiers to explore the link
between their political liking behaviour and actual voting intention. Through
this work, we show how even a single well-chosen Facebook like, can reveal as
much about our political voter intention as hundreds of random likes. Further,
by including the entire political like history of the respondents, our model
reaches prediction accuracies above previous multiparty studies (60-70%). We
conclude the paper by discussing how a parsimonious data strategy applied, with
some limitations, allow us to generalize our findings to the 1,4 million Danes
with at least one political like and even to other political multiparty
systems.
"
"  The development of positioning technologies has resulted in an increasing
amount of mobility data being available. While bringing a lot of convenience to
people's life, such availability also raises serious concerns about privacy. In
this paper, we concentrate on one of the most sensitive information that can be
inferred from mobility data, namely social relationships. We propose a novel
social relation inference attack that relies on an advanced feature learning
technique to automatically summarize users' mobility features. Compared to
existing approaches, our attack is able to predict any two individuals' social
relation, and it does not require the adversary to have any prior knowledge on
existing social relations. These advantages significantly increase the
applicability of our attack and the scope of the privacy assessment. Extensive
experiments conducted on a large dataset demonstrate that our inference attack
is effective, and achieves between 13% to 20% improvement over the best
state-of-the-art scheme. We propose three defense mechanisms -- hiding,
replacement and generalization -- and evaluate their effectiveness for
mitigating the social link privacy risks stemming from mobility data sharing.
Our experimental results show that both hiding and replacement mechanisms
outperform generalization. Moreover, hiding and replacement achieve a
comparable trade-off between utility and privacy, the former preserving better
utility and the latter providing better privacy.
"
"  Grasping is a complex process involving knowledge of the object, the
surroundings, and of oneself. While humans are able to integrate and process
all of the sensory information required for performing this task, equipping
machines with this capability is an extremely challenging endeavor. In this
paper, we investigate how deep learning techniques can allow us to translate
high-level concepts such as motor imagery to the problem of robotic grasp
synthesis. We explore a paradigm based on generative models for learning
integrated object-action representations, and demonstrate its capacity for
capturing and generating multimodal, multi-finger grasp configurations on a
simulated grasping dataset.
"
"  State-of-the-art speaker diarization systems utilize knowledge from external
data, in the form of a pre-trained distance metric, to effectively determine
relative speaker identities to unseen data. However, much of recent focus has
been on choosing the appropriate feature extractor, ranging from pre-trained
$i-$vectors to representations learned via different sequence modeling
architectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting
off-the-shelf metric learning solutions. In this paper, we argue that,
regardless of the feature extractor, it is crucial to carefully design a metric
learning pipeline, namely the loss function, the sampling strategy and the
discrimnative margin parameter, for building robust diarization systems.
Furthermore, we propose to adopt a fine-grained validation process to obtain a
comprehensive evaluation of the generalization power of metric learning
pipelines. To this end, we measure diarization performance across different
language speakers, and variations in the number of speakers in a recording.
Using empirical studies, we provide interesting insights into the effectiveness
of different design choices and make recommendations.
"
"  Social networks are typical attributed networks with node attributes.
Different from traditional attribute community detection problem aiming at
obtaining the whole set of communities in the network, we study an
application-oriented problem of mining an application-aware community
organization with respect to specific concerned attributes. The concerned
attributes are designated based on the requirements of any application by a
user in advance. The application-aware community organization w.r.t. concerned
attributes consists of the communities with feature subspaces containing these
concerned attributes. Besides concerned attributes, feature subspace of each
required community may contain some other relevant attributes. All relevant
attributes of a feature subspace jointly describe and determine the community
embedded in such subspace. Thus the problem includes two subproblems, i.e., how
to expand the set of concerned attributes to complete feature subspaces and how
to mine the communities embedded in the expanded subspaces. Two subproblems are
jointly solved by optimizing a quality function called subspace fitness. An
algorithm called ACM is proposed. In order to locate the communities
potentially belonging to the application-aware community organization, cohesive
parts of a network backbone composed of nodes with similar concerned attributes
are detected and set as the community seeds. The set of concerned attributes is
set as the initial subspace for all community seeds. Then each community seed
and its attribute subspace are adjusted iteratively to optimize the subspace
fitness. Extensive experiments on synthetic datasets demonstrate the
effectiveness and efficiency of our method and applications on real-world
networks show its application values.
"
"  In this paper, we propose a novel unfitted finite element method for the
simulation of multiple body contact. The computational mesh is generated
independently of the geometry of the interacting solids, which can be
arbitrarily complex. The key novelty of the approach is the combination of
elements of the CutFEM technology, namely the enrichment of the solution field
via the definition of overlapping fictitious domains with a dedicated
penalty-type regularisation of discrete operators, and the LaTIn hybrid-mixed
formulation of complex interface conditions. Furthermore, the novel P1-P1
discretisation scheme that we propose for the unfitted LaTIn solver is shown to
be stable, robust and optimally convergent with mesh refinement. Finally, the
paper introduces a high-performance 3D level-set/CutFEM framework for the
versatile and robust solution of contact problems involving multiple bodies of
complex geometries, with more than two bodies interacting at a single point.
"
"  We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.
"
"  A general formulation of optimization problems in which various candidate
solutions may use different feature-sets is presented, encompassing supervised
classification, automated program learning and other cases. A novel
characterization of the concept of a ""good quality feature"" for such an
optimization problem is provided; and a proposal regarding the integration of
quality based feature selection into metalearning is suggested, wherein the
quality of a feature for a problem is estimated using knowledge about related
features in the context of related problems. Results are presented regarding
extensive testing of this ""feature metalearning"" approach on supervised text
classification problems; it is demonstrated that, in this context, feature
metalearning can provide significant and sometimes dramatic speedup over
standard feature selection heuristics.
"
"  Portable computing devices, which include tablets, smart phones and various
types of wearable sensors, experienced a rapid development in recent years. One
of the most critical limitations for these devices is the power consumption as
they use batteries as the power supply. However, the bottleneck of the power
saving schemes in both hardware design and software algorithm is the huge
variability in power consumption. The variability is caused by a myriad of
factors, including the manufacturing process, the ambient environment
(temperature, humidity), the aging effects and etc. As the technology node
scaled down to 28nm and even lower, the variability becomes more severe. As a
result, a platform for variability characterization seems to be very necessary
and helpful.
"
"  Spinal cord stimulation has enabled humans with motor complete spinal cord
injury (SCI) to independently stand and recover some lost autonomic function.
Quantifying the quality of bipedal standing under spinal stimulation is
important for spinal rehabilitation therapies and for new strategies that seek
to combine spinal stimulation and rehabilitative robots (such as exoskeletons)
in real time feedback. To study the potential for automated electromyography
(EMG) analysis in SCI, we evaluated the standing quality of paralyzed patients
undergoing electrical spinal cord stimulation using both video and
multi-channel surface EMG recordings during spinal stimulation therapy
sessions. The quality of standing under different stimulation settings was
quantified manually by experienced clinicians. By correlating features of the
recorded EMG activity with the expert evaluations, we show that multi-channel
EMG recording can provide accurate, fast, and robust estimation for the quality
of bipedal standing in spinally stimulated SCI patients. Moreover, our analysis
shows that the total number of EMG channels needed to effectively predict
standing quality can be reduced while maintaining high estimation accuracy,
which provides more flexibility for rehabilitation robotic systems to
incorporate EMG recordings.
"
"  Media seems to have become more partisan, often providing a biased coverage
of news catering to the interest of specific groups. It is therefore essential
to identify credible information content that provides an objective narrative
of an event. News communities such as digg, reddit, or newstrust offer
recommendations, reviews, quality ratings, and further insights on journalistic
works. However, there is a complex interaction between different factors in
such online communities: fairness and style of reporting, language clarity and
objectivity, topical perspectives (like political viewpoint), expertise and
bias of community members, and more. This paper presents a model to
systematically analyze the different interactions in a news community between
users, news, and sources. We develop a probabilistic graphical model that
leverages this joint interaction to identify 1) highly credible news articles,
2) trustworthy news sources, and 3) expert users who perform the role of
""citizen journalists"" in the community. Our method extends CRF models to
incorporate real-valued ratings, as some communities have very fine-grained
scales that cannot be easily discretized without losing information. To the
best of our knowledge, this paper is the first full-fledged analysis of
credibility, trust, and expertise in news communities.
"
"  Currently, we are in an environment where the fraction of automated vehicles
is negligibly small. We anticipate that this fraction will increase in coming
decades before if ever, we have a fully automated transportation system.
Motivated by this we address the problem of provable safety of mixed traffic
consisting of both intelligent vehicles (IVs) as well as human-driven vehicles
(HVs). An important issue that arises is that such mixed systems may well have
lesser throughput than all human traffic systems if the automated vehicles are
expected to remain provably safe with respect to human traffic. This
necessitates the consideration of strategies such as platooning of automated
vehicles in order to increase the throughput. In this paper, we address the
design of provably safe systems consisting of a mix of automated and
human-driven vehicles including the use of platooning by automated vehicles.
We design motion planing policies and coordination rules for participants in
this novel mixed system. HVs are considered as nearsighted and modeled with
relatively loose constraints, while IVs are considered as capable of following
much tighter constraints. HVs are expected to follow reasonable and simple
rules. IVs are designed to move under a model predictive control (MPC) based
motion plans and coordination protocols. Our contribution of this paper is in
showing how to integrate these two types of models safely into a mixed system.
System safety is proved in single lane scenarios, as well as in multi-lane
situations allowing lane changes.
"
"  We consider the problem of streaming kernel regression, when the observations
arrive sequentially and the goal is to recover the underlying mean function,
assumed to belong to an RKHS. The variance of the noise is not assumed to be
known. In this context, we tackle the problem of tuning the regularization
parameter adaptively at each time step, while maintaining tight confidence
bounds estimates on the value of the mean function at each point. To this end,
we first generalize existing results for finite-dimensional linear regression
with fixed regularization and known variance to the kernel setup with a
regularization parameter allowed to be a measurable function of past
observations. Then, using appropriate self-normalized inequalities we build
upper and lower bound estimates for the variance, leading to Bersntein-like
concentration bounds. The later is used in order to define the adaptive
regularization. The bounds resulting from our technique are valid uniformly
over all observation points and all time steps, and are compared against the
literature with numerical experiments. Finally, the potential of these tools is
illustrated by an application to kernelized bandits, where we revisit the
Kernel UCB and Kernel Thompson Sampling procedures, and show the benefits of
the novel adaptive kernel tuning strategy.
"
"  Learning an encoding of feature vectors in terms of an over-complete
dictionary or a information geometric (Fisher vectors) construct is wide-spread
in statistical signal processing and computer vision. In content based
information retrieval using deep-learning classifiers, such encodings are
learnt on the flattened last layer, without adherence to the multi-linear
structure of the underlying feature tensor. We illustrate a variety of feature
encodings incl. sparse dictionary coding and Fisher vectors along with
proposing that a structured tensor factorization scheme enables us to perform
retrieval that can be at par, in terms of average precision, with Fisher vector
encoded image signatures. In short, we illustrate how structural constraints
increase retrieval fidelity.
"
"  We propose hMDAP, a hybrid framework for large-scale data analytical
processing on Spark, to support multi-paradigm process (incl. OLAP, machine
learning, and graph analysis etc.) in distributed environments. The framework
features a three-layer data process module and a business process module which
controls the former. We will demonstrate the strength of hMDAP by using traffic
scenarios in a real world.
"
"  A novel approach for unsupervised domain adaptation for neural networks is
proposed. It relies on metric-based regularization of the learning process. The
metric-based regularization aims at domain-invariant latent feature
representations by means of maximizing the similarity between domain-specific
activation distributions. The proposed metric results from modifying an
integral probability metric such that it becomes less translation-sensitive on
a polynomial function space. The metric has an intuitive interpretation in the
dual space as the sum of differences of higher order central moments of the
corresponding activation distributions. Under appropriate assumptions on the
input distributions, error minimization is proven for the continuous case. As
demonstrated by an analysis of standard benchmark experiments for sentiment
analysis, object recognition and digit recognition, the outlined approach is
robust regarding parameter changes and achieves higher classification
accuracies than comparable approaches. The source code is available at
this https URL.
"
"  Improving the quality of end-of-life care for hospitalized patients is a
priority for healthcare organizations. Studies have shown that physicians tend
to over-estimate prognoses, which in combination with treatment inertia results
in a mismatch between patients wishes and actual care at the end of life. We
describe a method to address this problem using Deep Learning and Electronic
Health Record (EHR) data, which is currently being piloted, with Institutional
Review Board approval, at an academic medical center. The EHR data of admitted
patients are automatically evaluated by an algorithm, which brings patients who
are likely to benefit from palliative care services to the attention of the
Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR
data from previous years, to predict all-cause 3-12 month mortality of patients
as a proxy for patients that could benefit from palliative care. Our
predictions enable the Palliative Care team to take a proactive approach in
reaching out to such patients, rather than relying on referrals from treating
physicians, or conduct time consuming chart reviews of all patients. We also
present a novel interpretation technique which we use to provide explanations
of the model's predictions.
"
"  Convolutional sparse coding (CSC) improves sparse coding by learning a
shift-invariant dictionary from the data. However, existing CSC algorithms
operate in the batch mode and are expensive, in terms of both space and time,
on large datasets. In this paper, we alleviate these problems by using online
learning. The key is a reformulation of the CSC objective so that convolution
can be handled easily in the frequency domain and much smaller history matrices
are needed. We use the alternating direction method of multipliers (ADMM) to
solve the resulting optimization problem and the ADMM subproblems have
efficient closed-form solutions. Theoretical analysis shows that the learned
dictionary converges to a stationary point of the optimization problem.
Extensive experiments show that convergence of the proposed method is much
faster and its reconstruction performance is also better. Moreover, while
existing CSC algorithms can only run on a small number of images, the proposed
method can handle at least ten times more images.
"
"  It is challenging to recognize facial action unit (AU) from spontaneous
facial displays, especially when they are accompanied by speech. The major
reason is that the information is extracted from a single source, i.e., the
visual channel, in the current practice. However, facial activity is highly
correlated with voice in natural human communications.
Instead of solely improving visual observations, this paper presents a novel
audiovisual fusion framework, which makes the best use of visual and acoustic
cues in recognizing speech-related facial AUs. In particular, a dynamic
Bayesian network (DBN) is employed to explicitly model the semantic and dynamic
physiological relationships between AUs and phonemes as well as measurement
uncertainty. A pilot audiovisual AU-coded database has been collected to
evaluate the proposed framework, which consists of a ""clean"" subset containing
frontal faces under well controlled circumstances and a challenging subset with
large head movements and occlusions. Experiments on this database have
demonstrated that the proposed framework yields significant improvement in
recognizing speech-related AUs compared to the state-of-the-art visual-based
methods especially for those AUs whose visual observations are impaired during
speech, and more importantly also outperforms feature-level fusion methods by
explicitly modeling and exploiting physiological relationships between AUs and
phonemes.
"
"  We propose an effective method for creating interpretable control agents, by
\textit{re-purposing} the function of a biological neural circuit model, to
govern simulated and real world reinforcement learning (RL) test-beds. Inspired
by the structure of the nervous system of the soil-worm, \emph{C. elegans}, we
introduce \emph{Neuronal Circuit Policies} (NCPs) as a novel recurrent neural
network instance with liquid time-constants, universal approximation
capabilities and interpretable dynamics. We theoretically show that they can
approximate any finite simulation time of a given continuous n-dimensional
dynamical system, with $n$ output units and some hidden units. We model
instances of the policies and learn their synaptic and neuronal parameters to
control standard RL tasks and demonstrate its application for autonomous
parking of a real rover robot on a pre-defined trajectory. For reconfiguration
of the \emph{purpose} of the neural circuit, we adopt a search-based RL
algorithm. We show that our neuronal circuit policies perform as good as deep
neural network policies with the advantage of realizing interpretable dynamics
at the cell-level. We theoretically find bounds for the time-varying dynamics
of the circuits, and introduce a novel way to reason about networks' dynamics.
"
"  We present a simple, yet useful result about the expected value of the
determinant of random sum of rank-one matrices. Computing such expectations in
general may involve a sum over exponentially many terms. Nevertheless, we show
that an interesting and useful class of such expectations that arise in, e.g.,
D-optimal estimation and random graphs can be computed efficiently via
computing a single determinant.
"
"  A general Boltzmann machine with continuous visible and discrete integer
valued hidden states is introduced. Under mild assumptions about the connection
matrices, the probability density function of the visible units can be solved
for analytically, yielding a novel parametric density function involving a
ratio of Riemann-Theta functions. The conditional expectation of a hidden state
for given visible states can also be calculated analytically, yielding a
derivative of the logarithmic Riemann-Theta function. The conditional
expectation can be used as activation function in a feedforward neural network,
thereby increasing the modelling capacity of the network. Both the Boltzmann
machine and the derived feedforward neural network can be successfully trained
via standard gradient- and non-gradient-based optimization techniques.
"
"  Representation learning has become an invaluable approach for learning from
symbolic data such as text and graphs. However, while complex symbolic datasets
often exhibit a latent hierarchical structure, state-of-the-art methods
typically learn embeddings in Euclidean vector spaces, which do not account for
this property. For this purpose, we introduce a new approach for learning
hierarchical representations of symbolic data by embedding them into hyperbolic
space -- or more precisely into an n-dimensional Poincaré ball. Due to the
underlying hyperbolic geometry, this allows us to learn parsimonious
representations of symbolic data by simultaneously capturing hierarchy and
similarity. We introduce an efficient algorithm to learn the embeddings based
on Riemannian optimization and show experimentally that Poincaré embeddings
outperform Euclidean embeddings significantly on data with latent hierarchies,
both in terms of representation capacity and in terms of generalization
ability.
"
"  Complex contagion models have been developed to understand a wide range of
social phenomena such as adoption of cultural fads, the diffusion of belief,
norms, and innovations in social networks, and the rise of collective action to
join a riot. Most existing works focus on contagions where individuals' states
are represented by {\em binary} variables, and propagation takes place over a
single isolated network. However, characterization of an individual's standing
on a given matter as a binary state might be overly simplistic as most of our
opinions, feelings, and perceptions vary over more than two states. Also, most
real-world contagions take place over multiple networks (e.g., Twitter and
Facebook) or involve {\em multiplex} networks where individuals engage in
different {\em types} of relationships (e.g., acquaintance, co-worker, family,
etc.). To this end, this paper studies {\em multi-stage} complex contagions
that take place over multi-layer or multiplex networks. Under a linear
threshold based contagion model, we give analytic results for the probability
and expected size of \textit{global} cascades, i.e., cases where a randomly
chosen node can initiate a propagation that eventually reaches a {\em positive}
fraction of the whole population. Analytic results are also confirmed and
supported by an extensive numerical study. In particular, we demonstrate how
the dynamics of complex contagions is affected by the extra weight exerted by
\textit{hyper-active} nodes and by the structural properties of the networks
involved. Among other things, we reveal an interesting connection between the
assortativity of a network and the impact of \textit{hyper-active} nodes on the
cascade size.
"
"  In network coding, we discuss the effect of sequential error injection on
information leakage. We show that there is no improvement when the operations
in the network are linear operations. However, when the operations in the
network contains non-linear operations, we find a counterexample to improve
Eve's obtained information. Furthermore, we discuss the asymptotic rate in a
linear network under the secrecy and robustness conditions as well as under the
secrecy condition alone. Finally, we apply our results to network quantum key
distribution, which clarifies the type of network that enables us to realize
secure long distance communication via short distance quantum key distribution.
"
"  Simultaneous Localization and Mapping (SLAM) is the problem of constructing a
map of an agent's environment while localizing or tracking the mobile agent's
position and orientation within the map. Algorithms for SLAM have high
computational requirements, which has hindered their use on embedded devices.
Approximation can be used to reduce the time and energy requirements of SLAM
implementations as long as the approximations do not prevent the agent from
navigating correctly through the environment. Previous studies of approximation
in SLAM have assumed that the entire trajectory of the agent is known before
the agent starts to move, and they have focused on offline controllers that use
features of the trajectory to set approximation knobs at the start of the
trajectory. In practice, the trajectory is not usually known ahead of time, and
allowing knob settings to change dynamically opens up more opportunities for
reducing computation time and energy.
We describe SLAMBooster, an application-aware online control system for SLAM
that adaptively controls approximation knobs during the motion of the agent.
SLAMBooster is based on a control technique called hierarchical proportional
control but our experiments showed this application-agnostic control led to an
unacceptable reduction in the quality of localization. To address this problem,
SLAMBooster exploits domain knowledge: it uses features extracted from input
frames and from the estimated motion of the agent in its algorithm for
controlling approximation.
We implemented SLAMBooster in the open-source SLAMBench framework. Our
experiments show that SLAMBooster reduces the computation time and energy
consumption by around half on the average on an embedded platform, while
maintaining the accuracy of the localization within reasonable bounds. These
improvements make it feasible to deploy SLAM on a wider range of devices.
"
"  Style transfer methods have achieved significant success in recent years with
the use of convolutional neural networks. However, many of these methods
concentrate on artistic style transfer with few constraints on the output image
appearance. We address the challenging problem of transferring face texture
from a style face image to a content face image in a photorealistic manner
without changing the identity of the original content image. Our framework for
face texture transfer (FaceTex) augments the prior work of MRF-CNN with a novel
facial semantic regularization that incorporates a face prior regularization
smoothly suppressing the changes around facial meso-structures (e.g eyes, nose
and mouth) and a facial structure loss function which implicitly preserves the
facial structure so that face texture can be transferred without changing the
original identity. We demonstrate results on face images and compare our
approach with recent state-of-the-art methods. Our results demonstrate superior
texture transfer because of the ability to maintain the identity of the
original face image.
"
"  This paper proposes an innovative method for segmentation of skin lesions in
dermoscopy images developed by the authors, based on fuzzy classification of
pixels and histogram thresholding.
"
"  In this paper, we explore deep reinforcement learning algorithms for
vision-based robotic grasping. Model-free deep reinforcement learning (RL) has
been successfully applied to a range of challenging environments, but the
proliferation of algorithms makes it difficult to discern which particular
approach would be best suited for a rich, diverse task like grasping. To answer
this question, we propose a simulated benchmark for robotic grasping that
emphasizes off-policy learning and generalization to unseen objects. Off-policy
learning enables utilization of grasping data over a wide variety of objects,
and diversity is important to enable the method to generalize to new objects
that were not seen during training. We evaluate the benchmark tasks against a
variety of Q-function estimation methods, a method previously proposed for
robotic grasping with deep neural network models, and a novel approach based on
a combination of Monte Carlo return estimation and an off-policy correction.
Our results indicate that several simple methods provide a surprisingly strong
competitor to popular algorithms such as double Q-learning, and our analysis of
stability sheds light on the relative tradeoffs between the algorithms.
"
"  The key issues pertaining to collection of epidemic disease data for our
analysis purposes are that it is a labour intensive, time consuming and
expensive process resulting in availability of sparse sample data which we use
to develop prediction models. To address this sparse data issue, we present
novel Incremental Transductive methods to circumvent the data collection
process by applying previously acquired data to provide consistent,
confidence-based labelling alternatives to field survey research. We
investigated various reasoning approaches for semisupervised machine learning
including Bayesian models for labelling data. The results show that using the
proposed methods, we can label instances of data with a class of vector density
at a high level of confidence. By applying the Liberal and Strict Training
Approaches, we provide a labelling and classification alternative to standalone
algorithms. The methods in this paper are components in the process of reducing
the proliferation of the Schistosomiasis disease and its effects.
"
"  We investigate an end-to-end method for automatically inducing task-based
dialogue systems from small amounts of unannotated dialogue data. It combines
an incremental semantic grammar - Dynamic Syntax and Type Theory with Records
(DS-TTR) - with Reinforcement Learning (RL), where language generation and
dialogue management are a joint decision problem. The systems thus produced are
incremental: dialogues are processed word-by-word, shown previously to be
essential in supporting natural, spontaneous dialogue. We hypothesised that the
rich linguistic knowledge within the grammar should enable a combinatorially
large number of dialogue variations to be processed, even when trained on very
few dialogues. Our experiments show that our model can process 74% of the
Facebook AI bAbI dataset even when trained on only 0.13% of the data (5
dialogues). It can in addition process 65% of bAbI+, a corpus we created by
systematically adding incremental dialogue phenomena such as restarts and
self-corrections to bAbI. We compare our model with a state-of-the-art
retrieval model, MemN2N. We find that, in terms of semantic accuracy, MemN2N
shows very poor robustness to the bAbI+ transformations even when trained on
the full bAbI dataset.
"
"  Multi-label classification is a practical yet challenging task in machine
learning related fields, since it requires the prediction of more than one
label category for each input instance. We propose a novel deep neural networks
(DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this
task. Aiming at better relating feature and label domain data for improved
classification, we uniquely perform joint feature and label embedding by
deriving a deep latent space, followed by the introduction of label-correlation
sensitive loss function for recovering the predicted label outputs. Our C2AE is
achieved by integrating the DNN architectures of canonical correlation analysis
and autoencoder, which allows end-to-end learning and prediction with the
ability to exploit label dependency. Moreover, our C2AE can be easily extended
to address the learning problem with missing labels. Our experiments on
multiple datasets with different scales confirm the effectiveness and
robustness of our proposed method, which is shown to perform favorably against
state-of-the-art methods for multi-label classification.
"
"  Combinatorial filters have been the subject of increasing interest from the
robotics community in recent years. This paper considers automatic reduction of
combinatorial filters to a given size, even if that reduction necessitates
changes to the filter's behavior. We introduce an algorithmic problem called
improper filter reduction, in which the input is a combinatorial filter F along
with an integer k representing the target size. The output is another
combinatorial filter F' with at most k states, such that the difference in
behavior between F and F' is minimal. We present two metrics for measuring the
distance between pairs of filters, describe dynamic programming algorithms for
computing these distances, and show that improper filter reduction is NP-hard
under these metrics. We then describe two heuristic algorithms for improper
filter reduction, one greedy sequential approach, and one randomized global
approach based on prior work on weighted improper graph coloring. We have
implemented these algorithms and analyze the results of three sets of
experiments.
"
"  Networked data, in which every training example involves two objects and may
share some common objects with others, is used in many machine learning tasks
such as learning to rank and link prediction. A challenge of learning from
networked examples is that target values are not known for some pairs of
objects. In this case, neither the classical i.i.d.\ assumption nor techniques
based on complete U-statistics can be used. Most existing theoretical results
of this problem only deal with the classical empirical risk minimization (ERM)
principle that always weights every example equally, but this strategy leads to
unsatisfactory bounds. We consider general weighted ERM and show new universal
risk bounds for this problem. These new bounds naturally define an optimization
problem which leads to appropriate weights for networked examples. Though this
optimization problem is not convex in general, we devise a new fully
polynomial-time approximation scheme (FPTAS) to solve it.
"
"  We investigate the similarities of pairs of articles which are co-cited at
the different co-citation levels of the journal, article, section, paragraph,
sentence and bracket. Our results indicate that textual similarity,
intellectual overlap (shared references), author overlap (shared authors),
proximity in publication time all rise monotonically as the co-citation level
gets lower (from journal to bracket). While the main gain in similarity happens
when moving from journal to article co-citation, all level changes entail an
increase in similarity, especially section to paragraph and paragraph to
sentence/bracket levels. We compare results from four journals over the years
2010-2015: Cell, the European Journal of Operational Research, Physics Letters
B and Research Policy, with consistent general outcomes and some interesting
differences. Our findings motivate the use of granular co-citation information
as defined by meaningful units of text, with implications for, among others,
the elaboration of maps of science and the retrieval of scholarly literature.
"
"  Comparing with traditional learning criteria, such as mean square error
(MSE), the minimum error entropy (MEE) criterion is superior in nonlinear and
non-Gaussian signal processing and machine learning. The argument of the
logarithm in Renyis entropy estimator, called information potential (IP), is a
popular MEE cost in information theoretic learning (ITL). The computational
complexity of IP is however quadratic in terms of sample number due to double
summation. This creates computational bottlenecks especially for large-scale
datasets. To address this problem, in this work we propose an efficient
quantization approach to reduce the computational burden of IP, which decreases
the complexity from O(N*N) to O (MN) with M << N. The new learning criterion is
called the quantized MEE (QMEE). Some basic properties of QMEE are presented.
Illustrative examples are provided to verify the excellent performance of QMEE.
"
"  A novel predictor for traffic flow forecasting, namely spatio-temporal
Bayesian network predictor, is proposed. Unlike existing methods, our approach
incorporates all the spatial and temporal information available in a
transportation network to carry our traffic flow forecasting of the current
site. The Pearson correlation coefficient is adopted to rank the input
variables (traffic flows) for prediction, and the best-first strategy is
employed to select a subset as the cause nodes of a Bayesian network. Given the
derived cause nodes and the corresponding effect node in the spatio-temporal
Bayesian network, a Gaussian Mixture Model is applied to describe the
statistical relationship between the input and output. Finally, traffic flow
forecasting is performed under the criterion of Minimum Mean Square Error
(M.M.S.E.). Experimental results with the urban vehicular flow data of Beijing
demonstrate the effectiveness of our presented spatio-temporal Bayesian network
predictor.
"
"  A person dependent network, called an AlterEgo net, is proposed for
development. The networks are created per person. It receives at input an
object descriptions and outputs a simulation of the internal person's
representation of the objects. The network generates a textual stream
resembling the narrative stream of consciousness depicting multitudinous
thoughts and feelings related to a perceived object. In this way, the object is
described not by a 'static' set of its properties, like a dictionary, but by
the stream of words and word combinations referring to the object. The network
simulates a person's dialogue with a representation of the object. It is based
on an introduced algorithmic scheme, where perception is modeled by two
interacting iterative cycles, reminding one respectively the forward and
backward propagation executed at training convolution neural networks. The
'forward' iterations generate a stream representing the 'internal world' of a
human. The 'backward' iterations generate a stream representing an internal
representation of the object. People perceive the world differently. Tuning
AlterEgo nets to a specific person or group of persons, will allow simulation
of their thoughts and feelings. Thereby these nets is potentially a new human
augmentation technology for various applications.
"
"  Univalent homotopy type theory (HoTT) may be seen as a language for the
category of $\infty$-groupoids. It is being developed as a new foundation for
mathematics and as an internal language for (elementary) higher toposes. We
develop the theory of factorization systems, reflective subuniverses, and
modalities in homotopy type theory, including their construction using a
""localization"" higher inductive type. This produces in particular the
($n$-connected, $n$-truncated) factorization system as well as internal
presentations of subtoposes, through lex modalities. We also develop the
semantics of these constructions.
"
"  One of the major hurdles toward automatic semantic understanding of computer
programs is the lack of knowledge about what constitutes functional equivalence
of code segments. We postulate that a sound knowledgebase can be used to
deductively understand code segments in a hierarchical fashion by first
de-constructing a code and then reconstructing it from elementary knowledge and
equivalence rules of elementary code segments. The approach can also be
engineered to produce computable programs from conceptual and abstract
algorithms as an inverse function. In this paper, we introduce the core idea
behind the MindReader online assessment system that is able to understand a
wide variety of elementary algorithms students learn in their entry level
programming classes such as Java, C++ and Python. The MindReader system is able
to assess student assignments and guide them how to develop correct and better
code in real time without human assistance.
"
"  In this paper we describe EasyInterface, an open-source toolkit for rapid
development of web-based graphical user interfaces (GUIs). This toolkit
addresses the need of researchers to make their research prototype tools
available to the community, and integrating them in a common environment,
rapidly and without being familiar with web programming or GUI libraries in
general. If a tool can be executed from a command-line and its output goes to
the standard output, then in few minutes one can make it accessible via a
web-interface or within Eclipse. Moreover, the toolkit defines a text-based
language that can be used to get more sophisticated GUIs, e.g., syntax
highlighting, dialog boxes, user interactions, etc. EasyInterface was
originally developed for building a common frontend for tools developed in the
Envisage project.
"
"  Multi-start algorithms are a common and effective tool for metaheuristic
searches. In this paper we amplify multi-start capabilities by employing the
parallel processing power of the graphics processer unit (GPU) to quickly
generate a diverse starting set of solutions for the Unconstrained Binary
Quadratic Optimization Problem which are evaluated and used to implement
screening methods to select solutions for further optimization. This method is
implemented as an initial high quality solution generation phase prior to a
secondary steepest ascent search and a comparison of results to best known
approaches on benchmark unconstrained binary quadratic problems demonstrates
that GPU-enabled diversified multi-start with screening quickly yields very
good results.
"
"  Estimation of the number of endmembers existing in a scene constitutes a
critical task in the hyperspectral unmixing process. The accuracy of this
estimate plays a crucial role in subsequent unsupervised unmixing steps i.e.,
the derivation of the spectral signatures of the endmembers (endmembers'
extraction) and the estimation of the abundance fractions of the pixels. A
common practice amply followed in literature is to treat endmembers' number
estimation and unmixing, independently as two separate tasks, providing the
outcome of the former as input to the latter. In this paper, we go beyond this
computationally demanding strategy. More precisely, we set forth a multiple
constrained optimization framework, which encapsulates endmembers' number
estimation and unsupervised unmixing in a single task. This is attained by
suitably formulating the problem via a low-rank and sparse nonnegative matrix
factorization rationale, where low-rankness is promoted with the use of a
sophisticated $\ell_2/\ell_1$ norm penalty term. An alternating proximal
algorithm is then proposed for minimizing the emerging cost function. The
results obtained by simulated and real data experiments verify the
effectiveness of the proposed approach.
"
"  The problem of three-user multiple-access channel (MAC) with noiseless
feedback is investigated. A new coding strategy is presented. The coding scheme
builds upon the natural extension of the Cover-Leung (CL) scheme; and uses
quasi-linear codes. A new single-letter achievable rate region is derived. The
new achievable region strictly contains the CL region. This is shown through an
example. In this example, the coding scheme achieves optimality in terms of
transmission rates. It is shown that any optimality achieving scheme for this
example must have a specific algebraic structure. Particularly, the codebooks
must be closed under binary addition.
"
"  The meta distribution of the signal-to-interference ratio (SIR) provides
fine-grained information about the performance of individual links in a
wireless network. This paper focuses on the analysis of the meta distribution
of the SIR for both the cellular network uplink and downlink with fractional
power control. For the uplink scenario, an approximation of the interfering
user point process with a non-homogeneous Poisson point process is used. The
moments of the meta distribution for both scenarios are calculated. Some
bounds, the analytical expression, the mean local delay, and the beta
approximation of the meta distribution are provided. The results give
interesting insights into the effect of the power control in both the uplink
and downlink. Detailed simulations show that the approximations made in the
analysis are well justified.
"
"  The analysis of industrial processes, modelled as descriptor systems, is
often computationally hard due to the presence of both algebraic couplings and
difference equations of high order. In this paper, we introduce a control
refinement notion for these descriptor systems that enables analysis and
control design over related reduced-order systems. Utilising the behavioural
framework, we extend upon the standard hierarchical control refinement for
ordinary systems and allow for algebraic couplings inherent to descriptor
systems.
"
"  This paper presents our approach to the quantitative modeling and analysis of
highly (re)configurable systems, such as software product lines. Different
combinations of the optional features of such a system give rise to
combinatorially many individual system variants. We use a formal modeling
language that allows us to model systems with probabilistic behavior, possibly
subject to quantitative feature constraints, and able to dynamically install,
remove or replace features. More precisely, our models are defined in the
probabilistic feature-oriented language QFLAN, a rich domain specific language
(DSL) for systems with variability defined in terms of features. QFLAN
specifications are automatically encoded in terms of a process algebra whose
operational behavior interacts with a store of constraints, and hence allows to
separate system configuration from system behavior. The resulting probabilistic
configurations and behavior converge seamlessly in a semantics based on
discrete-time Markov chains, thus enabling quantitative analysis. Our analysis
is based on statistical model checking techniques, which allow us to scale to
larger models with respect to precise probabilistic analysis techniques. The
analyses we can conduct range from the likelihood of specific behavior to the
expected average cost, in terms of feature attributes, of specific system
variants. Our approach is supported by a novel Eclipse-based tool which
includes state-of-the-art DSL utilities for QFLAN based on the Xtext framework
as well as analysis plug-ins to seamlessly run statistical model checking
analyses. We provide a number of case studies that have driven and validated
the development of our framework.
"
"  Many augmented reality (AR) applications operate within near-field reaching
distances, and require matching the depth of a virtual object with a real
object. The accuracy of this matching was measured in three experiments, which
examined the effect of focal distance, age, and brightness, within distances of
33.3 to 50 cm, using a custom-built AR haploscope. Experiment I examined the
effect of focal demand, at the levels of collimated (infinite focal distance),
consistent with other depth cues, and at the midpoint of reaching distance.
Observers were too young to exhibit age-related reductions in accommodative
ability. The depth matches of collimated targets were increasingly
overestimated with increasing distance, consistent targets were slightly
underestimated, and midpoint targets were accurately estimated. Experiment II
replicated Experiment I, with older observers. Results were similar to
Experiment I. Experiment III replicated Experiment I with dimmer targets, using
young observers. Results were again consistent with Experiment I, except that
both consistent and midpoint targets were accurately estimated. In all cases,
collimated results were explained by a model, where the collimation biases the
eyes' vergence angle outwards by a constant amount. Focal demand and brightness
affect near-field AR depth matching, while age-related reductions in
accommodative ability have no effect.
"
"  In this article, we study the problem of controlling a highway segment facing
stochastic perturbations, such as recurrent incidents and moving bottlenecks.
To model traffic flow under perturbations, we use the cell-transmission model
with Markovian capacities. The control inputs are: (i) the inflows that are
sent to various on-ramps to the highway (for managing traffic demand), and (ii)
the priority levels assigned to the on-ramp traffic relative to the mainline
traffic (for allocating highway capacity). The objective is to maximize the
throughput while ensuring that on-ramp queues remain bounded in the long-run.
We develop a computational approach to solving this stability-constrained,
throughput-maximization problem. Firstly, we use the classical drift condition
in stability analysis of Markov processes to derive a sufficient condition for
boundedness of on-ramp queues. Secondly, we show that our control design
problem can be formulated as a mixed integer program with linear or bilinear
constraints, depending on the complexity of Lyapunov function involved in the
stability condition. Finally, for specific types of capacity perturbations, we
derive intuitive criteria for managing demand and/or selecting priority levels.
These criteria suggest that inflows and priority levels should be determined
simultaneously such that traffic queues are placed at locations that discharge
queues fast. We illustrate the performance benefits of these criteria through a
computational study of a segment on Interstate 210 in California, USA.
"
"  In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an
increasing demand to reconstruct high quality images from limited number of
data. However, the existing solutions require either hardware changes or
computationally expansive algorithms. To overcome these limitations, here we
propose a novel deep learning approach that interpolates the missing RF data by
utilizing the sparsity of the RF data in the Fourier domain. Extensive
experimental results from sub-sampled RF data from a real US system confirmed
that the proposed method can effectively reduce the data rate without
sacrificing the image quality.
"
"  This paper extends a conventional, general framework for online adaptive
estimation problems for systems governed by unknown nonlinear ordinary
differential equations. The central feature of the theory introduced in this
paper represents the unknown function as a member of a reproducing kernel
Hilbert space (RKHS) and defines a distributed parameter system (DPS) that
governs state estimates and estimates of the unknown function. This paper 1)
derives sufficient conditions for the existence and stability of the infinite
dimensional online estimation problem, 2) derives existence and stability of
finite dimensional approximations of the infinite dimensional approximations,
and 3) determines sufficient conditions for the convergence of finite
dimensional approximations to the infinite dimensional online estimates. A new
condition for persistency of excitation in a RKHS in terms of its evaluation
functionals is introduced in the paper that enables proof of convergence of the
finite dimensional approximations of the unknown function in the RKHS. This
paper studies two particular choices of the RKHS, those that are generated by
exponential functions and those that are generated by multiscale kernels
defined from a multiresolution analysis.
"
"  Quantum computing technologies have become a hot topic in academia and
industry receiving much attention and financial support from all sides.
Building a quantum computer that can be used practically is in itself an
outstanding challenge that has become the 'new race to the moon'. Next to
researchers and vendors of future computing technologies, national authorities
are showing strong interest in maturing this technology due to its known
potential to break many of today's encryption techniques, which would have
significant impact on our society. It is however quite likely that quantum
computing has beneficial impact on many computational disciplines.
In this article we describe our vision of future developments in scientific
computing that would be enabled by the advent of software-programmable quantum
computers. We thereby assume that quantum computers will form part of a hybrid
accelerated computing platform like GPUs and co-processor cards do today. In
particular, we address the potential of quantum algorithms to bring major
breakthroughs in applied mathematics and its applications. Finally, we give
several examples that demonstrate the possible impact of quantum-accelerated
scientific computing on society.
"
"  Recently, encoder-decoder neural networks have shown impressive performance
on many sequence-related tasks. The architecture commonly uses an attentional
mechanism which allows the model to learn alignments between the source and the
target sequence. Most attentional mechanisms used today is based on a global
attention property which requires a computation of a weighted summarization of
the whole input sequence generated by encoder states. However, it is
computationally expensive and often produces misalignment on the longer input
sequence. Furthermore, it does not fit with monotonous or left-to-right nature
in several tasks, such as automatic speech recognition (ASR),
grapheme-to-phoneme (G2P), etc. In this paper, we propose a novel attention
mechanism that has local and monotonic properties. Various ways to control
those properties are also explored. Experimental results on ASR, G2P and
machine translation between two languages with similar sentence structures,
demonstrate that the proposed encoder-decoder model with local monotonic
attention could achieve significant performance improvements and reduce the
computational complexity in comparison with the one that used the standard
global attention architecture.
"
"  Approximate dynamic programming algorithms, such as approximate value
iteration, have been successfully applied to many complex reinforcement
learning tasks, and a better approximate dynamic programming algorithm is
expected to further extend the applicability of reinforcement learning to
various tasks. In this paper we propose a new, robust dynamic programming
algorithm that unifies value iteration, advantage learning, and dynamic policy
programming. We call it generalized value iteration (GVI) and its approximated
version, approximate GVI (AGVI). We show AGVI's performance guarantee, which
includes performance guarantees for existing algorithms, as special cases. We
discuss theoretical weaknesses of existing algorithms, and explain the
advantages of AGVI. Numerical experiments in a simple environment support
theoretical arguments, and suggest that AGVI is a promising alternative to
previous algorithms.
"
"  Spatiotemporal forecasting has various applications in neuroscience, climate
and transportation domain. Traffic forecasting is one canonical example of such
learning task. The task is challenging due to (1) complex spatial dependency on
road networks, (2) non-linear temporal dynamics with changing road conditions
and (3) inherent difficulty of long-term forecasting. To address these
challenges, we propose to model the traffic flow as a diffusion process on a
directed graph and introduce Diffusion Convolutional Recurrent Neural Network
(DCRNN), a deep learning framework for traffic forecasting that incorporates
both spatial and temporal dependency in the traffic flow. Specifically, DCRNN
captures the spatial dependency using bidirectional random walks on the graph,
and the temporal dependency using the encoder-decoder architecture with
scheduled sampling. We evaluate the framework on two real-world large scale
road network traffic datasets and observe consistent improvement of 12% - 15%
over state-of-the-art baselines.
"
"  Recently, machine learning has been used in every possible field to leverage
its amazing power. For a long time, the net-working and distributed computing
system is the key infrastructure to provide efficient computational resource
for machine learning. Networking itself can also benefit from this promising
technology. This article focuses on the application of Machine Learning
techniques for Networking (MLN), which can not only help solve the intractable
old network questions but also stimulate new network applications. In this
article, we summarize the basic workflow to explain how to apply the machine
learning technology in the networking domain. Then we provide a selective
survey of the latest representative advances with explanations on their design
principles and benefits. These advances are divided into several network design
objectives and the detailed information of how they perform in each step of MLN
workflow is presented. Finally, we shed light on the new opportunities on
networking design and community building of this new inter-discipline. Our goal
is to provide a broad research guideline on networking with machine learning to
help and motivate researchers to develop innovative algorithms, standards and
frameworks.
"
"  The spread of new products in a networked population is often modeled as an
epidemic. However, in the case of ""complex"" contagion, these models are
insufficient to properly model adoption behavior. In this paper, we investigate
a model of complex contagion which allows a coevolutionary interplay between
adoption, modeled as an SIS epidemic spreading process, and social
reinforcement effects, modeled as consensus opinion dynamics. Asymptotic
stability analysis of the all-adopt as well as the none-adopt equilibria of the
combined opinion-adoption model is provided through the use of Lyapunov
arguments. In doing so, sufficient conditions are provided which determine the
stability of the ""flop"" state, where no one adopts the product and everyone's
opinion of the product is least favorable, and the ""hit"" state, where everyone
adopts and their opinions are most favorable. These conditions are shown to
extend to the bounded confidence opinion dynamic under a stronger assumption on
the model parameters. To conclude, numerical simulations demonstrate behavior
of the model which reflect findings from the sociology literature on adoption
behavior.
"
"  We develop a linear algebraic framework for the shape-from-shading problem,
because tensors arise when scalar (e.g. image) and vector (e.g. surface normal)
fields are differentiated multiple times. The work is in two parts. In this
first part we investigate when image derivatives exhibit invariance to changing
illumination by calculating the statistics of image derivatives under general
distributions on the light source. We computationally validate the hypothesis
that image orientations (derivatives) provide increased invariance to
illumination by showing (for a Lambertian model) that a shape-from-shading
algorithm matching gradients instead of intensities provides more accurate
reconstructions when illumination is incorrectly estimated under a flatness
prior.
"
"  Modern cities are growing ecosystems that face new challenges due to the
increasing population demands. One of the many problems they face nowadays is
waste management, which has become a pressing issue requiring new solutions.
Swarm robotics systems have been attracting an increasing amount of attention
in the past years and they are expected to become one of the main driving
factors for innovation in the field of robotics. The research presented in this
paper explores the feasibility of a swarm robotics system in an urban
environment. By using bio-inspired foraging methods such as multi-place
foraging and stigmergy-based navigation, a swarm of robots is able to improve
the efficiency and autonomy of the urban waste management system in a realistic
scenario. To achieve this, a diverse set of simulation experiments was
conducted using real-world GIS data and implementing different garbage
collection scenarios driven by robot swarms. Results presented in this research
show that the proposed system outperforms current approaches. Moreover, results
not only show the efficiency of our solution, but also give insights about how
to design and customize these systems.
"
"  There are so many vehicles in the world and the number of vehicles is
increasing rapidly. To alleviate the parking problems caused by that, the smart
parking system has been developed. The parking planning is one of the most
important parts of it. An effective parking planning strategy makes the better
use of parking resources possible. In this paper, we present a feasible method
to do parking planning. We transform the parking planning problem into a kind
of linear assignment problem. We take vehicles as jobs and parking spaces as
agents. We take distances between vehicles and parking spaces as costs for
agents doing jobs. Then we design an algorithm for this particular assignment
problem and solve the parking planning problem. The method proposed can give
timely and efficient guide information to vehicles for a real time smart
parking system. Finally, we show the effectiveness of the method with
experiments over some data, which can simulate the situation of doing parking
planning in the real world.
"
"  We describe categorical models of a circuit-based (quantum) functional pro-
gramming language. We show that enriched categories play a crucial role.
Following earlier work on QWire by Paykin et al., we consider both a simple
first-order linear language for circuits, and a more powerful host language,
such that the circuit language is embedded inside the host language. Our
categorical semantics for the host language is standard, and involves cartesian
closed categories and monads. We interpret the circuit language not in an
ordinary category, but in a category that is enriched in the host category. We
show that this structure is also related to linear/non-linear models. As an
extended example, we recall an earlier result that the category of W*-algebras
is dcpo-enriched, and we use this model to extend the circuit language with
some recursive types.
"
"  This works presents a formulation for visual navigation that unifies map
based spatial reasoning and path planning, with landmark based robust plan
execution in noisy environments. Our proposed formulation is learned from data
and is thus able to leverage statistical regularities of the world. This allows
it to efficiently navigate in novel environments given only a sparse set of
registered images as input for building representations for space. Our
formulation is based on three key ideas: a learned path planner that outputs
path plans to reach the goal, a feature synthesis engine that predicts features
for locations along the planned path, and a learned goal-driven closed loop
controller that can follow plans given these synthesized features. We test our
approach for goal-driven navigation in simulated real world environments and
report performance gains over competitive baseline approaches.
"
"  Human collaborators coordinate effectively their actions through both verbal
and non-verbal communication. We believe that the the same should hold for
human-robot teams. We propose a formalism that enables a robot to decide
optimally between doing a task and issuing an utterance. We focus on two types
of utterances: verbal commands, where the robot expresses how it wants its
human teammate to behave, and state-conveying actions, where the robot explains
why it is behaving this way. Human subject experiments show that enabling the
robot to issue verbal commands is the most effective form of communicating
objectives, while retaining user trust in the robot. Communicating why
information should be done judiciously, since many participants questioned the
truthfulness of the robot statements.
"
"  We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking them to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability. Finally, principled interpretable strategies are
proposed and empirically evaluated on synthetic data, as well as on the largest
public olfaction dataset that was made recently available \cite{olfs}. We also
experiment on MNIST with a simple target model and different oracle models of
varying complexity. This leads to the insight that the improvement in the
target model is not only a function of the oracle model's performance, but also
its relative complexity with respect to the target model. Further experiments
on CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit
of our methods over Knowledge Distillation when the target models are simple
and the complex model is a neural network.
"
"  A statistical algorithm for categorizing different types of matches and fraud
in image databases is presented. The approach is based on a generative model of
a graph representing images and connections between pairs of identities,
trained using properties of a matching algorithm between images.
"
"  Brain-computer interfaces (BCIs) can provide an alternative means of
communication for individuals with severe neuromuscular limitations. The
P300-based BCI speller relies on eliciting and detecting transient
event-related potentials (ERPs) in electroencephalography (EEG) data, in
response to a user attending to rarely occurring target stimuli amongst a
series of non-target stimuli. However, in most P300 speller implementations,
the stimuli to be presented are randomly selected from a limited set of options
and stimulus selection and presentation are not optimized based on previous
user data. In this work, we propose a data-driven method for stimulus selection
based on the expected discrimination gain metric. The data-driven approach
selects stimuli based on previously observed stimulus responses, with the aim
of choosing a set of stimuli that will provide the most information about the
user's intended target character. Our approach incorporates knowledge of
physiological and system constraints imposed due to real-time BCI
implementation. Simulations were performed to compare our stimulus selection
approach to the row-column paradigm, the conventional stimulus selection method
for P300 spellers. Results from the simulations demonstrated that our adaptive
stimulus selection approach has the potential to significantly improve
performance from the conventional method: up to 34% improvement in accuracy and
43% reduction in the mean number of stimulus presentations required to spell a
character in a 72-character grid. In addition, our greedy approach to stimulus
selection provides the flexibility to accommodate design constraints.
"
"  We present a study on the impact of Mn$^{3+}$ substitution in the
geometrically frustrated Ising garnet Ho$_3$Ga$_5$O$_{12}$ using bulk magnetic
measurements and low temperature powder neutron diffraction. We find that the
transition temperature, $T_N$ = 5.8 K, for Ho$_3$MnGa$_4$O$_{12}$ is raised by
almost 20 when compared to Ho$_3$Ga$_5$O$_{12}$. Powder neutron diffraction on
Ho$_3$Mn$_x$Ga$_{5-x}$O$_{12}$ ($x$ = 0.5, 1) below $T_N$ shows the formation
of a long range ordered ordered state with $\mathbf{k}$ = (0,0,0). Ho$^{3+}$
spins are aligned antiferromagnetically along the six crystallographic axes
with no resultant moment while the Mn$^{3+}$ spins are oriented along the body
diagonals, such that there is a net moment along [111]. The magnetic structure
can be visualised as ten-membered rings of corner-sharing triangles of
Ho$^{3+}$ spins with the Mn$^{3+}$ spins ferromagnetically coupled to each
individual Ho$^{3+}$ spin in the triangle. Substitution of Mn$^{3+}$ completely
relieves the magnetic frustration with $f = \theta_{CW}/T_N \approx 1.1$ for
Ho$_3$MnGa$_4$O$_{12}$.
"
"  In this work, we have characterized changes in the dynamics of a
two-dimensional relativistic standard map in the presence of dissipation and
specially when it is submitted to thermal effects modeled by a Gaussian noise
reservoir. By the addition of thermal noise in the dissipative relativistic
standard map (DRSM) it is possible to suppress typical stable periodic
structures (SPSs) embedded in the chaotic domains of parameter space for large
enough temperature strengths. Smaller SPSs are first affected by thermal
effects, starting from their borders, as a function of temperature. To estimate
the necessary temperature strength capable to destroy those SPSs we use the
largest Lyapunov exponent to obtain the critical temperature ($T_C$) diagrams.
For critical temperatures the chaotic behavior takes place with the suppression
of periodic motion, although, the temperature strengths considered in this work
are not so large to convert the deterministic features of the underlying system
into a stochastic ones.
"
"  In relativistic quantum field theories, compact objects of interacting bosons
can become stable owing to conservation of an additive quantum number $Q$.
Discovering such $Q$-balls propagating in the Universe would confirm
supersymmetric extensions of the standard model and may shed light on the
mysteries of dark matter, but no unambiguous experimental evidence exists. We
report observation of a propagating long-lived $Q$-ball in superfluid $^3$He,
where the role of $Q$-ball is played by a Bose-Einstein condensate of magnon
quasiparticles. We achieve accurate representation of the $Q$-ball Hamiltonian
using the influence of the number of magnons, corresponding to the charge $Q$,
on the orbital structure of the superfluid $^3$He order parameter. This
realisation supports multiple coexisting $Q$-balls which in future allows
studies of $Q$-ball dynamics, interactions, and collisions.
"
"  In this work, we make two improvements on the staggered grid hydrodynamics
(SGH) Lagrangian scheme for modeling 2-dimensional compressible multi-material
flows on triangular mesh. The first improvement is the construction of a
dynamic local remeshing scheme for preventing mesh distortion. The remeshing
scheme is similar to many published algorithms except that it introduces some
special operations for treating grids around multi-material interfaces. This
makes the simulation of extremely deforming and topology-variable
multi-material processes possible, such as the complete process of a heavy
fluid dipping into a light fluid. The second improvement is the construction of
an Euler-like flow on each edge of the mesh to count for the ""edge-bending""
effect, so as to mitigate the ""checkerboard"" oscillation that commonly exists
in Lagrangian simulations, especially the triangular mesh based simulations.
Several typical hydrodynamic problems are simulated by the improved staggered
grid Lagrangian hydrodynamic method to test its performance.
"
"  The first billion years of the Universe is a pivotal time: stars, black holes
(BHs) and galaxies form and assemble, sowing the seeds of galaxies as we know
them today. Detecting, identifying and understand- ing the first galaxies and
BHs is one of the current observational and theoretical challenges in galaxy
formation. In this paper we present a population synthesis model aimed at
galaxies, BHs and Active Galactic Nuclei (AGNs) at high redshift. The model
builds a population based on empirical relations. Galaxies are characterized by
a spectral energy distribution determined by age and metallicity, and AGNs by a
spectral energy distribution determined by BH mass and accretion rate. We
validate the model against observational constraints, and then predict
properties of galaxies and AGN in other wavelength and/or luminosity ranges,
estimating the contamination of stellar populations (normal stars and high-mass
X-ray binaries) for AGN searches from the infrared to X-rays, and vice-versa
for galaxy searches. For high-redshift galaxies, with stellar ages < 1 Gyr, we
find that disentangling stellar and AGN emission is challenging at restframe
UV/optical wavelengths, while high-mass X-ray binaries become more important
sources of confusion in X-rays. We propose a color-color selection in JWST
bands to separate AGN vs star-dominated galaxies in photometric observations.
We also esti- mate the AGN contribution, with respect to massive, hot,
metal-poor stars, at driving high ionization lines, such as C IV and He II.
Finally, we test the influence of the minimum BH mass and occupa- tion fraction
of BHs in low mass galaxies on the restframe UV/near-IR and X-ray AGN
luminosity function.
"
"  We introduce the concept of Floquet topological magnons --- a mechanism by
which a synthetic tunable Dzyaloshinskii-Moriya interaction (DMI) can be
generated in quantum magnets using circularly polarized electric (laser) field.
The resulting effect is that Dirac magnons and nodal magnons in two-dimensional
(2D) and three-dimensional (3D) quantum magnets can be tuned to magnon Chern
insulators and Weyl magnons respectively under circularly polarized laser
field. The Floquet formalism also yields a tunable intrinsic DMI in insulating
quantum magnets without an inversion center. We demonstrate that the Floquet
topological magnons possess a finite thermal Hall conductivity tunable by the
laser field.
"
"  We present a thorough tight-binding analysis of the band structure of a wide
variety of lattices belonging to the class of honeycomb and Kagome systems
including several mixed forms combining both lattices. The band structure of
these systems are made of a combination of dispersive and flat bands. The
dispersive bands possess Dirac cones (linear dispersion) at the six corners (K
points) of the Brillouin zone although in peculiar cases Dirac cones at the
center of the zone $(\Gamma$ point) appear. The flat bands can be of different
nature. Most of them are tangent to the dispersive bands at the center of the
zone but some, for symmetry reasons, do not hybridize with other states. The
objective of our work is to provide an analysis of a wide class of so-called
ligand-decorated honeycomb Kagome lattices that are observed in 2D
metal-organic framework (MOF) where the ligand occupy honeycomb sites and the
metallic atoms the Kagome sites. We show that the $p_x$-$p_y$ graphene model is
relevant in these systems and there exists four types of flat bands: Kagome
flat (singly degenerate) bands, two kinds of ligand-centered flat bands (A$_2$
like and E like, respectively doubly and singly degenerate) and metal-centered
(three fold degenerate) flat bands.
"
"  The recently introduced mixed time-averaging semiclassical initial value
representation molecular dynamics method for spectroscopic calculations [M.
Buchholz, F. Grossmann, and M. Ceotto, J. Chem. Phys. 144, 094102 (2016)] is
applied to systems with up to 61 dimensions, ruled by a condensed phase
Caldeira-Leggett model potential. By calculating the ground state as well as
the first few excited states of the system Morse oscillator, changes of both
the harmonic frequency and the anharmonicity are determined. The method
faithfully reproduces blueshift and redshift effects and the importance of the
counter term, as previously suggested by other methods. Differently from
previous methods, the present semiclassical method does not take advantage of
the specific form of the potential and it can represent a practical tool that
opens the route to direct ab initio semiclassical simulation of condensed phase
systems.
"
"  One of the main challenges in probing the reionization epoch using the
redshifted 21 cm line is that the magnitude of the signal is several orders
smaller than the astrophysical foregrounds. One of the methods to deal with the
problem is to avoid a wedge-shaped region in the Fourier $k_{\perp} -
k_{\parallel}$ space which contains the signal from the spectrally smooth
foregrounds. However, measuring the spherically averaged power spectrum using
only modes outside this wedge (i.e., in the reionization window), leads to a
bias. We provide a prescription, based on expanding the power spectrum in terms
of the shifted Legendre polynomials, which can be used to compute the angular
moments of the power spectrum in the reionization window. The prescription
requires computation of the monopole, quadrupole and hexadecapole moments of
the power spectrum using the theoretical model under consideration and also the
knowledge of the effective extent of the foreground wedge in the $k_{\perp} -
k_{\parallel}$ plane. One can then calculate the theoretical power spectrum in
the window which can be directly compared with observations. The analysis
should have implications for avoiding any bias in the parameter constraints
using 21 cm power spectrum data.
"
"  We study topological excitations in two-component nematic superconductors,
with a particular focus on Cu$_x$Bi$_2$Se$_3$ as a candidate material. We find
that the lowest-energy topological excitations are coreless vortices: a bound
state of two spatially separated half-quantum vortices. These objects are
nematic Skyrmions, since they are characterized by an additional topological
charge. The inter-Skyrmion forces are dipolar in this model, i.e. attractive
for certain relative orientations of the Skyrmions, hence forming
multi-Skyrmion bound states.
"
"  The recent announcement of a Neptune-sized exomoon candidate around the
transiting Jupiter-sized object Kepler-1625 b could indicate the presence of a
hitherto unknown kind of gas giant moons, if confirmed. Three transits have
been observed, allowing radius estimates of both objects. Here we investigate
possible mass regimes of the transiting system that could produce the observed
signatures and study them in the context of moon formation in the solar system,
i.e. via impacts, capture, or in-situ accretion. The radius of Kepler-1625 b
suggests it could be anything from a gas giant planet somewhat more massive
than Saturn (0.4 M_Jup) to a brown dwarf (BD) (up to 75 M_Jup) or even a
very-low-mass star (VLMS) (112 M_Jup ~ 0.11 M_sun). The proposed companion
would certainly have a planetary mass. Possible extreme scenarios range from a
highly inflated Earth-mass gas satellite to an atmosphere-free water-rock
companion of about 180 M_Ear. Furthermore, the planet-moon dynamics during the
transits suggest a total system mass of 17.6_{-12.6}^{+19.2} M_Jup. A
Neptune-mass exomoon around a giant planet or low-mass BD would not be
compatible with the common mass scaling relation of the solar system moons
about gas giants. The case of a mini-Neptune around a high-mass BD or a VLMS,
however, would be located in a similar region of the satellite-to-host mass
ratio diagram as Proxima b, the TRAPPIST-1 system, and LHS 1140 b. The capture
of a Neptune-mass object around a 10 M_Jup planet during a close binary
encounter is possible in principle. The ejected object, however, would have had
to be a super-Earth object, raising further questions of how such a system
could have formed. In summary, this exomoon candidate is barely compatible with
established moon formation theories. If it can be validated as orbiting a
super-Jovian planet, then it would pose an exquisite riddle for formation
theorists to solve.
"
"  We investigate the effect of band-limited white Gaussian noise (BLWGN) on
electromagnetically induced transparency (EIT) and Autler-Townes (AT)
splitting, when performing atom-based continuous-wave (CW) radio-frequency (RF)
electric (E) field strength measurements with Rydberg atoms in an atomic vapor.
This EIT/AT-based E-field measurement approach is currently being investigated
by several groups around the world as a means to develop a new SI traceable RF
E-field measurement technique. For this to be a useful technique, it is
important to understand the influence of BLWGN. We perform EIT/AT based E-field
experiments with BLWGN centered on the RF transition frequency and for the
BLWGN blue-shifted and red-shifted relative to the RF transition frequency. The
EIT signal can be severely distorted for certain noise conditions (band-width,
center-frequency, and noise power), hence altering the ability to accurately
measure a CW RF E-field strength. We present a model to predict the changes in
the EIT signal in the presence of noise. This model includes AC Stark shifts
and on resonance transitions associated with the noise source. The results of
this model are compared to the experimental data and we find very good
agreement between the two.
"
"  As power electronics shrinks down to sub-micron scale, the thermal transport
from a solid surface to environment becomes significant. Under circumstances
when the device works in rare gas environment, the scale for thermal transport
is comparable to the mean free path of molecules, and is difficult to
characterize. In this work, we present an experimental study about thermal
transport around a microwire in rare gas environment by using a steady state
hot wire method. Unlike conventional hot wire technique of using transient heat
transfer process, this method considers both the heat conduction along the wire
and convection effect from wire surface to surroundings. Convection heat
transfer coefficient from a platinum wire in diameter 25 um to air is
characterized under different heating power and air pressures to comprehend the
effect of temperature and density of gas molecules. It is observed that
convection heat transfer coefficient varies from 14 Wm-2K-1 at 7 Pa to 629
Wm-2K-1 at atmosphere pressure. In free molecule regime, Nusselt number has a
linear relationship with inverse Knudsen number and the slope of 0.274 is
employed to determined equivalent thermal dissipation boundary as 7.03E10-4 m.
In transition regime, the equivalent thermal dissipation boundary is obtained
as 5.02E10-4 m. Under a constant pressure, convection heat transfer coefficient
decreases with increasing temperature, and this correlation is more sensitive
to larger pressure. This work provides a pathway for studying both heat
conduction and heat convection effect at micro/nanoscale under rare gas
environment, the knowledge of which is essential for regulating heat
dissipation in various industrial applications.
"
"  The ability of the mammalian ear in processing high frequency sounds, up to
$\sim$100 kHz, is based on the capability of outer hair cells (OHCs) responding
to stimulation at high frequencies. These cells show a unique motility in their
cell body coupled with charge movement. With this motile element, voltage
changes generated by stimuli at their hair bundles drives the cell body and
that, in turn, amplifies the stimuli. In vitro experiments show that the
movement of these charges significantly increases the membrane capacitance,
limiting the motile activity by additionally attenuating voltage changes. It
was found, however, that such an effect is due to the absence of mechanical
load. In the presence of mechanical resonance, such as in vivo conditions, the
movement of motile charges is expected to create negative capacitance near the
resonance frequency. Therefore this motile mechanism is effective at high
frequencies.
"
"  We derive general expressions for resonant inelastic x-ray scattering (RIXS)
operators for $t_{2g}$ orbital systems, which exhibit a rich array of
unconventional magnetism arising from unquenched orbital moments. Within the
fast collision approximation, which is valid especially for 4$d$ and 5$d$
transition metal compounds with short core-hole lifetimes, the RIXS operators
are expressed in terms of total spin and orbital angular momenta of the
constituent ions. We then map these operators onto pseudospins that represent
spin-orbit entangled magnetic moments in systems with strong spin-orbit
coupling. Applications of our theory to such systems as iridates and ruthenates
are discussed, with a particular focus on compounds based on $d^4$ ions with
Van Vleck-type nonmagnetic ground state.
"
"  We studied the emergence process of 42 active region (ARs) by analyzing the
time derivative, R(t), of the total unsigned flux. Line-of-sight magnetograms
acquired by the Helioseismic and Magnetic Imager (HMI) onboard the Solar
Dynamics Observatory (SDO) were used. A continuous piecewise linear fitting to
the R(t)-profile was applied to detect an interval, dt_2, of nearly-constant
R(t) covering one or several local maxima. The averaged over dt_2 magnitude of
R(t) was accepted as an estimate of the maximal value of the flux growth rate,
R_MAX, which varies in a range of (0.5-5)x10^20 Mx hour^-1 for active regions
with the maximal total unsigned flux of (0.5-3)x10^22 Mx. The normalized flux
growth rate, R_N, was defined under an assumption that the saturated total
unsigned flux, F_MAX, equals unity. Out of 42 ARs in our initial list, 36 event
were successfully fitted and they form two subsets (with a small overlap of 8
events): the ARs with a short (<13 hours) interval dt_2 and a high (>0.024
hour^-1) normalized flux emergence rate, R_N, form the ""rapid"" emergence event
subset. The second subset consists of ""gradual"" emergence events and it is
characterized by a long (>13 hours) interval dt_2 and a low R_N (<0.024
hour^-1). In diagrams of R_MAX plotted versus F_MAX, the events from different
subsets are not overlapped and each subset displays an individual power law.
The power law index derived from the entire ensemble of 36 events is
0.69+-0.10. The ""rapid"" emergence is consistent with a ""two-step"" emergence
process of a single twisted flux tube. The ""gradual"" emergence is possibly
related to a consecutive rising of several flux tubes emerging at nearly the
same location in the photosphere.
"
"  Gaining a detailed understanding of water transport behavior through
ultra-thin polymer membranes is increasingly becoming necessary due to the
recent interest in exploring applications such as water desalination using
nanoporous membranes. Current techniques only measure bulk water transport
rates and do not offer direct visualization of water transport which can
provide insights into the microscopic mechanisms affecting bulk behavior such
as the role of defects. We describe the use of a technique, referred here as
Bright-Field Nanoscopy (BFN) to directly image the transport of water across
thin polymer films using a regular bright-field microscope. The technique
exploits the strong thickness dependent color response of an optical stack
consisting of a thin (~25 nm) germanium film deposited over a gold substrate.
Using this technique, we were able to observe the strong influence of the
terminal layer and ambient conditions on the bulk water transport rates in thin
(~ 20 nm) layer-by-layer deposited multilayer films of weak polyelectrolytes
(PEMs).
"
"  Application of NaI(Tl) detectors in the search for galactic dark matter
particles through their elastic scattering off the target nuclei is well
motivated because of the long standing DAMA/LIBRA highly significant positive
result on annual modulation, still requiring confirmation. For such a goal, it
is mandatory to reach very low threshold in energy (at or below the keV level),
very low radioactive background (at a few counts/keV/kg/day), and high
detection mass (at or above the 100 kg scale). One of the most relevant
technical issues is the optimization of the crystal intrinsic scintillation
light yield and the efficiency of the light collecting system for large mass
crystals. In the frame of the ANAIS (Annual modulation with NaI Scintillators)
dark matter search project large NaI(Tl) crystals from different providers
coupled to two photomultiplier tubes (PMTs) have been tested at the Canfranc
Underground Laboratory. In this paper we present the estimates of the NaI(Tl)
scintillation light collected using full-absorption peaks at very low energy
from external and internal sources emitting gammas/electrons, and
single-photoelectron events populations selected by using very low energy
pulses tails. Outstanding scintillation light collection at the level of
15~photoelectrons/keV can be reported for the final design and provider chosen
for ANAIS detectors. Taking into account the Quantum Efficiency of the PMT
units used, the intrinsic scintillation light yield in these NaI(Tl) crystals
is above 40~photoelectrons/keV for energy depositions in the range from 3 up to
25~keV. This very high light output of ANAIS crystals allows triggering below
1~keV, which is very important in order to increase the sensitivity in the
direct detection of dark matter.
"
"  The finite-difference time-domain (FDTD) method is a well established method
for solving the time evolution of Maxwell's equations. Unfortunately the scheme
introduces numerical dispersion and therefore phase and group velocities which
deviate from the correct values. The solution to Maxwell's equations in more
than one dimension results in non-physical predictions such as numerical
dispersion or numerical Cherenkov radiation emitted by a relativistic electron
beam propagating in vacuum.
Improved solvers, which keep the staggered Yee-type grid for electric and
magnetic fields, generally modify the spatial derivative operator in the
Maxwell-Faraday equation by increasing the computational stencil. These
modified solvers can be characterized by different sets of coefficients,
leading to different dispersion properties. In this work we introduce a norm
function to rewrite the choice of coefficients into a minimization problem. We
solve this problem numerically and show that the minimization procedure leads
to phase and group velocities that are considerably closer to $c$ as compared
to schemes with manually set coefficients available in the literature.
Depending on a specific problem at hand (e.g. electron beam propagation in
plasma, high-order harmonic generation from plasma surfaces, etc), the norm
function can be chosen accordingly, for example, to minimize the numerical
dispersion in a certain given propagation direction. Particle-in-cell
simulations of an electron beam propagating in vacuum using our solver are
provided.
"
"  The origin and nature of extreme energy cosmic rays (EECRs), which have
energies above the 50 EeV, the Greisen-Zatsepin-Kuzmin (GZK) energy limit, is
one of the most interesting and complicated problems in modern cosmic-ray
physics. Existing ground-based detectors have helped to obtain remarkable
results in studying cosmic rays before and after the GZK limit, but have also
produced some contradictions in our understanding of cosmic ray mass
composition. Moreover, each of these detectors covers only a part of the
celestial sphere, which poses problems for studying the arrival directions of
EECRs and identifying their sources. As a new generation of EECR space
detectors, TUS (Tracking Ultraviolet Set-up), KLYPVE and JEM-EUSO, are intended
to study the most energetic cosmic-ray particles, providing larger, uniform
exposures of the entire celestial sphere. The TUS detector, launched on board
the Lomonosov satellite on April 28, 2016, from Vostochny Cosmodrome in Russia,
is the first of these. It employs a single-mirror optical system and a
photomultiplier tube matrix as a photo-detector and will test the fluorescent
method of measuring EECRs from space. Utilizing the Earth's atmosphere as a
huge calorimeter, it is expected to detect EECRs with energies above 100 EeV.
It will also be able to register slower atmospheric transient events:
atmospheric fluorescence in electrical discharges of various types including
precipitating electrons escaping the magnetosphere and from the radiation of
meteors passing through the atmosphere. We describe the design of the TUS
detector and present results of different ground-based tests and simulations.
"
"  We propose a precise ellipsometric method for the investigation of coherent
light with a small ellipticity. The main feature of this method is the use of
compensators with phase delays providing the maximum accuracy of measurements
for the selected range of ellipticities and taking into account the
interference of multiple reflections of coherent light. The relative error of
the ellipticity measurement in the range of mesurement does not exceed 0.02.
"
"  Transition metal dichalcogenides (TMDs) exhibit a remarkable exciton physics
including optically accessible (bright) as well as spin- and momentum-forbidden
(dark) excitonic states. So far the dark exciton landscape has not been
revealed leaving in particular the spectral position of momentum-forbidden dark
states completely unclear. This has a significant impact on the technological
application potential of TMDs, since the nature of the energetically lowest
state determines, if the material is a direct-gap semiconductor. Here, we show
how dark states can be experimentally revealed by probing the intra-excitonic
1s-2p transition. Distinguishing the optical response shortly after the
excitation (< 100$\,$fs) and after the exciton thermalization (> 1$\,$ps)
allows us to demonstrate the relative position of bright and dark excitons. We
find both in theory and experiment a clear blue-shift in the optical response
demonstrating for the first time the transition of bright exciton populations
into lower lying momentum- and spin-forbidden dark excitonic states in
monolayer WSe$_2$.
"
"  In this work, which is based on an essential linear analysis carried out by
Christodoulou, we study the evolution of tidal energy for the motion of two
gravitating incompressible fluid balls with free boundaries obeying the
Euler-Poisson equations. The orbital energy is defined as the mechanical energy
of the two bodies' center of mass. According to the classical analysis of
Kepler and Newton, when the fluids are replaced by point masses, the conic
curve describing the trajectories of the masses is a hyperbola when the orbital
energy is positive and an ellipse when the orbital energy is negative. The
orbital energy is conserved in the case of point masses. If the point masses
are initially very far, then the orbital energy is positive, corresponding to
hyperbolic motion. However, in the motion of fluid bodies the orbital energy is
no longer conserved because part of the conserved energy is used in deforming
the boundaries of the bodies. In this case the total energy
$\tilde{\mathcal{E}}$ can be decomposed into a sum
$\tilde{\mathcal{E}}:=\widetilde{\mathcal{E}_{\mathrm{orbital}}}+\widetilde{\mathcal{E}_{\mathrm{tidal}}}$,
with $\widetilde{\mathcal{E}_{\mathrm{tidal}}}$ measuring the energy used in
deforming the boundaries, such that if
$\widetilde{\mathcal{E}_{\mathrm{orbital}}}<-c<0$ for some absolute constant
$c>0$, then the orbit of the bodies must be bounded. In this work we prove that
under appropriate conditions on the initial configuration of the system, the
fluid boundaries and velocity remain regular up to the point of the first
closest approach in the orbit, and that the tidal energy
$\widetilde{\mathcal{E}_{\mathrm{tidal}}}$ can be made arbitrarily large
relative to the total energy $\tilde{\mathcal{E}}$. In particular under these
conditions $\widetilde{\mathcal{E}_{\mathrm{orbital}}}$, which is initially
positive, becomes negative before the point of the first closest approach.
"
"  Here we report small-angle neutron scattering (SANS) measurements and
theoretical modeling of U$_3$Al$_2$Ge$_3$. Analysis of the SANS data reveals a
phase transition to sinusoidally modulated magnetic order, at
$T_{\mathrm{N}}=63$~K to be second order, and a first order phase transition to
ferromagnetic order at $T_{\mathrm{c}}=48$~K. Within the sinusoidally modulated
magnetic phase ($T_{\mathrm{c}} < T < T_{\mathrm{N}}$), we uncover a dramatic
change, by a factor of three, in the ordering wave-vector as a function of
temperature. These observations all indicate that U$_3$Al$_2$Ge$_3$ is a close
realization of the three-dimensional Axial Next-Nearest-Neighbor Ising model, a
prototypical framework for describing commensurate to incommensurate phase
transitions in frustrated magnets.
"
"  We derive asymptotic formulas for the solution of the derivative nonlinear
Schrödinger equation on the half-line under the assumption that the initial
and boundary values lie in the Schwartz class. The formulas clearly show the
effect of the boundary on the solution. The approach is based on a nonlinear
steepest descent analysis of an associated Riemann-Hilbert problem.
"
"  Understanding the nature of bulges in disc galaxies can provide important
insights into the formation and evolution of galaxies. For instance, the
presence of a classical bulge suggests a relatively violent history, in
contrast, the presence of simply an inner disc (also referred to as a
""pseudobulge"") indicates the occurrence of secular evolution processes in the
main disc. However, we still lack criteria to effectively categorise bulges,
limiting our ability to study their impact on the evolution of the host
galaxies. Here we present a recipe to separate inner discs from classical
bulges by combining four different parameters from photometric and kinematic
analyses: The bulge Sérsic index $n_\mathrm{b}$, the concentration index
$C_{20,50}$, the Kormendy (1977) relation and the inner slope of the radial
velocity dispersion profile $\nabla\sigma$. With that recipe we provide a
detailed bulge classification for a sample of 45 galaxies from the
integral-field spectroscopic survey CALIFA. To aid in categorising bulges
within these galaxies, we perform 2D image decomposition to determine bulge
Sérsic index, bulge-to-total light ratio, surface brightness and effective
radius of the bulge and use growth curve analysis to derive a new concentration
index, $C_{20,50}$. We further extract the stellar kinematics from CALIFA data
cubes and analyse the radial velocity dispersion profile. The results of the
different approaches are in good agreement and allow a safe classification for
approximately $95\%$ of the galaxies. In particular, we show that our new
""inner"" concentration index performs considerably better than the traditionally
used $C_{50,90}$ when yielding the nature of bulges. We also found that a
combined use of this index and the Kormendy (1977) relation gives a very robust
indication of the physical nature of the bulge.
"
"  We report a measurement of $KLL$ dielectronic recombination in charge states
from Kr$^{+34}$ through Kr$^{+28}$, in order to investigate the contribution of
Breit interaction for a wide range of resonant states. Highly charged Kr ions
were produced in an electron beam ion trap, while the electron-ion collision
energy was scanned over a range of dielectronic recombination resonances. The
subsequent $K\alpha$ x rays were recorded both along and perpendicular to the
electron beam axis, which allowed the observation of the influence of Breit
interaction on the angular distribution of the x rays. Experimental results are
in good agreement with distorted-wave calculations. We demonstrate, both
theoretically and experimentally, that there is a strong state-selective
influence of the Breit interaction that can be traced back to the angular and
radial properties of the wavefunctions in the dielectronic capture.
"
"  Precision pulsar timing requires optimization against measurement errors and
astrophysical variance from the neutron stars themselves and the interstellar
medium. We investigate optimization of arrival time precision as a function of
radio frequency and bandwidth. We find that increases in bandwidth that reduce
the contribution from receiver noise are countered by the strong chromatic
dependence of interstellar effects and intrinsic pulse-profile evolution. The
resulting optimal frequency range is therefore telescope and pulsar dependent.
We demonstrate the results for five pulsars included in current pulsar timing
arrays and determine that they are not optimally observed at current center
frequencies. For those objects, we find that better choices of total bandwidth
as well as center frequency can improve the arrival-time precision. Wideband
receivers centered at somewhat higher frequencies with respect to the currently
adopted receivers can reduce required overall integration times and provide
significant improvements in arrival time uncertainty by a factor of ~sqrt(2) in
most cases, assuming a fixed integration time. We also discuss how timing
programs can be extended to pulsars with larger dispersion measures through the
use of higher-frequency observations.
"
"  We examine dense self-gravitating stellar systems dominated by a central
potential, such as nuclear star clusters hosting a central supermassive black
hole. Different dynamical properties of these systems evolve on vastly
different timescales. In particular, the orbital-plane orientations are
typically driven into internal thermodynamic equilibrium by vector resonant
relaxation before the orbital eccentricities or semimajor axes relax. We show
that the statistical mechanics of such systems exhibit a striking resemblance
to liquid crystals, with analogous ordered-nematic and disordered-isotropic
phases. The ordered phase consists of bodies orbiting in a disk in both
directions, with the disk thickness depending on temperature, while the
disordered phase corresponds to a nearly isotropic distribution of the orbit
normals. We show that below a critical value of the total angular momentum, the
system undergoes a first-order phase transition between the ordered and
disordered phases. At the critical point the phase transition becomes
second-order while for higher angular momenta there is a smooth crossover. We
also find metastable equilibria containing two identical disks with mutual
inclinations between $90^{\circ}$ and $180^\circ$.
"
"  We propose a new type of Hopf semimetals indexed by a pair of numbers
$(p,q)$, where the Hopf number is given by $pq$. The Fermi surface is given by
the preimage of the Hopf map, which is nontrivially linked for a nonzero Hopf
number. The Fermi surface forms a torus link, whose examples are the Hopf link
indexed by $(1,1)$, the Solomon's knot $(2,1)$, the double Hopf-link $(2,2)$
and the double trefoil-knot $(3,2)$. We may choose $p$ or $q$ as a half
integer, where torus-knot Fermi surfaces such as the trefoil knot $(3/2,1)$ are
realized. It is even possible to make the Hopf number an arbitrary rational
number, where a semimetal whose Fermi surface forms open strings is generated.
"
"  We report experimental studies of the influence of symmetric dual-loop
optical feedback on the RF linewidth and timing jitter of self-mode-locked
two-section quantum dash lasers emitting at 1550 nm. Various feedback schemes
were investigated and optimum levels determined for narrowest RF linewidth and
low timing jitter, for single-loop and symmetric dual-loop feedback. Two
symmetric dual-loop configurations, with balanced and unbalanced feedback
ratios, were studied. We demonstrate that unbalanced symmetric dual loop
feedback, with the inner cavity resonant and fine delay tuning of the outer
loop, gives narrowest RF linewidth and reduced timing jitter over a wide range
of delay, unlike single and balanced symmetric dual-loop configurations. This
configuration with feedback lengths 80 and 140 m narrows the RF linewidth by
4-67x and 10-100x, respectively, across the widest delay range, compared to
free-running. For symmetric dual-loop feedback, the influence of different
power split ratios through the feedback loops was determined. Our results show
that symmetric dual-loop feedback is markedly more effective than single-loop
feedback in reducing RF linewidth and timing jitter, and is much less sensitive
to delay phase, making this technique ideal for applications where robustness
and alignment tolerance are essential.
"
"  Radiative alpha-capture, ($\alpha,\gamma$), reactions play a critical role in
nucleosynthesis and nuclear energy generation in a variety of astrophysical
environments. The St. George recoil separator at the University of Notre Dame's
Nuclear Science Laboratory was developed to measure ($\alpha,\gamma$) reactions
in inverse kinematics via recoil detection in order to obtain nuclear reaction
cross sections at the low energies of astrophysical interest, while avoiding
the $\gamma$-background that plagues traditional measurement techniques. Due to
the $\gamma$-ray produced by the nuclear reaction at the target location,
recoil nuclei are produced with a variety of energies and angles, all of which
must be accepted by St. George in order to accurately determine the reaction
cross section. We demonstrate the energy acceptance of the St. George recoil
separator using primary beams of helium, hydrogen, neon, and oxygen, spanning
the magnetic and electric rigidity phase space populated by recoils of
anticipated ($\alpha,\gamma$) reaction measurements. We found the performance
of St. George meets the design specifications, demonstrating its suitability
for ($\alpha,\gamma$) reaction measurements of astrophysical interest.
"
"  Based on the results published recently [J. Phys. A: Math. Theor. 50, 065201
(2017)], the universal finite-size contributions to the free energy of the
square lattice Ising model on the $L\times M$ rectangle, with open boundary
conditions in both directions, are calculated exactly in the finite-size
scaling limit $L,M\to\infty$, $T\to T_\mathrm{c}$, with fixed temperature
scaling variable $x\propto(T/T_\mathrm{c}-1)M$ and fixed aspect ratio
$\rho\propto L/M$. We derive exponentially fast converging series for the
related Casimir potential and Casimir force scaling functions. At the critical
point $T=T_\mathrm{c}$ we confirm predictions from conformal field theory by
Cardy & Peschel [Nucl. Phys. B 300, 377 (1988)] and by Kleban & Vassileva [J.
Phys. A: Math. Gen. 24, 3407 (1991)]. The presence of corners and the related
corner free energy has dramatic impact on the Casimir scaling functions and
leads to a logarithmic divergence of the Casimir potential scaling function at
criticality.
"
"  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
"
"  We analyze three-dimensional hydrodynamical simulations of the interaction of
jets and the bubbles they inflate with the intra-cluster medium (ICM), and show
that the heating of the ICM by mixing hot bubble gas with the ICM operates over
tens of millions of years, and hence can smooth the sporadic activity of the
jets. The inflation process of hot bubbles by propagating jets forms many
vortices, and these vortices mix the hot bubble gas with the ICM. The mixing,
hence the heating of the ICM, starts immediately after the jets are launched,
but continues for tens of millions of years. We suggest that the smoothing of
the active galactic nucleus (AGN) sporadic activity by the long-lived vortices
accounts for the recent finding of a gentle energy coupling between AGN heating
and the ICM.
"
"  Random impedance networks are widely used as a model to describe plasmon
resonances in disordered metal-dielectric nanocomposites. In order to study
thin films, two-dimensional networks are often used despite the fact that such
networks correspond to a two-dimensional electrodynamics [J.P. Clerc et al, J.
Phys. A 29, 4781 (1996)]. In the present work, we propose a model of
two-dimensional systems with three-dimensional Coulomb interaction and show
that this model is equivalent to a planar network with long-range capacitive
connections between sites. In a case of a metal film, we get a known dispersion
$\omega \propto \sqrt{k}$ of plane-wave two-dimensional plasmons. In the
framework of the proposed model, we study the evolution of resonances with
decreasing of metal filling factor. In the subcritical region with metal
filling $p$ lower than the percolation threshold $p_c$, we observe a gap with
Lifshitz tails in the spectral density of states (DOS). In the supercritical
region $p>p_c$, the DOS demonstrates a crossover between plane-wave
two-dimensional plasmons and resonances associated with small clusters.
"
"  Purpose: To compare two methods that use x-ray spectral information to image
externally administered contrast agents: K-edge subtraction and basis-function
decomposition (the A-space method), Methods: The K-edge method uses narrow band
x-ray spectra with energies infinitesimally below and above the contrast
material K-edge energy. The A-space method uses a broad spectrum x-ray tube
source and measures the transmitted spectrum with photon counting detectors
with pulse height analysis. The methods are compared by their signal to noise
ratio (SNR) divided by the patient dose for an imaging task to decide whether
contrast material is present in a soft tissue background. The performance with
iodine or gadolinium containing contrast material is evaluated as a function of
object thickness and the x-ray tube voltage of the A-space method. Results: For
a tube voltages above 60 kV and soft tissue thicknesses from 5 to 25 g/cm^2,
the A-space method has a larger SNR per dose than the K-edge subtraction method
for either iodine or gadolinium containing contrast agent. Conclusion: Even
with the unrealistic spectra assumed for the K-edge method, the A-space method
has a substantially larger SNR per patient dose.
"
"  We study the dynamics of overdamped Brownian particles diffusing in
conservative force fields and undergoing stochastic resetting to a given
location with a generic space-dependent rate of resetting. We present a
systematic approach involving path integrals and elements of renewal theory
that allows to derive analytical expressions for a variety of statistics of the
dynamics such as (i) the propagator prior to first reset; (ii) the distribution
of the first-reset time, and (iii) the spatial distribution of the particle at
long times. We apply our approach to several representative and hitherto
unexplored examples of resetting dynamics. A particularly interesting example
for which we find analytical expressions for the statistics of resetting is
that of a Brownian particle trapped in a harmonic potential with a rate of
resetting that depends on the instantaneous energy of the particle. We find
that using energy-dependent resetting processes is more effective in achieving
spatial confinement of Brownian particles on a faster timescale than by
performing quenches of parameters of the harmonic potential.
"
"  Positron Emission Tomography (PET) is a functional imaging modality widely
used in neuroscience studies. To obtain meaningful quantitative results from
PET images, attenuation correction is necessary during image reconstruction.
For PET/MR hybrid systems, PET attenuation is challenging as Magnetic Resonance
(MR) images do not reflect attenuation coefficients directly. To address this
issue, we present deep neural network methods to derive the continuous
attenuation coefficients for brain PET imaging from MR images. With only Dixon
MR images as the network input, the existing U-net structure was adopted and
analysis using forty patient data sets shows it is superior than other Dixon
based methods. When both Dixon and zero echo time (ZTE) images are available,
we have proposed a modified U-net structure, named GroupU-net, to efficiently
make use of both Dixon and ZTE information through group convolution modules
when the network goes deeper. Quantitative analysis based on fourteen real
patient data sets demonstrates that both network approaches can perform better
than the standard methods, and the proposed network structure can further
reduce the PET quantification error compared to the U-net structure.
"
"  Electrochemistry is the underlying mechanism in a variety of energy
conversion and storage systems, and it is well known that the composition,
structure, and properties of electrochemical materials near active interfaces
often deviates substantially and inhomogeneously from the bulk properties. A
universal challenge facing the development of electrochemical systems is our
lack of understanding of physical and chemical rates at local length scales,
and the recently developed electrochemical strain microscopy (ESM) provides a
promising method to probe crucial local information regarding the underlying
electrochemical mechanisms. Here we develop a computational model that couples
mechanics and electrochemistry relevant for ESM experiments, with the goal to
enable quantitative analysis of electrochemical processes underneath a charged
scanning probe. We show that the model captures the essence of a number of
different ESM experiments, making it possible to de-convolute local ionic
concentration and diffusivity via combined ESM mapping, spectroscopy, and
relaxation studies. Through the combination of ESM experiments and
computations, it is thus possible to obtain deep insight into the local
electrochemistry at the nanoscale.
"
"  For the analysis of molecular processes, the estimation of time-scales, i.e.,
transition rates, is very important. Estimating the transition rates between
molecular conformations is -- from a mathematical point of view -- an invariant
subspace projection problem. A certain infinitesimal generator acting on
function space is projected to a low-dimensional rate matrix. This projection
can be performed in two steps. First, the infinitesimal generator is
discretized, then the invariant subspace is approxi-mated and used for the
subspace projection. In our approach, the discretization will be based on a
Voronoi tessellation of the conformational space. We will show that the
discretized infinitesimal generator can simply be approximated by the geometric
average of the Boltzmann weights of the Voronoi cells. Thus, there is a direct
correla-tion between the potential energy surface of molecular structures and
the transition rates of conformational changes. We present results for a
2d-diffusion process and Alanine dipeptide.
"
"  We present microlensing events in the 2015 Korea Microlensing Telescope
Network (KMTNet) data and our procedure for identifying these events. In
particular, candidates were detected with a novel ""completed event""
microlensing event-finder algorithm. The algorithm works by making linear fits
to a (t0,teff,u0) grid of point-lens microlensing models. This approach is
rendered computationally efficient by restricting u0 to just two values (0 and
1), which we show is quite adequate. The implementation presented here is
specifically tailored to the commission-year character of the 2015 data, but
the algorithm is quite general and has already been applied to a completely
different (non-KMTNet) data set. We outline expected improvements for 2016 and
future KMTNet data. The light curves of the 660 ""clear microlensing"" and 182
""possible microlensing"" events that were found in 2015 are presented along with
our policy for their public release.
"
"  Shape memory alloys often show a complex hierarchical morphology in the
martensitic state. To understand the formation of this twin-within-twins
microstructure, we examine epitaxial Ni-Mn-Ga films as a model system. In-situ
scanning electron microscopy experiments show beautiful complex twinning
patterns with a number of different mesoscopic twin boundaries and macroscopic
twin boundaries between already twinned regions. We explain the appearance and
geometry of these patterns by constructing an internally twinned martensitic
nucleus, which can take the shape of a diamond or a parallelogram, within the
basic phenomenological theory of martensite. These nucleus contains already the
seeds of different possible mesoscopic twin boundaries. Nucleation and growth
of these nuclei determines the creation of the hierarchical space-filling
martensitic microstructure. This is in contrast to previous approaches to
explain a hierarchical martensitic microstructure. This new picture of creation
and anisotropic, well-oriented growth of twinned martensitic nuclei explains
the morphology and exact geometrical features of our experimentally observed
twins-within-twins microstructure on the meso- and macroscopic scale.
"
"  We introduce a Bayesian approach for modeling Voigt profiles in absorption
spectroscopy and its implementation in the python package, BayesVP, publicly
available at this https URL. The code fits the
absorption line profiles within specified wavelength ranges and generates
posterior distributions for the column density, Doppler parameter, and
redshifts of the corresponding absorbers. The code uses publicly available
efficient parallel sampling packages to sample posterior and thus can be run on
parallel platforms. BayesVP supports simultaneous fitting for multiple
absorption components in high-dimensional parameter space. We provide other
useful utilities in the package, such as explicit specification of priors of
model parameters, continuum model, Bayesian model comparison criteria, and
posterior sampling convergence check.
"
"  The evolution from superconducting LiTi2O4-delta to insulating Li4Ti5O12 thin
films has been studied by precisely adjusting the oxygen pressure during the
sample fabrication process. In the superconducting LiTi2O4-delta films, with
the increase of oxygen pressure, the oxygen vacancies are filled, and the
c-axis lattice constant decreases gradually. With the increase of the oxygen
pressure to a certain critical value, the c-axis lattice constant becomes
stable, which implies that the Li4Ti5O12 phase comes into being. The process of
oxygen filling is manifested by the angular bright-field images of the scanning
transmission electron microscopy techniques. The temperature of
magnetoresistance changed from positive and negative shows a non-monotonous
behavior with the increase of oxygen pressure. The theoretical explanation of
the oxygen effects on the structure and superconductivity of LiTi2O4-delta has
also been discussed in this work.
"
"  We present the second release of value-added catalogues of the LAMOST
Spectroscopic Survey of the Galactic Anticentre (LSS-GAC DR2). The catalogues
present values of radial velocity $V_{\rm r}$, atmospheric parameters ---
effective temperature $T_{\rm eff}$, surface gravity log$g$, metallicity
[Fe/H], $\alpha$-element to iron (metal) abundance ratio [$\alpha$/Fe]
([$\alpha$/M]), elemental abundances [C/H] and [N/H], and absolute magnitudes
${\rm M}_V$ and ${\rm M}_{K_{\rm s}}$ deduced from 1.8 million spectra of 1.4
million unique stars targeted by the LSS-GAC since September 2011 until June
2014. The catalogues also give values of interstellar reddening, distance and
orbital parameters determined with a variety of techniques, as well as proper
motions and multi-band photometry from the far-UV to the mid-IR collected from
the literature and various surveys. Accuracies of radial velocities reach
5kms$^{-1}$ for late-type stars, and those of distance estimates range between
10 -- 30 per cent, depending on the spectral signal-to-noise ratios. Precisions
of [Fe/H], [C/H] and [N/H] estimates reach 0.1dex, and those of [$\alpha$/Fe]
and [$\alpha$/M] reach 0.05dex. The large number of stars, the contiguous sky
coverage, the simple yet non-trivial target selection function and the robust
estimates of stellar radial velocities and atmospheric parameters, distances
and elemental abundances, make the catalogues a valuable data set to study the
structure and evolution of the Galaxy, especially the solar-neighbourhood and
the outer disk.
"
"  During Rutherford cable production the wires are plastically deformed and
their initially round shape is distorted. Using X-ray absorption tomography we
have determined the 3D shape of an unreacted Nb3Sn 11 T dipole Rutherford
cable, and of a reacted and impregnated Nb3Sn cable double stack.
State-of-the-art image processing was applied to correct for tomographic
artefacts caused by the large cable aspect ratio, for the segmentation of the
individual wires and subelement bundles inside the wires, and for the
calculation of the wire cross sectional area and shape variations. The 11 T
dipole cable cross section oscillates by 2% with a frequency of 1.24 mm (1/80
of the transposition pitch length of the 40 wire cable). A comparatively
stronger cross sectional area variation is observed in the individual wires at
the thin edge of the keystoned cable where the wire aspect ratio is largest.
"
"  Interpreting the small-scale clustering of galaxies with halo models can
elucidate the connection between galaxies and dark matter halos. Unfortunately,
the modelling is typically not sufficiently accurate for ruling out models
statistically. It is thus difficult to use the information encoded in small
scales to test cosmological models or probe subtle features of the galaxy-halo
connection. In this paper, we attempt to push halo modelling into the
""accurate"" regime with a fully numerical mock-based methodology and careful
treatment of statistical and systematic errors. With our forward-modelling
approach, we can incorporate clustering statistics beyond the traditional
two-point statistics. We use this modelling methodology to test the standard
$\Lambda\mathrm{CDM}$ + halo model against the clustering of SDSS DR7 galaxies.
Specifically, we use the projected correlation function, group multiplicity
function and galaxy number density as constraints. We find that while the model
fits each statistic separately, it struggles to fit them simultaneously. Adding
group statistics leads to a more stringent test of the model and significantly
tighter constraints on model parameters. We explore the impact of varying the
adopted halo definition and cosmological model and find that changing the
cosmology makes a significant difference. The most successful model we tried
(Planck cosmology with Mvir halos) matches the clustering of low luminosity
galaxies, but exhibits a 2.3$\sigma$ tension with the clustering of luminous
galaxies, thus providing evidence that the ""standard"" halo model needs to be
extended. This work opens the door to adding interesting freedom to the halo
model and including additional clustering statistics as constraints.
"
"  The synthesis, physical, photocatalytic, and antibacterial properties of MgO
and graphene nanoplatelets (GNPs) nanocomposites are reported. The
crystallinity, phase, morphology, chemical bonding, and vibrational modes of
prepared nanomaterials are studied. The conductive nature of GNPs is tailored
via photocatalysis and enhanced antibacterial activity. It is interestingly
observed that the MgO/GNPs nanocomposite with optimized GNPs content show a
significant photocatalytic activity (97.23% degradation) as compared to bare
MgO (43%) which makes it the potential photocatalyst for purification of
industrial waste water. In addition, the effect of increased amount of GNPs on
antibacterial performance of nanocomposites against pathogenic micro-organisms
is researched, suggesting them toxic. MgO/GNPs 25% nanocomposite may have
potential applications in waste water treatment and nanomedicine due its
multifunctionality.
"
"  We present an implementation of the relativistic quantum-chemical density
matrix renormalization group (DMRG) approach based on a matrix-product
formalism. Our approach allows us to optimize matrix product state (MPS) wave
functions including a variational description of scalar-relativistic effects
and spin-orbit coupling from which we can calculate, for example, first-order
electric and magnetic properties in a relativistic framework. While
complementing our pilot implementation (S. Knecht et al., J. Chem. Phys., 140,
041101 (2014)) this work exploits all features provided by its underlying
non-relativistic DMRG implementation based on an matrix product state and
operator formalism. We illustrate the capabilities of our relativistic DMRG
approach by studying the ground-state magnetization as well as current density
of a paramagnetic $f^9$ dysprosium complex as a function of the active orbital
space employed in the MPS wave function optimization.
"
"  Single molecule magnets (SMMs) with single-ion anisotropies $\mathbf d$,
comparable to exchange interactions J, between spins have recently been
synthesized. In this paper, we provide theoretical insights into the magnetism
of such systems. We study spin chains with site spins, s=1, 3/2 and 2 and
on-site anisotropy $\mathbf d$ comparable to the exchange constants between the
spins. We find that large $\mathbf d$ leads to crossing of the states with
different $M_S$ values in the same spin manifold of the $\mathbf d = 0$ limit.
For very large $\mathbf d$'s we also find that the $M_S$ states of the higher
energy spin states descend below the $M_S$ states of the ground state spin
manifold. Total spin in this limit is no longer conserved and describing the
molecular anisotropy by the constants $D_M$ and $E_M$ is not possible. However,
the total spin of the low-lying large $M_S$ states is very nearly an integer
and using this spin value it is possible to construct an effective spin
Hamiltonian and compute the molecular magnetic anisotropy constants $D_M$ and
$E_M$. We report effect of finite sizes, rotations of site anisotropies and
chain dimerization on the effective anisotropy of the spin chains.
"
"  Compressive sensing (CS) combines data acquisition with compression coding to
reduce the number of measurements required to reconstruct a sparse signal. In
optics, this usually takes the form of projecting the field onto sequences of
random spatial patterns that are selected from an appropriate random ensemble.
We show here that CS can be exploited in `native' optics hardware without
introducing added components. Specifically, we show that random sub-Nyquist
sampling of an interferogram helps reconstruct the field modal structure. The
distribution of reduced sensing matrices corresponding to random measurements
is provably incoherent and isotropic, which helps us carry out CS successfully.
"
"  In recent years, realistic hydrodynamical simulations of galaxies like the
Milky Way have become available, enabling a reliable estimate of the dark
matter density and velocity distribution in the Solar neighborhood. We review
here the status of hydrodynamical simulations and their implications for the
interpretation of direct dark matter searches. We focus in particular on: the
criteria to identify Milky Way-like galaxies; the impact of baryonic physics on
the dark matter velocity distribution; the possible presence of substructures
like clumps, streams, or dark disks; and on the implications for the direct
detection of dark matter with standard and non-standard interactions.
"
"  Charge-neutral 180$^\circ$ domain walls that separate domains of antiparallel
polarization directions are common structural topological defects in
ferroelectrics. In normal ferroelectrics, charged 180$^\circ$ domain walls
running perpendicular to the polarization directions are highly energetically
unfavorable because of the depolarization field and are difficult to stabilize.
We explore both neutral and charged 180$^\circ$ domain walls in
hyperferroelectrics, a class of proper ferroelectrics with persistent
polarization in the presence of a depolarization field, using density
functional theory. We obtain zero temperature equilibrium structures of
head-to-head and tail-to-tail walls in recently discovered $ABC$-type hexagonal
hyperferroelectrics. Charged domain walls can also be stabilized in canonical
ferroelectrics represented by LiNbO$_3$ without any dopants, defects or
mechanical clamping. First-principles electronic structure calculations show
that charged domain walls can reduce and even close the band gap of host
materials and support quasi-two-dimensional electron(hole) gas with enhanced
electrical conductivity.
"
"  Strongly disordered spin chains invariant under the SO(N) group are shown to
display antiferromagnetic phases with emergent SU(N) symmetry without fine
tuning. The phases with emergent SU(N) symmetry are of two kinds: one has a
ground state formed of randomly distributed singlets of strongly bound pairs of
SO(N) spins (the `mesonic' phase), while the other has a ground state composed
of singlets made out of strongly bound integer multiples of N SO(N) spins (the
`baryonic' phase). Although the mechanism is general, we argue that the cases
N=2,3,4 and 6 can in principle be realized with the usual spin and orbital
degrees of freedom.
"
"  The use of opto-thermal molecular energy storage at the nanoscale creates new
opportunities for powering future microdevices with flexible synthetic
tailorability. Practical application of these molecular materials, however,
requires a deeper microscopic understanding of how their behavior is altered by
the presence of different types of substrates. Here we present single-molecule
resolved scanning tunneling microscopy imaging of thermally- and
optically-induced structural transitions in (fulvalene)tetracarbonyldiruthenium
molecules adsorbed onto a Ag(100) surface as a prototype system. Both the
parent complex and the photoisomer display distinct thermally-driven phase
transformations when they are in contact with a Ag(100) surface. This behavior
is consistent with the loss of carbonyl ligands due to strong molecule-surface
coupling. Ultraviolet radiation induces marked structural changes only in the
intact parent complex, thus indicating a photoisomerization reaction. These
results demonstrate how stimuli-induced structural transitions in this class of
molecule depend on the nature of the underlying substrate.
"
"  Using hybrid exchange-correlation functional in ab initio density functional
theory calculations, we study magnetic properties and strain effect on the
electronic properties of $\alpha$-graphyne monolayer. We find that a
spontaneous antiferromagnetic (AF) ordering occurs with energy band gap ($\sim$
0.5 eV) in the equilibrated $\alpha$-graphyne. Bi-axial tensile strain enhances
the stability of AF state as well as the staggered spin moment and value of the
energy gap. The antiferromagnetic semiconductor phase is quite robust against
moderate carrier filling with threshold carrier density up to
1.7$\times$10$^{14}$ electrons/cm$^2$ to destabilize the phase. The spontaneous
AF ordering and strain effect in $\alpha$-graphyne can be well described by the
framework of the Hubbard model. Our study shows that it is essential to
consider the electronic correlation effect properly in $\alpha$-graphyne and
may pave an avenue for exploring magnetic ordering in other carbon allotropes
with mixed hybridization of s and p orbitals.
"
"  We present a complete and consistent quantum theory of generalised X waves
with orbital angular momentum (OAM) in dispersive media. We show that the
resulting quantised light pulses are affected by neither dispersion nor
diffraction and are therefore resilient against external perturbations. The
nonlinear interaction of quantised X waves in quadratic and Kerr nonlinear
media is also presented and studied in detail.
"
"  We investigate drag reduction due to the flow-induced reconfiguration of a
flexible thin plate in presence of skin friction drag at low Reynolds Number.
The plate is subjected to a uniform free stream and is tethered at one end. We
extend existing models in the literature to account for the skin friction drag.
The total drag on the plate with respect to a rigid upright plate decreases due
to flow-induced reconfiguration and further reconfiguration increases the total
drag due to increase in skin friction drag. A critical value of Cauchy number
($Ca$) exists at which the total drag on the plate with respect to a rigid
upright plate is minimum at a given Reynolds number. The reconfigured shape of
the plate for this condition is unique, beyond which the total drag increases
on the plate even with reconfiguration. The ratio of the form drag coefficient
for an upright rigid plate and skin drag coefficient for a horizontal rigid
plate ($\lambda$) determines the critical Cauchy number ($Ca_{cr}$). We propose
modification in the drag scaling with free stream velocity ($F_{x}$ ${\propto}$
$U^{n}$) in presence of the skin friction drag. The following expressions of
$n$ are found for $0.01 \leq Re \leq 1$, $n = 4/5 + {\lambda}/5$ for 1 $\leq$
$Ca$ $<$ $Ca_{cr}$ and $n = 1 + {\lambda}/5$ for $Ca_{cr} \leq Ca \leq 300$,
where $Re$ is Reynolds number. We briefly discuss the combined effect of the
skin friction drag and buoyancy on the drag reduction. An assessment of the
feasibility of experiments is presented in order to translate the present model
to physical systems.
"
"  We study transient behaviour in the dynamics of complex systems described by
a set of non-linear ODE's. Destabilizing nature of transient trajectories is
discussed and its connection with the eigenvalue-based linearization procedure.
The complexity is realized as a random matrix drawn from a modified May-Wigner
model. Based on the initial response of the system, we identify a novel
stable-transient regime. We calculate exact abundances of typical and extreme
transient trajectories finding both Gaussian and Tracy-Widom distributions
known in extreme value statistics. We identify degrees of freedom driving
transient behaviour as connected to the eigenvectors and encoded in a
non-orthogonality matrix $T_0$. We accordingly extend the May-Wigner model to
contain a phase with typical transient trajectories present. An exact norm of
the trajectory is obtained in the vanishing $T_0$ limit where it describes a
normal matrix.
"
"  Recently proposed model of foam impact on the air sea drag coefficient Cd has
been employed for the estimation of the efficient foam-bubble radius Rb
variation with wind speed U10 in hurricane conditions. The model relates Cd
(U10) with the efficient roughness length Zeff (U10) represented as a sum of
aerodynamic roughness lengths of the foam free and foam covered sea surfaces Zw
(U10 ), and Zf (U10) weighted with the foam coverage coefficient. This relation
is treated for known phenomenological distributions Cd (U10), Zw (U10) at
strong wind speeds as an inverse problem for the efficient roughness parameter
of foam-covered sea surface Zf (U10).
"
"  This paper discusses the synthesis, characterization, and comprehensive study
of Ba-122 single crystals with various substitutions and various $T_c$. The
paper uses five complementary techniques to obtain a self-consistent set of
data on the superconducting properties of Ba-122. A major conclusion of the
authors work is the coexistence of two superconducting condensates differing in
the electron-boson coupling strength. The two gaps that develop in distinct
Fermi surface sheets are nodeless in the $k_xk_y$-plane and exhibit s-wave
symmetry, the two-band model represents a sufficient data description tool. A
moderate interband coupling and a considerable Coulomb repulsion in the
description of the two-gap superconducting state of barium pnictides favor the
$s^{++}$-model.
"
"  The Limb-imaging Ionospheric and Thermospheric Extreme-ultraviolet
Spectrograph (LITES) experiment is one of thirteen instruments aboard the Space
Test Program Houston 5 (STP-H5) payload on the International Space Station.
Along with the complementary GPS Radio Occultation and Ultraviolet Photometry
-- Colocated (GROUP-C) experiment, LITES will investigate ionospheric
structures and variability relevant to the global ionosphere. The ISS has an
orbital inclination of 51.6° which combined with its altitude of about 410
km enables middle- and low-latitude measurements from slightly above the peak
region of the ionosphere. The LITES instrument features a 10° by 10°
field of view which is collapsed horizontally, combining all information from a
given altitude. The instrument is installed such it looks in the wake of the
ISS and about 14.5° downwards in order to image altitudes ranging from
about 350 km to 150 km. The actual viewing altitude and geometry is directly
dependent on the pitch of the ISS, affecting the geophysical information
captured by the instrument.
"
"  Calculating one-body density profiles in equilibrium via particle-based
simulation methods involves counting of events of particle occurrences at
(histogram-resolved) space points. Here we investigate an alternative method
based on a histogram of the local force density. Via an exact sum rule the
density profile is obtained with a simple spatial integration. The method
circumvents the inherent ideal gas fluctuations. We have tested the method in
Monte Carlo, Brownian Dynamics and Molecular Dynamics simulations. The results
carry a statistical uncertainty smaller than that of the standard, counting,
method, reducing therefore the computation time.
"
"  We study the quantum dynamics of the Bose-Hubbard model on a ladder formed by
two rings coupled by tunneling effect. By implementing the Bogoliubov
approximation scheme, we prove that, despite the presence of the inter-ring
coupling term, the Hamiltonian decouples in many independent sub-Hamiltonians
$\hat{H}_k$ associated to momentum-mode pairs $\pm k$. Each sub-Hamiltonian
$\hat{H}_k$ is then shown to be part of a specific dynamical algebra. The
properties of the latter allow us to perform the diagonalization process, to
find energy spectrum, the conserved quantities of the model, and to derive the
time evolution of important physical observables. We then apply this solution
scheme to the simplest possible closed ladder, the double trimer. After
observing that the excitations of the system are weakly-populated vortices, we
explore the corresponding dynamics by varying the initial conditions and the
model parameters. Finally, we show that the inter-ring tunneling determines a
spectral collapse when approaching the border of the dynamical-stability
region.
"
"  We analyse Kepler light-curves of the exoplanet KOI-13b transiting its
moderately rapidly rotating (gravity-darkened) parent star. A physical model,
with minimal ad hoc free parameters, reproduces the time-averaged light-curve
at the ca. 10 parts per million level. We demonstrate that this Roche-model
solution allows the absolute dimensions of the system to be determined from the
star's projected equatorial rotation speed, v(e)sin(i), without any additional
assumptions; we find a planetary radius 1.33+/-0.05 R(Jup), stellar polar
radius 1.55+/-0.06 R(sun), combined mass M(*) + M(P) (\simeq M*) = 1.47 +/-
0.17 M(sun), and distance d \simeq 370+/-25 pc, where the errors are dominated
by uncertainties in relative flux contribution of the visual-binary companion
KOI-13B. The implied stellar rotation period is within ca. 5% of the
non-orbital, 25.43-hr signal found in the Kepler photometry. We show that the
model accurately reproduces independent tomographic observations, and yields an
offset between orbital and stellar-rotation angular-momentum vectors of
60.25+/-0.05 degrees.
"
"  We report the results of X-ray spectroscopy and Raman measurements of
as-prepared graphene on a high quality copper surface and the same materials
after 1.5 years under different conditions (ambient and low humidity). The
obtained results were compared with density functional theory calculations of
the formation energies and electronic structures of various structural defects
in graphene/Cu interfaces. For evaluation of the stability of the carbon cover,
we propose a two-step model. The first step is oxidation of the graphene, and
the second is perforation of graphene with the removal of carbon atoms as part
of the carbon dioxide molecule. Results of the modeling and experimental
measurements provide evidence that graphene grown on high-quality copper
substrate becomes robust and stable in time (1.5 years). However, the stability
of this interface depends on the quality of the graphene and the number of
native defects in the graphene and substrate. The effect of the presence of a
metallic substrate with defects on the stability and electronic structure of
graphene is also discussed.
"
"  We explore the impact of dimensionality on the scattering of a small bosonic
ensemble in an elongated harmonic trap off a centered repulsive barrier,
thereby taking particle correlations into account. The loss of coherence as
well as the oscillation of the center of mass are studied and we analyze the
influence of both particle and spatial correlations. Two different mechanisms
of coherence losses in dependence of the aspect ratio are found. For small
aspect ratios, loss of coherence between the region close to the barrier and
outer regions occurs, due to spatial correlations, and for large aspect ratios,
incoherence between the two density fragments of the left and right side of the
barrier arises, due to particle correlations. Apart form the decay of the
center of mass motion induced by the reflection and transmission, further
effects due to the particle and spatial correlations are explored. For tight
transversal traps, the amplitude of the center of mass oscillation experiences
a weaker damping, which can be traced back to the population of a second
natural orbital, and for a weaker transversal confinement, we detect a strong
decay, due to the possibility of transferring energy to transversal excited
modes. These effects are enhanced if the aspect ratio is integer valued.
"
"  The classical Galois theory deals with certain finite algebraic extensions
and establishes a bijective order reversing correspondence between the
intermediate fields and the subgroups of a group of permutations called the
Galois group of the extension. It has been the dream of many mathematicians at
the end of the nineteenth century to generalize these results to systems of
algebraic partial differential (PD) equations and the corresponding finitely
generated differential extensions, in order to be able to add the word
differential in front of any classical statement. The achievement of the
Picard-Vessiot theory by E. Kolchin between 1950 and 1970 is now well known.
The purpose of this paper is to sketch the general theory for such differential
extensions and algebraic pseudogroups by means of new methods mixing
differential algebra, differential geometry and algebraic geometry. As already
discovered by E. Vessiot in 1904 through the use of automorphic systems, a
concept never acknowledged, the main point is to notice that the Galois theory
(old and new) is mainly a study of principal homogeneous spaces (PHS) for
algebraic groups or pseudogroups. Hence, all the formal theory of PD equations
developped by D.C. Spencer around 1970 must be used together with modern
algebraic geometry, in particular tensor products of rings and fields. However,
the combination of these new tools is not sufficient and we have to create the
analogue for Lie pseudogroups of the so-called invariant derivations introduced
by A. Bialynicki-Birula after 1960 in the study of algebraic groups and fields
with derivations. We shall finally prove the usefulness of the resulting
differential Galois theory through striking applications to mechanics,
revisiting shell theory, chain theory, the Frenet-Serret formulas and the
integration of Hamilton-Jacobi equations.
"
"  Plasticity in zirconium alloys is mainly controlled by the interaction of 1/3
1210 screw dislocations with oxygen atoms in interstitial octahedral sites of
the hexagonal close-packed lattice. This process is studied here using ab
initio calculations based on the density functional theory. The atomic
simulations show that a strong repulsion exists only when the O atoms lie in
the dislocation core and belong to the prismatic dislocation habit plane. This
is a consequence of the destruction of the octahedral sites by the stacking
fault arising from the dislocation dissociation. Because of the repulsion, the
dislocation partially cross-slips to an adjacent prismatic plane, in agreement
with experiments where the lattice friction on screw dislocations in Zr-O
alloys has been attributed to the presence of jogs on the dislocations due to
local cross-slip.
"
"  Fourier optics, the principle of using Fourier Transformation to understand
the functionalities of optical elements, lies at the heart of modern optics,
and has been widely applied to optical information processing, imaging,
holography etc. While a simple thin lens is capable of resolving Fourier
components of an arbitrary optical wavefront, its operation is limited to near
normal light incidence, i.e. the paraxial approximation, which put a severe
constraint on the resolvable Fourier domain. As a result, high-order Fourier
components are lost, resulting in extinction of high-resolution information of
an image. Here, we experimentally demonstrate a dielectric metasurface
consisting of high-aspect-ratio silicon waveguide array, which is capable of
performing Fourier transform for a large incident angle range and a broad
operating bandwidth. Thus our device significantly expands the operational
Fourier space, benefitting from the large numerical aperture (NA), and
negligible angular dispersion at large incident angles. Our Fourier metasurface
will not only facilitate efficient manipulation of spatial spectrum of
free-space optical wavefront, but also be readily integrated into micro-optical
platforms due to its compact size.
"
"  A set of density functionals coming from different rungs on Jacob's ladder
are employed to evaluate the electronic excited states of three Ru(II)
complexes. While most studies on the performance of density functionals compare
the vertical excitation energies, in this work we focus on the energy gaps
between the electronic excited states, of the same and different multiplicity.
Excited state energy gaps are important for example to determine radiationless
transition probabilities. Besides energies, a functional should deliver the
correct state character and state ordering. Therefore, wavefunction overlaps
are introduced to systematically evaluate the effect of different functionals
on the character of the excited states. As a reference, the energies and state
characters from multi-state second-order perturbation theory complete active
space (MS-CASPT2) are used. In comparison to MS-CASPT2, it is found that while
hybrid functionals provide better vertical excitation energies, pure
functionals typically give more accurate excited state energy gaps. Pure
functionals are also found to reproduce the state character and ordering in
closer agreement to MS-CASPT2 than the hybrid functionals.
"
"  We use a direct numerical integration of the Vlasov equation in spherical
symmetry with a background gravitational potential to determine the evolution
of a collection of particles in different models of a galactic halo. Such a
collection is assumed to represent a dark matter inhomogeneity which reaches a
stationary state determined by the virialization of the system. We describe
some features of the stationary states and, by using several halo models,
obtain distinctive signatures for the evolution of the inhomogeneities in each
of the models.
"
"  We examine the velocity profile of coherent vortices appearing as a
consequence of the inverse cascade of two-dimensional turbulence in a finite
box in the case of static pumping. We demonstrate that in the passive regime
the flat velocity profile is realized, as in the case of pumping short
correlated in time. However, in the static case the energy flux to large scales
is dependent on the system parameters. We demonstrate that it is proportional
to $f^{4/3}$ where $f$ is the characteristic force exciting turbulence.
"
"  A boundary value problem, which could represent a transcendent temperature
conduction problem with evaporation in a part of the boundary, was studied to
determine unknown thermophysical parameters, which can be constants or time
dependent functions. The goal of this paper was elucidate which parameters may
be determined using only the measured superficial temperature in part of the
boundary of the domain. We formulated a nonlinear inverse problem to determine
the unknown parameters and a sensitivity analysis was also performed. In
particular, we introduced a new way of computing a sensitivity analysis of a
parameter which is variable in time. We applied the proposed method to model
tissue temperature changes under transient conditions in a biological problem:
the hamster cheek pouch. In this case, the time dependent unknown parameter can
be associated to the loss of heat due to water evaporation at the superficial
layer of the pouch. Finally, we performed the sensitivity analysis to determine
the most sensible parameters to variations of the superficial experimental data
in the hamster cheek pouch.
"
"  When a d-dimensional quantum system is subjected to a periodic drive, it may
be treated as a (d+1)-dimensional system, where the extra dimension is a
synthetic one. In this work, we take these ideas to the next level by showing
that non-uniform potentials, and particularly edges, in the synthetic dimension
are created whenever the dynamics of system has a memory component. We
demonstrate that topological states appear on the edges of these synthetic
dimensions and can be used as a basis for a wave packet construction. Such
systems may act as an optical isolator which allows transmission of light in a
directional way. We supplement our ideas by an example of a physical system
that shows this type of physics.
"
"  The data torrent unleashed by current and upcoming astronomical surveys
demands scalable analysis methods. Many machine learning approaches scale well,
but separating the instrument measurement from the physical effects of
interest, dealing with variable errors, and deriving parameter uncertainties is
often an after-thought. Classic forward-folding analyses with Markov Chain
Monte Carlo or Nested Sampling enable parameter estimation and model
comparison, even for complex and slow-to-evaluate physical models. However,
these approaches require independent runs for each data set, implying an
unfeasible number of model evaluations in the Big Data regime. Here I present a
new algorithm, collaborative nested sampling, for deriving parameter
probability distributions for each observation. Importantly, the number of
physical model evaluations scales sub-linearly with the number of data sets,
and no assumptions about homogeneous errors, Gaussianity, the form of the model
or heterogeneity/completeness of the observations need to be made.
Collaborative nested sampling has immediate application in speeding up analyses
of large surveys, integral-field-unit observations, and Monte Carlo
simulations.
"
"  The ancient phrase, ""All roads lead to Rome"" applies to Chemistry and
Physics. Both are highly evolved sciences, with their own history, traditions,
language, and approaches to problems. Despite all these differences, these two
roads generally lead to the same place. For high temperature cuprate
superconductors however, the Chemistry and Physics roads do not meet or even
come close to each other. In this paper, we analyze the physics and chemistry
approaches to the doped electronic structure of cuprates and find the chemistry
doped hole (out-of-the-CuO$\mathrm{_2}$-planes) leads to explanations of a vast
array of normal state cuprate phenomenology using simple counting arguments.
The chemistry picture suggests that phonons are responsible for
superconductivity in cuprates. We identify the important phonon modes, and show
that the observed T$\mathrm{_c} \sim 100$ K, the T$\mathrm{_c}$-dome as a
function of hole doping, the change in T$\mathrm{_c}$ as a function of the
number of CuO$\mathrm{_2}$ layers per unit cell, the lack of an isotope effect
at optimal T$\mathrm{_c}$ doping, and the D-wave symmetry of the
superconducting Cooper pair wavefunction are all explained by the chemistry
picture. Finally, we show that ""crowding"" the dopants in cuprates leads to a
pair wavefunction with S-wave symmetry and T$\mathrm{_c}\approx280-390$ K.
Hence, we believe there is enormous ""latent"" T$\mathrm{_c}$ remaining in the
cuprate class of superconductors.
"
"  We perform Zeeman spectroscopy on a Rydberg electromagnetically induced
transparency (EIT) system in a room-temperature Cs vapor cell, in magnetic
fields up to 50~Gauss and for several polarization configurations. The magnetic
interactions of the $\vert 6S_{1/2}, F_g=4 \rangle$ ground, $\vert 6P_{3/2},
F_e=5 \rangle$ intermediate, and $\vert 33S_{1/2} \rangle$ Rydberg states that
form the ladder-type EIT system are in the linear Zeeman, quadratic Zeeman, and
the deep hyperfine Paschen-Back regimes, respectively. Starting in magnetic
fields of about 5~Gauss, the spectra develop an asymmetry that becomes
paramount in fields $\gtrsim40$~Gauss. We use a quantum Monte Carlo
wave-function approach to quantitatively model the spectra. Simulated spectra
are in good agreement with experimental data. The asymmetry in the spectra is,
in part, due to level shifts caused by the quadratic Zeeman effect, but it also
reflects the complicated interplay between optical pumping and EIT in the
magnetic field. Relevance to measurement applications is discussed. %The
simulations are also used to study optical pumping in the magnetic field and to
investigate the interplay between optical pumping and EIT, which reduces photon
scattering and optical pumping.
"
"  The past decade has seen significant advances in cm-wave VLBI extragalactic
observations due to a wide range of technical successes, including the increase
in processed field-of-view and bandwidth. The future inclusion of MeerKAT into
global VLBI networks would provide further enhancement, particularly the
dramatic sensitivity boost to >7000 km baselines. This will not be without its
limitations, however, considering incomplete MeerKAT band overlap with current
VLBI arrays and the small (real-time) field-of-view afforded by the phased up
MeerKAT array. We provide a brief overview of the significant contributions
MeerKAT-VLBI could make, with an emphasis on the scientific output of several
MeerKAT extragalactic Large Survey Projects.
"
"  Controlling nanocircuits at the single electron spin level is a possible
route for large-scale quantum information processing. In this context,
individual electron spins have been identified as versatile quantum information
carriers to interconnect different nodes of a spin-based semiconductor quantum
circuit. Despite important experimental efforts to control the electron
displacement over long distances, keeping the electron spin coherence after
transfer remained up to now elusive. Here we demonstrate that individual
electron spins can be displaced coherently over a distance of 5 micrometers.
This displacement is realized on a closed path made of three tunnel-coupled
lateral quantum dots. Using fast quantum dot control, the electrons tunnel from
one dot to another at a speed approaching 100 m/s. We find that the spin
coherence length is 8 times longer than expected from the electron spin
coherence without displacement. Such an enhanced spin coherence points at a
process similar to motional narrowing observed in nuclear magnetic resonance
experiments6. The demonstrated coherent displacement will enable long-range
interaction between distant spin-qubits and will open the route towards
non-abelian and holonomic manipulation of a single electron spin.
"
"  We compute the effects of generic short-range interactions on gapless
electrons residing at the quantum critical point separating a two-dimensional
Dirac semimetal (DSM) and a symmetry-preserving band insulator (BI). The
electronic dispersion at this critical point is anisotropic ($E_{\mathbf k}=\pm
\sqrt{v^2 k^2_x + b^2 k^{2n}_y}$ with $n=2$), which results in unconventional
scaling of physical observables. Due to the vanishing density of states
($\varrho(E) \sim |E|^{1/n}$), this anisotropic semimetal (ASM) is stable
against weak short-range interactions. However, for stronger interactions the
direct DSM-BI transition can either $(i)$ become a first-order transition, or
$(ii)$ get avoided by an intervening broken-symmetry phase (BSP). We perform a
renormalization group analysis by perturbing away from the one-dimensional
limit with the small parameter $\epsilon = 1/n$, augmented with a $1/n$
expansion (parametrically suppressing quantum fluctuations in higher
dimension). We identify charge density wave (CDW), antiferromagnet (AFM) and
singlet s-wave superconductor as the three dominant candidates for the BSP. The
onset of any such order at strong coupling $(\sim \epsilon)$ takes place
through a continuous quantum phase transition across multicritical point. We
also present the phase diagram of an extended Hubbard model for the ASM,
obtained via the controlled deformation of its counterpart in one dimension.
The latter displays spin-charge separation and instabilities to CDW, spin
density wave, and Luther-Emery liquid phases at arbitrarily weak coupling. The
spin density wave and Luther-Emery liquid phases deform into pseudospin
SU(2)-symmetric quantum critical points separating the ASM from the AFM and
superconducting orders, respectively. Our results can be germane for a
uniaxially strained honeycomb lattice or organic compound
$\alpha$-(BEDT-TTF)$_2\text{I}_3$.
"
"  The refraction index of the quantized lossy composite right-left handed
transmission line (CRLH-TL) is deduced in the thermal coherence state. The
results show that the negative refraction index (herein the left-handedness)
can be implemented by the electric circuit dissipative factors(i.e., the
resistances \(R\) and conductances \( G\)) in a higher frequency band
(1.446GHz\(\leq\omega\leq \) 15GHz), and flexibly adjusted by the left-handed
circuit components (\(C_l\), \(L_l\)) and the right-handed circuit components
(\(C_r\), \(L_r\)) at a lower frequency (\(\omega\)=0.995GHz) . The flexible
adjustment for left-handedness in a wider bandwidth will be significant for the
microscale circuit design of the CRLH-TL and may make the theoretical
preparation for its compact applications.
"
"  The Kotliar and Ruckenstein slave-boson representation of the Hubbard model
allows to obtain an approximation of the charge dynamical response function
resulting from the Gaussian fluctuations around the paramagnetic saddle-point
in analytical form. Numerical evaluation in the thermodynamical limit yields
charge excitation spectra consisting of a continuum, a gapless collective mode
with anisotropic zero-sound velocity, and a correlation induced high-frequency
mode at $\omega\approx U$. In this work we show that this analytical expression
obeys the particle-hole symmetry of the model on any bipartite lattice with one
atom in the unit cell. Other formal aspects of the approach are also addressed.
"
"  We find that negative charges on an armchair single-walled carbon nanotube
(SWCNT) can significantly enhance the migration of a carbon adatom on the
external surfaces of SWCNTs, along the direction of the tube axis. Nanotube
charging results in stronger binding of adatoms to SWCNTs and consequent longer
lifetimes of adatoms before desorption, which in turn increases their migration
distance several orders of magnitude. These results support the hypothesis of
diffusion enhanced SWCNT growth in the volume of arc plasma. This process could
enhance effective carbon flux to the metal catalyst.
"
"  Thermochemical models have been used in the past to constrain the deep oxygen
abundance in the gas and ice giant planets from tropospheric CO spectroscopic
measurements. Knowing the oxygen abundance of these planets is a key to better
understand their formation. These models have widely used dry and/or moist
adiabats to extrapolate temperatures from the measured values in the upper
troposphere down to the level where the thermochemical equilibrium between
H$_2$O and CO is established. The mean molecular mass gradient produced by the
condensation of H$_2$O stabilizes the atmosphere against convection and results
in a vertical thermal profile and H$_2$O distribution that departs
significantly from previous estimates. We revisit O/H estimates using an
atmospheric structure that accounts for the inhibition of the convection by
condensation. We use a thermochemical network and the latest observations of CO
in Uranus and Neptune to calculate the internal oxygen enrichment required to
satisfy both these new estimates of the thermal profile and the observations.
We also present the current limitations of such modeling.
"
"  K$_3$Cu$_3$AlO$_2$(SO$_4$)$_4$ is a highly one-dimensional spin-1/2
inequilateral diamond-chain antiferromagnet. Spinon continuum and spin-singlet
dimer excitations are observed in the inelastic neutron scattering spectra,
which is in excellent agreement with a theoretical prediction: a dimer-monomer
composite structure, where the dimer is caused by strong antiferromagnetic
(AFM) coupling and the monomer forms an almost isolated quantum AFM chain
controlling low-energy excitations. Moreover, muon spin rotation/relaxation
spectroscopy shows no long-range ordering down to 90~mK, which is roughly three
orders of magnitude lower than the exchange interaction of the quantum AFM
chain. K$_3$Cu$_3$AlO$_2$(SO$_4$)$_4$ is, thus, regarded as a compound that
exhibits a Tomonaga-Luttinger spin liquid behavior at low temperatures close to
the ground state.
"
"  We report the finding of unidirectional electronic properties, analogous to a
semiconductor diode, in two-dimensional artificial permalloy honeycomb lattice
of ultra-small bond, with a typical length of ~ 12 nm. The unidirectional
transport behavior, characterized by the asymmetric colossal enhancement in
differential conductivity at a modest current application of ~ 10-15 $\mu$A,
persists to T = 300 K in honeycomb lattice of thickness ~ 6 nm. The asymmetric
behavior arises without the application of magnetic field. A qualitative
analysis of experimental data suggests the role of magnetic charge or monopoles
in the unusual observations with strong implication for spintronics.
"
"  We use Boltzmann transport equation (BE) to study time evolution of a
photo-excited state in a nanoparticle including phonon-mediated exciton
relaxation and the multiple exciton generation (MEG) processes, such as
exciton-to-biexciton multiplication and biexciton-to-exciton recombination. BE
collision integrals are computed using Kadanoff-Baym-Keldysh many-body
perturbation theory (MBPT) based on density functional theory (DFT)
simulations, including exciton effects. We compute internal quantum efficiency
(QE), which is the number of excitons generated from an absorbed photon in the
course of the relaxation. We apply this approach to chiral single-wall carbon
nanotubes (SWCNTs), such as (6,2), and (6,5). We predict efficient MEG in the
(6,2) and (6,5) SWCNTs within the solar spectrum range starting at the $2 E_g$
energy threshold and with QE reaching $\sim 1.6$ at about $3 E_g,$ where $E_g$
is the electronic gap.
"
"  The goal of the present study is to develop polymeric matrix films loaded
with a combination of free diclofenac sodium (DFSfree) and DFS:Ion exchange
resin complexes (DFS:IR) for immediate and sustained release profiles,
respectively. Effect of ratio of DFS and IR on the DFS:IR complexation
efficiency was studied using batch processing. DFS:IR complex, DFSfree, or a
combination of DFSfree+DFS:IR loaded matrix films were prepared by melt-cast
technology. DFS content was 20% w/w in these matrix films. In vitro
transcorneal permeability from the film formulations were compared against DFS
solution, using a side-by-side diffusion apparatus, over a 6 h period. Ocular
disposition of DFS from the solution, films and corresponding suspensions were
evaluated in conscious New Zealand albino rabbits, 4 h and 8 h post-topical
administration. All in vivo studies were carried out as per the University of
Mississippi IACUC approved protocol. Complexation efficiency of DFS:IR was
found to be 99% with a 1:1 ratio of DFS:IR. DFS release from DFS:IR suspension
and the film were best-fit to a Higuchi model. In vitro transcorneal flux with
the DFSfree+DFS:IR(1:1)(1 + 1) was twice that of only DFS:IR(1:1) film. In
vivo, DFS solution and DFS:IR(1:1) suspension formulations were not able to
maintain therapeutic DFS levels in the aqueous humor (AH). Both DFSfree and
DFSfree+DFS:IR(1:1)(3 + 1) loaded matrix films were able to achieve and
maintain high DFS concentrations in the AH, but elimination of DFS from the
ocular tissues was much faster with the DFSfree formulation. DFSfree+DFS:IR
combination loaded matrix films were able to deliver and maintain therapeutic
DFS concentrations in the anterior ocular chamber for up to 8 h. Thus, free
drug/IR complex loaded matrix films could be a potential topical ocular
delivery platform for achieving immediate and sustained release
characteristics.
"
"  This work introduces a novel reinterpretation of structured illumination (SI)
microscopy for coherent imaging that allows three-dimensional imaging of
complex refractive index (RI). To do so, we show that coherent SI is
mathematically equivalent to a superposition of angled illuminations. It
follows that raw acquisitions for standard SI-enhanced quantitative-phase
images can be processed into complex electric-field maps describing sample
diffraction under angled illuminations. Standard diffraction tomography (DT)
computation can then be used to reconstruct the sample 3D RI distribution at
sub-diffraction resolutions. We demonstrate this concept by using a
SI-quantitative-phase imaging system to computationally reconstruct 3D RI
distributions of human breast (MCF-7) and colorectal (HT-29) adenocarcinoma
cells. Our experimental setup uses a spatial light modulator to generate
structured patterns at the sample and collects angle-dependent sample
diffraction using a common-path, off-axis interference configuration with no
moving components. Furthermore, this technique holds promise for easy pairing
with SI fluorescence microscopy, and important future extensions may include
multimodal, sub-diffraction resolution, 3D RI and fluorescent visualizations.
"
"  The arrangements of particles and forces in granular materials have a complex
organization on multiple spatial scales that ranges from local structures to
mesoscale and system-wide ones. This multiscale organization can affect how a
material responds or reconfigures when exposed to external perturbations or
loading. The theoretical study of particle-level, force-chain, domain, and bulk
properties requires the development and application of appropriate physical,
mathematical, statistical, and computational frameworks. Traditionally,
granular materials have been investigated using particulate or continuum
models, each of which tends to be implicitly agnostic to multiscale
organization. Recently, tools from network science have emerged as powerful
approaches for probing and characterizing heterogeneous architectures across
different scales in complex systems, and a diverse set of methods have yielded
fascinating insights into granular materials. In this paper, we review work on
network-based approaches to studying granular matter and explore the potential
of such frameworks to provide a useful description of these systems and to
enhance understanding of their underlying physics. We also outline a few open
questions and highlight particularly promising future directions in the
analysis and design of granular matter and other kinds of material networks.
"
"  We show that non-linear Schwarzian differential equations emerging from
covariance symmetry conditions imposed on linear differential operators with
hypergeometric function solutions, can be generalized to arbitrary order linear
differential operators with polynomial coefficients having selected
differential Galois groups. For order three and order four linear differential
operators we show that this pullback invariance up to conjugation eventually
reduces to symmetric powers of an underlying order-two operator. We give,
precisely, the conditions to have modular correspondences solutions for such
Schwarzian differential equations, which was an open question in a previous
paper. We analyze in detail a pullbacked hypergeometric example generalizing
modular forms, that ushers a pullback invariance up to operator homomorphisms.
We expect this new concept to be well-suited in physics and enumerative
combinatorics. We finally consider the more general problem of the equivalence
of two different order-four linear differential
Calabi-Yau operators up to pullbacks and conjugation, and clarify the cases
where they have the same Yukawa couplings.
"
"  We study a squeezed vacuum field generated in hot Rb vapor via the
polarization self-rotation effect. Our previous experiments showed that the
amount of observed squeezing may be limited by the contamination of the
squeezed vacuum output with higher-order spatial modes, also generated inside
the cell. Here, we demonstrate that the squeezing can be improved by making the
light interact several times with a less dense atomic ensemble. With
optimization of some parameters we can achieve up to -2.6 dB of squeezing in
the multi-pass case, which is 0.6 dB improvement compared to the single-pass
experimental configuration. Our results show that other than the optical depth
of the medium, the spatial mode structure and cell configuration also affect
the squeezing level.
"
"  Recently, Ciufolini et al. reported on a test of the general relativistic
gravitomagnetic Lense-Thirring effect by analyzing about 3.5 years of laser
ranging data to the LAGEOS, LAGEOS II, LARES geodetic satellites orbiting the
Earth. By using the GRACE-based GGM05S Earth's global gravity model and a
linear combination of the nodes $\Omega$ of the three satellites designed to
remove the impact of errors in the first two even zonal harmonic coefficients
$J_2,~J_4$ of the multipolar expansion of the Newtonian part of the Earth's
gravitational potential, they claimed an overall accuracy of $5\%$ for the
Lense-Thirring caused node motion. We show that the scatter in the nominal
values of the uncancelled even zonals of degree $\ell = 6,~8,~10$ from some of
the most recent global gravity models does not yet allow to reach unambiguously
and univocally the expected $\approx 1\%$ level, being large up to $\lesssim
15\%~(\ell=6),~6\%~(\ell=8),~36\%~(\ell=10)$ for some pairs of models.
"
"  NGC 7793 P13 is an ultraluminous X-ray source harboring an accreting pulsar.
We report on the detection of a ~65 d period X-ray modulation with Swift
observations in this system. The modulation period found in the X-ray band is
P=65.05+/-0.10 d and the profile is asymmetric with a fast rise and a slower
decay. On the other hand, the u-band light curve collected by Swift UVOT
confirmed an optical modulation with a period of P=64.24+/-0.13 d. We explored
the phase evolution of the X-ray and optical periodicities and propose two
solutions. A superorbital modulation with a period of ~2,700-4,700 d probably
caused by the precession of a warped accretion disk is necessary to interpret
the phase drift of the optical data. We further discuss the implication if this
~65d periodicity is caused by the superorbital modulation. Estimated from the
relationship between the spin-orbital and orbital-superorbital periods of known
disk-fed high-mass X-ray binaries, the orbital period of P13 is roughly
estimated as 3-7 d. In this case, an unknown mechanism with a much longer time
scale is needed to interpret the phase drift. Further studies on the stability
of these two periodicities with a long-term monitoring could help us to probe
their physical origins.
"
"  A new large-scale parallel multiconfigurational self-consistent field (MCSCF)
implementation in the open-source NWChem computational chemistry code is
presented. The generalized active space (GAS) approach is used to partition
large configuration interaction (CI) vectors and generate a sufficient number
of batches that can be distributed to the available nodes. Massively parallel
CI calculations with large active spaces can be treated. The performance of the
new parallel MCSCF implementation is presented for the chromium trimer and for
an active space of 20 electrons in 20 orbitals. Unprecedented CI calculations
with an active space of 22 electrons in 22 orbitals for the pentacene systems
were performed and a single CI iteration calculation with an active space of 24
electrons in 24 orbitals for the chromium tetramer was possible. The chromium
tetramer corresponds to a CI expansion of one trillion SDs (914 058 513 424)
and is largest conventional CI calculation attempted up to date.
"
"  With the start of the Gaia era, the time has come to address the major
challenge of deriving the star formation history and evolution of the disk of
our MilkyWay. Here we review our present knowledge of the outer regions of the
Milky Way disk population. Its stellar content, its structure and its dynamical
and chemical evolution are summarized, focussing on our lack of understanding
both from an observational and a theoretical viewpoint. We describe the
unprecedented data that Gaia and the upcoming ground-based spectroscopic
surveys will provide in the next decade. More in detail, we quantify the expect
accuracy in position, velocity and astrophysical parameters of some of the key
tracers of the stellar populations in the outer Galactic disk. Some insights on
the future capability of these surveys to answer crucial and fundamental issues
are discussed, such as the mechanisms driving the spiral arms and the warp
formation. Our Galaxy, theMilkyWay, is our cosmological laboratory for
understanding the process of formation and evolution of disk galaxies. What we
learn in the next decades will be naturally transferred to the extragalactic
domain.
"
"  These are lecture notes based on three lectures given by Antonello
Scardicchio at the December 2016 Topical School on Many-Body-Localization
organized by the Statistical Physics Group of the Institute Jean Lamour in
Nancy. They were compiled and put in a coherent logical form by Thimothée
Thiery.
"
"  We report the detection of extended Halpha emission from the tip of the HI
disk of the nearby edge-on galaxy UGC 7321, observed with the Multi Unit
Spectroscopic Explorer (MUSE) instrument at the Very Large Telescope. The
Halpha surface brightness fades rapidly where the HI column density drops below
N(HI) = 10^19 cm^-2 , consistent with fluorescence arising at the ionisation
front from gas that is photoionized by the extragalactic ultraviolet background
(UVB). The surface brightness measured at this location is (1.2 +/- 0.5)x10^-19
erg/s/cm^2/arcsec^2, where the error is mostly systematic and results from the
proximity of the signal to the edge of the MUSE field of view, and from the
presence of a sky line next to the redshifted Halpha wavelength. By combining
the Halpha and the HI 21 cm maps with a radiative transfer calculation of an
exponential disk illuminated by the UVB, we derive a value for the HI
photoionization rate of Gamma ~ (6-8)x10^-14 1/s . This value is consistent
with transmission statistics of the Lyalpha forest and with recent models of a
UVB which is dominated by quasars.
"
"  We study a scenario in which the baryon asymmetry of the universe arises from
a cosmological phase transition where lepton-number is spontaneously broken. If
the phase transition is first order, a lepton-number asymmetry can arise at the
bubble wall, through dynamics similar to electroweak baryogenesis, but
involving right-handed neutrinos. In addition to the usual neutrinoless double
beta decay in nuclear experiments, the model may be probed through a variety of
""baryogenesis by-products,"" which include a stochastic background of
gravitational waves created by the colliding bubbles. Depending on the model,
other aspects may include a network of topological defects that produce their
own gravitational waves, additional contribution to dark radiation, and a light
pseudo-Goldstone boson (majoron) as dark matter candidate.
"
"  No-insulation (NI) REBCO magnets have many advantages. They are
self-protecting, therefore do not need quench detection and protection which
can be very challenging in a high Tc superconducting magnet. Moreover, by
removing insulation and allowing thinner copper stabilizer, NI REBCO magnets
have significantly higher engineering current density and higher mechanical
strength. On the other hand, NI REBCO magnets have drawbacks of long magnet
charging time and high field-ramp-loss. In principle, these drawbacks can be
mitigated by managing the turn-to-turn contact resistivity (Rc). Evidently the
first step toward managing Rc is to establish a reliable method of accurate Rc
measurement. In this paper, we present experimental Rc measurements of REBCO
tapes as a function of mechanical load up to 144 MPa and load cycles up to 14
times. We found that Rc is in the range of 26-100 uOhm-cm2; it decreases with
increasing pressure, and gradually increases with number of load cycles. The
results are discussed in the framework of Holm's electric contact theory.
"
"  Solitary waves propagation of baryonic density perturbations, ruled by the
Korteweg--de Vries equation in a mean-field quark-gluon plasma model, are
investigated from the point of view of the theory of information. A recently
proposed continuous logarithmic measure of information, called configurational
entropy, is used to derive the soliton width, defining the pulse, for which the
informational content of the soliton spatial profile is more compressed, in the
Shannon's sense.
"
"  Recent Einstein-Podolsky-Rosen-Bohm experiments [M. Giustina et al. Phys.
Rev. Lett. 115, 250401 (2015); L. K. Shalm et al. Phys. Rev. Lett. 115, 250402
(2015)] that claim to be loophole free are scrutinized and are shown to suffer
a photon identification loophole. The combination of a digital computer and
discrete-event simulation is used to construct a minimal but faithful model of
the most perfected realization of these laboratory experiments. In contrast to
prior simulations, all photon selections are strictly made, as they are in the
actual experiments, at the local station and no other ""post-selection"" is
involved. The simulation results demonstrate that a manifestly non-quantum
model that identifies photons in the same local manner as in these experiments
can produce correlations that are in excellent agreement with those of the
quantum theoretical description of the corresponding thought experiment, in
conflict with Bell's theorem. The failure of Bell's theorem is possible because
of our recognition of the photon identification loophole. Such identification
measurement-procedures are necessarily included in all actual experiments but
are not included in the theory of Bell and his followers.
"
"  Detection of a planetary ring of exoplanets remains as one of the most
attractive but challenging goals in the field. We present a methodology of a
systematic search for exoplanetary rings via transit photometry of long-period
planets. The methodology relies on a precise integration scheme we develop to
compute a transit light curve of a ringed planet. We apply the methodology to
89 long-period planet candidates from the Kepler data so as to estimate, and/or
set upper limits on, the parameters of possible rings. While a majority of our
samples do not have a sufficiently good signal-to-noise ratio for meaningful
constraints on ring parameters, we find that six systems with a higher
signal-to-noise ratio are inconsistent with the presence of a ring larger than
1.5 times the planetary radius assuming a grazing orbit and a tilted ring.
Furthermore, we identify five preliminary candidate systems whose light curves
exhibit ring-like features. After removing four false positives due to the
contamination from nearby stars, we identify KIC 10403228 as a reasonable
candidate for a ringed planet. A systematic parameter fit of its light curve
with a ringed planet model indicates two possible solutions corresponding to a
Saturn-like planet with a tilted ring. There also remain other two possible
scenarios accounting for the data; a circumstellar disk and a hierarchical
triple. Due to large uncertain factors, we cannot choose one specific model
among the three.
"
"  Scientific discovery via numerical simulations is important in modern
astrophysics. This relatively new branch of astrophysics has become possible
due to the development of reliable numerical algorithms and the high
performance of modern computing technologies. These enable the analysis of
large collections of observational data and the acquisition of new data via
simulations at unprecedented accuracy and resolution. Ideally, simulations run
until they reach some pre-determined termination condition, but often other
factors cause extensive numerical approaches to break down at an earlier stage.
In those cases, processes tend to be interrupted due to unexpected events in
the software or the hardware. In those cases, the scientist handles the
interrupt manually, which is time-consuming and prone to errors. We present the
Simulation Monitor (SiMon) to automatize the farming of large and extensive
simulation processes. Our method is light-weight, it fully automates the entire
workflow management, operates concurrently across multiple platforms and can be
installed in user space. Inspired by the process of crop farming, we perceive
each simulation as a crop in the field and running simulation becomes analogous
to growing crops. With the development of SiMon we relax the technical aspects
of simulation management. The initial package was developed for extensive
parameter searchers in numerical simulations, but it turns out to work equally
well for automating the computational processing and reduction of observational
data reduction.
"
"  We investigate, using 3D hydrodynamic simulations, the fragmentation of
pressure-confined, vertically stratified, self-gravitating gaseous layers. The
confining pressure is either thermal pressure acting on both surfaces, or
thermal pressure acting on one surface and ram-pressure on the other. In the
linear regime of fragmentation, the dispersion relation we obtain agrees well
with that derived by Elmegreen & Elmegreen (1978), and consequently deviates
from the dispersion relations based on the thin shell approximation (Vishniac
1983) or pressure assisted gravitational instability (Wünsch et al. 2010). In
the non-linear regime, the relative importance of the confining pressure to the
self-gravity is a crucial parameter controlling the qualitative course of
fragmentation. When confinement of the layer is dominated by external pressure,
self- gravitating condensations are delivered by a two-stage process: first the
layer fragments into gravitationally bound but stable clumps, and then these
clumps coalesce until they assemble enough mass to collapse. In contrast, when
external pressure makes a small contribution to confinement of the layer, the
layer fragments monolithically into gravitationally unstable clumps and there
is no coalescence. This dichotomy persists whether the external pressure is
thermal or ram. We apply these results to fragments forming in a shell swept up
by an expanding H II region, and find that, unless the swept up gas is quite
hot or the surrounding medium has low density, the fragments have low-mass ( ~<
3 M_Sun ), and therefore they are unlikely to spawn stars that are sufficiently
massive to promote sequential self-propagating star formation.
"
"  Stellar shells are low surface brightness arcs of overdense stellar regions,
extending to large galactocentric distances. In a companion study, we
identified 39 shell galaxies in a sample of 220 massive ellipticals
($\mathrm{M}_{\mathrm{200crit}}>6\times10^{12}\,\mathrm{M}_\odot$) from the
Illustris cosmological simulation. We used stellar history catalogs to trace
the history of each individual star particle inside the shell substructures,
and we found that shells in high-mass galaxies form through mergers with
massive satellites (stellar mass ratios $\mu_{\mathrm{stars}}\gtrsim1:10$).
Using the same sample of shell galaxies, the current study extends the stellar
history catalogs in order to investigate the metallicity of stellar shells
around massive galaxies. Our results indicate that outer shells are often times
more metal-rich than the surrounding stellar material in a galaxy's halo. For a
galaxy with two different satellites forming $z=0$ shells, we find a
significant difference in the metallicity of the shells produced by each
progenitor. We also find that shell galaxies have higher mass-weighted
logarithmic metallicities ([Z/H]) at $2$-$4\,\mathrm{R}_{\mathrm{eff}}$
compared to galaxies without shells. Our results indicate that observations
comparing the metallicities of stars in tidal features, such as shells, to the
average metallicities in the stellar halo can provide information about the
assembly histories of galaxies.
"
"  By way of the nonequilibrium Green's function simulations and first
principles calculations, we report that borophene, a single layer of boron
atoms that was fabricated recently, possesses an extraordinarily high lattice
thermal conductance in the ballistic transport regime, which even exceeds
graphene. In addition to the obvious reasons of light mass and strong bonding
of boron atoms, the superior thermal conductance is mainly rooted in its strong
structural anisotropy and unusual phonon transmission. For low-frequency
phonons, the phonon transmission within borophene is nearly isotropic, similar
to that of graphene. For high frequency phonons, however, the transmission is
one dimensional, that is, all the phonons travel in one direction, giving rise
to its ultrahigh thermal conductance. The present study suggests that borophene
is promising for applications in efficient heat dissipation and thermal
management, and also an ideal material for revealing fundamentals of
dimensionality effect on phonon transport in ballistic regime.
"
"  We discuss the three spacetime dimensional $\mathbb{C}\mathbb{P}^N$ model and
specialize to the $\mathbb{C}\mathbb{P}^1$ model. Because of the Hopf map
$\pi_3(\mathbb{C}\mathbb{P}^1)=\mathbb{Z}$ one might try to couple the model to
a periodic $\theta$ parameter. However, we argue that only the values
$\theta=0$ and $\theta=\pi$ are consistent. For these values the Skyrmions in
the model are bosons and fermions respectively, rather than being anyons. We
also extend the model by coupling it to a topological quantum field theory,
such that the Skyrmions are anyons. We use techniques from geometry and
topology to construct the $\theta =\pi $ theory on arbitrary 3-manifolds, and
use recent results about invertible field theories to prove that no other
values of $\theta $ satisfy the necessary locality.
"
"  We observed 15 of the solar-type binaries within 67 pc of the Sun previously
observed by the Robo-AO system in the visible, with the PHARO near-IR camera
and the PALM-3000 adaptive optics system on the 5 m Hale telescope. The
physical status of the binaries is confirmed through common proper motion and
detection of orbital motion. In the process we detected a new candidate
companion to HIP 95309. We also resolved the primary of HIP 110626 into a close
binary making that system a triple. These detections increase the completeness
of the multiplicity survey of the solar-type stars within 67 pc of the Sun.
Combining our observations of HIP 103455 with archival astrometric measurements
and RV measurements, we are able to compute the first orbit of HIP 103455
showing that the binary has a 68 yr period. We place the components on a
color-magnitude diagram and discuss each multiple system individually.
"
"  Measurements of plasma electric fields are essential to the advancement of
plasma science and applications. Methods for non-invasive in situ measurements
of plasma fields on sub-millimeter length scales with high sensitivity over a
large field range remain an outstanding challenge. Here, we introduce and
demonstrate a new method for plasma electric field measurement that employs
electromagnetically induced transparency as a high-resolution quantum-optical
probe for the Stark energy level shifts of plasma-embedded Rydberg atoms, which
serve as highly-sensitive field sensors with a large dynamic range. The method
is applied in diagnostics of plasmas photo-excited out of a cesium vapor. The
plasma electric fields are extracted from spatially-resolved measurements of
field-induced shape changes and shifts of Rydberg resonances in rubidium tracer
atoms. Measurement capabilities over a range of plasma densities and
temperatures are exploited to characterize plasmas in applied magnetic fields
and to image electric-field distributions in cyclotron-heated plasmas.
"
"  The problem of determining those multiplets of forces, or sets of force
multiplets, acting at a set of points, such that there exists a truss
structure, or wire web, that can support these force multiplets with all the
elements of the truss or wire web being under tension, is considered. The
two-dimensional problem where the points are at the vertices of a convex
polygon is essentially solved: each multiplet of forces must be such that the
net anticlockwise torque around any vertex of the forces summed over any number
of consecutive points clockwise past the vertex must be non-negative; and one
can find a truss structure that supports under tension, and only supports,
those force multiplets in a convex polyhedron of force multiplets that is
generated by a finite number of force multiplets each satisfying the torque
condition. Progress is also made on the problem where only a subset of the
points are at the vertices of a convex polygon, and the other points are
inside. In particular, in the case where only one point is inside, an explicit
procedure is described for constructing a suitable truss, if one exists. An
alternative recipe to that provided by Guevara-Vasquez, Milton, and Onofrei
(2011), based on earlier work of Camar Eddine and Seppecher (2003), is given
for constructing a truss structure, with elements under either compression or
tension, that supports an arbitrary collection of balanced forces at the
vertices of a convex polygon. Finally some constraints are given on the forces
that a three-dimension truss, or wire web, under tension must satisfy.
"
"  We present a measurement of baryon acoustic oscillations (BAO) in the
cross-correlation of quasars with the Ly$\alpha$-forest flux-transmission at a
mean redshift $z=2.40$. The measurement uses the complete SDSS-III data sample:
168,889 forests and 234,367 quasars from the SDSS Data Release DR12. In
addition to the statistical improvement on our previous study using DR11, we
have implemented numerous improvements at the analysis level allowing a more
accurate measurement of this cross-correlation. We also developed the first
simulations of the cross-correlation allowing us to test different aspects of
our data analysis and to search for potential systematic errors in the
determination of the BAO peak position. We measure the two ratios
$D_{H}(z=2.40)/r_{d} = 9.01 \pm 0.36$ and $D_{M}(z=2.40)/r_{d} = 35.7 \pm 1.7$,
where the errors include marginalization over the non-linear velocity of
quasars and the metal - quasar cross-correlation contribution, among other
effects. These results are within $1.8\sigma$ of the prediction of the
flat-$\Lambda$CDM model describing the observed CMB anisotropies. We combine
this study with the Ly$\alpha$-forest auto-correlation function
[2017A&A...603A..12B], yielding $D_{H}(z=2.40)/r_{d} = 8.94 \pm 0.22$ and
$D_{M}(z=2.40)/r_{d} = 36.6 \pm 1.2$, within $2.3\sigma$ of the same
flat-$\Lambda$CDM model.
"
"  We examine the 2008-2016 $\gamma$-ray and optical light curves of three
bright BL Lac objects, 0716+714, MRK 421, BL Lac, which exhibit large
structured variability. We searched for periodicities by using a fully Bayesian
approach. For two out of three sources investigated no significant periodic
variability was found. In the case of BL Lac we detected a periodicity of ~ 680
days. Although the signal related to this is modest, the coincidence of the
periods in both gamma and optical bands is indicative of a physical relevance.
Considering previous literature results, possibly related $\gamma$-ray and
optical periodicities of about one year time scale are proposed in 4 bright
$\gamma$-ray blazars out of the 10 examined in detail. Comparing with results
from periodicity search of optical archives of quasars, the presence of
quasi-periodicities in blazars might be more frequent by a large factor. This
suggests the intriguing possibility that the basic conditions for their
observability are related to the relativistic jet in the observer direction,
but the overall picture remains uncertain.
"
"  The ground state of the diatomic molecules in nature is inevitably bonding,
and its first excited state is antibonding. We demonstrate theoretically that,
for a pair of distant adatoms placed buried in three-dimensional-Dirac
semimetals, this natural order of the states can be reversed and an antibonding
ground state occurs at the lowest energy of the so-called bound states in the
continuum. We propose an experimental protocol with the use of a scanning
tunneling microscope tip to visualize the topographic map of the local density
of states on the surface of the system to reveal the emerging physics.
"
"  T2K (Tokai-to-Kamioka) is a long-baseline neutrino experiment in Japan
designed to study various parameters of neutrino oscillations. A near detector
complex (ND280) is located 280~m downstream of the production target and
measures neutrino beam parameters before any oscillations occur. ND280's
measurements are used to predict the number and spectra of neutrinos in the
Super-Kamiokande detector at the distance of 295~km. The difference in the
target material between the far (water) and near (scintillator, hydrocarbon)
detectors leads to the main non-cancelling systematic uncertainty for the
oscillation analysis. In order to reduce this uncertainty a new
WAter-Grid-And-SCintillator detector (WAGASCI) has been developed. A magnetized
iron neutrino detector (Baby MIND) will be used to measure momentum and charge
identification of the outgoing muons from charged current interactions. The
Baby MIND modules are composed of magnetized iron plates and long plastic
scintillator bars read out at the both ends with wavelength shifting fibers and
silicon photomultipliers. The front-end electronics board has been developed to
perform the readout and digitization of the signals from the scintillator bars.
Detector elements were tested with cosmic rays and in the PS beam at CERN. The
obtained results are presented in this paper.
"
"  We present the first simultaneous photometric and spectroscopic investigation
of a large set of RR Lyrae variables in a globular cluster. The radial-velocity
data presented comprise the largest sample of RVs of RR Lyrae stars ever
obtained. The target is M3; $BVI_{\mathrm{C}}$ time-series of 111 and $b$ flux
data of further 64 RRab stars, and RV data of 79 RR Lyrae stars are published.
Blazhko modulation of the light curves of 47 percent of the RRab stars are
detected. The mean value of the center-of-mass velocities of RR Lyrae stars is
$-146.8$ km s$^{-1}$ with 4.52 km s$^{-1}$ standard deviation, which is in good
agreement with the results obtained for the red giants of the cluster. The
${\Phi_{21}}^{\mathrm RV}$ phase difference of the RV curves of RRab stars is
found to be uniformly constant both for the M3 and for Galactic field RRab
stars; no period or metallicity dependence of the ${\Phi_{21}}^{\mathrm RV}$ is
detected. The Baade-Wesselink distances of 26 non-Blazhko variables with the
best phase-coverage radial-velocity curves are determined; the corresponding
distance of the cluster, $10480\pm210$ pc, agrees with the previous literature
information. A quadratic formula for the $A_{\mathrm{puls}}-A_V$ relation of
RRab stars is given, which is valid for both OoI and OoII variables. We also
show that the $(V-I)_0$ of RRab stars measured at light minimum is period
dependent, there is at least 0.1 mag difference between the colours at minimum
light of the shortest- and longest-period variables.
"
"  We realize scattering states in a lossy and chaotic two-dimensional microwave
cavity which follow bundles of classical particle trajectories. To generate
such particlelike scattering states we measure the system's transmission matrix
and apply an adapted Wigner-Smith time-delay formalism to it. The necessary
shaping of the incident wave is achieved in situ using phase and amplitude
regulated microwave antennas. Our experimental findings pave the way for
establishing spatially confined communication channels that avoid possible
intruders or obstacles in wave-based communication systems.
"
"  A theoretical study of the current-driven dynamics of magnetic skyrmions in
disordered perpendicularly-magnetized ultrathin films is presented. The
disorder is simulated as a granular structure in which the local anisotropy
varies randomly from grain to grain. The skyrmion velocity is computed for
different disorder parameters and ensembles. Similar behavior is seen for
spin-torques due to in-plane currents and the spin Hall effect, where a pinning
regime can be identified at low currents with a transition towards the
disorder-free case at higher currents, similar to domain wall motion in
disordered films. Moreover, a current-dependent skyrmion Hall effect and
fluctuations in the core radius are found, which result from the interaction
with the pinning potential.
"
"  A conformal coating technique with nanocarbon was developed to enhance the
surface properties of alumina nanoparticles for bio-applications. The
ultra-thin carbon layer induces new surface properties such as water
dispersion, cytocompatibility and tuneable surface chemistry, while maintaining
the optical properties of the core particle. The possibility of using these
particles as agents for DNA sensing was demonstrated in a competitive assay.
Additionally, the inherent fluorescence of the core alumina particles provided
a unique platform for localization and monitoring of living organisms, allowing
simultaneous cell monitoring and intra-cellular sensing. Nanoparticles were
able to carry genes to the cells and release them in an environment where
specific biomarkers were present.
"
"  Context. The 4th release of the SDSS Moving Object Catalog (SDSSMOC) is
presently the largest photometric dataset of asteroids. Up to this point, the
release of large asteroid datasets has always been followed by a redefinition
of asteroid taxonomy. In the years that followed the release of the first
SDSSMOC, several classification schemes using its data were proposed, all using
the taxonomic classes from previous taxonomies. However, no successful attempt
has been made to derive a new taxonomic system directly from the SDSS dataset.
Aims. The scope of the work is to propose a different interpretation scheme for
gauging u0g0r0i0z0 asteroid observations based on the continuity of spectral
features. The scheme is integrated into previous taxonomic labeling, but is not
dependent on them. Methods. We analyzed the behavior of asteroid sampling
through principal components analysis to understand the role of uncertainties
in the SDSSMOC. We identified that asteroids in this space follow two separate
linear trends using reflectances in the visible, which is characteristic of
their spectrophotometric features. Results. Introducing taxonomic classes, we
are able to interpret both trends as representative of featured and featureless
spectra. The evolution within the trend is connected mainly to the band depth
for featured asteroids and to the spectral slope for featureless ones. We
defined a different taxonomic system that allowed us to only classify asteroids
by two labels. Conclusions. We have classified 69% of all SDSSMOC sample, which
is a robustness higher than reached by previous SDSS classifications.
Furthermore, as an example, we present the behavior of asteroid (5129) Groom,
whose taxonomic labeling changes according to one of the trends owing to phase
reddening. Now, such behavior can be characterized by the variation of one
single parameter, its position in the trend.
"
"  The critical behavior of the random transverse-field Ising model in finite
dimensional lattices is governed by infinite disorder fixed points, several
properties of which have already been calculated by the use of the strong
disorder renormalization group (SDRG) method. Here we extend these studies and
calculate the connected transverse-spin correlation function by a numerical
implementation of the SDRG method in $d=1,2$ and $3$ dimensions. At the
critical point an algebraic decay of the form $\sim r^{-\eta_t}$ is found, with
a decay exponent being approximately $\eta_t \approx 2+2d$. In $d=1$ the
results are related to dimer-dimer correlations in the random AF XX-chain and
have been tested by numerical calculations using free-fermionic techniques.
"
"  We study the effects of quantum fluctuations on the dynamical generation of a
gap and on the evolution of the spin-wave spectra of a frustrated magnet on a
triangular lattice with bond-dependent Ising couplings, analog of the Kitaev
honeycomb model. The quantum fluctuations lift the subextensive degeneracy of
the classical ground-state manifold by a quantum order-by-disorder mechanism.
Nearest-neighbor chains remain decoupled and the surviving discrete degeneracy
of the ground state is protected by a hidden model symmetry. We show how the
four-spin interaction, emergent from the fluctuations, generates a spin gap
shifting the nodal lines of the linear spin-wave spectrum to finite energies.
"
"  Fundamental atomic parameters, such as oscillator strengths, play a key role
in modelling and understanding the chemical composition of stars in the
universe. Despite the significant work underway to produce these parameters for
many astrophysically important ions, uncertainties in these parameters remain
large and can propagate throughout the entire field of astronomy. The Belgian
repository of fundamental atomic data and stellar spectra (BRASS) aims to
provide the largest systematic and homogeneous quality assessment of atomic
data to date in terms of wavelength, atomic and stellar parameter coverage. To
prepare for it, we first compiled multiple literature occurrences of many
individual atomic transitions, from several atomic databases of astrophysical
interest, and assessed their agreement. Several atomic repositories were
searched and their data retrieved and formatted in a consistent manner. Data
entries from all repositories were cross-matched against our initial BRASS
atomic line list to find multiple occurrences of the same transition. Where
possible we used a non-parametric cross-match depending only on electronic
configurations and total angular momentum values. We also checked for duplicate
entries of the same physical transition, within each retrieved repository,
using the non-parametric cross-match. We report the cross-matched transitions
for each repository and compare their fundamental atomic parameters. We find
differences in log(gf) values of up to 2 dex or more. We also find and report
that ~2% of our line list and Vienna Atomic Line Database retrievals are
composed of duplicate transitions. Finally we provide a number of examples of
atomic spectral lines with different log(gf) values, and discuss the impact of
these uncertain log(gf) values on quantitative spectroscopy. All cross-matched
atomic data and duplicate transitions are available to download at
brass.sdf.org.
"
"  Hamiltonian Truncation (a.k.a. Truncated Spectrum Approach) is an efficient
numerical technique to solve strongly coupled QFTs in d=2 spacetime dimensions.
Further theoretical developments are needed to increase its accuracy and the
range of applicability. With this goal in mind, here we present a new variant
of Hamiltonian Truncation which exhibits smaller dependence on the UV cutoff
than other existing implementations, and yields more accurate spectra. The key
idea for achieving this consists in integrating out exactly a certain class of
high energy states, which corresponds to performing renormalization at the
cubic order in the interaction strength. We test the new method on the strongly
coupled two-dimensional quartic scalar theory. Our work will also be useful for
the future goal of extending Hamiltonian Truncation to higher dimensions d >=
3.
"
"  Intensive studies for more than three decades have elucidated multiple
superconducting phases and odd-parity Cooper pairs in a heavy fermion
superconductor UPt$_3$. We identify a time-reversal invariant superconducting
phase of UPt$_3$ as a recently proposed topological nonsymmorphic
superconductivity. Combining the band structure of UPt$_3$, order parameter of
$E_{\rm 2u}$ representation allowed by $P6_3/mmc$ space group symmetry, and
topological classification by $K$-theory, we demonstrate the nontrivial
$Z_2$-invariant of three-dimensional DIII class enriched by glide symmetry.
Correspondingly, double Majorana cone surface states appear at the surface
Brillouin zone boundary. Furthermore, we show a variety of surface states and
clarify the topological protection by crystal symmetry. Majorana arcs
corresponding to tunable Weyl points appear in the time-reversal symmetry
broken B-phase. Majorana cone protected by mirror Chern number and Majorana
flat band by glide-winding number are also revealed.
"
"  By investigating information flow between a general parity-time (PT)
-symmetric non-Hermitian system and an environment, we find that the complete
information retrieval from the environment can be achieved in the PT-unbroken
phase, whereas no information can be retrieved in the PT-broken phase. The
PT-transition point thus marks the reversible-irreversible criticality of
information flow, around which many physical quantities such as the recurrence
time and the distinguishability between quantum states exhibit power-law
behavior. Moreover, by embedding a PT-symmetric system into a larger Hilbert
space so that the entire system obeys unitary dynamics, we reveal that behind
the information retrieval lies a hidden entangled partner protected by PT
symmetry. Possible experimental situations are also discussed.
"
"  Random network models play a prominent role in modeling, analyzing and
understanding complex phenomena on real-life networks. However, a key property
of networks is often neglected: many real-world networks exhibit spatial
structure, the tendency of a node to select neighbors with a probability
depending on physical distance. Here, we introduce a class of random spatial
networks (RSNs) which generalizes many existing random network models but adds
spatial structure. In these networks, nodes are placed randomly in space and
joined in edges with a probability depending on their distance and their
individual expected degrees, in a manner that crucially remains analytically
tractable. We use this network class to propose a new generalization of
small-world networks, where the average shortest path lengths in the graph are
small, as in classical Watts-Strogatz small-world networks, but with close
spatial proximity of nodes that are neighbors in the network playing the role
of large clustering. Small-world effects are demonstrated on these spatial
small-world networks without clustering. We are able to derive partial
integro-differential equations governing susceptible-infectious-recovered
disease spreading through an RSN, and we demonstrate the existence of traveling
wave solutions. If the distance kernel governing edge placement decays slower
than exponential, the population-scale dynamics are dominated by long-range
hops followed by local spread of traveling waves. This provides a theoretical
modeling framework for recent observations of how epidemics like Ebola evolve
in modern connected societies, with long-range connections seeding new focal
points from which the epidemic locally spreads in a wavelike manner.
"
"  Large-scale dipolar surface magnetic fields have been detected in a fraction
of OB stars, however only few stellar evolution models of massive stars have
considered the impact of these fossil fields. We are performing 1D
hydrodynamical model calculations taking into account evolutionary consequences
of the magnetospheric-wind interactions in a simplified parametric way. Two
effects are considered: i) the global mass-loss rates are reduced due to
mass-loss quenching, and ii) the surface angular momentum loss is enhanced due
to magnetic braking. As a result of the magnetic mass-loss quenching, the mass
of magnetic massive stars remains close to their initial masses. Thus magnetic
massive stars - even at Galactic metallicity - have the potential to be
progenitors of `heavy' stellar mass black holes. Similarly, at Galactic
metallicity, the formation of pair instability supernovae is plausible with a
magnetic progenitor.
"
"  The focusing NLS equation is the simplest universal model describing the
modulation instability (MI) of quasi monochromatic waves in weakly nonlinear
media, considered the main physical mechanism for the appearance of rogue
(anomalous) waves (RWs) in Nature. In this paper we study, using the finite gap
method, the NLS Cauchy problem for periodic initial perturbations of the
unstable background solution of NLS exciting just one of the unstable modes. We
distinguish two cases. In the case in which only the corresponding unstable gap
is theoretically open, the solution describes an exact deterministic alternate
recurrence of linear and nonlinear stages of MI, and the nonlinear RW stages
are described by the 1-breather Akhmediev solution, whose parameters, different
at each RW appearance, are always given in terms of the initial data through
elementary functions. If the number of unstable modes is >1, this uniform in t
dynamics is sensibly affected by perturbations due to numerics and/or real
experiments, provoking O(1) corrections to the result. In the second case in
which more than one unstable gap is open, a detailed investigation of all these
gaps is necessary to get a uniform in $t$ dynamics, and this study is postponed
to a subsequent paper. It is however possible to obtain the elementary
description of the first nonlinear stage of MI, given again by the Akhmediev
1-breather solution, and how perturbations due to numerics and/or real
experiments can affect this result.
"
"  We present the first discoveries from a survey of $z\gtrsim6$ quasars using
imaging data from the DECam Legacy Survey (DECaLS) in the optical, the UKIRT
Deep Infrared Sky Survey (UKIDSS) and a preliminary version of the UKIRT
Hemisphere Survey (UHS) in the near-IR, and ALLWISE in the mid-IR. DECaLS will
image 9000 deg$^2$ of sky down to $z_{\rm AB}\sim23.0$, and UKIDSS and UHS,
which will map the northern sky at $0<DEC<+60^{\circ}$, reaching $J_{\rm
VEGA}\sim19.6$ (5-$\sigma$). The combination of these datasets allows us to
discover quasars at redshift $z\gtrsim7$ and to conduct a complete census of
the faint quasar population at $z\gtrsim6$. In this paper, we report on the
selection method of our search, and on the initial discoveries of two new,
faint $z\gtrsim6$ quasars and one new $z=6.63$ quasar in our pilot
spectroscopic observations. The two new $z\sim6$ quasars are at $z=6.07$ and
$z=6.17$ with absolute magnitudes at rest-frame wavelength 1450 \AA\ being
$M_{1450}=-25.83$ and $M_{1450}=-25.76$, respectively. These discoveries
suggest that we can find quasars close to or fainter than the break magnitude
of the Quasar Luminosity Function (QLF) at $z\gtrsim6$. The new $z=6.63$ quasar
has an absolute magnitude of $M_{1450}=-25.95$. This demonstrates the potential
of using the combined DECaLS and UKIDSS/UHS datasets to find $z\gtrsim7$
quasars. Extrapolating from previous QLF measurements, we predict that these
combined datasets will yield $\sim200$ $z\sim6$ quasars to $z_{\rm AB} < 21.5$,
$\sim1{,}000$ $z\sim6$ quasars to $z_{\rm AB}<23$, and $\sim 30$ quasars at
$z>6.5$ to $J_{\rm VEGA}<19.5$.
"
"  One of the most puzzling features of high-temperature cuprate superconductors
is the pseudogap state, which appears above the temperature at which
superconductivity is destroyed. There remain fundamental questions regarding
its nature and its relation to superconductivity. But to address these
questions, we must first determine whether the pseudogap and superconducting
states share a common property: particle-hole symmetry. We introduce a new
technique to test particle-hole symmetry by using laser pulses to manipulate
and measure the chemical potential on picosecond time scales. The results
strongly suggest that the asymmetry in the density of states is inverted in the
pseudogap state, implying a particle-hole asymmetric gap. Independent of
interpretation, these results can test theoretical predictions of the density
of states in cuprates.
"
"  Thermal noise is expected to be one of the noise sources limiting the
astrophysical reach of Advanced LIGO (once commissioning is complete) and
third-generation detectors. Adopting crystalline materials for thin, reflecting
mirror coatings, rather than the amorphous coatings used in current-generation
detectors, could potentially reduce thermal noise. Understanding and reducing
thermal noise requires accurate theoretical models, but modeling thermal noise
analytically is especially challenging with crystalline materials. Thermal
noise models typically rely on the fluctuation-dissipation theorem, which
relates the power spectral density of the thermal noise to an auxiliary elastic
problem. In this paper, we present results from a new, open-source tool that
numerically solves the auxiliary elastic problem to compute the Brownian
thermal noise for both amorphous and crystalline coatings. We employ
open-source frameworks to solve the auxiliary elastic problem using a
finite-element method, adaptive mesh refinement, and parallel processing that
enables us to use high resolutions capable of resolving the thin reflective
coating. We compare with approximate analytic solutions for amorphous
materials, and we verify that our solutions scale as expected. Finally, we
model the crystalline coating thermal noise in an experiment reported by Cole
and collaborators (2013), comparing our results to a simpler numerical
calculation that treats the coating as an ""effectively amorphous"" material. We
find that treating the coating as a cubic crystal instead of as an effectively
amorphous material increases the thermal noise by about 3%. Our results are a
step toward better understanding and reducing thermal noise to increase the
reach of future gravitational-wave detectors. (Abstract abbreviated.)
"
"  We investigate how star formation efficiency can be significantly decreased
by the removal of a molecular cloud's envelope by feedback from an external
source. Feedback from star formation has difficulties halting the process in
dense gas but can easily remove the less dense and warmer envelopes where star
formation does not occur. However, the envelopes can play an important role
keeping their host clouds bound by deepening the gravitational potential and
providing a constraining pressure boundary. We use numerical simulations to
show that removal of the cloud envelopes results in all cases in a fall in the
star formation efficiency (SFE). At 1.38 free-fall times our 4 pc cloud
simulation experienced a drop in the SFE from 16 to six percent, while our 5 pc
cloud fell from 27 to 16 per cent. At the same time, our 3 pc cloud (the least
bound) fell from an SFE of 5.67 per cent to zero when the envelope was lost.
The star formation efficiency per free-fall time varied from zero to $\approx$
0.25 according to $\alpha$, defined to be the ratio of the kinetic plus thermal
to gravitational energy, and irrespective of the absolute star forming mass
available. Furthermore the fall in SFE associated with the loss of the envelope
is found to even occur at later times. We conclude that the SFE will always
fall should a star forming cloud lose its envelope due to stellar feedback,
with less bound clouds suffering the greatest decrease.
"
"  We present an experimental study on the non-equilibrium tunnel dynamics of
two coupled one-dimensional Bose-Einstein quasi-condensates deep in the
Josephson regime. Josephson oscillations are initiated by splitting a single
one-dimensional condensate and imprinting a relative phase between the
superfluids. Regardless of the initial state and experimental parameters, the
dynamics of the relative phase and atom number imbalance shows a relaxation to
a phase-locked steady state. The latter is characterized by a high phase
coherence and reduced fluctuations with respect to the initial state. We
propose an empirical model based on the analogy with the anharmonic oscillator
to describe the effect of various experimental parameters. A microscopic theory
compatible with our observations is still missing.
"
"  We have studied the peculiarities of selective reflection from Rb vapor cell
with thickness $L <$ 70 nm, which is over an order of magnitude smaller than
the resonant wavelength for Rb atomic D$_1$ line $\lambda$ = 795 nm. A huge
($\approx$ 240 MHz) red shift and spectral broadening of reflection signal is
recorded for $L =$ 40 nm caused by the atom-surface interaction. Also
completely frequency resolved hyperfine Paschen-Back splitting of atomic
transitions to four components for $^{87}$Rb and six components for $^{85}$Rb
is recorded in strong magnetic field ($B >$ 2 kG).
"
"  The interaction of (CH3-C5H4)Pt(CH3)3
((methylcyclopentadienyl)trimethylplatinum)) molecules on fully and partially
hydroxylated SiO2 surfaces, as well as the dynamics of this interaction were
investigated using density functional theory (DFT) and finite temperature
DFT-based molecular dynamics simulations. Fully and partially hydroxylated
surfaces represent substrates before and after electron beam treatment and this
study examines the role of electron beam pretreatment on the substrates in the
initial stages of precursor dissociation and formation of Pt deposits. Our
simulations show that on fully hydroxylated surfaces or untreated surfaces, the
precursor molecules remain inactivated while we observe fragmentation of
(CH3-C5H4)Pt(CH3)3 on partially hydroxylated surfaces. The behavior of
precursor molecules on the partially hydroxylated surfaces has been found to
depend on the initial orientation of the molecule and the distribution of
surface active sites. Based on the observations from the simulations and
available experiments, we discuss possible dissociation channels of the
precursor.
"
"  We examine the H$\beta$ Lick index in a sample of $\sim 24000$ massive ($\rm
log(M/M_{\odot})>10.75$) and passive early-type galaxies extracted from SDSS at
z<0.3, in order to assess the reliability of this index to constrain the epoch
of formation and age evolution of these systems. We further investigate the
possibility of exploiting this index as ""cosmic chronometer"", i.e. to derive
the Hubble parameter from its differential evolution with redshift, hence
constraining cosmological models independently of other probes. We find that
the H$\beta$ strength increases with redshift as expected in passive evolution
models, and shows at each redshift weaker values in more massive galaxies.
However, a detailed comparison of the observed index with the predictions of
stellar population synthesis models highlights a significant tension, with the
observed index being systematically lower than expected. By analyzing the
stacked spectra, we find a weak [NII]$\lambda6584$ emission line (not
detectable in the single spectra) which anti-correlates with the mass, that can
be interpreted as a hint of the presence of ionized gas. We estimated the
correction of the H$\beta$ index by the residual emission component exploiting
different approaches, but find it very uncertain and model-dependent. We
conclude that, while the qualitative trends of the observed H$\beta$-z
relations are consistent with the expected passive and downsizing scenario, the
possible presence of ionized gas even in the most massive and passive galaxies
prevents to use this index for a quantitative estimate of the age evolution and
for cosmological applications.
"
"  We present a collective coordinate approach to study the collective behaviour
of a finite ensemble of $N$ stochastic Kuramoto oscillators using two degrees
of freedom; one describing the shape dynamics of the oscillators and one
describing their mean phase. Contrary to the thermodynamic limit $N\to\infty$
in which the mean phase of the cluster of globally synchronized oscillators is
constant in time, the mean phase of a finite-size cluster experiences Brownian
diffusion with a variance proportional to $1/N$. This finite-size effect is
quantitatively well captured by our collective coordinate approach.
"
"  We consider light-induced binding and motion of dielectric microparticles in
an optical waveguide that gives rise to a back-action effect such as light
transmission oscillating with time. Modeling the particles by dielectric slabs
allows us to solve the problem analytically and obtain a rich variety of
dynamical regimes both for Newtonian and damped motion. This variety is clearly
reflected in temporal oscillations of the light transmission. The
characteristic frequencies of the oscillations are within the ultrasound range
of the order of $10^{5}$ Hz for micron size particles and injected power of the
order of 100 mW. In addition, we consider driven by propagating light dynamics
of a dielectric particle inside a Fabry-Perot resonator. These phenomena pave a
way for optical driving and monitoring of motion of particles in waveguides and
resonators.
"
"  A magnetic adatom chain, proximity coupled to a conventional superconductor
with spin-orbit coupling, exhibits locally an odd-parity, spin-triplet pairing
amplitude. We show that the singlet-triplet junction, thus formed, leads to a
net spin accumulation in the near vicinity of the chain. The accumulated spins
are polarized along the direction of the local $\mathbf{d}$-vector for triplet
pairing and generate an enhanced persistent current flowing around the chain.
The spin polarization and the ""supercurrent"" reverse their directions beyond a
critical exchange coupling strength at which the singlet superconducting order
changes its sign on the chain. The current is strongly enhanced in the
topological superconducting regime where Majorana bound states appear at the
chain ends. The current and the spin profile offer alternative routes to
characterize the topological superconducting state in adatom chains and
islands.
"
"  Computational Fluid Dynamics (CFD) is a hugely important subject with
applications in almost every engineering field, however, fluid simulations are
extremely computationally and memory demanding. Towards this end, we present
Lat-Net, a method for compressing both the computation time and memory usage of
Lattice Boltzmann flow simulations using deep neural networks. Lat-Net employs
convolutional autoencoders and residual connections in a fully differentiable
scheme to compress the state size of a simulation and learn the dynamics on
this compressed form. The result is a computationally and memory efficient
neural network that can be iterated and queried to reproduce a fluid
simulation. We show that once Lat-Net is trained, it can generalize to large
grid sizes and complex geometries while maintaining accuracy. We also show that
Lat-Net is a general method for compressing other Lattice Boltzmann based
simulations such as Electromagnetism.
"
"  We study existence and properties of one-dimensional edge domain walls in
ultrathin ferromagnetic films with uniaxial in-plane magnetic anisotropy. In
these materials, the magnetization vector is constrained to lie entirely in the
film plane, with the preferred directions dictated by the magnetocrystalline
easy axis. We consider magnetization profiles in the vicinity of a straight
film edge oriented at an arbitrary angle with respect to the easy axis. To
minimize the micromagnetic energy, these profiles form transition layers in
which the magnetization vector rotates away from the direction of the easy axis
to align with the film edge. We prove existence of edge domain walls as
minimizers of the appropriate one-dimensional micromagnetic energy functional
and show that they are classical solutions of the associated Euler-Lagrange
equation with Dirichlet boundary condition at the edge. We also perform a
numerical study of these one-dimensional domain walls and uncover further
properties of these domain wall profiles.
"
"  We present the properties of a magneto-optical trap (MOT) of CaF molecules.
We study the process of loading the MOT from a decelerated buffer-gas-cooled
beam, and how best to slow this molecular beam in order to capture the most
molecules. We determine how the number of molecules, the photon scattering
rate, the oscillation frequency, damping constant, temperature, cloud size and
lifetime depend on the key parameters of the MOT, especially the intensity and
detuning of the main cooling laser. We compare our results to analytical and
numerical models, to the properties of standard atomic MOTs, and to MOTs of SrF
molecules. We load up to $2 \times 10^4$ molecules, and measure a maximum
scattering rate of $2.5 \times 10^6$ s$^{-1}$ per molecule, a maximum
oscillation frequency of 100 Hz, a maximum damping constant of 500 s$^{-1}$,
and a minimum MOT rms radius of 1.5 mm. A minimum temperature of 730 $\mu$K is
obtained by ramping down the laser intensity to low values. The lifetime,
typically about 100 ms, is consistent with a leak out of the cooling cycle with
a branching ratio of about $6 \times 10^{-6}$. The MOT has a capture velocity
of about 11 m/s.
"
"  In this paper we construct a non-autonomous version of the Hietarinta
equation [Hietarinta J., J. Phys. A: Math. Gen. 37 (2004), L67-L73] and study
its integrability properties. We show that this equation possess linear growth
of the degrees of iterates, generalized symmetries depending on arbitrary
functions, and that it is Darboux integrable. We use the first integrals to
provide a general solution of this equation. In particular we show that this
equation is a sub-case of the non-autonomous $Q_{\rm V}$ equation, and we
provide a non-autonomous Möbius transformation to another equation found in
[Hietarinta J., J. Nonlinear Math. Phys. 12 (2005), suppl. 2, 223-230] and
appearing also in Boll's classification [Boll R., Ph.D. Thesis, Technische
Universität Berlin, 2012].
"
"  Fractional quantum Hall-superconductor heterostructures may provide a
platform towards non-abelian topological modes beyond Majoranas. However their
quantitative theoretical study remains extremely challenging. We propose and
implement a numerical setup for studying edge states of fractional quantum Hall
droplets with a superconducting instability. The fully gapped edges carry a
topological degree of freedom that can encode quantum information protected
against local perturbations. We simulate such a system numerically using exact
diagonalization by restricting the calculation to the quasihole-subspace of a
(time-reversal symmetric) bilayer fractional quantum Hall system of Laughlin
$\nu=1/3$ states. We show that the edge ground states are permuted by
spin-dependent flux insertion and demonstrate their fractional $6\pi$ Josephson
effect, evidencing their topological nature and the Cooper pairing of
fractionalized quasiparticles.
"
"  The discovery of topological insulators has reformed modern materials
science, promising to be a platform for tabletop relativistic physics,
electronic transport without scattering, and stable quantum computation.
Topological invariants are used to label distinct types of topological
insulators. But it is not generally known how many or which invariants can
exist in any given crystalline material. Using a new and efficient counting
algorithm, we study the topological invariants that arise in time-reversal
symmetric crystals. This results in a unified picture that explains the
relations between all known topological invariants in these systems. It also
predicts new topological phases and one entirely new topological invariant. We
present explicitly the classification of all two-dimensional crystalline
fermionic materials, and give a straightforward procedure for finding the
analogous result in any three-dimensional structure. Our study represents a
single, intuitive physical picture applicable to all topological invariants in
real materials, with crystal symmetries.
"
"  The formation of large voids in the Cosmic Web from the initial adiabatic
cosmological perturbations of space-time metric, density and velocity of matter
is investigated in cosmological model with the dynamical dark energy
accelerating expansion of the Universe. It is shown that the negative density
perturbations with the initial radius of about 50 Mpc in comoving to the
cosmological background coordinates and the amplitude corresponding to the
r.m.s. temperature fluctuations of the cosmic microwave background lead to the
formation of voids with the density contrast up to $-$0.9, maximal peculiar
velocity about 400 km/s and the radius close to the initial one. An important
feature of voids formation from the analyzed initial amplitudes and profiles is
establishing the surrounding overdensity shell. We have shown that the ratio of
the peculiar velocity in units of the Hubble flow to the density contrast in
the central part of a void does not depend or weakly depends on the distance
from the center of the void. It is also shown that this ratio is sensitive to
the values of dark energy parameters and can be used to find them based on the
observational data on mass density and peculiar velocities of galaxies in the
voids.
"
"  We present a first internal delensing of CMB maps, both in temperature and
polarization, using the public foreground-cleaned (SMICA) Planck 2015 maps.
After forming quadratic estimates of the lensing potential, we use the
corresponding displacement field to undo the lensing on the same data. We build
differences of the delensed spectra to the original data spectra specifically
to look for delensing signatures. After taking into account reconstruction
noise biases in the delensed spectra, we find an expected sharpening of the
power spectrum acoustic peaks with a delensing efficiency of $29\,\%$ ($TT$)
$25\,\%$ ($TE$) and $22\,\%$ ($EE$). The detection significance of the
delensing effects is very high in all spectra: $12\,\sigma$ in $EE$
polarization; $18\,\sigma$ in $TE$; and $20\,\sigma$ in $TT$. The null
hypothesis of no lensing in the maps is rejected at $26\,\sigma$. While direct
detection of the power in lensing $B$-modes themselves is not possible at high
significance at Planck noise levels, we do detect (at $4.5\,\sigma$ under the
null hypothesis) delensing effects in the $B$-mode map, with $7\,\%$ reduction
in lensing power. Our results provide a first demonstration of polarization
delensing, and generally of internal CMB delensing, and stand in agreement with
the baseline $\Lambda$CDM Planck 2015 cosmology expectations.
"
"  The geodetic VLBI technique is capable of measuring the Sun's gravity light
deflection from distant radio sources around the whole sky. This light
deflection is equivalent to the conventional gravitational delay used for the
reduction of geodetic VLBI data. While numerous tests based on a global set of
VLBI data have shown that the parameter 'gamma' of the post-Newtonian
approximation is equal to unity with a precision of about 0.02 percent, more
detailed analysis reveals some systematic deviations depending on the angular
elongation from the Sun. In this paper a limited set of VLBI observations near
the Sun were adjusted to obtain the estimate of the parameter 'gamma' free of
the elongation angle impact. The parameter 'gamma' is still found to be close
to unity with precision of 0.06 percent, two subsets of VLBI data measured at
short and long baselines produce some statistical inconsistency.
"
"  An unbiased estimator for the ellipticity of an object in a noisy image is
given in terms of the image moments. Three assumptions are made: i) the pixel
noise is normally distributed, although with arbitrary covariance matrix, ii)
the image moments are taken about a fixed centre, and iii) the point-spread
function is known. The relevant combinations of image moments are then jointly
normal and their covariance matrix can be computed. A particular estimator for
the ratio of the means of jointly normal variates is constructed and used to
provide the unbiased estimator for the ellipticity. Furthermore, an unbiased
estimate of the covariance of the new estimator is also given.
"
"  The dust-forming nova V2676 Oph is unique in that it was the first nova to
provide evidence of C_2 and CN molecules during its near-maximum phase and
evidence of CO molecules during its early decline phase. Observations of this
nova have revealed the slow evolution of its lightcurves and have also shown
low isotopic ratios of carbon (12C/13C) and nitrogen (14N/15N) in its nova
envelope. These behaviors indicate that the white dwarf (WD) star hosting V2676
Oph is a CO-rich WD rather than an ONe-rich WD (typically larger in mass than
the former). We performed mid-infrared spectroscopic and photometric
observations of V2676 Oph in 2013 and 2014 (respectively 452 and 782 days after
its discovery). No significant [Ne II] emission at 12.8 micron was detected at
either epoch. These provided evidence for a CO-rich WD star hosting V2676 Oph.
Both carbon-rich and oxygen-rich grains were detected in addition to an
unidentified infrared feature at 11.4 micron originating from polycyclic
aromatic hydrocarbon molecules or hydrogenated amorphous carbon grains in the
envelope of V2676 Oph.
"
"  We investigate the low-energy scaling behavior of an interacting 3D Weyl
semimetal in the presence of disorder. In order to achieve a renormalization
group analysis of the theory, we focus on the effects of a
short-ranged-correlated disorder potential, checking nevertheless that this
choice is not essential to locate the different phases of the Weyl semimetal.
We show that there is a line of fixed-points in the renormalization group flow
of the interacting theory, corresponding to the disorder-driven transition to a
diffusive metal phase. Along that boundary, the critical disorder strength
undergoes a strong increase with respect to the noninteracting theory, as a
consequence of the unconventional screening of the Coulomb and disorder-induced
interactions. A complementary resolution of the Schwinger-Dyson equations
allows us to determine the full phase diagram of the system, showing the
prevalence of a renormalized semimetallic phase in the regime of intermediate
interaction strength, and adjacent to the non-Fermi liquid phase characteristic
of the strong interaction regime of 3D Weyl semimetals.
"
"  We present a finite difference time domain (FDTD) model for computation of A
line scans in time domain optical coherence tomography (OCT). By simulating
only the end of the two arms of the interferometer and computing the
interference signal in post processing, it is possible to reduce the
computational time required by the simulations and, thus, to simulate much
bigger environments. Moreover, it is possible to simulate successive A lines
and thus obtaining a cross section of the sample considered. In this paper we
present the model applied to two different samples: a glass rod filled with
water-sucrose solution at different concentrations and a peripheral nerve. This
work demonstrates the feasibility of using OCT for non-invasive, direct optical
monitoring of peripheral nerve activity, which is a long-sought goal of
neuroscience.
"
"  Using focused electron-beam-induced deposition (FEBID), we fabricate
vertical, platinum-coated cobalt nanowires with a controlled three-dimensional
structure. The latter is engineered to feature bends along the height: these
are used as pinning sites for domain walls, the presence of which we
investigate using X-ray Magnetic Circular Dichroism (XMCD) coupled to
PhotoEmission Electron Microscopy (PEEM). The vertical geometry of our sample
combined with the low incidence of the X-ray beam produce an extended wire
shadow which we use to recover the wire's magnetic configuration. In this
transmission configuration, the whole sample volume is probed, thus
circumventing the limitation of PEEM to surfaces. This article reports on the
first study of magnetic nanostructures standing perpendicular to the substrate
with XMCD-PEEM. The use of this technique in shadow mode enabled us to confirm
the presence of a domain wall (DW) without direct imaging of the nanowire.
"
"  We study optical forces acting upon semiconductor quantum dots and the force
driven motion of the dots in a colloid. In the spectral range of exciton
transitions in uantum dots, when the photon energy is close to the exciton
energy, the polarizability of the dots is drastically increased. It leads to a
resonant increase of both the gradient and the scattering contributions to the
optical force, which enables the efficient manipulation with the dots. We
reveal that the optical grating of the colloid leads to the formation of a
fluid photonic crystal with spatially periodic circulating fluxes and density
of the dots. Pronounced resonant dielectric response of semiconductor quantum
dots enables a separation of the quantum dots with different exciton
frequencies.
"
"  Bell inequalities are usually derived by assuming locality and realism, and
therefore experimental violations of Bell inequalities are usually taken to
imply violations of either locality or realism, or both. But, after reviewing
an oversight by Bell, here we derive the Bell-CHSH inequality by assuming only
that Bob can measure along the directions b and b' simultaneously while Alice
measures along either a or a', and likewise Alice can measure along the
directions a and a' simultaneously while Bob measures along either b or b',
without assuming locality. The observed violations of the Bell-CHSH inequality
therefore simply verify the manifest impossibility of measuring along the
directions b and b' (or along the directions a and a') simultaneously, in any
realizable EPR-Bohm type experiment.
"
"  e-ASTROGAM (enhanced ASTROGAM) is a breakthrough Observatory space mission,
with a detector composed by a Silicon tracker, a calorimeter, and an
anticoincidence system, dedicated to the study of the non-thermal Universe in
the photon energy range from 0.3 MeV to 3 GeV - the lower energy limit can be
pushed to energies as low as 150 keV for the tracker, and to 30 keV for
calorimetric detection. The mission is based on an advanced space-proven
detector technology, with unprecedented sensitivity, angular and energy
resolution, combined with polarimetric capability. Thanks to its performance in
the MeV-GeV domain, substantially improving its predecessors, e-ASTROGAM will
open a new window on the non-thermal Universe, making pioneering observations
of the most powerful Galactic and extragalactic sources, elucidating the nature
of their relativistic outflows and their effects on the surroundings. With a
line sensitivity in the MeV energy range one to two orders of magnitude better
than previous generation instruments, e-ASTROGAM will determine the origin of
key isotopes fundamental for the understanding of supernova explosion and the
chemical evolution of our Galaxy. The mission will provide unique data of
significant interest to a broad astronomical community, complementary to
powerful observatories such as LIGO-Virgo-GEO600-KAGRA, SKA, ALMA, E-ELT, TMT,
LSST, JWST, Athena, CTA, IceCube, KM3NeT, and LISA.
"
"  We report magnetic and thermodynamic properties of a $4d^1$ (Mo$^{5+}$)
magnetic insulator MoOPO$_4$ single crystal, which realizes a $J_1$-$J_2$
Heisenberg spin-$1/2$ model on a stacked square lattice. The specific-heat
measurements show a magnetic transition at 16 K which is also confirmed by
magnetic susceptibility, ESR, and neutron diffraction measurements. Magnetic
entropy deduced from the specific heat corresponds to a two-level degree of
freedom per Mo$^{5+}$ ion, and the effective moment from the susceptibility
corresponds to the spin-only value. Using {\it ab initio} quantum chemistry
calculations we demonstrate that the Mo$^{5+}$ ion hosts a purely spin-$1/2$
magnetic moment, indicating negligible effects of spin-orbit interaction. The
quenched orbital moments originate from the large displacement of Mo ions
inside the MoO$_6$ octahedra along the apical direction. The ground state is
shown by neutron diffraction to support a collinear Néel-type magnetic order,
and a spin-flop transition is observed around an applied magnetic field of 3.5
T. The magnetic phase diagram is reproduced by a mean-field calculation
assuming a small easy-axis anisotropy in the exchange interactions. Our results
suggest $4d$ molybdates as an alternative playground to search for model
quantum magnets.
"
"  We propose a universal experiment to measure the differential Casimir force
between a Au-coated sphere and two halves of a structured plate covered with a
P-doped Si overlayer. The concentration of free charge carriers in the
overlayer is chosen slightly below the critical one, f or which the phase
transition from dielectric to metal occurs. One ha f of the structured plate is
insulating, while its second half is made of gold. For the former we consider
two different structures, one consisting of bulk high-resistivity Si and the
other of a layer of silica followed by bulk high-resistivity Si. The
differential Casimir force is computed within the Lifshitz theory using four
approaches that have been proposed in the literature to account for the role of
free charge carriers in metallic and dielectric materials interacting with
quantum fluctuations. According to these approaches, Au at low frequencies is
described by either the Drude or the plasma model, whereas the free charge
carriers in dielectric materials at room temperature are either taken into
account or disregarded. It is shown that the values of differential Casimir
forces, computed in the micrometer separation range using these four
approaches, are widely distinct from each other and can be easily discriminated
experimentally. It is shown that for all approaches the thermal component of
the differential Casimir force is sufficiently large for direct observation.
The possible errors and uncertainties in the proposed experiment are estimated
and its importance for the theory of quantum fluctuations is discussed.
"
"  We report observations of magnetoresistance, quantum oscillations and
angle-resolved photoemission in RhSb$_3$, a unfilled skutterudite semimetal
with low carrier density. The calculated electronic band structure of RhSb$_3$
entails a $Z_2$ quantum number $\nu_0=0,\nu_1=\nu_2=\nu_3=1$ in analogy to
strong topological insulators, and inverted linear valence/conduction bands
that touch at discrete points close to the Fermi level, in agreement with
angle-resolved photoemission results. Transport experiments reveal an
unsaturated linear magnetoresistance that approaches a factor of 200 at 60 T
magnetic fields, and quantum oscillations observable up to 150~K that are
consistent with a large Fermi velocity ($\sim 1.3\times 10^6$ ms$^{-1}$), high
carrier mobility ($\sim 14$ $m^2$/Vs), and small three dimensional hole pockets
with nontrivial Berry phase. A very small, sample-dependent effective mass that
falls as low as $0.015(7)$ bare masses scales with Fermi velocity, suggesting
RhSb$_3$ is a new class of zero-gap three-dimensional Dirac semimetal.
"
"  The ""backward simulation"" of a stochastic process is defined as the
stochastic dynamics that trace a time-reversed path from the target region to
the initial configuration. If the probabilities calculated by the original
simulation are easily restored from those obtained by backward dynamics, we can
use it as a computational tool. It is shown that the naive approach to backward
simulation does not work as expected. As a remedy, the Time Reverse Monte Carlo
method (TRMC) based on the ideas of Sequential Importance Sampling (SIS) and
Sequential Monte Carlo (SMC) is proposed and successfully tested with a
stochastic typhoon model and the Lorenz 96 model. TRMC with SMC, which contains
resampling steps, is shown to be more efficient for simulations with a larger
number of time steps. A limitation of TRMC and its relation to the Bayes
formula are also discussed.
"
"  We spectroscopically investigate the hyperfine, rotational and Zeeman
structure of the vibrational levels $\text{v}'=0$, $7$, $13$ within the
electronically excited $c^3\Sigma_g^+$ state of $^{87}\text{Rb}_2$ for magnetic
fields of up to $1000\,\text{G}$. As spectroscopic methods we use short-range
photoassociation of ultracold Rb atoms as well as photoexcitation of ultracold
molecules which have been previously prepared in several well-defined quantum
states of the $a^3\Sigma_u^+$ potential. As a byproduct, we present optical
two-photon transfer of weakly bound Feshbach molecules into $a^3\Sigma_u^+$,
$\text{v}=0$ levels featuring different nuclear spin quantum numbers. A simple
model reproduces well the molecular level structures of the $c^3\Sigma_g^+$
vibrational states and provides a consistent assignment of the measured
resonance lines. Furthermore, the model can be used to predict the relative
transition strengths of the lines. From fits to the data we extract for each
vibrational level the rotational constant, the effective spin-spin interaction
constant, as well as the Fermi contact parameter and (for the first time) the
anisotropic hyperfine constant. In an alternative approach, we perform
coupled-channel calculations where we fit the relevant potential energy curves,
spin-orbit interactions and hyperfine functions. The calculations reproduce the
measured hyperfine level term frequencies with an average uncertainty of
$\pm9\:$MHz, similar as for the simple model. From these fits we obtain a
section of the potential energy curve for the $c^3\Sigma_g^+$ state which can
be used for predicting the level structure for the vibrational manifold
$\text{v}'=0$ to $13$ of this electronic state.
"
"  In the study of subdiffusive wave-packet spreading in disordered Klein-Gordon
(KG) nonlinear lattices, a central open question is whether the motion
continues to be chaotic despite decreasing densities, or tends to become
quasi-periodic as nonlinear terms become negligible. In a recent study of such
KG particle chains with quartic (4th order) anharmonicity in the on-site
potential, it was shown that $q-$Gaussian probability distribution functions of
sums of position observables with $q > 1$ always approach pure Gaussians
($q=1$) in the long time limit and hence the motion of the full system is
ultimately ""strongly chaotic"". In the present paper, we show that these results
continue to hold even when a sextic (6th order) term is gradually added to the
potential and ultimately prevails over the 4th order anharmonicity, despite
expectations that the dynamics is more ""regular"", at least in the regime of
small oscillations. Analyzing this system in the subdiffusive energy domain
using $q$-statistics, we demonstrate that groups of oscillators centered around
the initially excited one (as well as the full chain) possess strongly chaotic
dynamics and are thus far from any quasi-periodic torus, for times as long as
$t=10^9$.
"
"  The origin of the activity in the solar corona is a long-standing problem in
solar physics. Recent satellite observations, such as Hinode, Solar Dynamics
Observatory (SDO), Interface Region Imaging Spectrograph (IRIS), show the
detail characteristics of the solar atmosphere and try to reveal the energy
transfer from the photosphere to the corona through the magnetic fields and its
energy conversion by various processes. However, quantitative estimation of
energy transfer along the magnetic field is not enough. There are mainly two
reason why it is difficult to observe the energy transfer from photosphere to
corona; 1) spatial resolution gap between photosphere (a few 0.1 arcsec) and
corona (a few arcsec), 2) lack in temperature coverage. Furthermore, there is
not enough observational knowledge of the physical parameters in the energy
dissipation region. There are mainly three reason why it is difficult to
observe in the vicinity of the energy dissipation region; 1) small spatial
scale, 2) short time scale, 3) low emission. It is generally believed that the
energy dissipation occurs in the very small scale and its duration is very
short (10 second). Further, the density in the dissipation region might be very
low. Therefore, the high spatial and temporal resolution UV/EUV spectroscopic
observation with wide temperature coverage is crucial to estimate the energy
transport from photosphere to corona quantitatively and diagnose the plasma
dynamics in the vicinity of the energy dissipation region. Main Science Target
for the telescope is quantitative estimation for the energy transfer from the
photosphere to the corona, and clarification of the plasma dynamics in the
vicinity of the energy dissipation region, where is the key region for coronal
heating, solar wind acceleration, and/or solar flare, by the high spatial and
temporal resolution UV/EUV spectroscopy.
"
"  Fermilab is committed to upgrading its accelerator complex towards the
intensity frontier to pursue HEP research in the neutrino sector and beyond.
The upgrade has two steps: 1) the Proton Improvement Plan (PIP), which is
underway, has its primary goal to start providing 700 kW beam power on NOvA
target by the end of 2017 and 2) the foreseen PIP-II will replace the existing
LINAC, a 400 MeV injector to the Booster, by an 800 MeV superconducting LINAC
by the middle of next decade, with output beam intensity from the Booster
increased significantly and the beam power on the NOvA target increased to <1.2
MW. In any case, the Fermilab Booster is going to play a very significant role
for the next two decades. In this context, we have recently developed and
commissioned an innovative beam injection scheme for the Booster called ""early
injection scheme."" This scheme is already in operation and has a potential to
increase the Booster beam intensity from the PIP design goal by a considerable
amount with a reduced beam emittance and beam loss. In this paper, we will
present results from our experience from the new scheme in operation, current
status and future plans.
"
"  We present the results of smoothed particle hydrodynamic simulations
investigating the evolution and fragmentation of filaments that are accreting
from a turbulent medium. We show that the presence of turbulence, and the
resulting inhomogeneities in the accretion flow, play a significant role in the
fragmentation process. Filaments which experience a weakly turbulent accretion
flow fragment in a two-tier hierarchical fashion, similar to the fragmentation
pattern seen in the Orion Integral Shaped Filament. Increasing the energy in
the turbulent velocity field results in more sub-structure within the
filaments, and one sees a shift from gravity-dominated fragmentation to
turbulence-dominated fragmentation. The sub-structure formed in the filaments
is elongated and roughly parallel to the longitudinal axis of the filament,
similar to the fibres seen in observations of Taurus, and suggests that the
fray and fragment scenario is a possible mechanism for the production of
fibres. We show that the formation of these fibre-like structures is linked to
the vorticity of the velocity field inside the filament and the filament's
accretion from an inhomogeneous medium. Moreover, we find that accretion is
able to drive and sustain roughly sonic levels of turbulence inside the
filaments, but is not able to prevent radial collapse once the filaments become
supercritical. However, the supercritical filaments which contain fibre-like
structures do not collapse radially, suggesting that fibrous filaments may not
necessarily become radially unstable once they reach the critical line-density.
"
"  Redox flow batteries (RFBs) are potential solutions for grid-scale energy
storage, and deeper understanding of the effect of flow rate on RFB performance
is needed to develop efficient, low-cost designs. In this study we highlight
the importance of modeling tanks, which can limit the charge/discharge capacity
of redox-active polymer (RAP) based RFBs. The losses due to tank mixing
dominate over the polarization-induced capacity losses that arise due to
resistive processes in the reactor. A porous electrode model is used to
separate these effects by predicting the time variation of active species
concentration in electrodes and tanks. A simple transient model based on
species conservation laws developed in this study reveals that charge
utilization and polarization are affected by two dimensionless numbers
quantifying (1) flow rate relative to stoichiometric flow and (2) size of flow
battery tanks relative to the reactor. The RFB's utilization is shown to
increase monotonically with flow rate, reaching 90% of the theoretical value
only when flow rate exceeds twenty-fold of the stoichiometric value. We also
identify polarization due to irreversibilities inherent to RFB architecture as
a result of tank mixing and current distribution internal to the reactor, and
this polarization dominates over that resulting from ohmic resistances
particularly when cycling RFBs at low flow rates and currents. These findings
are summarized in a map of utilization and polarization that can be used to
select adequate flow rate for a given tank size.
"
"  In this work we have analyzed the magnetocaloric effect (MCE) from the
Tsallis thermostatistics formalism (TTF) point of view. The problem discussed
here is a two level system MCE. We have calculated, both analytically and
numerically, the entropy of this system as a function of the Tsallis' parameter
(the well known q-parameter) which value depends on the extensivity (q<1) or
non-extensivity (q>1) of the system. Since we consider this MCE not depending
on the initial conditions, which classify our system as a non-extensive one, we
used several greater than one q-parameters to understand the effect of the
nonextensive formalism in the entropy as well as the magnetocaloric potential,
$\Delta S$. We have plotted several curves that shows precisely the behavior of
this effect when dealt with non-extensive statistics.
"
"  In the framework of MSSM inflation, matter and gravitino production are here
investigated through the decay of the fields which are coupled to the udd
inflaton, a gauge invariant combination of squarks. After the end of inflation,
the flat direction oscillates about the minimum of its potential, losing at
each oscillation about 56% of its energy into bursts of gauge/gaugino and
scalar quanta when crossing the origin. These particles then acquire a large
inflaton VEV-induced mass and decay perturbatively into the MSSM quanta and
gravitinos, transferring the inflaton energy very efficiently via instant
preheating. Regarding thermalization, we show that the MSSM degrees of freedom
thermalize very quickly, yet not immediately by virtue of the large vacuum
expectation value of the inflaton, which breaks the $SU(3)_C\times U(1)_Y$
symmetry into a residual $U(1)$. The energy transfer to the MSSM quanta is very
efficient, since full thermalization is achieved after only $\mathcal{O}(40)$
complete oscillations. The udd inflaton thus provides an extremely efficient
reheating of the Universe, with a temperature
$T_{reh}=\mathcal{O}(10^8\mathrm{GeV})$ that allows for instance several
mechanisms of baryogenesis. We also compute the gravitino number density from
the perturbative decay of the flat direction and of the SUSY multiplet. We find
that the gravitinos are produced in negligible amount and satisfy cosmological
bounds such as the Big Bang Nucleosynthesis (BBN) and Dark Matter (DM)
constraints.
"
"  Electromagnetic properties of single crystal terbium gallium garnet (TGG) are
characterised from room down to millikelvin temperatures using the whispering
gallery mode method. Microwave spectroscopy is performed at low powers
equivalent to a few photons in energy and conducted as functions of the
magnetic field and temperature. A phase transition is detected close to the
temperature of 3.5 K. This is observed for multiple whispering gallery modes
causing an abrupt negative frequency shift and a change in transmission due to
extra losses in the new phase caused by a change in complex magnetic
susceptibility.
"
"  We set out to quantify the number density of quiescent massive compact
galaxies at intermediate redshifts. We determine structural parameters based on
i-band imaging using the CFHT equatorial SDSS Stripe 82 (CS82) survey (~170 sq.
degrees) taking advantage of an exquisite median seeing of ~0.6''. We select
compact massive (M > 5x10^10 M_sun) galaxies within the redshift range of
0.2<z<0.6. The large volume sampled allows to decrease the effect of cosmic
variance that has hampered the calculation of the number density for this
enigmatic population in many previous studies. We undertake an exhaustive
analysis in an effort to untangle the various findings inherent to the diverse
definition of compactness present in the literature. We find that the absolute
number of compact galaxies is very dependent on the adopted definition and can
change up to a factor of >10. We systematically measure a factor of ~5 more
compacts at the same redshift than what was previously reported on smaller
fields with HST imaging, which are more affected by cosmic variance. This means
that the decrease in number density from z ~ 1.5 to z ~ 0.2 might be only of a
factor of ~2-5, significantly smaller than what previously reported. This
supports progenitor bias as the main contributor to the size evolution. This
milder decrease is roughly compatible with the predictions from recent
numerical simulations. Only the most extreme compact galaxies, with Reff <
1.5x( M/10^11 M_sun)^0.75 and M > 10^10.7 M_sun, appear to drop in number by a
factor of ~20 and hence likely experience a noticeable size evolution.
"
"  The theory of Hitchin systems is something like a ""global theory of Lie
groups"", where one works over a Riemann surface rather than just at a point.
We'll describe how one can take this analogy a few steps further by attempting
to make precise the class of rich geometric objects that appear in this story
(including the non-compact case), and discuss their classification, outlining a
theory of ""Dynkin diagrams"" as a step towards classifying some examples of such
objects.
"
"  Despite its attractive features, Congruent-melted Lithium Niobate (CLN)
suffers from Photo-Refractive Damage (PRD). This light-induced refractive-index
change hampers the use of CLN when high-power densities are in play, a typical
regime in integrated optics. The resistance to PRD can be largely improved by
doping the lithium-niobate substrates with magnesium oxide. However, the
fabrication of waveguides on MgO-doped substrates is not as effective as for
CLN: either the resistance to PRD is strongly reduced by the waveguide
fabrication process (as it happens in Ti-indiffused waveguides) or the
nonlinear conversion efficiency is lowered (as it occurs in annealed-proton
exchange). Here we fabricate, for the first time, waveguides starting from
MgO-doped substrates using the Soft-Proton Exchange (SPE) technique and we show
that this third way represents a promising alternative. We demonstrate that SPE
allows to produce refractive-index profiles almost identical to those produced
on CLN without reducing the nonlinearity in the substrate. We also prove that
the SPE does not affect substantially the resistance to PRD. Since the
fabrication recipe is identical between CLN and MgO-doped substrates, we
believe that SPE might outperform standard techniques to fabricate robust and
efficient waveguides for high-intensity-beam confinement.
"
"  We report on the design and performance of a mixed-signal application
specific integrated circuit (ASIC) dedicated to avalanche photodiodes (APDs) in
order to detect hard X-ray emissions in a wide energy band onboard the
International Space Station. To realize wide-band detection from 20 keV to 1
MeV, we use Ce:GAGG scintillators, each coupled to an APD, with low-noise
front-end electronics capable of achieving a minimum energy detection threshold
of 20 keV. The developed ASIC has the ability to read out 32-channel APD
signals using 0.35 $\mu$m CMOS technology, and an analog amplifier at the input
stage is designed to suppress the capacitive noise primarily arising from the
large detector capacitance of the APDs. The ASIC achieves a performance of 2099
e$^{-}$ + 1.5 e$^{-}$/pF at root mean square (RMS) with a wide 300 fC dynamic
range. Coupling a reverse-type APD with a Ce:GAGG scintillator, we obtain an
energy resolution of 6.7% (FWHM) at 662 keV and a minimum detectable energy of
20 keV at room temperature (20 $^{\circ}$C). Furthermore, we examine the
radiation tolerance for space applications by using a 90 MeV proton beam,
confirming that the ASIC is free of single-event effects and can operate
properly without serious degradation in analog and digital processing.
"
"  Careful analyses of photometric and star count data available for the nine
putative young clusters identified by Camargo et al. (2015, 2016) at high
Galactic latitudes reveal that none of the groups contain early-type stars, and
most are not significant density enhancements above field level. 2MASS colours
for stars in the groups match those of unreddened late-type dwarfs and giants,
as expected for contamination by (mostly) thin disk objects. A simulation of
one such field using only typical high latitude foreground stars yields a
colour-magnitude diagram that is very similar to those constructed by Camargo
et al. (2015, 2016) as evidence for their young groups as well as the means of
deriving their reddenings and distances. Although some of the fields are
coincident with clusters of galaxies, one must conclude that there is no
evidence that the putative clusters are extremely young stellar groups.
"
"  The self-annihilation of dark matter particles with mass in the MeV range can
produce gamma rays via prompt or secondary radiation. The annihilation rate for
such light dark matter particles is however tightly constrained by cosmic
microwave background (CMB) data. Here we explore the possibility of discovering
MeV dark matter annihilation with future MeV gamma-ray telescopes taking into
account the latest and future CMB constraints. We study the optimal energy
window as a function of the dominant annihilation final state. We consider both
the (conservative) case of the dwarf spheroidal galaxy Draco and the (more
optimistic) case of the Galactic center. We find that for certain channels,
including those with one or two monochromatic photon(s) and one or two neutral
pion(s), a detectable gamma-ray signal is possible for both targets under
consideration, and compatible with CMB constraints. For other annihilation
channels, however, including all leptonic annihilation channels and two charged
pions, CMB data rule out any significant signal of dark matter annihilation at
future MeV gamma-ray telescopes from dwarf galaxies, but possibly not for the
Galactic center.
"
"  Direct imaging of exoplanets requires the detection of very faint objects
orbiting close to very bright stars. In this context, the SPICES mission was
proposed to the European Space Agency for planet characterization at visible
wavelength. SPICES is a 1.5m space telescope which uses a coronagraph to
strongly attenuate the central source. However, small optical aberrations,
which appear even in space telescopes, dramatically decrease coronagraph
performance. To reduce these aberrations, we want to estimate, directly on the
coronagraphic image, the electric field, and, with the help of a deformable
mirror, correct the wavefront upstream of the coronagraph. We propose an
instrument, the Self-Coherent Camera (SCC) for this purpose. By adding a small
""reference hole"" into the Lyot stop, located after the coronagraph, we can
produce interferences in the focal plane, using the coherence of the stellar
light. We developed algorithms to decode the information contained in these
Fizeau fringes and retrieve an estimation of the field in the focal plane.
After briefly recalling the SCC principle, we will present the results of a
study, based on both experiment and numerical simulation, analyzing the impact
of the size of the reference hole.
"
"  We study phase transitions in a two dimensional weakly interacting Bose gas
in a random potential at finite temperatures. We identify superfluid, normal
fluid, and insulator phases and construct the phase diagram. At T=0 one has a
tricritical point where the three phases coexist. The truncation of the energy
distribution at the trap barrier, which is a generic phenomenon in cold atom
systems, limits the growth of the localization length and in contrast to the
thermodynamic limit the insulator phase is present at any temperature.
"
"  In this work, the study of thermal conductivity before and after in-situ
ring-opening polymerization of cyclic butylene terephthalate into poly
(butylene terephthalate) in presence of graphene-related materials (GRM) is
addressed, to gain insight in the modification of nanocomposites morphology
upon polymerization. Five types of GRM were used: one type of graphite
nanoplatelets, two different grades of reduced graphene oxide (rGO) and the
same rGO grades after thermal annealing for 1 hour at 1700°C under vacuum
to reduce their defectiveness. Polymerization of CBT into pCBT, morphology and
nanoparticle organization were investigated by means of differential scanning
calorimetry, electron microscopy and rheology. Electrical and thermal
properties were investigated by means of volumetric resistivity and bulk
thermal conductivity measurement. In particular, the reduction of nanoflake
aspect ratio during ring-opening polymerization was found to have a detrimental
effect on both electrical and thermal conductivities in nanocomposites.
"
"  In this paper we develop a conservative sharp-interface method dedicated to
simulating multiple compressible fluids. Numerical treatments for a cut cell
shared by more than two materials are proposed. First, we simplify the
interface interaction inside such a cell with a reduced model to avoid explicit
interface reconstruction and complex flux calculation. Second, conservation is
strictly preserved by an efficient conservation correction procedure for the
cut cell. To improve the robustness, a multi-material scale separation model is
developed to consistently remove non-resolved interface scales. In addition,
the multi-resolution method and local time-stepping scheme are incorporated
into the proposed multi-material method to speed up the high-resolution
simulations. Various numerical test cases, including the multi-material shock
tube problem, inertial confinement fusion implosion, triple-point shock
interaction and shock interaction with multi-material bubbles, show that the
method is suitable for a wide range of complex compressible multi-material
flows.
"
"  Two-dimensional (2D) materials, such as graphene and MoS2, have been
attracting wide interest in surface enhancement Raman spectroscopy. This
perspective gives an overview of recent developments in 2D materials'
application in surface enhanced Raman spectroscopy. This review focuses on the
applications of using bare 2D materials and metal/2D material hybrid substrate
for Raman enhancement. The Raman enhancing mechanism of 2D materials will also
be discussed. The progress covered herein shows great promise for widespread
adoption of 2D materials in SERS application.
"
"  We perform ultrasound velocity measurements on a single crystal of
nearly-metallic spinel Co$_{1.21}$V$_{1.79}$O$_4$ which exhibits a
ferrimagnetic phase transition at $T_C \sim$ 165 K. The experiments reveal a
variety of elastic anomalies in not only the paramagnetic phase above $T_C$ but
also the ferrimagnetic phase below $T_C$, which should be driven by the
nearly-itinerant character of the orbitally-degenerate V 3$d$ electrons. In the
paramagnetic phase above $T_C$, the elastic moduli exhibit
elastic-mode-dependent unusual temperature variations, suggesting the existence
of a dynamic spin-cluster state. Furthermore, above $T_C$, the sensitive
magnetic-field response of the elastic moduli suggests that, with the negative
magnetoresistance, the magnetic-field-enhanced nearly-itinerant character of
the V 3$d$ electrons emerges from the spin-cluster state. This should be
triggered by the inter-V-site interactions acting on the orbitally-degenerate
3$d$ electrons. In the ferrimagnetic phase below $T_C$, the elastic moduli
exhibit distinct anomalies at $T_1\sim$ 95 K and $T_2\sim$ 50 K, with a sign
change of the magnetoresistance at $T_1$ (positive below $T_1$) and an
enhancement of the positive magnetoresistance below $T_2$, respectively. These
observations below $T_C$ suggest the successive occurrence of an orbital glassy
order at $T_1$ and a structural phase transition at $T_2$, where the rather
localized character of the V 3$d$ electrons evolves below $T_1$ and is further
enhanced below $T_2$.
"
"  Following the advent of electromagnetic metamaterials at the turn of the
century, researchers working in other areas of wave physics have translated
concepts of electromagnetic metamaterials to acoustics, elastodynamics, as well
as to heat, mass and light diffusion processes. In elastodynamics, seismic
metamaterials have emerged in the last decade for soft soils structured at the
meter scale, and have been tested thanks to full-scale experiments on holey
soils five years ago. Born in the soil, seismic metamaterials grow
simultaneously on the field of tuned-resonators buried in the soil, around
building's foundations or near the soil-structure's interface, and on the field
of above-surface resonators. In this perspective article, we quickly recall
some research advances made in all these types of seismic metamaterials and we
further dress an inventory of which material parameters can be achieved and
which cannot, notably from the effective medium theory perspective. We finally
envision perspectives on future developments of large scale auxetic
metamaterials for building's foundations, forests of trees for seismic
protection and metamaterial-like transformed urbanism at the city scale.
"
"  Weak attractive interactions in a spin-imbalanced Fermi gas induce a
multi-particle instability, binding multiple fermions together. The maximum
binding energy per particle is achieved when the ratio of the number of up- and
down-spin particles in the instability is equal to the ratio of the up- and
down-spin densities of states in momentum at the Fermi surfaces, to utilize the
variational freedom of all available momentum states. We derive this result
using an analytical approach, and verify it using exact diagonalization. The
multi-particle instability extends the Cooper pairing instability of balanced
Fermi gases to the imbalanced case, and could form the basis of a many-body
state, analogously to the construction of the Bardeen-Cooper-Schrieffer theory
of superconductivity out of Cooper pairs.
"
"  We investigate the basic thermal, mechanical and structural properties of
body centred cubic iron ($\alpha$-Fe) at several temperatures and positive
loading by means of Molecular Dynamics simulations in conjunction with the
embedded-atom method potential and its modified counterpart one. Computations
of its thermal properties like average energy and density of atoms, transport
sound velocities at finite temperatures and pressures are detailed studied as
well. Moreover, there are suggestions to obtain hexagonal close- packed
structure ($\varepsilon$-phase) of this metal under positive loading. To
demonstrate that, one can increase sufficiently the pressure of simulated
system at several temperature's ranges; these structural changes depend only on
potential type used. The ensuring structures are studied via the pair radial
distribution functions (PRDF) and precise common- neighbour analysis method
(CNA) as well.
"
"  We present Magnetohydrodynamic (MHD) simulations of the magnetic interactions
between a solar type star and short period hot Jupiter exoplanets, using the
publicly available MHD code PLUTO. It has been predicted that emission due to
magnetic interactions such as the electron cyclotron maser instability (ECMI)
will be observable. In our simulations, a planetary outflow, due to UV
evaporation of the exoplanets atmosphere, results in the build-up of
circumplanetary material. We predict the ECMI emission and determine that the
emission is prevented from escaping from the system. This is due to the
evaporated material leading to a high plasma frequency in the vicinity of the
planet, which inhibits the ECMI process.
"
"  We studied intermediate filaments (IFs) in the retina of the Pied flycatcher
(Ficedula hypoleuca) in the foveolar zone. Single IFs span Müller cells (MC)
lengthwise; cylindrical bundles of IFs (IFBs) appear inside the cone inner
segment (CIS) at the outer limiting membrane (OLM) level. IFBs adjoin the cone
cytoplasmatic membrane, following lengthwise regularly spaced, forming a
skeleton of the CIS, located above the OLM. IFBs follow along the cone outer
segment (COS), with single IFs separating from the IFB, touching and entering
in-between the light-sensitive disks of the cone membrane. We propose a
mechanism of exciton transfer from the inner retinal surface to the visual
pigments in the photoreceptor cells. This includes excitation transfer in
donor-acceptor systems, from the IF donors to the rhodopsin acceptors, with
theoretic efficiency over 80%. This explains high image contrast in fovea and
foveola in daylight, while the classical mechanism that describes Müller
cells as optical lightguides operates in night vision, with loss of resolution
traded for sensitivity. Our theory receives strong confirmation in morphology
and function of the cones and pigment cells. In daylight the lateral surface of
the photosensor disks is blocked from the (scattered or oblique) light by the
pigment cells. Thus the light energy can only get to the cone via intermediate
filaments that absorb photons in the Müller cell endfeet and conduct excitons
to the cone. Thus, the disks are consumed at their lateral surfaces, moving to
the apex of the cone, with new disks produced below. An alternative hypothesis
of direct light passing through the cone with its organelles and hitting the
lowest disk contradicts morphological evidence, as thus all of the other disks
would have no useful function in daylight vision.
"
"  Using the Purple Mountain Observatory Delingha (PMODLH) 13.7 m telescope, we
report a 96-square-degree 12CO/13CO/C18O mapping observation toward the
Galactic region of l = [139.75, 149.75]$^\circ$, b = [-5.25, 5.25]$^\circ$. The
molecular structure of the Local Arm and Perseus Arm are presented. Combining
HI data and part of the Outer Arm results, we obtain that the warp structure of
both atomic and molecular gas is obvious, while the flare structure only exists
in atomic gas in this observing region. In addition, five filamentary giant
molecular clouds on the Perseus Arm are identified. Among them, four are newly
identified. Their relations with the Milky Way large-scale structure are
discussed.
"
"  SrRuO$_3$ (SRO) films are known to exhibit insulating behavior as their
thickness approaches four unit cells. We employ electron energy$-$loss (EEL)
spectroscopy to probe the spatially resolved electronic structures of both
insulating and conducting SRO to correlate them with the metal$-$insulator
transition (MIT). Importantly, the central layer of the ultrathin insulating
film exhibits distinct features from the metallic SRO. Moreover, EEL near edge
spectra adjacent to the SrTiO$_3$ (STO) substrate or to the capping layer are
remarkably similar to those of STO. The site$-$projected density of states
based on density functional theory (DFT) partially reflects the characteristics
of the spectra of these layers. These results may provide important information
on the possible influence of STO on the electronic states of ultrathin SRO.
"
"  We prove local well-posedness in regular spaces and a Beale-Kato-Majda
blow-up criterion for a recently derived stochastic model of the 3D Euler fluid
equation for incompressible flow. This model describes incompressible fluid
motions whose Lagrangian particle paths follow a stochastic process with
cylindrical noise and also satisfy Newton's 2nd Law in every Lagrangian domain.
"
"  Te NMR studies were carried out for the bismuth telluride topological
insulator in a wide range from room temperature down to 12.5 K. The
measurements were made on a Bruker Avance 400 pulse spectrometer. The NMR
spectra were collected for the mortar and pestle powder sample and for single
crystalline stacks with orientations c parallel and perpendicular to field. The
activation energy responsible for thermal activation. The spectra for the stack
with c parallel to field showed some particular behavior below 91 K.
"
"  We carried out molecular dynamics simulations (MD) using realistic empirical
potentials for the vapor deposition (VD) of CuZrAl glasses. VD glasses have
higher densities and lower potential and inherent structure energies than the
melt-quenched glasses for the same alloys. The optimal substrate temperature
for the deposition process is 0.625$\times T_\mathrm{g}$. In VD metallic
glasses (MGs), the total number of icosahedral like clusters is higher than in
the melt-quenched MGs. Surprisingly, the VD glasses have a lower degree of
chemical mixing than the melt-quenched glasses. The reason for it is that the
melt-quenched MGs can be viewed as frozen liquids, which means that their
chemical order is the same as in the liquid state. In contrast, during the
formation of the VD MGs, the absence of the liquid state results in the
creation of a different chemical order with more Zr-Zr homonuclear bonds
compared with the melt-quenched MGs. In order to obtain MGs from melt-quench
technique with similarly low energies as in the VD process, the cooling rate
during quenching would have to be many orders of magnitude lower than currently
accessible to MD simulations. The method proposed in this manuscript is a more
efficient way to create MGs by using MD simulations.
"
"  The SoLid collaboration have developed an intelligent readout system to
reduce their 3200 silicon photomultiplier detector's data rate by a factor of
10000 whilst maintaining high efficiency for storing data from anti-neutrino
interactions. The system employs an FPGA-level waveform characterisation to
trigger on neutron signals. Following a trigger, data from a space time region
of interest around the neutron will be read out using the IPbus protocol. In
these proceedings the design of the readout system is explained and results
showing the performance of a prototype version of the system are presented.
"
"  We demonstrate the presence of chaos in stochastic simulations that are
widely used to study biodiversity in nature. The investigation deals with a set
of three distinct species that evolve according to the standard rules of
mobility, reproduction and predation, with predation following the cyclic rules
of the popular rock, paper and scissors game. The study uncovers the
possibility to distinguish between time evolutions that start from slightly
different initial states, guided by the Hamming distance which heuristically
unveils the chaotic behavior. The finding opens up a quantitative approach that
relates the correlation length to the average density of maxima of a typical
species, and an ensemble of stochastic simulations is implemented to support
the procedure. The main result of the work shows how a single and simple
experimental realization that counts the density of maxima associated with the
chaotic evolution of the species serves to infer its correlation length. We use
the result to investigate others distinct complex systems, one dealing with a
set of differential equations that can be used to model a diversity of natural
and artificial chaotic systems, and another one, focusing on the ocean water
level.
"
"  Thunderstorms produce strong electric fields over regions on the order of
kilometer. The corresponding electric potential differences are on the order of
100 MV. Secondary cosmic rays reaching these regions may be significantly
accelerated and even amplified in relativistic runaway avalanche processes.
These phenomena lead to enhancements of the high-energy background radiation
observed by detectors on the ground and on board aircraft. Moreover, intense
submillisecond gamma-ray bursts named terrestrial gamma-ray flashes (TGFs)
produced in thunderstorms are detected from low Earth orbit satellites. When
passing through the atmosphere, these gamma-rays are recognized to produce
secondary relativistic electrons and positrons rapidly trapped in the
geomagnetic field and injected into the near-Earth space environment. In the
present work, we attempt to give an overview of the current state of research
on high-energy phenomena associated with thunderstorms.
"
"  An accurate calculation of proton ranges in phantoms or detector geometries
is crucial for decision making in proton therapy and proton imaging. To this
end, several parameterizations of the range-energy relationship exist, with
different levels of complexity and accuracy. In this study we compare the
accuracy four different parameterizations models: Two analytical models derived
from the Bethe equation, and two different interpolation schemes applied to
range-energy tables. In conclusion, a spline interpolation scheme yields the
highest reproduction accuracy, while the shape of the energy loss-curve is best
reproduced with the differentiated Bragg-Kleeman equation.
"
"  The recent rapid progress in observations of circumstellar disks and
extrasolar planets has reinforced the importance of understanding an intimate
coupling between star and planet formation. Under such a circumstance, it may
be invaluable to attempt to specify when and how planet formation begins in
star-forming regions and to identify what physical processes/quantities are the
most significant to make a link between star and planet formation. To this end,
we have recently developed a couple of projects. These include an observational
project about dust growth in Class 0 YSOs and a theoretical modeling project of
the HL Tauri disk. For the first project, we utilize the archive data of radio
interferometric observations, and examine whether dust growth, a first step of
planet formation, occurs in Class 0 YSOs. We find that while our observational
results can be reproduced by the presence of large ($\sim$ mm) dust grains for
some of YSOs under the single-component modified blackbody formalism, an
interpretation of no dust growth would be possible when a more detailed model
is used. For the second project, we consider an origin of the disk
configuration around HL Tauri, focusing on magnetic fields. We find that
magnetically induced disk winds may play an important role in the HL Tauri
disk. The combination of these attempts may enable us to move towards a
comprehensive understanding of how star and planet formation are intimately
coupled with each other.
"
"  The observed constraints on the variability of the proton to electron mass
ratio $\mu$ and the fine structure constant $\alpha$ are used to establish
constraints on the variability of the Quantum Chromodynamic Scale and a
combination of the Higgs Vacuum Expectation Value and the Yukawa couplings.
Further model dependent assumptions provide constraints on the Higgs VEV and
the Yukawa couplings separately. A primary conclusion is that limits on the
variability of dimensionless fundamental constants such as $\mu$ and $\alpha$
provide important constraints on the parameter space of new physics and
cosmologies.
"
"  We study the role of environment in the evolution of central and satellite
galaxies with the Sloan Digital Sky Survey. We begin by studying the size-mass
relation, replicating previous studies, which showed no difference between the
sizes of centrals and satellites at fixed stellar mass, before turning our
attention to the size-core velocity dispersion ($\sigma_0$) and mass-$\sigma_0$
relations. By comparing the median size and mass of the galaxies at fixed
velocity dispersion we find that the central galaxies are consistently larger
and more massive than their satellite counterparts in the quiescent population.
In the star forming population we find there is no difference in size and only
a small difference in mass. To analyse why these difference may be present we
investigate the radial mass profiles and stellar metallicity of the galaxies.
We find that in the cores of the galaxies there is no difference in mass
surface density between centrals and satellites, but there is a large
difference at larger radii. We also find almost no difference between the
stellar metallicity of centrals and satellites when they are separated into
star forming and quiescent groups. Under the assumption that $\sigma_0$ is
invariant to environmental processes, our results imply that central galaxies
are likely being increased in mass and size by processes such as minor mergers,
particularly at high $\sigma_0$, while satellites are being slightly reduced in
mass and size by tidal stripping and harassment, particularly at low
$\sigma_0$, all of which predominantly affect the outer regions of the
galaxies.
"
"  In the increasing interests on spin-orbit torque (SOT) with various magnetic
materials, we investigated SOT in rare earth-transition metal ferrimagnetic
alloys. The harmonic Hall measurements were performed in Pt/GdFeCo bilayers to
quantify the effective fields resulting from the SOT. It is found that the
damping-like torque rapidly increases near the magnetization compensation
temperature TM of the GdFeCo, which is attributed to the reduction of the net
magnetic moment.
"
"  A major challenge in solar and heliospheric physics is understanding how
highly localized regions, far smaller than 1 degree at the Sun, are the source
of solar-wind structures spanning more than 20 degrees near Earth. The Sun's
atmosphere is divided into magnetically open regions, coronal holes, where
solar-wind plasma streams out freely and fills the solar system, and closed
regions, where the plasma is confined to coronal loops. The boundary between
these regions extends outward as the heliospheric current sheet (HCS).
Measurements of plasma composition imply that the solar wind near the HCS, the
so-called slow solar wind, originates in closed regions, presumably by the
processes of field-line opening or interchange reconnection. Mysteriously,
however, slow wind is also often seen far from the HCS. We use high-resolution,
three-dimensional magnetohydrodynamic simulations to calculate the dynamics of
a coronal hole whose geometry includes a narrow corridor flanked by closed
field and which is driven by supergranule-like flows at the coronal-hole
boundary. We find that these dynamics result in the formation of giant arcs of
closed-field plasma that extend far from the HCS and span tens of degrees in
latitude and longitude at Earth, accounting for the slow solar wind
observations.
"
"  In line with its terms of reference the ICFA Neutrino Panel has developed a
roadmapfor the international, accelerator-based neutrino programme. A ""roadmap
discussion document"" was presented in May 2016 taking into account the
peer-group-consultation described in the Panel's initial report. The ""roadmap
discussion document"" was used to solicit feedback from the neutrino
community---and more broadly, the particle- and astroparticle-physics
communities---and the various stakeholders in the programme. The roadmap, the
conclusions and recommendations presented in this document take into account
the comments received following the publication of the roadmap discussion
document.
With its roadmap the Panel documents the approved objectives and milestones
of the experiments that are presently in operation or under construction.
Approval, construction and exploitation milestones are presented for
experiments that are being considered for approval. The timetable proposed by
the proponents is presented for experiments that are not yet being considered
formally for approval. Based on this information, the evolution of the
precision with which the critical parameters governinger the neutrino are known
has been evaluated. Branch or decision points have been identified based on the
anticipated evolution in precision. The branch or decision points have in turn
been used to identify desirable timelines for the neutrino-nucleus cross
section and hadro-production measurements that are required to maximise the
integrated scientific output of the programme. The branch points have also been
used to identify the timeline for the R&D required to take the programme beyond
the horizon of the next generation of experiments. The theory and phenomenology
programme, including nuclear theory, required to ensure that maximum benefit is
derived from the experimental programme is also discussed.
"
"  We present a tutorial on the determination of the physical conditions and
chemical abundances in gaseous nebulae. We also include a brief review of
recent results on the study of gaseous nebulae, their relevance for the study
of stellar evolution, galactic chemical evolution, and the evolution of the
universe. One of the most important problems in abundance determinations is the
existence of a discrepancy between the abundances determined with collisionally
excited lines and those determined by recombination lines, this is called the
ADF (abundance discrepancy factor) problem; we review results related to this
problem. Finally, we discuss possible reasons for the large t$^2$ values
observed in gaseous nebulae.
"
"  We consider the factorization problem of matrix symbols relative to a closed
contour, i.e., a Riemann-Hilbert problem, where the symbol depends analytically
on parameters. We show how to define a function $\tau$ which is locally
analytic on the space of deformations and that is expressed as a Fredholm
determinant of an operator of ""integrable"" type in the sense of
Its-Izergin-Korepin-Slavnov. The construction is not unique and the
non-uniqueness highlights the fact that the tau function is really the section
of a line bundle.
"
"  We use the LDA+U approach to search for possible ordered ground states of
LaSrCoO$_4$. We find a staggered arrangement of magnetic multipoles to be
stable over a broad range of Co $3d$ interaction parameters. This ordered state
can be described as a spin-denity-wave-type condensate of $d_{xy} \otimes
d_{x^2-y^2}$ excitons carrying spin $S=1$. Further, we construct an effective
strong-coupling model, calculate the exciton dispersion and investigate closing
of the exciton gap, which marks the exciton condensation instability. Comparing
the layered LaSrCoO$_4$ with its pseudo cubic analog LaCoO$_3$, we find that
for the same interaction parameters the excitonic gap is smaller (possibly
vanishing) in the layered cobaltite.
"
"  In the last decades a vaste amount of evidence for the existence of dark
matter has been accumulated. At the same time, many efforts have been
undertaken to try to identify what dark matter is. Indirect searches look at
places in the Universe where dark matter is believed to be abundant and seek
for possible annihilation or decay signatures. The Cherenkov Telescope Array
(CTA) represents the next generation of imaging Cherenkov telescopes and, with
one site in the Southern hemisphere and one in the Northern hemisphere, will be
able to observe all the sky with unprecedented sensitivity and angular
resolution above a few tens of GeV. The CTA Consortium will undertake an
ambitious program of indirect dark matter searches for which we report here the
brightest prospects.
"
"  Nuclear starburst discs (NSDs) are star-forming discs that may be residing in
the nuclear regions of active galaxies at intermediate redshifts. One
dimensional (1D) analytical models developed by Thompson et al. (2005) show
that these discs can possess an inflationary atmosphere when dust is sublimated
on parsec scales. This make NSDs a viable source for AGN obscuration. We model
the two dimensional (2D) structure of NSDs using an iterative method in order
to compute the explicit vertical solutions for a given annulus. These solutions
satisfy energy and hydrostatic balance, as well as the radiative transfer
equation. In comparison to the 1D model, the 2D calculation predicts a less
extensive expansion of the atmosphere by orders of magnitude at the
parsec/sub-parsec scale, but the new scale-height $h$ may still exceed the
radial distance $R$ for various physical conditions. A total of 192 NSD models
are computed across the input parameter space in order to predict distributions
of a line of sight column density $N_H$. Assuming a random distribution of
input parameters, the statistics yield 56% of Type 1, 23% of Compton-thin Type
2s (CN), and 21% of Compton-thick (CK) AGNs. Depending on a viewing angle
($\theta$) of a particular NSD (fixed physical conditions), any central AGN can
appear to be Type 1, CN, or CK which is consistent with the basic unification
theory of AGNs. Our results show that $\log[N_H(\text{cm}^{-2})]\in$ [23,25.5]
can be oriented at any $\theta$ from 0$^\circ$ to $\approx$80$^\circ$ due to
the degeneracy in the input parameters.
"
"  Polyethylene Naphtalate (PEN) is a mechanically very favorable polymer.
Earlier it was found that thin foils made from PEN can have very high
radio-purity compared to other commercially available foils. In fact, PEN is
already in use for low background signal transmission applications (cables).
Recently it has been realized that PEN also has favorable scintillating
properties. In combination, this makes PEN a very promising candidate as a
self-vetoing structural material in low background experiments. Components
instrumented with light detectors could be built from PEN. This includes
detector holders, detector containments, signal transmission links, etc. The
current R\&D towards qualification of PEN as a self-vetoing low background
structural material is be presented.
"
"  The derivation of approximate wave functions for an electron submitted to
both a coulomb and a time-dependent laser electric fields, the so-called
Coulomb-Volkov (CV) state, is addressed. Despite its derivation for continuum
states does not exhibit any particular problem within the framework of the
standard theory of quantum mechanics (QM), difficulties arise when considering
an initially bound atomic state. Indeed the natural way of translating the
unperturbed momentum by the laser vector potential is no longer possible since
a bound state does not exhibit a plane wave form including explicitely a
momentum. The use of a fractal space permits to naturally define a momentum for
a bound wave function. Within this framework, it is shown how the derivation of
laser-dressed bound states can be performed. Based on a generalized eikonal
approach, a new expression for the laser-dressed states is also derived, fully
symmetric relative to the continuum or bound nature of the initial unperturbed
wave function. It includes an additional crossed term in the Volkov phase which
was not obtained within the standard theory of quantum mechanics. The
derivations within this fractal framework have highlighted other possible ways
to derive approximate laser-dressed states in QM. After comparing the various
obtained wave functions, an application to the prediction of the ionization
probability of hydrogen targets by attosecond XUV pulses within the sudden
approximation is provided. This approach allows to make predictions in various
regimes depending on the laser intensity, going from the non-resonant
multiphoton absorption to tunneling and barrier-suppression ionization.
"
"  'Oumuamua, the first bona-fide interstellar planetesimal, was discovered
passing through our Solar System on a hyperbolic orbit. This object was likely
dynamically ejected from an extrasolar planetary system after a series of close
encounters with gas giant planets. To account for 'Oumuamua's detection, simple
arguments suggest that ~1 Earth mass of planetesimals are ejected per Solar
mass of Galactic stars. However, that value assumes mono-sized planetesimals.
If the planetesimal mass distribution is instead top-heavy the inferred mass in
interstellar planetesimals increases to an implausibly high value. The tension
between theoretical expectations for the planetesimal mass function and the
observation of 'Oumuamua can be relieved if a small fraction (~0.1-1%) of
planetesimals are tidally disrupted on the pathway to ejection into
'Oumuamua-sized fragments. Using a large suite of simulations of giant planet
dynamics including planetesimals, we confirm that 0.1-1% of planetesimals pass
within the tidal disruption radius of a gas giant on their pathway to ejection.
'Oumuamua may thus represent a surviving fragment of a disrupted planetesimal.
Finally, we argue that an asteroidal composition is dynamically disfavoured for
'Oumuamua, as asteroidal planetesimals are both less abundant and ejected at a
lower efficiency than cometary planetesimals.
"
"  We present a systematical study via scanning tunneling microscopy (STM) and
low-energy electron diffraction (LEED) on the effect of the exposure of Lithium
(Li) on graphene on silicon carbide (SiC). We have investigated Li deposition
both on epitaxial monolayer graphene and on buffer layer surfaces on the
Si-face of SiC. At room temperature, Li immediately intercalates at the
interface between the SiC substrate and the buffer layer and transforms the
buffer layer into a quasi-free-standing graphene. This conclusion is
substantiated by LEED and STM evidence. We show that intercalation occurs
through the SiC step sites or graphene defects. We obtain a good quantitative
agreement between the number of Li atoms deposited and the number of available
Si bonds at the surface of the SiC crystal. Through STM analysis, we are able
to determine the interlayer distance induced by Li-intercalation at the
interface between the SiC substrate and the buffer layer.
"
"  The atmospheres of exoplanets reveal all their properties beyond mass,
radius, and orbit. Based on bulk densities, we know that exoplanets larger than
1.5 Earth radii must have gaseous envelopes, hence atmospheres. We discuss
contemporary techniques for characterization of exoplanetary atmospheres. The
measurements are difficult, because - even in current favorable cases - the
signals can be as small as 0.001-percent of the host star's flux. Consequently,
some early results have been illusory, and not confirmed by subsequent
investigations. Prominent illusions to date include polarized scattered light,
temperature inversions, and the existence of carbon planets. The field moves
from the first tentative and often incorrect conclusions, converging to the
reality of exoplanetary atmospheres. That reality is revealed using transits
for close-in exoplanets, and direct imaging for young or massive exoplanets in
distant orbits. Several atomic and molecular constituents have now been
robustly detected in exoplanets as small as Neptune. In our current
observations, the effects of clouds and haze appear ubiquitous. Topics at the
current frontier include the measurement of heavy element abundances in giant
planets, detection of carbon-based molecules, measurement of atmospheric
temperature profiles, definition of heat circulation efficiencies for tidally
locked planets, and the push to detect and characterize the atmospheres of
super-Earths. Future observatories for this quest include the James Webb Space
Telescope, and the new generation of Extremely Large Telescopes on the ground.
On a more distant horizon, NASA's concepts for the HabEx and LUVOIR missions
could extend the study of exoplanetary atmospheres to true twins of Earth.
"
"  Molecular adsorption on surfaces plays an important part in catalysis,
corrosion, desalination, and various other processes that are relevant to
industry and in nature. As a complement to experiments, accurate adsorption
energies can be obtained using various sophisticated electronic structure
methods that can now be applied to periodic systems. The adsorption energy of
water on boron nitride substrates, going from zero to 2-dimensional
periodicity, is particularly interesting as it calls for an accurate treatment
of polarizable electrostatics and dispersion interactions, as well as posing a
practical challenge to experiments and electronic structure methods. Here, we
present reference adsorption energies, static polarizabilities, and dynamic
polarizabilities, for water on BN substrates of varying size and dimension.
Adsorption energies are computed with coupled cluster theory, fixed-node
quantum Monte Carlo (FNQMC), the random phase approximation (RPA), and second
order M{\o}ller-Plesset (MP2) theory. These explicitly correlated methods are
found to agree in molecular as well as periodic systems. The best estimate of
the water/h-BN adsorption energy is $-107\pm7$ meV from FNQMC. In addition, the
water adsorption energy on the BN substrates could be expected to grow
monotonically with the size of the substrate due to increased dispersion
interactions but interestingly, this is not the case here. This peculiar
finding is explained using the static polarizabilities and molecular dispersion
coefficients of the systems, as computed from time-dependent density functional
theory (DFT). Dynamic as well as static polarizabilities are found to be highly
anisotropic in these systems. In addition, the many-body dispersion method in
DFT emerges as a particularly useful estimation of finite size effects for
other expensive, many-body wavefunction based methods.
"
"  We theoretically investigate the mechanism to generate large intrinsic spin
Hall effect in iridates or more broadly in 5d transition metal oxides with
strong spin-orbit coupling. We demonstrate such a possibility by taking the
example of orthorhombic perovskite iridate with nonsymmorphic lattice symmetry,
SrIrO$_3$, which is a three-dimensional semimetal with nodal line spectrum. It
is shown that large intrinsic spin Hall effect arises in this system via the
spin-Berry curvature originating from the nearly degenerate electronic spectra
surrounding the nodal line. This effect exists even when the nodal line is
gently gapped out, due to the persistent nearly degenerate electronic
structure, suggesting a distinct robustness. The magnitude of the spin Hall
conductivity is shown to be comparable to the best known example such as doped
topological insulators and the biggest in any transition metal oxides. To gain
further insight, we compute the intrinsic spin Hall conductivity in both of the
bulk and thin film systems. We find that the geometric confinement in thin
films leads to significant modifications of the electronic states, leading to
even bigger spin Hall conductivity in certain cases. We compare our findings
with the recent experimental report on the discovery of large spin Hall effect
in SrIrO$_3$ thin films.
"
"  We present an extragalactic survey using observations from the Atacama Large
Millimeter/submillimeter Array (ALMA) to characterise galaxy populations up to
$z=0.35$: the Valparaíso ALMA Line Emission Survey (VALES). We use ALMA
Band-3 CO(1--0) observations to study the molecular gas content in a sample of
67 dusty normal star-forming galaxies selected from the $Herschel$
Astrophysical Terahertz Large Area Survey ($H$-ATLAS). We have spectrally
detected 49 galaxies at $>5\sigma$ significance and 12 others are seen at low
significance in stacked spectra. CO luminosities are in the range of
$(0.03-1.31)\times10^{10}$ K km s$^{-1}$ pc$^2$, equivalent to $\log({\rm
M_{gas}/M_{\odot}}) =8.9-10.9$ assuming an $\alpha_{\rm CO}$=4.6(K km s$^{-1}$
pc$^{2}$)$^{-1}$, which perfectly complements the parameter space previously
explored with local and high-z normal galaxies. We compute the optical to CO
size ratio for 21 galaxies resolved by ALMA at $\sim 3$.""$5$ resolution (6.5
kpc), finding that the molecular gas is on average $\sim$ 0.6 times more
compact than the stellar component. We obtain a global Schmidt-Kennicutt
relation, given by $\log [\Sigma_{\rm SFR}/({\rm M_{\odot}
yr^{-1}kpc^{-2}})]=(1.26 \pm 0.02) \times \log [\Sigma_{\rm M_{H2}}/({\rm
M_{\odot}\,pc^{-2}})]-(3.6 \pm 0.2)$. We find a significant fraction of
galaxies lying at `intermediate efficiencies' between a long-standing mode of
star-formation activity and a starburst, specially at $\rm L_{IR}=10^{11-12}
L_{\odot}$. Combining our observations with data taken from the literature, we
propose that star formation efficiencies can be parameterised by $\log [{\rm
SFR/M_{H2}}]=0.19 \times {\rm (\log {L_{IR}}-11.45)}-8.26-0.41 \times
\arctan[-4.84 (\log {\rm L_{IR}}-11.45) ]$. Within the redshift range we
explore ($z<0.35$), we identify a rapid increase of the gas content as a
function of redshift.
"
"  The Doppler tracking data of the Chang'e 3 lunar mission is used to constrain
the stochastic background of gravitational wave in cosmology within the 1 mHz
to 0.05 Hz frequency band. Our result improves on the upper bound on the energy
density of the stochastic background of gravitational wave in the 0.02 Hz to
0.05 Hz band obtained by the Apollo missions, with the improvement reaching
almost one order of magnitude at around 0.05 Hz. Detailed noise analysis of the
Doppler tracking data is also presented, with the prospect that these noise
sources will be mitigated in future Chinese deep space missions. A feasibility
study is also undertaken to understand the scientific capability of the Chang'e
4 mission, due to be launched in 2018, in relation to the stochastic
gravitational wave background around 0.01 Hz. The study indicates that the
upper bound on the energy density may be further improved by another order of
magnitude from the Chang'e 3 mission, which will fill the gap in the frequency
band from 0.02 Hz to 0.1 Hz in the foreseeable future.
"
"  We numerically investigate the electronic transport properties of graphene
nanoribbons and carbon nanotubes with inter-valley coupling, e.g., in \sqrt{3}N
\times \sqrt{3}N and 3N \times 3N superlattices. By taking the \sqrt{3} \times
\sqrt{3} graphene superlattice as an example, we show that tailoring the bulk
graphene superlattice results in rich structural configurations of nanoribbons
and nanotubes. After studying the electronic characteristics of the
corresponding armchair and zigzag nanoribbon geometries, we find that the
linear bands of carbon nanotubes can lead to the Klein tunnelling-like
phenomenon, i.e., electrons propagate along tubes without backscattering even
in the presence of a barrier. Due to the coupling between K and K' valleys of
pristine graphene by \sqrt{3} \times \sqrt{3} supercells,we propose a
valley-field-effect transistor based on the armchair carbon nanotube, where the
valley polarization of the current can be tuned by applying a gate voltage or
varying the length of the armchair carbon nanotubes.
"
"  The Short-Baseline Neutrino (SBN) Program is a short-baseline neutrino
oscillation experiment in the Booster Neutrino Beam-line (BNB) at Fermilab. It
consists of three Liquid Argon Time Projection Chambers (LArTPCs) from the
Short-Baseline Near Detector (SBND), Micro Booster Neutrino Experiment
(MicroBooNE), and Imaging Cosmic And Rare Underground Signals (ICARUS)
experiments. The SBN Program will definitively search for short-baseline
neutrino oscillations in the 1 eV mass range, make precision neutrino-argon
interaction measurements, and further develop the LArTPC technology. The
physics program and current status of the program, and its constituent
experiments, are presented.
"
"  The J-integral is recognized as a fundamental parameter in fracture mechanics
that characterizes the inherent resistance of materials to crack growth.
However, the conventional methods to calculate the J-integral, which require
knowledge of the exact position of a crack tip and the continuum fields around
it, are unable to precisely measure the J-integral of polymer composites at the
nanoscale. This work aims to propose an effective calculation method based on
coarse-grained (CG) simulations for predicting the J-integral of carbon
nanotube (CNT)/polymer composites. In the proposed approach, the J-integral is
determined from the load displacement curve of a single specimen. The
distinguishing feature of the method is the calculation of J-integral without
need of information about the crack tip, which makes it applicable to complex
polymer systems. The effects of the CNT weight fraction and covalent
cross-links between the polymer matrix and nanotubes, and polymer chains on the
fracture behavior of the composites are studied in detail. The dependence of
the J-integral on the crack length and the size of representative volume
element (RVE) is also explored.
"
"  In this article, we discuss a verification study of an operational solar
flare forecast in the Regional Warning Center (RWC) Japan. The RWC Japan has
been issuing four-categorical deterministic solar flare forecasts for a long
time. In this forecast verification study, we used solar flare forecast data
accumulated over 16 years (from 2000 to 2015). We compiled the forecast data
together with solar flare data obtained with the Geostationary Operational
Environmental Satellites (GOES). Using the compiled data sets, we estimated
some conventional scalar verification measures with 95% confidence intervals.
We also estimated a multi-categorical scalar verification measure. These scalar
verification measures were compared with those obtained by the persistence
method and recurrence method. As solar activity varied during the 16 years, we
also applied verification analyses to four subsets of forecast-observation pair
data with different solar activity levels. We cannot conclude definitely that
there are significant performance difference between the forecasts of RWC Japan
and the persistence method, although a slightly significant difference is found
for some event definitions. We propose to use a scalar verification measure to
assess the judgment skill of the operational solar flare forecast. Finally, we
propose a verification strategy for deterministic operational solar flare
forecasting.
"
"  It is shown that the total set of equations, which determines the dynamics of
the domain bounds (DB) in a weak ferromagnet, has the same type of specific
solution as the well-known Walker's solution for ferromagnets. We calculated
the functional dependence of the velocity of the DB on the magnetic field,
which is described by the obtained solution. This function has a maximum at a
finite field and a section of the negative differential mobility of the DB.
According to the calculation, the maximum velocity $ c \approx 2 \times 10^6$
cm/sec in YFeO$_3$ is reached at $H_m \approx 4 \times 10^3$ Oe.
"
"  New numerical solutions to the so-called selection problem for one and two
steadily translating bubbles in an unbounded Hele-Shaw cell are presented. Our
approach relies on conformal mapping which, for the two-bubble problem,
involves the Schottky-Klein prime function associated with an annulus. We show
that a countably infinite number of solutions exist for each fixed value of
dimensionless surface tension, with the bubble shapes becoming more exotic as
the solution branch number increases. Our numerical results suggest that a
single solution is selected in the limit that surface tension vanishes, with
the scaling between the bubble velocity and surface tension being different to
the well-studied problems for a bubble or a finger propagating in a channel
geometry.
"
"  We report on SPT-CLJ2011-5228, a giant system of arcs created by a cluster at
$z=1.06$. The arc system is notable for the presence of a bright central image.
The source is a Lyman Break galaxy at $z_s=2.39$ and the mass enclosed within
the 14 arc second radius Einstein ring is $10^{14.2}$ solar masses. We perform
a full light profile reconstruction of the lensed images to precisely infer the
parameters of the mass distribution. The brightness of the central image
demands that the central total density profile of the lens be shallow. By
fitting the dark matter as a generalized Navarro-Frenk-White profile---with a
free parameter for the inner density slope---we find that the break radius is
$270^{+48}_{-76}$ kpc, and that the inner density falls with radius to the
power $-0.38\pm0.04$ at 68 percent confidence. Such a shallow profile is in
strong tension with our understanding of relaxed cold dark matter halos; dark
matter only simulations predict the inner density should fall as $r^{-1}$. The
tension can be alleviated if this cluster is in fact a merger; a two halo model
can also reconstruct the data, with both clumps (density going as $r^{-0.8}$
and $r^{-1.0}$) much more consistent with predictions from dark matter only
simulations. At the resolution of our Dark Energy Survey imaging, we are unable
to choose between these two models, but we make predictions for forthcoming
Hubble Space Telescope imaging that will decisively distinguish between them.
"
"  The paper aims to apply the complex octonion to explore the influence of the
energy gradient on the Eotvos experiment, impacting the gravitational mass in
the ultra-strong magnetic fields. Until now the Eotvos experiment has never
been validated under the ultra-strong magnetic field. It is aggravating the
existing serious qualms about the Eotvos experiment. According to the
electromagnetic and gravitational theory described with the complex octonions,
the ultra-strong magnetic field must result in a tiny variation of the
gravitational mass. The magnetic field with the gradient distribution will
generate the energy gradient. These influencing factors will exert an influence
on the state of equilibrium in the Eotvos experiment. That is, the
gravitational mass will depart from the inertial mass to a certain extent, in
the ultra-strong magnetic fields. Only under exceptional circumstances,
especially in the case of the weak field strength, the gravitational mass may
be equal to the inertial mass approximately. The paper appeals intensely to
validate the Eotvos experiment in the ultra-strong electromagnetic strengths.
It is predicted that the physical property of gravitational mass will be
distinct from that of inertial mass.
"
"  Thanks to multi-spacecraft mission, it has recently been possible to directly
estimate the current density in space plasmas, by using magnetic field time
series from four satellites flying in a quasi perfect tetrahedron
configuration. The technique developed, commonly called 'curlometer' permits a
good estimation of the current density when the magnetic field time series vary
linearly in space. This approximation is generally valid for small spacecraft
separation. The recent space missions Cluster and Magnetospheric Multiscale
(MMS) have provided high resolution measurements with inter-spacecraft
separation up to 100 km and 10 km, respectively. The former scale corresponds
to the proton gyroradius/ion skin depth in 'typical' solar wind conditions,
while the latter to sub-proton scale. However, some works have highlighted an
underestimation of the current density via the curlometer technique with
respect to the current computed directly from the velocity distribution
functions, measured at sub-proton scales resolution with MMS. In this paper we
explore the limit of the curlometer technique studying synthetic data sets
associated to a cluster of four artificial satellites allowed to fly in a
static turbulent field, spanning a wide range of relative separation. This
study tries to address the relative importance of measuring plasma moments at
very high resolution from a single spacecraft with respect to the
multi-spacecraft missions in the current density evaluation.
"
"  High-signal to noise observations of the Ly$\alpha$ forest transmissivity in
the z = 7.085 QSO ULAS J1120+0641 show seven narrow transmission spikes
followed by a long 240 cMpc/h trough. Here we use radiative transfer
simulations of cosmic reionization previously calibrated to match a wider range
of Ly$\alpha$ forest data to show that the occurrence of seven transmission
spikes in the narrow redshift range z = 5.85 - 6.1 is very sensitive to the
exact timing of reionization. Occurrence of the spikes requires the most under
dense regions of the IGM to be already fully ionised. The rapid onset of a long
trough at z = 6.12 requires a strong decrease of the photo-ionisation rate at
z$\sim$6.1 in this line-of-sight, consistent with the end of percolation at
this redshift. The narrow range of reionisation histories that we previously
found to be consistent with a wider range of Ly$\alpha$ forest data have a
reasonable probability of showing seven spikes and the mock absorption spectra
provide an excellent match to the spikes and the trough in the observed
spectrum of ULAS J1120+0641. Despite the large overall opacity of Ly$\alpha$ at
z > 5.8, larger samples of high signal-to-noise observations of rare
transmission spikes should therefore provide important further insights into
the exact timing of the percolation of HII bubbles at the tail-end of
reionization
"
"  The intermediate-valence compound SmB6 is a well-known Kondo insulator, in
which hybridization of itinerant 5d electrons with localized 4f electrons leads
to a transition from metallic to insulating behavior at low temperatures.
Recent studies suggest that SmB6 is a topological insulator, with topological
metallic surface states emerging from a fully insulating hybridized bulk band
structure. Here we locally probe the bulk magnetic properties of pure and 0.5 %
Fe-doped SmB6 by muon spin rotation/relaxation methods. Below 6 K the Fe
impurity induces simultaneous changes in the bulk local magnetism and the
electrical conductivity. In the low-temperature insulating bulk state we
observe a temperature-independent dynamic relaxation rate indicative of
low-lying magnetic excitations driven primarily by quantum fluctuations.
"
"  Recently, an open geometry Fourier modal method based on a new combination of
an open boundary condition and a non-uniform $k$-space discretization was
introduced for rotationally symmetric structures providing a more efficient
approach for modeling nanowires and micropillar cavities [J. Opt. Soc. Am. A
33, 1298 (2016)]. Here, we generalize the approach to three-dimensional (3D)
Cartesian coordinates allowing for the modeling of rectangular geometries in
open space. The open boundary condition is a consequence of having an infinite
computational domain described using basis functions that expand the whole
space. The strength of the method lies in discretizing the Fourier integrals
using a non-uniform circular ""dartboard"" sampling of the Fourier $k$ space. We
show that our sampling technique leads to a more accurate description of the
continuum of the radiation modes that leak out from the structure. We also
compare our approach to conventional discretization with direct and inverse
factorization rules commonly used in established Fourier modal methods. We
apply our method to a variety of optical waveguide structures and demonstrate
that the method leads to a significantly improved convergence enabling more
accurate and efficient modeling of open 3D nanophotonic structures.
"
"  Recent space missions have provided information on the physical and chemical
properties of interstellar grains such as the ratio $\beta$ of radiation
pressure to gravity acting on the grains in addition to the composition,
structure, and size distribution of the grains. Numerical simulation on the
trajectories of interstellar grains captured by Stardust and returned to Earth
constrained the $\beta$ ratio for the Stardust samples of interstellar origin.
However, recent accurate calculations of radiation pressure cross sections for
model dust grains have given conflicting stories in the $\beta$ ratio of
interstellar grains. The $\beta$ ratio for model dust grains of so-called
""astronomical silicate"" in the femto-kilogram range lies below unity, in
conflict with $\beta \sim 1$ for the Stardust interstellar grains. Here, I
tackle this conundrum by re-evaluating the $\beta$ ratio of interstellar grains
on the assumption that the grains are aggregated particles grown by coagulation
and composed of amorphous MgSiO$_{3}$ with the inclusion of metallic iron. My
model is entirely consistent with the depletion and the correlation of major
rock-forming elements in the Local Interstellar Cloud surrounding the Sun and
the mineralogical identification of interstellar grains in the Stardust and
Cassini missions. I find that my model dust particles fulfill the constraints
on the $\beta$ ratio derived from not only the Stardust mission but also the
Ulysses and Cassini missions. My results suggest that iron is not incorporated
into silicates but exists as metal, contrary to the majority of interstellar
dust models available to date.
"
"  Titanium dioxide (TiO2) is a wide band gap semiconducting material which is
promising for photocatalysis. Here we present first-principles calculations to
study the pressure dependence of structural and electronic properties of two
TiO2 phases: the cotunnite-type and the Fe2P-type structure. The band gaps are
calculated using density functional theory (DFT) with the generalized gradient
approximation (GGA), as well as the many-body perturbation theory with the GW
approximation. The band gaps of both phases are found to be unexpectedly robust
across a broad range pressures. The corresponding pressure coefficients are
significantly smaller than that of diamond and silicon carbide (SiC), whose
pressure coefficient is the smallest value ever measured by experiment. The
robustness originates from the synchronous change of valence band maximum (VBM)
and conduction band minimum (CBM) with nearly identical rates of changes. A
step-like jump of band gaps around the phase transition pressure point is
expected and understood in light of the difference in crystal structures.
"
"  The control of the electron spin by external means is a key issue for
spintronic devices. Using spin- and angle-resolved photoemission spectroscopy
(SARPES) with three-dimensional spin detection, we demonstrate operando
electrostatic spin manipulation in ferroelectric GeTe and multiferroic
Ge1-xMnxTe. We not only demonstrate for the first time electrostatic spin
manipulation in Rashba semiconductors due to ferroelectric polarization
reversal, but are also able to follow the switching pathway in detail, and show
a gain of the Rashba-splitting strength under external fields. In multiferroic
Ge1-xMnxTe operando SARPES reveals switching of the perpendicular spin
component due to electric field induced magnetization reversal. This provides
firm evidence of effective multiferroic coupling which opens up magnetoelectric
functionality with a multitude of spin-switching paths in which the magnetic
and electric order parameters are coupled through ferroelastic relaxation
paths. This work thus provides a new type of magnetoelectric switching
entangled with Rashba-Zeeman splitting in a multiferroic system.
"
"  We report on the observation of phase space modulations in the correlated
electron emission after strong field double ionization of helium using laser
pulses with a wavelength of 394~nm and an intensity of $3\cdot10^{14}$W/cm$^2$.
Those modulations are identified as direct results of quantum mechanical
selection rules predicted by many theoretical calculations. They only occur for
an odd number of absorbed photons. By that we attribute this effect to the
parity of the continuum wave function.
"
"  The mid-infrared (MIR) spectral range, pertaining to important applications
such as molecular 'fingerprint' imaging, remote sensing, free space
telecommunication and optical radar, is of particular scientific interest and
technological importance. However, state-of-the-art materials for MIR detection
are limited by intrinsic noise and inconvenient fabrication processes,
resulting in high cost photodetectors requiring cryogenic operation. We report
black arsenic-phosphorus-based long wavelength infrared photodetectors with
room temperature operation up to 8.2 um, entering the second MIR atmospheric
transmission window. Combined with a van der Waals heterojunction, room
temperature specific detectivity higher than 4.9*10^9 Jones was obtained in the
3-5 um range. The photodetector works in a zero-bias photovoltaic mode,
enabling fast photoresponse and low dark noise. Our van der Waals
heterojunction photodector not only exemplify black arsenic-phosphorus as a
promising candidate for MIR opto-electronic applications, but also pave the way
for a general strategy to suppress 1/f noise in photonic devices.
"
"  Networks of elastic fibers are ubiquitous in biological systems and often
provide mechanical stability to cells and tissues. Fiber reinforced materials
are also common in technology. An important characteristic of such materials is
their resistance to failure under load. Rupture occurs when fibers break under
excessive force and when that failure propagates. Therefore it is crucial to
understand force distributions. Force distributions within such networks are
typically highly inhomogeneous and are not well understood. Here we construct a
simple one-dimensional model system with periodic boundary conditions by
randomly placing linear springs on a circle. We consider ensembles of such
networks that consist of $N$ nodes and have an average degree of connectivity
$z$, but vary in topology. Using a graph-theoretical approach that accounts for
the full topology of each network in the ensemble, we show that, surprisingly,
the force distributions can be fully characterized in terms of the parameters
$(N,z)$. Despite the universal properties of such $(N,z)$-ensembles, our
analysis further reveals that a classical mean-field approach fails to capture
force distributions correctly. We demonstrate that network topology is a
crucial determinant of force distributions in elastic spring networks.
"
"  We use the coupled cluster method (CCM) to study a frustrated
spin-$\frac{1}{2}$ $J_{1}$--$J_{2}$--$J_{1}^{\perp}$ Heisenberg antiferromagnet
on a bilayer honeycomb lattice with $AA$ stacking. Both nearest-neighbor (NN)
and frustrating next-nearest-neighbor antiferromagnetic (AFM) exchange
interactions are present in each layer, with respective exchange coupling
constants $J_{1}>0$ and $J_{2} \equiv \kappa J_{1} > 0$. The two layers are
coupled with NN AFM exchanges with coupling strength $J_{1}^{\perp}\equiv
\delta J_{1}>0$. We calculate to high orders of approximation within the CCM
the zero-field transverse magnetic susceptibility $\chi$ in the Néel phase.
We thus obtain an accurate estimate of the full boundary of the Néel phase in
the $\kappa\delta$ plane for the zero-temperature quantum phase diagram. We
demonstrate explicitly that the phase boundary derived from $\chi$ is fully
consistent with that obtained from the vanishing of the Néel magnetic order
parameter. We thus conclude that at all points along the Néel phase boundary
quasiclassical magnetic order gives way to a nonclassical paramagnetic phase
with a nonzero energy gap. The Néel phase boundary exhibits a marked
reentrant behavior, which we discuss in detail.
"
"  We develop a theory of weakly interacting fermionic atoms in shaken optical
lattices based on the orbital mixing in the presence of time-periodic
modulations. Specifically, we focus on fermionic atoms in circularly shaken
square lattice with near resonance frequencies, i.e., tuned close to the energy
separation between $s$-band and the $p$-bands. First, we derive a
time-independent four-band effective Hamiltonian in the non-interacting limit.
Diagonalization of the effective Hamiltonian yields a quasi-energy spectrum
consistent with the full numerical Floquet solution that includes all higher
bands. In particular, we find that the hybridized $s$-band develops multiple
minima and therefore non-trivial Fermi surfaces at different fillings. We then
obtain the effective interactions for atoms in the hybridized $s$-band
analytically and show that they acquire momentum dependence on the Fermi
surface even though the bare interaction is contact-like. We apply the theory
to find the phase diagram of fermions with weak attractive interactions and
demonstrate that the pairing symmetry is $s+d$-wave. Our theory is valid for a
range of shaking frequencies near resonance, and it can be generalized to other
phases of interacting fermions in shaken lattices.
"
"  We use a secular model to describe the non-resonant dynamics of
trans-Neptunian objects in the presence of an external ten-earth-mass
perturber. The secular dynamics is analogous to an ""eccentric Kozai mechanism""
but with both an inner component (the four giant planets) and an outer one (the
eccentric distant perturber). By the means of Poincaré sections, the cases of
a non-inclined or inclined outer planet are successively studied, making the
connection with previous works. In the inclined case, the problem is reduced to
two degrees of freedom by assuming a non-precessing argument of perihelion for
the perturbing body.
The size of the perturbation is typically ruled by the semi-major axis of the
small body: we show that the classic integrable picture is still valid below
about 70 AU, but it is progressively destroyed when we get closer to the
external perturber. In particular, for a>150 AU, large-amplitude orbital flips
become possible, and for a>200 AU, the Kozai libration islands are totally
submerged by the chaotic sea. Numerous resonance relations are highlighted. The
most large and persistent ones are associated to apsidal alignments or
anti-alignments with the orbit of the distant perturber.
"
"  We present a novel analysis of the metal-poor star sample in the complete
Radial Velocity Experiment (RAVE) Data Release 5 catalog with the goal of
identifying and characterizing all very metal-poor stars observed by the
survey. Using a three-stage method, we first identified the candidate stars
using only their spectra as input information. We employed an algorithm called
t-SNE to construct a low-dimensional projection of the spectrum space and
isolate the region containing metal-poor stars. Following this step, we
measured the equivalent widths of the near-infrared CaII triplet lines with a
method based on flexible Gaussian processes to model the correlated noise
present in the spectra. In the last step, we constructed a calibration relation
that converts the measured equivalent widths and the color information coming
from the 2MASS and WISE surveys into metallicity and temperature estimates. We
identified 877 stars with at least a 50% probability of being very metal-poor
$(\rm [Fe/H] < -2\,\rm dex)$, out of which 43 are likely extremely metal-poor
$(\rm [Fe/H] < -3\,\rm dex )$. The comparison of the derived values to a small
subsample of stars with literature metallicity values shows that our method
works reliably and correctly estimates the uncertainties, which typically have
values $\sigma_{\rm [Fe/H]} \approx 0.2\,\mathrm{dex}$. In addition, when
compared to the metallicity results derived using the RAVE DR5 pipeline, it is
evident that we achieve better accuracy than the pipeline and therefore more
reliably evaluate the very metal-poor subsample. Based on the repeated
observations of the same stars, our method gives very consistent results. The
method used in this work can also easily be extended to other large-scale data
sets, including to the data from the Gaia mission and the upcoming 4MOST
survey.
"
"  The technical details of a balloon stratospheric mission that is aimed at
measuring the Schumann resonances are described. The gondola is designed
specifically for the measuring of faint effects of ELF (Extremely Low Frequency
electromagnetic waves) phenomena. The prototype met the design requirements.
The ELF measuring system worked properly for entire mission; however, the level
of signal amplification that was chosen taking into account ground-level
measurements was too high. Movement of the gondola in the Earth magnetic field
induced the signal in the antenna that saturated the measuring system. This
effect will be taken into account in the planning of future missions. A large
telemetry dataset was gathered during the experiment and is currently under
processing. The payload consists also of biological material as well as
electronic equipment that was tested under extreme conditions.
"
"  A synoptic view on the long-established theory of light propagation in
crystalline dielectrics is presented, providing a new exact solution for the
microscopic local electromagnetic field thus disclosing the role of the
divergence-free (transversal) and curl-free (longitudinal) parts of the
electromagnetic field inside a material as a function of the density of
polarizable atoms. Our results enable fast and efficient calculation of the
photonic bandstructure and also the (non-local) dielectric tensor, solely with
the crystalline symmetry and atom-individual polarizabilities as input.
"
"  The nucleation and growth of calcite is an important research in scientific
and industrial field. Both the macroscopic and microscopic observation of
calcite growth have been reported. Now, with the development of microfluidic
device, we could focus the nucleation and growth of one single calcite. By
changing the flow rate of fluid, the concentration of fluid is controlled. We
introduced a new method to study calcite growth in situ and measured the growth
rate of calcite in microfluidic channel.
"
"  The Galactic magnetic field (GMF) plays a role in many astrophysical
processes and is a significant foreground to cosmological signals, such as the
Epoch of Reionization (EoR), but is not yet well understood. Dispersion and
Faraday rotation measurements (DMs and RMs, respectively) towards a large
number of pulsars provide an efficient method to probe the three-dimensional
structure of the GMF. Low-frequency polarisation observations with large
fractional bandwidth can be used to measure precise DMs and RMs. This is
demonstrated by a catalogue of RMs (corrected for ionospheric Faraday rotation)
from the Low Frequency Array (LOFAR), with a growing complementary catalogue in
the southern hemisphere from the Murchison Widefield Array (MWA). These data
further our knowledge of the three-dimensional GMF, particularly towards the
Galactic halo. Recently constructed or upgraded pathfinder and precursor
telescopes, such as LOFAR and the MWA, have reinvigorated low-frequency science
and represent progress towards the construction of the Square Kilometre Array
(SKA), which will make significant advancements in studies of astrophysical
magnetic fields in the future. A key science driver for the SKA-Low is to study
the EoR, for which pulsar and polarisation data can provide valuable insights
in terms of Galactic foreground conditions.
"
"  The mechanisms by which organs acquire their functional structure and realize
its maintenance (or homeostasis) over time are still largely unknown. In this
paper, we investigate this question on adipose tissue. Adipose tissue can
represent 20 to 50% of the body weight. Its investigation is key to overcome a
large array of metabolic disorders that heavily strike populations worldwide.
Adipose tissue consists of lobular clusters of adipocytes surrounded by an
organized collagen fiber network. By supplying substrates needed for
adipogenesis, vasculature was believed to induce the regroupment of adipocytes
near capillary extremities. This paper shows that the emergence of these
structures could be explained by simple mechanical interactions between the
adipocytes and the collagen fibers. Our assumption is that the fiber network
resists the pressure induced by the growing adipocytes and forces them to
regroup into clusters. Reciprocally, cell clusters force the fibers to merge
into a well-organized network. We validate this hypothesis by means of a
two-dimensional Individual Based Model (IBM) of interacting adipocytes and
extra-cellular-matrix fiber elements. The model produces structures that
compare quantitatively well to the experimental observations. Our model seems
to indicate that cell clusters could spontaneously emerge as a result of simple
mechanical interactions between cells and fibers and surprisingly, vasculature
is not directly needed for these structures to emerge.
"
"  We investigated an out-of-plane exchange bias system that is based on the
antiferromagnet MnN. Polycrystalline, highly textured film stacks of Ta / MnN /
CoFeB / MgO / Ta were grown on SiO$_x$ by (reactive) magnetron sputtering and
studied by x-ray diffraction and Kerr magnetometry. Nontrivial modifications of
the exchange bias and the perpendicular magnetic anisotropy were observed both
as functions of film thicknesses as well as field cooling temperatures. In
optimized film stacks, a giant perpendicular exchange bias of 3600 Oe and a
coercive field of 350 Oe were observed at room temperature. The effective
interfacial exchange energy is estimated to be $J_\mathrm{eff} = 0.24$ mJ/m$^2$
and the effective uniaxial anisotropy constant of the antiferromagnet is
$K_\mathrm{eff} = 24$ kJ/m$^3$. The maximum effective perpendicular anisotropy
field of the CoFeB layer is $H_\mathrm{ani} = 3400$ Oe. These values are larger
than any previously reported values. These results possibly open a route to
magnetically stable, exchange biased perpendicularly magnetized spin valves.
"
"  In electroencephalography (EEG) source imaging, the inverse source estimates
are depth biased in such a way that their maxima are often close to the
sensors. This depth bias can be quantified by inspecting the statistics (mean
and co-variance) of these estimates. In this paper, we find weighting factors
within a Bayesian framework for the used L1/L2 sparsity prior that the
resulting maximum a posterior (MAP) estimates do not favor any particular
source location. Due to the lack of an analytical expression for the MAP
estimate when this sparsity prior is used, we solve the weights indirectly.
First, we calculate the Gaussian prior variances that lead to depth un-biased
maximum a posterior (MAP) estimates. Subsequently, we approximate the
corresponding weight factors in the sparsity prior based on the solved Gaussian
prior variances. Finally, we reconstruct focal source configurations using the
sparsity prior with the proposed weights and two other commonly used choices of
weights that can be found in literature.
"
"  We present a multi-wavelength compilation of new and previously-published
photometry for 55 Galactic field RR Lyrae variables. Individual studies,
spanning a time baseline of up to 30 years, are self-consistently phased to
produce light curves in 10 photometric bands covering the wavelength range from
0.4 to 4.5 microns. Data smoothing via the GLOESS technique is described and
applied to generate high-fidelity light curves, from which mean magnitudes,
amplitudes, rise-times, and times of minimum and maximum light are derived.
60,000 observations were acquired using the new robotic Three-hundred
MilliMeter Telescope (TMMT), which was first deployed at the Carnegie
Observatories in Pasadena, CA, and is now permanently installed and operating
at Las Campanas Observatory in Chile. We provide a full description of the TMMT
hardware, software, and data reduction pipeline. Archival photometry
contributed approximately 31,000 observations. Photometric data are given in
the standard Johnson UBV, Kron-Cousins RI, 2MASS JHK, and Spitzer [3.6] & [4.5]
bandpasses.
"
"  We show, that in contrast to the free electron model (standard BCS model), a
particular gap in the spectrum of multiband superconductors opens at some
distance from the Fermi energy, if conduction band is composed of hybridized
atomic orbitals of different symmetries. This gap has composite
superconducting-hybridization origin, because it exists only if both the
superconductivity and the hybridization between different kinds of orbitals are
present. So for many classes of superconductors with multiorbital structure
such spectrum changes should take place. These particular changes in the
spectrum at some distance from the Fermi level result in slow convergence of
the spectral weight of the optical conductivity even in quite conventional
superconductors with isotropic s-wave pairing mechanism.
"
"  We study the entanglement entropy of gapped phases of matter in three spatial
dimensions. We focus in particular on size-independent contributions to the
entropy across entanglement surfaces of arbitrary topologies. We show that for
low energy fixed-point theories, the constant part of the entanglement entropy
across any surface can be reduced to a linear combination of the entropies
across a sphere and a torus. We first derive our results using strong
sub-additivity inequalities along with assumptions about the entanglement
entropy of fixed-point models, and identify the topological contribution by
considering the renormalization group flow; in this way we give an explicit
definition of topological entanglement entropy $S_{\mathrm{topo}}$ in (3+1)D,
which sharpens previous results. We illustrate our results using several
concrete examples and independent calculations, and show adding ""twist"" terms
to the Lagrangian can change $S_{\mathrm{topo}}$ in (3+1)D. For the generalized
Walker-Wang models, we find that the ground state degeneracy on a 3-torus is
given by $\exp(-3S_{\mathrm{topo}}[T^2])$ in terms of the topological
entanglement entropy across a 2-torus. We conjecture that a similar
relationship holds for Abelian theories in $(d+1)$ dimensional spacetime, with
the ground state degeneracy on the $d$-torus given by
$\exp(-dS_{\mathrm{topo}}[T^{d-1}])$.
"
"  Gravitational clustering in the nonlinear regime remains poorly understood.
Gravity dual of gravitational clustering has recently been proposed as a means
to study the nonlinear regime. The stable clustering ansatz remains a key
ingredient to our understanding of gravitational clustering in the highly
nonlinear regime. We study certain aspects of violation of the stable
clustering ansatz in the gravity dual of Large Scale Structure (LSS). We extend
the recent studies of gravitational clustering using AdS gravity dual to take
into account possible departure from the stable clustering ansatz and to
arbitrary dimensions. Next, we extend the recently introduced consistency
relations to arbitrary dimensions. We use the consistency relations to test the
commonly used models of gravitational clustering including the halo models and
hierarchical ansätze. In particular we establish a tower of consistency
relations for the hierarchical amplitudes: $Q, R_a, R_b, S_a,S_b,S_c$ etc. as a
functions of the scaled peculiar velocity $h$. We also study the variants of
popular halo models in this context. In contrast to recent claims, none of
these models, in their simplest incarnation, seem to satisfy the consistency
relations in the soft limit.
"
"  The round trip time of the light pulse limits the maximum detectable
frequency response range of vibration in phase-sensitive optical time domain
reflectometry ({\phi}-OTDR). We propose a method to break the frequency
response range restriction of {\phi}-OTDR system by modulating the light pulse
interval randomly which enables a random sampling for every vibration point in
a long sensing fiber. This sub-Nyquist randomized sampling method is suits for
detecting sparse-wideband-frequency vibration signals. Up to MHz resonance
vibration signal with over dozens of frequency components and 1.153MHz single
frequency vibration signal are clearly identified for a sensing range of 9.6km
with 10kHz maximum sampling rate.
"
"  The continuity of the gauge fixing condition $n\cdot\partial n\cdot A=0$ for
$SU(2)$ gauge theory on the manifold $R\bigotimes S^{1}\bigotimes
S^{1}\bigotimes S^{1}$ is studied here, where $n^{\mu}$ stands for directional
vector along $x_{i}$-axis($i=1,2,3$). It is proved that the gauge fixing
condition is continuous given that gauge potentials are differentiable with
continuous derivatives on the manifold $R\bigotimes S^{1}\bigotimes
S^{1}\bigotimes S^{1}$ which is compact.
"
"  When studying tropical cyclones using the $f$-plane, axisymmetric, gradient
balanced model, there arises a second-order elliptic equation for the
transverse circulation. Similarly, when studying zonally symmetric meridional
circulations near the equator (the tropical Hadley cells) or the katabatically
forced meridional circulation over Antarctica, there also arises a second order
elliptic equation. These elliptic equations are usually derived in the pressure
coordinate or the potential temperature coordinate, since the thermal wind
equation has simple non-Jacobian forms in these two vertical coordinates.
Because of the large variations in surface pressure that can occur in tropical
cyclones and over the Antarctic ice sheet, there is interest in using other
vertical coordinates, e.g., the height coordinate, the classical
$\sigma$-coordinate, or some type of hybrid coordinate typically used in global
numerical weather prediction or climate models. Because the thermal wind
equation in these coordinates takes a Jacobian form, the derivation of the
elliptic transverse circulation equation is not as simple. Here we present a
method for deriving the elliptic transverse circulation equation in a
generalized vertical coordinate, which allows for many particular vertical
coordinates, such as height, pressure, log-pressure, potential temperature,
classical $\sigma$, and most hybrid cases. Advantages and disadvantages of the
various coordinates are discussed.
"
"  We study the impact of quenched disorder (random exchange couplings or site
dilution) on easy-plane pyrochlore antiferromagnets. In the clean system,
order-by-disorder selects a magnetically ordered state from a classically
degenerate manifold. In the presence of randomness, however, different orders
can be chosen locally depending on details of the disorder configuration. Using
a combination of analytical considerations and classical Monte-Carlo
simulations, we argue that any long-range-ordered magnetic state is destroyed
beyond a critical level of randomness where the system breaks into magnetic
domains due to random exchange anisotropies, becoming, therefore, a glass of
spin clusters, in accordance with the available experimental data. These random
anisotropies originate from off-diagonal exchange couplings in the microscopic
Hamiltonian, establishing their relevance to other magnets with strong
spin-orbit coupling.
"
"  Nanostructures with open shell transition metal or molecular constituents
host often strong electronic correlations and are highly sensitive to atomistic
material details. This tutorial review discusses method developments and
applications of theoretical approaches for the realistic description of the
electronic and magnetic properties of nanostructures with correlated electrons.
First, the implementation of a flexible interface between density functional
theory and a variant of dynamical mean field theory (DMFT) highly suitable for
the simulation of complex correlated structures is explained and illustrated.
On the DMFT side, this interface is largely based on recent developments of
quantum Monte Carlo and exact diagonalization techniques allowing for efficient
descriptions of general four fermion Coulomb interactions, reduced symmetries
and spin-orbit coupling, which are explained here. With the examples of the Cr
(001) surfaces, magnetic adatoms, and molecular systems it is shown how the
interplay of Hubbard U and Hund's J determines charge and spin fluctuations and
how these interactions drive different sorts of correlation effects in
nanosystems. Non-local interactions and correlations present a particular
challenge for the theory of low dimensional systems. We present our method
developments addressing these two challenges, i.e., advancements of the
dynamical vertex approximation and a combination of the constrained random
phase approximation with continuum medium theories. We demonstrate how
non-local interaction and correlation phenomena are controlled not only by
dimensionality but also by coupling to the environment which is typically
important for determining the physics of nanosystems.
"
"  Conventional textbook treatments on electromagnetic wave propagation consider
the induced charge and current densities as ""bound"", and therefore absorb them
into a refractive index. In principle it must also be possible to treat the
medium as vacuum, but with explicit charge and current densities. This gives a
more direct, physical description. However, since the induced waves propagate
in vacuum in this picture, it is not straightforward to realize that the
wavelength becomes different compared to that in vacuum. We provide an
explanation, and also associated time-domain simulations. As an extra bonus the
results turn out to illuminate the behavior of metamaterials.
"
"  Surface-functionalized nanomaterials can act as theranostic agents that
detect disease and track biological processes using hyperpolarized magnetic
resonance imaging (MRI). Candidate materials are sparse however, requiring
spinful nuclei with long spin-lattice relaxation (T1) and spin-dephasing times
(T2), together with a reservoir of electrons to impart hyperpolarization. Here,
we demonstrate the versatility of the nanodiamond material system for
hyperpolarized 13C MRI, making use of its intrinsic paramagnetic defect
centers, hours-long nuclear T1 times, and T2 times suitable for spatially
resolving millimeter-scale structures. Combining these properties, we enable a
new imaging modality that exploits the phase-contrast between spins encoded
with a hyperpolarization that is aligned, or anti-aligned with the external
magnetic field. The use of phase-encoded hyperpolarization allows nanodiamonds
to be tagged and distinguished in an MRI based on their spin-orientation alone,
and could permit the action of specific bio-functionalized complexes to be
directly compared and imaged.
"
"  It has been recently demonstrated that textured closed surfaces which are
made out of perfect electric conductors (PECs) can mimic highly localized
surface plasmons (LSPs). Here, we propose an effective medium which can
accurately model LSP resonances in a two-dimensional periodically decorated PEC
cylinder. The accuracy of previous models is limited to structures with
deep-subwavelength and high number of grooves. However, we show that our model
can successfully predict the ultra-sharp LSP resonances which exist in
structures with relatively lower number of grooves. Such resonances are not
correctly predictable with previous models that give some spurious resonances.
The success of the proposed model is indebted to the incorporation of an
effective surface conductivity which is created at the interface of the
cylinder and the homogeneous medium surrounding the structure. This surface
conductivity models the effect of higher diffracted orders which are excited in
the periodic structure. The validity of the proposed model is verified by
full-wave simulations.
"
"  The radially outward flow of fluid through a porous medium occurs in many
practical problems, from transport across vascular walls to the pressurisation
of boreholes in the subsurface. When the driving pressure is non-negligible
relative to the stiffness of the solid structure, the poromechanical coupling
between the fluid and the solid can control both the steady-state and the
transient mechanics of the system. Very large pressures or very soft materials
lead to large deformations of the solid skeleton, which introduce kinematic and
constitutive nonlinearity that can have a nontrivial impact on these mechanics.
Here, we study the transient response of a poroelastic cylinder to sudden fluid
injection. We consider the impacts of kinematic and constitutive nonlinearity,
both separately and in combination, and we highlight the central role of
driving method in the evolution of the response. We show that the various
facets of nonlinearity may either accelerate or decelerate the transient
response relative to linear poroelasticity, depending on the boundary
conditions and the initial geometry, and that an imposed fluid pressure leads
to a much faster response than an imposed fluid flux.
"
"  The Hohenberg-Kohn theorem plays a fundamental role in density functional
theory, which has become a basic tool for the study of electronic structure of
matter. In this article, we study the Hohenberg-Kohn theorem for a class of
external potentials based on a unique continuation principle.
"
"  Forecasting fault failure is a fundamental but elusive goal in earthquake
science. Here we show that by listening to the acoustic signal emitted by a
laboratory fault, machine learning can predict the time remaining before it
fails with great accuracy. These predictions are based solely on the
instantaneous physical characteristics of the acoustical signal, and do not
make use of its history. Surprisingly, machine learning identifies a signal
emitted from the fault zone previously thought to be low-amplitude noise that
enables failure forecasting throughout the laboratory quake cycle. We
hypothesize that applying this approach to continuous seismic data may lead to
significant advances in identifying currently unknown signals, in providing new
insights into fault physics, and in placing bounds on fault failure times.
"
"  New upper limit on a mixing parameter for hidden photons with a mass from 5
eV till 10 keV has been obtained from the results of measurements during 78
days in two configurations R1 and R2 of a multicathode counter. For a region of
a maximal sensitivity from 10 eV till 30 eV the upper limit obtained is less
than 4 x 10-11. The measurements have been performed at three temperatures:
26C, 31C and 36C. A positive effect for the spontaneous emission of single
electrons has been obtained at the level of more than 7{\sigma}. A falling
tendency of a temperature dependence of the spontaneous emission rate indicates
that the effect of thermal emission from a copper cathode can be neglected.
"
"  21 st century astrophysicists are confronted with the herculean task of
distilling the maximum scientific return from extremely expensive and complex
space- or ground-based instrumental projects. This paper concentrates in the
mining of the time series catalog produced by the European Space Agency Gaia
mission, launched in December 2013. We tackle in particular the problem of
inferring the true distribution of the variability properties of Cepheid stars
in the Milky Way satellite galaxy known as the Large Magellanic Cloud (LMC).
Classical Cepheid stars are the first step in the so-called distance ladder: a
series of techniques to measure cosmological distances and decipher the
structure and evolution of our Universe. In this work we attempt to unbias the
catalog by modelling the aliasing phenomenon that distorts the true
distribution of periods. We have represented the problem by a 2-level
generative Bayesian graphical model and used a Markov chain Monte Carlo (MCMC)
algorithm for inference (classification and regression). Our results with
synthetic data show that the system successfully removes systematic biases and
is able to infer the true hyperparameters of the frequency and magnitude
distributions.
"
"  A detailed Monte Carlo-study of the satisfiability threshold for random 3-SAT
has been undertaken. In combination with a monotonicity assumption we find that
the threshold for random 3-SAT satisfies $\alpha_3 \leq 4.262$. If the
assumption is correct, this means that the actual threshold value for $k=3$ is
lower than that given by the cavity method. In contrast the latter has recently
been shown to give the correct value for large $k$. Our result thus indicate
that there are distinct behaviors for $k$ above and below some critical $k_c$,
and the cavity method may provide a correct mean-field picture for the range
above $k_c$.
"
"  Planar magnetic structures (PMSs) are periods in the solar wind during which
interplanetary magnetic field vectors are nearly parallel to a single plane.
One of the specific regions where PMSs have been reported are coronal mass
ejection (CME)-driven sheaths. We use here an automated method to identify PMSs
in 95 CME sheath regions observed in-situ by the Wind and ACE spacecraft
between 1997 and 2015. The occurrence and location of the PMSs are related to
various shock, sheath and CME properties. We find that PMSs are ubiquitous in
CME sheaths; 85% of the studied sheath regions had PMSs with the mean duration
of 6.0 hours. In about one-third of the cases the magnetic field vectors
followed a single PMS plane that covered a significant part (at least 67%) of
the sheath region. Our analysis gives strong support for two suggested PMS
formation mechanisms: the amplification and alignment of solar wind
discontinuities near the CME-driven shock and the draping of the magnetic field
lines around the CME ejecta. For example, we found that the shock and PMS plane
normals generally coincided for the events where the PMSs occurred near the
shock (68% of the PMS plane normals near the shock were separated by less than
20° from the shock normal), while deviations were clearly larger when PMSs
occurred close to the ejecta leading edge. In addition, PMSs near the shock
were generally associated with lower upstream plasma beta than the cases where
PMSs occurred near the leading edge of the CME. We also demonstrate that the
planar parts of the sheath contain a higher amount of strongly southward
magnetic field than the non-planar parts, suggesting that planar sheaths are
more likely to drive magnetospheric activity.
"
"  The dark energy plus cold dark matter ($\Lambda$CDM) cosmological model has
been a demonstrably successful framework for predicting and explaining the
large-scale structure of Universe and its evolution with time. Yet on length
scales smaller than $\sim 1$ Mpc and mass scales smaller than $\sim 10^{11}
M_{\odot}$, the theory faces a number of challenges. For example, the observed
cores of many dark-matter dominated galaxies are both less dense and less cuspy
than naively predicted in $\Lambda$CDM. The number of small galaxies and dwarf
satellites in the Local Group is also far below the predicted count of low-mass
dark matter halos and subhalos within similar volumes. These issues underlie
the most well-documented problems with $\Lambda$CDM: Cusp/Core, Missing
Satellites, and Too-Big-to-Fail. The key question is whether a better
understanding of baryon physics, dark matter physics, or both will be required
to meet these challenges. Other anomalies, including the observed planar and
orbital configurations of Local Group satellites and the tight baryonic/dark
matter scaling relations obeyed by the galaxy population, have been less
thoroughly explored in the context of $\Lambda$CDM theory. Future surveys to
discover faint, distant dwarf galaxies and to precisely measure their masses
and density structure hold promising avenues for testing possible solutions to
the small-scale challenges going forward. Observational programs to constrain
or discover and characterize the number of truly dark low-mass halos are among
the most important, and achievable, goals in this field over then next decade.
These efforts will either further verify the $\Lambda$CDM paradigm or demand a
substantial revision in our understanding of the nature of dark matter.
"
"  For primordial black holes (PBH) to be the dark matter in single-field
inflation, the slow-roll approximation must be violated by at least ${\cal
O}(1)$ in order to enhance the curvature power spectrum within the required
number of efolds between CMB scales and PBH mass scales. Power spectrum
predictions which rely on the inflaton remaining on the slow-roll attractor can
fail dramatically leading to qualitatively incorrect conclusions in models like
an inflection potential and misestimate the mass scale in a running mass model.
We show that an optimized temporal evaluation of the Hubble slow-roll
parameters to second order remains a good description for a wide range of PBH
formation models where up to a $10^7$ amplification of power occurs in $10$
efolds or more.
"
"  We report the first experimental demonstration of frequency-locking of an
extended-cavity quantum-cascade-laser (EC-QCL) to a near-infrared frequency
comb. The locking scheme is applied to carry out absolute spectroscopy of N2O
lines near 7.87 {\mu}m with an accuracy of ~60 kHz. Thanks to a single mode
operation over more than 100 cm^{-1}, the comb-locked EC-QCL shows great
potential for the accurate retrieval of line center frequencies in a spectral
region that is currently outside the reach of broadly tunable cw sources,
either based on difference frequency generation or optical parametric
oscillation. The approach described here can be straightforwardly extended up
to 12 {\mu}m, which is the current wavelength limit for commercial cw EC-QCLs.
"
"  In the past decade, the discovery of active pharmaceutical substances with
high therapeutic value but poor aqueous solubility has increased, thus making
it challenging to formulate these compounds as oral dosage forms. The
bioavailability of these drugs can be increased by formulating these drugs as
an amorphous drug delivery system. Use of porous media like mesoporous silica
has been investigated as a potential means to increase the solubility of poorly
soluble drugs and to stabilize the amorphous drug delivery system. These
materials have nanosized capillaries and the large surface area which enable
the materials to accommodate high drug loading and promote the controlled and
fast release. Therefore, mesoporous silica has been used as a carrier in the
solid dispersion to form an amorphous solid dispersion (ASD). Mesoporous silica
is also being used as an adsorbent in a conventional solid dispersion, which
has many useful aspects. This review focuses on the use of mesoporous silica in
ASD as potential means to increase the dissolution rate and to provide or
increase the stability of the ASD. First, an overview of mesoporous silica and
the classification is discussed. Subsequently, methods of drug incorporation,
the stability of dispersion and, much more are discussed.
"
"  Scalable quantum photonic systems require efficient single photon sources
coupled to integrated photonic devices. Solid-state quantum emitters can
generate single photons with high efficiency, while silicon photonic circuits
can manipulate them in an integrated device structure. Combining these two
material platforms could, therefore, significantly increase the complexity of
integrated quantum photonic devices. Here, we demonstrate hybrid integration of
solid-state quantum emitters to a silicon photonic device. We develop a
pick-and-place technique that can position epitaxially grown InAs/InP quantum
dots emitting at telecom wavelengths on a silicon photonic chip
deterministically with nanoscale precision. We employ an adiabatic tapering
approach to transfer the emission from the quantum dots to the waveguide with
high efficiency. We also incorporate an on-chip silicon-photonic beamsplitter
to perform a Hanbury-Brown and Twiss measurement. Our approach could enable
integration of pre-characterized III-V quantum photonic devices into
large-scale photonic structures to enable complex devices composed of many
emitters and photons.
"
"  We investigate the ground-state properties and the collective modes of a
two-dimensional two-component Rydberg-dressed Fermi liquid in the
dipole-blockade regime. We find instability of the homogeneous system toward
phase separated and density ordered phases, using the Hartree-Fock and
random-phase approximations, respectively. The spectral weight of collective
density oscillations in the homogenous phase also signals the emergence of
density-wave instability. We examine the effect of exchange-hole on the
density-wave instability and on the collective mode dispersion using the
Hubbard local-field factor.
"
"  Waves can be used to probe and image an unknown medium. Passive imaging uses
ambient noise sources to illuminate the medium. This paper considers passive
imaging with moving sensors. The motivation is to generate large synthetic
apertures, which should result in enhanced resolution. However Doppler effects
and lack of reciprocity significantly affect the imaging process. This paper
discusses the consequences in terms of resolution and it shows how to design
appropriate imaging functions depending on the sensor trajectory and velocity.
"
"  We investigate the Anderson localization in non-Hermitian
Aubry-André-Harper (AAH) models with imaginary potentials added to lattice
sites to represent the physical gain and loss during the interacting processes
between the system and environment. By checking the mean inverse participation
ratio (MIPR) of the system, we find that different configurations of physical
gain and loss have very different impacts on the localization phase transition
in the system. In the case with balanced physical gain and loss added in an
alternate way to the lattice sites, the critical region (in the case with
p-wave superconducting pairing) and the critical value (both in the situations
with and without p-wave pairing) for the Anderson localization phase transition
will be significantly reduced, which implies an enhancement of the localization
process. However, if the system is divided into two parts with one of them
coupled to physical gain and the other coupled to the corresponding physical
loss, the transition process will be impacted only in a very mild way. Besides,
we also discuss the situations with imbalanced physical gain and loss and find
that the existence of random imaginary potentials in the system will also
affect the localization process while constant imaginary potentials will not.
"
"  Using a 10D lift of non-perturbative volume stabilization in type IIB string
theory we study the limitations for obtaining de Sitter vacua. Based on this we
find that the simplest KKLT vacua with a single Kahler modulus stabilized by a
gaugino condensate cannot be uplifted to de Sitter. Rather, the uplift flattens
out due to stronger backreaction on the volume modulus than has previously been
anticipated, resulting in vacua which are meta-stable and SUSY breaking, but
that are always AdS. However, we also show that setups such as racetrack
stabilization can avoid this issue. In these models it is possible to obtain
supersymmetric AdS vacua with a cosmological constant that can be tuned to zero
while retaining finite moduli stabilization. In this regime, it seems that de
Sitter uplifts are possible with negligible backreaction on the internal
volume. We exhibit this behavior also from the 10D perspective.
"
"  We demonstrate the full functionality of a circuit that generates single
microwave photons on demand, with a wave packet that can be modulated with a
near-arbitrary shape. We achieve such a high tunability by coupling a
superconducting qubit near the end of a semi-infinite transmission line. A dc
superconducting quantum interference device shunts the line to ground and is
employed to modify the spatial dependence of the electromagnetic mode structure
in the transmission line. This control allows us to couple and decouple the
qubit from the line, shaping its emission rate on fast time scales. Our
decoupling scheme is applicable to all types of superconducting qubits and
other solid-state systems and can be generalized to multiple qubits as well as
to resonators.
"
"  Earthquakes at seismogenic plate boundaries are a response to the
differential motions of tectonic blocks embedded within a geometrically complex
network of branching and coalescing faults. Elastic strain is accumulated at a
slow strain rate of the order of $10^{-15}$ s$^{-1}$, and released
intermittently at intervals $>100$ years, in the form of rapid (seconds to
minutes) coseismic ruptures. The development of macroscopic models of
quasi-static planar tectonic dynamics at these plate boundaries has remained
challenging due to uncertainty with regard to the spatial and kinematic
complexity of fault system behaviors. In particular, the characteristic length
scale of kinematically distinct tectonic structures is poorly constrained. Here
we analyze fluctuations in GPS recordings of interseismic velocities from the
southern California plate boundary, identifying heavy-tailed scaling behavior.
This suggests that the plate boundary can be understood as a densely packed
granular medium near the jamming transition, with a characteristic length scale
of $91 \pm 20$ km. In this picture fault and block systems may rapidly
rearrange the distribution of forces within them, driving a mixture of
transient and intermittent fault slip behaviors over tectonic time scales.
"
"  We seek to infer the parameters of an ergodic Markov process from samples
taken independently from the steady state. Our focus is on non-equilibrium
processes, where the steady state is not described by the Boltzmann measure,
but is generally unknown and hard to compute, which prevents the application of
established equilibrium inference methods. We propose a quantity we call
propagator likelihood, which takes on the role of the likelihood in equilibrium
processes. This propagator likelihood is based on fictitious transitions
between those configurations of the system which occur in the samples. The
propagator likelihood can be derived by minimising the relative entropy between
the empirical distribution and a distribution generated by propagating the
empirical distribution forward in time. Maximising the propagator likelihood
leads to an efficient reconstruction of the parameters of the underlying model
in different systems, both with discrete configurations and with continuous
configurations. We apply the method to non-equilibrium models from statistical
physics and theoretical biology, including the asymmetric simple exclusion
process (ASEP), the kinetic Ising model, and replicator dynamics.
"
"  A pair of type-II Dirac cones in PdTe$_2$ was recently predicted by theories
and confirmed in experiments, making PdTe$_2$ the first material that processes
both superconductivity and type-II Dirac fermions. In this work, we study the
evolution of Dirac cones in PdTe$_2$ under hydrostatic pressure by the
first-principles calculations. Our results show that the pair of type-II Dirac
points disappears at 6.1 GPa. Interestingly, a new pair of type-I Dirac points
from the same two bands emerges at 4.7 GPa. Due to the distinctive band
structures compared with those of PtSe$_2$ and PtTe$_2$, the two types of Dirac
points can coexist in PdTe$_2$ under proper pressure (4.7-6.1 GPa). The
emergence of type-I Dirac cones and the disappearance of type-II Dirac ones are
attributed to the increase/decrease of the energy of the states at $\Gamma$ and
$A$ point, which have the anti-bonding/bonding characters of interlayer Te-Te
atoms. On the other hand, we find that the superconductivity of PdTe$_2$
slightly decreases with pressure. The pressure-induced different types of Dirac
cones combined with superconductivity may open a promising way to investigate
the complex interactions between Dirac fermions and superconducting
quasi-particles.
"
"  We present a weak lensing analysis of a sample of SDSS Compact Groups (CGs).
Using the measured radial density contrast profile, we derive the average
masses under the assumption of spherical symmetry, obtaining a velocity
dispersion for the Singular Isothermal Spherical model, $\sigma_V = 270 \pm 40
\rm ~km~s^{-1}$, and for the NFW model, $R_{200}=0.53\pm0.10\,h_{70}^{-1}\,\rm
Mpc$. We test three different definitions of CGs centres to identify which best
traces the true dark matter halo centre, concluding that a luminosity weighted
centre is the most suitable choice. We also study the lensing signal dependence
on CGs physical radius, group surface brightness, and morphological mixing. We
find that groups with more concentrated galaxy members show steeper mass
profiles and larger velocity dispersions. We argue that both, a possible lower
fraction of interloper and a true steeper profile, could be playing a role in
this effect. Straightforward velocity dispersion estimates from member
spectroscopy yields $\sigma_V \approx 230 \rm ~km~s^{-1}$ in agreement with our
lensing results.
"
"  The three exceptional lattices, $E_6$, $E_7$, and $E_8$, have attracted much
attention due to their anomalously dense and symmetric structures which are of
critical importance in modern theoretical physics. Here, we study the
electronic band structure of a single spinless quantum particle hopping between
their nearest-neighbor lattice points in the tight-binding limit. Using Markov
chain Monte Carlo methods, we numerically sample their lattice Green's
functions, densities of states, and random walk return probabilities. We find
and tabulate a plethora of Van Hove singularities in the densities of states,
including degenerate ones in $E_6$ and $E_7$. Finally, we use brute force
enumeration to count the number of distinct closed walks of length up to eight,
which gives the first eight moments of the densities of states.
"
"  Analysis of conjugate natural convection with surface radiation in a
two-dimensional enclosure is carried out in order to search the optimal
location of the heat source with entropy generation minimization (EGM) approach
and conventional heat transfer parameters. The air as an incompressible fluid
and transparent media is considered the fluid filling the enclosure with the
steady and laminar regime. The enclosure internal surfaces are also gray,
opaque and diffuse. The governing equations with stream function and vorticity
formulation are solved using finite difference approach. Results include the
effect of Rayleigh number and emissivity on the dimensionless average rate of
entropy generation and its optimum location. The optimum location search with
conventional heat transfer parameters including maximum temperature and Nusselt
numbers are also examined.
"
"  In this work, by using strong gravitational lensing (SGL) observations along
with Type Ia Supernovae (Union2.1) and gamma ray burst data (GRBs), we propose
a new method to study a possible redshift evolution of $\gamma(z)$, the mass
density power-law index of strong gravitational lensing systems. In this
analysis, we assume the validity of cosmic distance duality relation and the
flat universe. In order to explore the $\gamma(z)$ behavior, three different
parametrizations are considered, namely: (P1) $\gamma(z_l)=\gamma_0+\gamma_1
z_l$, (P2) $\gamma(z_l)=\gamma_0+\gamma_1 z_l/(1+z_l)$ and (P3)
$\gamma(z_l)=\gamma_0+\gamma_1 \ln(1+z_l)$, where $z_l$ corresponds to lens
redshift. If $\gamma_0=2$ and $\gamma_1=0$ the singular isothermal sphere model
is recovered. Our method is performed on SGL sub-samples defined by different
lens redshifts and velocity dispersions. For the former case, the results are
in full agreement with each other, while a 1$\sigma$ tension between the
sub-samples with low ($\leq 250$ km/s) and high ($>250$ km/s) velocity
dispersions was obtained on the ($\gamma_0$-$\gamma_1$) plane. By considering
the complete SGL sample, we obtain $\gamma_0 \approx 2$ and $ \gamma_1 \approx
0$ within 1$\sigma$ c.l. for all $\gamma(z)$ parametrizations. However, we find
the following best fit values of $\gamma_1$: $-0.085$, $-0.16$ and $-0.12$ for
P1, P2 and P3 parametrizations, respectively, suggesting a mild evolution for
$\gamma(z)$. By repeating the analysis with Type Ia Supernovae from JLA
compilation, GRBs and SGL systems this mild evolution is reinforced.
"
"  Hydrogen peroxide (H2O2) is an important signaling molecule in cancer cells.
However, the significant secretion of H2O2 by cancer cells have been rarely
observed. Cold atmospheric plasma (CAP) is a near room temperature ionized gas
composed of neutral particles, charged particles, reactive species, and
electrons. Here, we first demonstrated that breast cancer cells and pancreatic
adenocarcinoma cells generated micromolar level H2O2 during just 1 min of
direct CAP treatment on these cells. The cell-based H2O2 generation is affected
by the medium volume, the cell confluence, as well as the discharge voltage.
The application of cold atmospheric plasma (CAP) in the cancer treatment has
been intensively investigated over the past decade. Several cellular responses
to the CAP treatment have been observed including the consumption of the
CAP-originated reactive species, the rise of intracellular reactive oxygen
species, the damage on DNA and mitochondria, as well as the activation of
apoptotic events. This is a new previously unknown cellular response to CAP,
which provides a new prospective to understand the interaction between CAP and
cells.
"
"  We present an investigation into the intrinsic magnetic properties of the
compounds YCo5 and GdCo5, members of the RETM5 class of permanent magnets (RE =
rare earth, TM = transition metal). Focusing on Y and Gd provides direct
insight into both the TM magnetization and RE-TM interactions without the
complication of strong crystal field effects. We synthesize single crystals of
YCo5 and GdCo5 using the optical floating zone technique and measure the
magnetization from liquid helium temperatures up to 800 K. These measurements
are interpreted through calculations based on a Green's function formulation of
density-functional theory, treating the thermal disorder of the local magnetic
moments within the coherent potential approximation. The rise in the
magnetization of GdCo5 with temperature is shown to arise from a faster
disordering of the Gd magnetic moments compared to the antiferromagnetically
aligned Co sublattice. We use the calculations to analyze the different Curie
temperatures of the compounds and also compare the molecular (Weiss) fields at
the RE site with previously published neutron scattering experiments. To gain
further insight into the RE-TM interactions, we perform substitutional doping
on the TM site, studying the compounds RECo4.5Ni0.5, RECo4Ni, and RECo4.5Fe0.5.
Both our calculations and experiments on powdered samples find an
increased/decreased magnetization with Fe/Ni doping, respectively. The
calculations further reveal a pronounced dependence on the location of the
dopant atoms of both the Curie temperatures and the Weiss field at the RE site.
"
"  The Muon g-2 Experiment plans to use the Fermilab Recycler Ring for forming
the proton bunches that hit its production target. The proposed scheme uses one
RF system, 80 kV of 2.5 MHz RF. In order to avoid bunch rotations in a
mismatched bucket, the 2.5 MHz is ramped adiabatically from 3 to 80 kV in 90
ms. In this study, the interaction of the primary proton beam with the
production target for the Muon g-2 Experiment is numerically examined.
"
"  The existence or absence of non-analytic cusps in the Loschmidt-echo return
rate is traditionally employed to distinguish between a regular dynamical phase
(regular cusps) and a trivial phase (no cusps) in quantum spin chains after a
global quench. However, numerical evidence in a recent study [J. C. Halimeh and
V. Zauner-Stauber, arXiv:1610.02019] suggests that instead of the trivial phase
a distinct anomalous dynamical phase characterized by a novel type of
non-analytic cusps occurs in the one-dimensional transverse-field Ising model
when interactions are sufficiently long-range. Using an analytic semiclassical
approach and exact diagonalization, we show that this anomalous phase also
arises in the fully-connected case of infinite-range interactions, and we
discuss its defining signature. Our results show that the transition from the
regular to the anomalous dynamical phase coincides with Z2-symmetry breaking in
the infinite-time limit, thereby showing a connection between two different
concepts of dynamical criticality. Our work further expands the dynamical phase
diagram of long-range interacting quantum spin chains, and can be tested
experimentally in ion-trap setups and ultracold atoms in optical cavities,
where interactions are inherently long-range.
"
"  We explore ways of creating cold keV-scale dark matter by means of decays and
scatterings. The main observation is that certain thermal freeze-in processes
can lead to a cold dark matter distribution in regions with small available
phase space. In this way the free-streaming length of keV particles can be
suppressed without decoupling them too much from the Standard Model. In all
cases, dark matter needs to be produced together with a heavy particle that
carries away most of the initial momentum. For decays, this simply requires an
off-diagonal DM coupling to two heavy particles; for scatterings, the coupling
of soft DM to two heavy particles needs to be diagonal, in particular in spin
space. Decays can thus lead to cold light DM of any spin, while scatterings
only work for bosons with specific couplings. We explore a number of simple
models and also comment on the connection to the tentative 3.5 keV line.
"
"  Superhydrophobic surfaces (SHSs) have the potential to achieve large drag
reduction for internal and external flow applications. However, experiments
have shown inconsistent results, with many studies reporting significantly
reduced performance. Recently, it has been proposed that surfactants,
ubiquitous in flow applications, could be responsible, by creating adverse
Marangoni stresses. Yet, testing this hypothesis is challenging. Careful
experiments with purified water show large interfacial stresses and,
paradoxically, adding surfactants yields barely measurable drag increases. This
suggests that other physical processes, such as thermal Marangoni stresses or
interface deflection, could explain the lower performance. To test the
surfactant hypothesis, we perform the first numerical simulations of flows over
a SHS inclusive of surfactant kinetics. These simulations reveal that
surfactant-induced stresses are significant at extremely low concentrations,
potentially yielding a no-slip boundary condition on the air--water interface
(the ""plastron"") for surfactant amounts below typical environmental values.
These stresses decrease as the streamwise distance between plastron stagnation
points increases. We perform microchannel experiments with thermally-controlled
SHSs consisting of streamwise parallel gratings, which confirm this numerical
prediction. We introduce a new, unsteady test of surfactant effects. When we
rapidly remove the driving pressure following a loading phase, a backflow
develops at the plastron, which can only be explained by surfactant gradients
formed in the loading phase. This demonstrates the significance of surfactants
in deteriorating drag reduction, and thus the importance of including
surfactant stresses in SHS models. Our time-dependent protocol can assess the
impact of surfactants in SHS testing and guide future mitigating designs.
"
"  We assess the range of validity of sgoldstino-less inflation in a scenario of
low energy supersymmetry breaking. We first analyze the consistency conditions
that an effective theory of the inflaton and goldstino superfields should
satisfy in order to be faithfully described by a sgoldstino-less model.
Enlarging the scope of previous studies, we investigate the case where the
effective field theory cut-off, and hence also the sgoldstino mass, are
inflaton-dependent. We then introduce a UV complete model where one can realize
successfully sgoldstino-less inflation and gauge mediation of supersymmetry
breaking, combining the alpha-attractor mechanism and a weakly coupled model of
spontaneous breaking of supersymmetry. In this class of models we find that,
given current limits on superpartner masses, the gravitino mass has a lower
bound of the order of the MeV, i.e. we cannot reach very low supersymmetry
breaking scales. On the plus side, we recognize that in this framework, one can
derive the complete superpartner spectrum as well as compute inflation
observables, the reheating temperature, and address the gravitino overabundance
problem. We then show that further constraints come from collider results and
inflation observables. Their non trivial interplay seems a staple feature of
phenomenological studies of supersymmetric inflationary models.
"
"  In a previous paper, we assembled a collection of medium-resolution spectra
of 35 carbon stars, covering optical and near-infrared wavelengths from 400 to
2400 nm. The sample includes stars from the Milky Way and the Magellanic
Clouds, with a variety of $(J-K_s)$ colors and pulsation properties. In the
present paper, we compare these observations to a new set of high-resolution
synthetic spectra, based on hydrostatic model atmospheres. We find that the
broad-band colors and the molecular-band strengths measured by
spectrophotometric indices match those of the models when $(J-K_s)$ is bluer
than about 1.6, while the redder stars require either additional reddening or
dust emission or both. Using a grid of models to fit the full observed spectra,
we estimate the most likely atmospheric parameters $T_\mathrm{eff}$, $\log(g)$,
$[\mathrm{Fe/H}]$ and C/O. These parameters derived independently in the
optical and near-infrared are generally consistent when $(J-K_s)<1.6$. The
temperatures found based on either wavelength range are typically within
$\pm$100K of each other, and $\log(g)$ and $[\mathrm{Fe/H}]$ are consistent
with the values expected for this sample. The reddest stars ($(J-K_s)$ $>$ 1.6)
are divided into two families, characterized by the presence or absence of an
absorption feature at 1.53\,$\mu$m, generally associated with HCN and
C$_2$H$_2$. Stars from the first family begin to be more affected by
circumstellar extinction. The parameters found using optical or near-infrared
wavelengths are still compatible with each other, but the error bars become
larger. In stars showing the 1.53\,$\mu$m feature, which are all
large-amplitude variables, the effects of pulsation are strong and the spectra
are poorly matched with hydrostatic models. For these, atmospheric parameters
could not be derived reliably, and dynamical models are needed for proper
interpretation.
"
"  We discuss the parametric oscillatory instability in a Fabry-Perot cavity of
the Einstein Telescope. Unstable combinations of elastic and optical modes for
two possible configurations of gravitational wave third-generation detector are
deduced. The results are compared with the results for gravita- tional wave
interferometers LIGO and LIGO Voyager.
"
"  In 1835 Lobachevski entertained the possibility of multiple (rival)
geometries. This idea has reappeared on occasion (e.g., Poincaré) but
didn't become key in space-time foundations prior to Brown's \emph{Physical
Relativity} (at the end, the interpretive key to the book). A crucial
difference between his constructivism and orthodox ""space-time realism"" is
modal scope. Constructivism applies to all local classical field theories,
including those with multiple geometries. But the orthodox view provincially
assumes a unique geometry, as familiar theories (Newton, Special Relativity,
Nordström, and GR) have. They serve as the orthodox ""canon."" Their
historical roles suggest a story of inevitable progress. Physics literature
after c. 1920 is relevant to orthodoxy mostly as commentary on the canon, which
closed in the 1910s. The orthodox view explains the behavior of matter as the
manifestation of the real space-time geometry, which works within the canon.
The orthodox view, Whiggish history, and the canon relate symbiotically.
If one considers a theory outside the canon, space-time realism sheds little
light on matter's behavior. Worse, it gives the wrong answer when applied to an
example arguably in the canon, massive scalar gravity with universal coupling.
Which is the true geometry---the flat metric from the Poincaré symmetry,
the conformally flat metric exhibited by material rods and clocks, or both---or
is the question bad? How does space-time realism explain that all matter fields
see the same curved geometry, given so many ways to mix and match?
Constructivist attention to dynamical details is vindicated; geometrical
shortcuts disappoint. The more exhaustive exploration of relativistic field
theories (especially massive) in particle physics is an underused resource for
foundations.
"
"  We present the analysis of microlensing event MOA-2010-BLG-117, and show that
the light curve can only be explained by the gravitational lensing of a binary
source star system by a star with a Jupiter mass ratio planet. It was necessary
to modify standard microlensing modeling methods to find the correct light
curve solution for this binary-source, binary-lens event. We are able to
measure a strong microlensing parallax signal, which yields the masses of the
host star, $M_* = 0.58\pm 0.11 M_\odot$, and planet $m_p = 0.54\pm 0.10 M_{\rm
Jup}$ at a projected star-planet separation of $a_\perp = 2.42\pm 0.26\,$AU,
corresponding to a semi-major axis of $a = 2.9{+1.6\atop -0.6}\,$AU. Thus, the
system resembles a half-scale model of the Sun-Jupiter system with a
half-Jupiter mass planet orbiting a half-solar mass star at very roughly half
of Jupiter's orbital distance from the Sun. The source stars are slightly
evolved, and by requiring them to lie on the same isochrone, we can constrain
the source to lie in the near side of the bulge at a distance of $D_S = 6.9 \pm
0.7\,$kpc, which implies a distance to the planetary lens system of $D_L =
3.5\pm 0.4\,$kpc. The ability to model unusual planetary microlensing events,
like this one, will be necessary to extract precise statistical information
from the planned large exoplanet microlensing surveys, such as the WFIRST
microlensing survey.
"
"  When an upstream steady uniform supersonic flow impinges onto a symmetric
straight-sided wedge, governed by the Euler equations, there are two possible
steady oblique shock configurations if the wedge angle is less than the
detachment angle -- the steady weak shock with supersonic or subsonic
downstream flow (determined by the wedge angle that is less or larger than the
sonic angle) and the steady strong shock with subsonic downstream flow, both of
which satisfy the entropy condition. The fundamental issue -- whether one or
both of the steady weak and strong shocks are physically admissible solutions
-- has been vigorously debated over the past eight decades. In this paper, we
survey some recent developments on the stability analysis of the steady shock
solutions in both the steady and dynamic regimes. For the static stability, we
first show how the stability problem can be formulated as an initial-boundary
value type problem and then reformulate it into a free boundary problem when
the perturbation of both the upstream steady supersonic flow and the wedge
boundary are suitably regular and small, and we finally present some recent
results on the static stability of the steady supersonic and transonic shocks.
For the dynamic stability for potential flow, we first show how the stability
problem can be formulated as an initial-boundary value problem and then use the
self-similarity of the problem to reduce it into a boundary value problem and
further reformulate it into a free boundary problem, and we finally survey some
recent developments in solving this free boundary problem for the existence of
the Prandtl-Meyer configurations that tend to the steady weak supersonic or
transonic oblique shock solutions as time goes to infinity. Some further
developments and mathematical challenges in this direction are also discussed.
"
"  We provide a new perspective on fracton topological phases, a class of
three-dimensional topologically ordered phases with unconventional
fractionalized excitations that are either completely immobile or only mobile
along particular lines or planes. We demonstrate that a wide range of these
fracton phases can be constructed by strongly coupling mutually intersecting
spin chains and explain via a concrete example how such a coupled-spin-chain
construction illuminates the generic properties of a fracton phase. In
particular, we describe a systematic translation from each coupled-spin-chain
construction into a parton construction where the partons correspond to the
excitations that are mobile along lines. Remarkably, our construction of
fracton phases is inherently based on spin models involving only two-spin
interactions and thus brings us closer to their experimental realization.
"
"  The electronic and magneto transport properties of reduced anatase TiO2
epitaxial thin films are analyzed considering various polaronic effects.
Unexpectedly, with increasing carrier concentration, the mobility increases,
which rarely happens in common metallic systems. We find that the screening of
the electron-phonon (e-ph) coupling by excess carriers is necessary to explain
this unusual dependence. We also find that the magnetoresistance (MR) could be
decomposed into a linear and a quadratic component, separately characterizing
the transport and trap behavior of carriers as a function of temperature. The
various transport behaviors could be organized into a single phase diagram
which clarifies the nature of large polaron in this material.
"
"  The superconductivity of the 4-angstrom single-walled carbon nanotubes
(SWCNTs) was discovered more than a decade ago, and marked the breakthrough of
finding superconductivity in pure elemental undoped carbon compounds. The van
Hove singularities in the electronic density of states at the Fermi level in
combination with a large Debye temperature of the SWCNTs are expected to cause
an impressively large superconducting gap. We have developed an innovative
computational algorithm specially tailored for the investigation of
superconductivity in ultrathin SWCNTs. We predict the superconducting
transition temperature of various thin carbon nanotubes resulting from
electron-phonon coupling by an ab-initio method, taking into account the effect
of radial pressure, symmetry, chirality (N,M) and bond lengths. By optimizing
the geometry of the carbon nanotubes, a maximum Tc of 60K is found. We also use
our method to calculate the Tc of a linear carbon chain embedded in the center
of (5,0) SWCNTs. The strong curvature in the (5,0) carbon nanotubes in the
presence of the inner carbon chain provides an alternative path to increase the
Tc of this carbon composite by a factor of 2.2 with respect to the empty (5,0)
SWCNTs.
"
"  The discovery of topological states of matter has profoundly augmented our
understanding of phase transitions in physical systems. Instead of local order
parameters, topological phases are described by global topological invariants
and are therefore robust against perturbations. A prominent example thereof is
the two-dimensional integer quantum Hall effect. It is characterized by the
first Chern number which manifests in the quantized Hall response induced by an
external electric field. Generalizing the quantum Hall effect to
four-dimensional systems leads to the appearance of a novel non-linear Hall
response that is quantized as well, but described by a 4D topological invariant
- the second Chern number. Here, we report on the first observation of a bulk
response with intrinsic 4D topology and the measurement of the associated
second Chern number. By implementing a 2D topological charge pump with
ultracold bosonic atoms in an angled optical superlattice, we realize a
dynamical version of the 4D integer quantum Hall effect. Using a small atom
cloud as a local probe, we fully characterize the non-linear response of the
system by in-situ imaging and site-resolved band mapping. Our findings pave the
way to experimentally probe higher-dimensional quantum Hall systems, where new
topological phases with exotic excitations are predicted.
"
"  Explaining the unexpected presence of dune-like patterns at the surface of
the comet 67P/Churyumov-Gerasimenko requires conceptual and quantitative
advances in the understanding of surface and outgassing processes. We show here
that vapor flow emitted by the comet around its perihelion spreads laterally in
a surface layer, due to the strong pressure difference between zones
illuminated by sunlight and those in shadow. For such thermal winds to be dense
enough to transport grains -- ten times greater than previous estimates --
outgassing must take place through a surface porous granular layer, and that
layer must be composed of grains whose roughness lowers cohesion consistently
with contact mechanics. The linear stability analysis of the problem, entirely
tested against laboratory experiments, quantitatively predicts the emergence of
bedforms in the observed wavelength range, and their propagation at the scale
of a comet revolution. Although generated by a rarefied atmosphere, they are
paradoxically analogous to ripples emerging on granular beds submitted to
viscous shear flows. This quantitative agreement shows that our understanding
of the coupling between hydrodynamics and sediment transport is able to account
for bedform emergence in extreme conditions and provides a reliable tool to
predict the erosion and accretion processes controlling the evolution of small
solar system bodies.
"
"  Fusing satellite observations and station measurements to estimate
ground-level PM2.5 is promising for monitoring PM2.5 pollution. A
geo-intelligent approach, which incorporates geographical correlation into an
intelligent deep learning architecture, is developed to estimate PM2.5.
Specifically, it considers geographical distance and spatiotemporally
correlated PM2.5 in a deep belief network (denoted as Geoi-DBN). Geoi-DBN can
capture the essential features associated with PM2.5 from latent factors. It
was trained and tested with data from China in 2015. The results show that
Geoi-DBN performs significantly better than the traditional neural network. The
cross-validation R increases from 0.63 to 0.94, and RMSE decreases from 29.56
to 13.68${\mu}$g/m3. On the basis of the derived PM2.5 distribution, it is
predicted that over 80% of the Chinese population live in areas with an annual
mean PM2.5 of greater than 35${\mu}$g/m3. This study provides a new perspective
for air pollution monitoring in large geographic regions.
"
"  The goal of this paper is to examine experimental progress in laser wakefield
acceleration over the past decade (2004-2014), and to use trends in the data to
understand some of the important physical processes. By examining a set of over
50 experiments, various trends concerning the relationship between plasma
density, accelerator length, laser power and the final electron beam en- ergy
are revealed. The data suggest that current experiments are limited by
dephasing and that current experiments typically require some pulse evolution
to reach the trapping threshold.
"
"  We present a non-perturbative numerical technique for calculating strong
light shifts in atoms under the influence of multiple optical fields with
arbitrary polarization. We confirm our technique experimentally by performing
spectroscopy of a cloud of cold $^{87}$Rb atoms subjected to $\sim$ kW/cm$^2$
intensities of light at 1560.492 nm simultaneous with 1529.269 nm or 1529.282
nm. In these conditions the excited state resonances at 1529.26 nm and 1529.36
nm induce strong level mixing and the shifts are highly nonlinear. By
absorption spectroscopy, we observe that the induced shifts of the 5P3/2
hyperfine Zeeman sublevels agree well with our theoretical predictions.. We
propose the application of our theory and experiment to accurate measurements
of excited-state electric-dipole matrix elements.
"
"  In several geophysical applications, such as full waveform inversion and data
modelling, we are facing the solution of inhomogeneous Helmholtz equation. The
difficulties of solving the Helmholtz equa- tion are two fold. Firstly, in the
case of large scale problems we cannot calculate the inverse of the Helmholtz
operator directly. Hence, iterative algorithms should be implemented. Secondly,
the Helmholtz operator is non-unitary and non-diagonalizable which in turn
deteriorates the performances of the iterative algorithms (especially for high
wavenumbers). To overcome this issue, we need to im- plement proper
preconditioners for a Krylov subspace method to solve the problem efficiently.
In this paper we incorporated shifted-Laplace operators to precondition the
system of equations and then generalized minimal residual (GMRES) method used
to solve the problem iteratively. The numerical results show the performance of
the preconditioning operator in improving the convergence rate of the GMRES
algorithm for data modelling case. In the companion paper we discussed the
application of preconditioned data modelling algorithm in the context of
frequency domain full waveform inversion. However, the analysis of the degree
of suitability of the preconditioners in the solution of Helmholtz equation is
an ongoing field of study.
"
"  A network-based approach is presented to investigate the cerebrovascular flow
patterns during atrial fibrillation (AF) with respect to normal sinus rhythm
(NSR). AF, the most common cardiac arrhythmia with faster and irregular
beating, has been recently and independently associated with the increased risk
of dementia. However, the underlying hemodynamic mechanisms relating the two
pathologies remain mainly undetermined so far; thus the contribution of
modeling and refined statistical tools is valuable. Pressure and flow rate
temporal series in NSR and AF are here evaluated along representative cerebral
sites (from carotid arteries to capillary brain circulation), exploiting
reliable artificially built signals recently obtained from an in silico
approach. The complex network analysis evidences, in a synthetic and original
way, a dramatic signal variation towards the distal/capillary cerebral regions
during AF, which has no counterpart in NSR conditions. At the large artery
level, networks obtained from both AF and NSR hemodynamic signals exhibit
elongated and chained features, which are typical of pseudo-periodic series.
These aspects are almost completely lost towards the microcirculation during
AF, where the networks are topologically more circular and present random-like
characteristics. As a consequence, all the physiological phenomena at
microcerebral level ruled by periodicity - such as regular perfusion, mean
pressure per beat, and average nutrient supply at cellular level - can be
strongly compromised, since the AF hemodynamic signals assume irregular
behaviour and random-like features. Through a powerful approach which is
complementary to the classical statistical tools, the present findings further
strengthen the potential link between AF hemodynamic and cognitive decline.
"
"  Weyl points with monopole charge $\pm 1$ have been extensively studied,
however, real materials of multi-Weyl points, whose monopole charges are higher
than $1$, have yet to be found. In this Rapid Communication, we show that
nodal-line semimetals with nontrivial line connectivity provide natural
platforms for realizing Floquet multi-Weyl points. In particular, we show that
driving crossing nodal lines by circularly polarized light generates
double-Weyl points. Furthermore, we show that monopole combination and
annihilation can be observed in crossing-nodal-line semimetals and nodal-chain
semimetals. These proposals can be experimentally verified in pump-probe
angle-resolved photoemission spectroscopy.
"
"  A theoretical investigation of extremely high field transport in an emerging
wide-bandgap material $\beta-Ga_2O_3$ is reported from first principles. The
signature high-field effect explored here is impact ionization. Interaction
between a valence-band electron and an excited electron is computed from the
matrix elements of a screened Coulomb operator. Maximally localized Wannier
functions (MLWF) are utilized in computing the impact ionization rates. A
full-band Monte Carlo (FBMC) simulation is carried out incorporating the impact
ionization rates, and electron-phonon scattering rates. This work brings out
valuable insights on the impact ionization coefficient (IIC) of electrons in
$\beta-Ga_2O_3$. The isolation of the $\Gamma$ point conduction band minimum by
a significantly high energy from other satellite band pockets play a vital role
in determining ionization co-efficients. IICs are calculated for electric
fields ranging up to 8 MV/cm for different crystal directions. A Chynoweth
fitting of the computed IICs is done to calibrate ionization models in device
simulators.
"
"  We investigate the effect on disorder potential on exciton valley
polarization and valley coherence in monolayer WSe2. By analyzing polarization
properties of photoluminescence, the valley coherence (VC) and valley
polarization (VP) is quantified across the inhomogeneously broadened exciton
resonance. We find that disorder plays a critical role in the exciton VC, while
minimally affecting VP. For different monolayer samples with disorder
characterized by their Stokes Shift (SS), VC decreases in samples with higher
SS while VP again remains unchanged. These two methods consistently demonstrate
that VC as defined by the degree of linearly polarized photoluminescence is
more sensitive to disorder potential, motivating further theoretical studies.
"
"  Free Electron Lasers (FEL) are commonly regarded as the potential key
application of laser wakefield accelerators (LWFA). It has been found that
electron bunches exiting from state-of-the-art LWFAs exhibit a normalized
6-dimensional beam brightness comparable to those in conventional linear
accelerators. Effectively exploiting this beneficial beam property for
LWFA-based FELs is challenging due to the extreme initial conditions
particularly in terms of beam divergence and energy spread. Several different
approaches for capturing, reshaping and matching LWFA beams to suited
undulators, such as bunch decompression or transverse-gradient undulator
schemes, are currently being explored. In this article the transverse gradient
undulator concept will be discussed with a focus on recent experimental
achievements.
"
"  As part of the Fornax Deep Survey with the ESO VLT Survey Telescope, we
present new $g$ and $r$ bands mosaics of the SW group of the Fornax cluster. It
covers an area of $3 \times 2$ square degrees around the central galaxy
NGC1316. The deep photometry, the high spatial resolution of OmegaCam and the
large covered area allow us to study the galaxy structure, to trace stellar
halo formation and look at the galaxy environment. We map the surface
brightness profile out to 33arcmin ($\sim 200$kpc $\sim15R_e$) from the galaxy
centre, down to $\mu_g \sim 31$ mag arcsec$^{-2}$ and $\mu_r \sim 29$ mag
arcsec$^{-2}$. This allow us to estimate the scales of the main components
dominating the light distribution, which are the central spheroid, inside 5.5
arcmin ($\sim33$ kpc), and the outer stellar envelope. Data analysis suggests
that we are catching in act the second phase of the mass assembly in this
galaxy, since the accretion of smaller satellites is going on in both
components. The outer envelope of NGC1316 still hosts the remnants of the
accreted satellite galaxies that are forming the stellar halo. We discuss the
possible formation scenarios for NGC1316, by comparing the observed properties
(morphology, colors, gas content, kinematics and dynamics) with predictions
from cosmological simulations of galaxy formation. We find that {\it i)} the
central spheroid could result from at least one merging event, it could be a
pre-existing early-type disk galaxy with a lower mass companion, and {\it ii)}
the stellar envelope comes from the gradual accretion of small satellites.
"
"  In Kondo lattice systems with mixed valence, such as YbAl3, interactions
between localized electrons in a partially filled f shell and delocalized
conduction electrons can lead to fluctuations between two different valence
configurations with changing temperature or pressure. The impact of this change
on the momentum-space electronic structure and Fermi surface topology is
essential for understanding their emergent properties, but has remained
enigmatic due to a lack of appropriate experimental probes. Here by employing a
combination of molecular beam epitaxy (MBE) and in situ angle-resolved
photoemission spectroscopy (ARPES) we show that valence fluctuations can lead
to dramatic changes in the Fermi surface topology, even resulting in a Lifshitz
transition. As the temperature is lowered, a small electron pocket in YbAl3
becomes completely unoccupied while the low-energy ytterbium (Yb) 4f states
become increasingly itinerant, acquiring additional spectral weight, longer
lifetimes, and well-defined dispersions. Our work presents the first unified
picture of how local valence fluctuations connect to momentum space concepts
including band filling and Fermi surface topology in the longstanding problem
of mixed-valence systems.
"
"  A method of transmitting information in interstellar space at superluminal,
or $> c$, speeds is proposed. The information is encoded as phase modulation of
an electromagnetic wave of constant intensity, i.e. fluctuations in the rate of
energy transport plays no role in the communication, and no energy is
transported at speed $>$ c. Of course, such a constant wave can ultimately last
only the duration of its enveloping wave packet. However, as a unique feature
of this paper, we assume the source is sufficiently steady to be capable of
emitting wave packets, or pulses, of size much larger than the separation
between sender and receiver. Therefore, if a pre-existing and enduring wave
envelope already connects the two sides, the subluminal nature of the
envelope's group velocity will no longer slow down the communication, which is
now limited by the speed at which information encoded as phase modulation
propagates through the plasma, i.e. the phase velocity $v_p > c$. The method
involves no sharp structure in either time or frequency. As a working example,
we considered two spaceships separated by 1 lt-s in the local hot bubble.
Provided the bandwidth of the extra Fourier modes generated by the phase
modulation is much smaller than the carrier wave frequency, the radio
communication of a message, encoded as a specific alignment between the carrier
wave phase and the anomalous (modulated) phase, can take place at a speed in
excess of light by a few parts in 10$^{11}$ at $\nu\approx 1$~GHz, and higher
at smaller $\nu$.
"
"  We perform direct numerical simulations (DNS) of passive heavy inertial
particles (dust) in homogeneous and isotropic two-dimensional turbulent flows
(gas) for a range of Stokes number, ${\rm St} < 1$, using both Lagrangian and
Eulerian approach (with a shock-capturing scheme). We find that: The
dust-density field in our Eulerian simulations have the same correlation
dimension $d_2$ as obtained from the clustering of particles in the Lagrangian
simulations for ${\rm St} < 1$; The cumulative probability distribution
function of the dust-density coarse-grained over a scale $r$ in the inertial
range has a left-tail with a power-law fall-off indicating presence of voids;
The energy spectrum of the dust-velocity has a power-law range with an exponent
that is same as the gas-velocity spectrum except at very high Fourier modes;
The compressibility of the dust-velocity field is proportional to ${\rm St}^2$.
We quantify the topological properties of the dust-velocity and the
gas-velocity through their gradient matrices, called $\mathcal{A}$ and
$\mathcal{B}$, respectively. The topological properties of $\mathcal{B}$ are
the same in Eulerian and Lagrangian frames only if the Eulerian data are
weighed by the dust-density -- a correspondence that we use to study Lagrangian
properties of $\mathcal{A}$. In the Lagrangian frame, the mean value of the
trace of $\mathcal{A} \sim - \exp(-C/{\rm St}$, with a constant $C\approx 0.1$.
The topology of the dust-velocity fields shows that as ${\rm St} increases the
contribution to negative divergence comes mostly from saddles and the
contribution to positive divergence comes from both vortices and saddles.
Compared to the Eulerian case, the density-weighed Eulerian case has less
inward spirals and more converging saddles. Outward spirals are the least
probable topological structures in both cases.
"
"  Ordered chains (such as chains of amino acids) are ubiquitous in biological
cells, and these chains perform specific functions contingent on the sequence
of their components. Using the existence and general properties of such
sequences as a theoretical motivation, we study the statistical physics of
systems whose state space is defined by the possible permutations of an ordered
list, i.e., the symmetric group, and whose energy is a function of how certain
permutations deviate from some chosen correct ordering. Such a non-factorizable
state space is quite different from the state spaces typically considered in
statistical physics systems and consequently has novel behavior in systems with
interacting and even non-interacting Hamiltonians. Various parameter choices of
a mean-field model reveal the system to contain five different physical regimes
defined by two transition temperatures, a triple point, and a quadruple point.
Finally, we conclude by discussing how the general analysis can be extended to
state spaces with more complex combinatorial properties and to other standard
questions of statistical mechanics models.
"
"  Based on ab initio evolutionary crystal structure search computation, we
report a new phase of phosphorus called green phosphorus ({\lambda}-P), which
exhibits the direct band gaps ranging from 0.7 to 2.4 eV and the strong
anisotropy in optical and transport properties. Free energy calculations show
that a single-layer form, termed green phosphorene, is energetically more
stable than blue phosphorene and a phase transition from black to green
phosphorene can occur at temperatures above 87 K. Due to its buckled structure,
green phosphorene can be synthesized on corrugated metal surfaces rather than
clean surfaces.
"
"  We apply a generalized Kepler map theory to describe the qualitative chaotic
dynamics around cometary nuclei, based on accessible observational data for
five comets whose nuclei are well-documented to resemble dumb-bells. The sizes
of chaotic zones around the nuclei and the Lyapunov times of the motion inside
these zones are estimated. In the case of Comet 1P/Halley, the circumnuclear
chaotic zone seems to engulf an essential part of the Hill sphere, at least for
orbits of moderate to high eccentricity.
"
"  The basins of convergence, associated with the roots (attractors) of a
complex equation, are revealed in the Hill problem with oblateness and
radiation, using a large variety of numerical methods. Three cases are
investigated, regarding the values of the oblateness and radiation. In all
cases, a systematic and thorough scan of the complex plane is performed in
order to determine the basins of attraction of the several iterative schemes.
The correlations between the attracting domains and the corresponding required
number of iterations are also illustrated and discussed. Our numerical analysis
strongly suggests that the basins of convergence, with the highly fractal basin
boundaries, produce extraordinary and beautiful formations on the complex
plane.
"
"  Anions of the molecules ZnO, O2 and atomic Zn and O constitute mass spectra
of the species sputtered from pellets of molecular solid of ZnO under Cs+
irradiation. Their normalized yields are independent of energy of the
irradiating Cs+. Collision cascades cannot explain the simultaneous sputtering
of atoms and molecules. We propose that the origin of the molecular
sublimation, dissociation and subsequent emission is the result of localized
thermal spikes induced by individual Cs+ ions. The fractal dimension of binary
collision cascades of atomic recoils in the irradiated ZnO solid increases with
reduction in the energy of recoils. Upon reaching the collision diameters of
atomic dimensions, the space-filling fractal-like transition occurs where
cascades transform into thermal spikes. These localized thermal spikes induce
sublimation, dissociation and sputtering from the region. The calculated rates
of the subliming and dissociating species due to localized thermal spikes agree
well with the experimental results.
"
"  We have developed a system combining a back-illuminated
Complementary-Metal-Oxide-Semiconductor (CMOS) imaging sensor and Xilinx Zynq
System-on-Chip (SoC) device for a soft X-ray (0.5-10 keV) imaging spectroscopy
observation of the Sun to investigate the dynamics of the solar corona. Because
typical timescales of energy release phenomena in the corona span a few minutes
at most, we aim to obtain the corresponding energy spectra and derive the
physical parameters, i.e., temperature and emission measure, every few tens of
seconds or less for future solar X-ray observations. An X-ray photon-counting
technique, with a frame rate of a few hundred frames per second or more, can
achieve such results. We used the Zynq SoC device to achieve the requirements.
Zynq contains an ARM processor core, which is also known as the Processing
System (PS) part, and a Programmable Logic (PL) part in a single chip. We use
the PL and PS to control the sensor and seamless recording of data to a storage
system, respectively. We aim to use the system for the third flight of the
Focusing Optics Solar X-ray Imager (FOXSI-3) sounding rocket experiment for the
first photon-counting X-ray imaging and spectroscopy of the Sun.
"
"  The aim of this comment is to show that anisotropic effects and image fields
should not be omitted as they are in the publication of A. Leonardi, S. Ryu, N.
M. Pugno, and P. Scardi (LRPS) [J. Appl. Phys. 117, 164304 (2015)] on Pd <011>
cylindrical nanowires containing an axial screw dislocation. Indeed, according
to our previous study [Phys. Rev. B 88, 224101 (2013)], the axial displacement
field along the nanowire exhibits both a radial and an azimuthal dependence
with a twofold symmetry due the <011> orientation. As a consequence, the
deviatoric strain term used by LRPS is not suitable to analyze the anisotropic
strain fields that should be observed in their atomistic simulations. In this
comment, we first illustrate the importance of anisotropy in <011> Pd nanowire
by calculating the azimuthal dependence of the deviatoric strain term. Then the
expression of the anisotropic elastic field is recalled in term of strain
tensor components to show that image fields should be also considered. The
other aspect of this comment concerns the supposedly loss of correlation along
the nanorod caused by the twist. It is claimed for instance by LRPS that : ""As
an effect of the dislocation strain and twist, if the cylinder is long enough,
upper/lower regions tend to lose correlation, as if the rod were made of
different sub-domains."". This assertion appears to us misleading since for any
twist the position of all the atoms in the nanorod is perfectly defined and
therefore prevents any loss of correlation. To clarify this point, it should be
specified that this apparent loss of correlation can not be ascribed to the
twisted state of the nanowire but is rather due to a limitation of the X-ray
powder diffraction. Considering for instance coherent X-ray diffraction, we
show an example of high twist where the simulated diffractogram presents a
clear signature of the perfect correlation.
"
"  We investigate possible pathways for the formation of the low density
Neptune-mass planet HAT-P-26b. We use two formation different models based on
pebbles and planetesimals accretion, and includes gas accretion, disk migration
and simple photoevaporation. The models tracks the atmospheric oxygen
abundance, in addition to the orbital period, and mass of the forming planets,
that we compare to HAT-P-26b. We find that pebbles accretion can explain this
planet more naturally than planetesimals accretion that fails completely unless
we artificially enhance the disk metallicity significantly. Pebble accretion
models can reproduce HAT-P-26b with either a high initial core mass and low
amount of envelope enrichment through core erosion or pebbles dissolution, or
the opposite, with both scenarios being possible. Assuming a low envelope
enrichment factor as expected from convection theory and comparable to the
values we can infer from the D/H measurements in Uranus and Neptune, our most
probable formation pathway for HAT-P-26b is through pebble accretion starting
around 10 AU early in the disk's lifetime.
"
"  We study three-dimensional gauge theories based on orthogonal groups.
Depending on the global form of the group these theories admit discrete
$\theta$-parameters, which control the weights in the sum over topologically
distinct gauge bundles. We derive level-rank duality for these topological
field theories. Our results may also be viewed as level-rank duality for
$SO(N)_{K}$ Chern-Simons theory in the presence of background fields for
discrete global symmetries. In particular, we include the required counterterms
and analysis of the anomalies. We couple our theories to charged matter and
determine how these counterterms are shifted by integrating out massive
fermions. By gauging discrete global symmetries we derive new boson-fermion
dualities for vector matter, and present the phase diagram of theories with
two-index tensor fermions, thus extending previous results for $SO(N)$ to other
global forms of the gauge group.
"
"  We derive equations of motion for the reduced density matrix of a molecular
system which undergoes energy transfer dynamics competing with fast internal
conversion channels. Environmental degrees of freedom of such a system have no
time to relax to quasi-equilibrium in the electronic excited state of the donor
molecule, and thus the conditions of validity of Foerster and Modified Redfield
theories in their standard formulations do not apply. We derive non-equilibrium
versions of the two well-known rate theories and apply them to the case of
carotenoid-chlorophyll energy transfer. Although our reduced density matrix
approach does not account for the formation of vibronic excitons, it still
confirms the important role of the donor ground-state vibrational states in
establishing the resonance energy transfer conditions. We show that it is
essential to work with a theory valid in strong system-bath interaction regime
to obtain correct dependence of the rates on donor-acceptor energy gap.
"
"  The Weyl semimetallic compound Eu2Ir2O7 along with its hole doped derivatives
(which is achieved by substituting trivalent Eu by divalent Sr) are
investigated through transport, magnetic and calorimetric studies. The
metal-insulator transition (MIT) temperature is found to get substantially
reduced with hole doping and for 10% Sr doping the composition is metallic down
to temperature as low as 5 K. These doped compounds are found to violate the
Mott-Ioffe-Regel condition for minimum electrical conductivity and show
distinct signature of non-Fermi liquid behavior at low temperature. The MIT in
the doped compounds does not correlate with the magnetic transition point and
Anderson-Mott type disorder induced localization may be attributed to the
ground state insulating phase. The observed non-Fermi liquid behavior can be
understood on the basis of disorder induced distribution of spin orbit coupling
parameter which is markedly different in case of Ir4+ and Ir5+ ions.
"
"  Winds are predicted to be ubiquitous in low-mass, actively star-forming
galaxies. Observationally, winds have been detected in relatively few local
dwarf galaxies, with even fewer constraints placed on their timescales. Here,
we compare galactic outflows traced by diffuse, soft X-ray emission from
Chandra Space Telescope archival observations to the star formation histories
derived from Hubble Space Telescope imaging of the resolved stellar populations
in six starburst dwarfs. We constrain the longevity of a wind to have an upper
limit of 25 Myr based on galaxies whose starburst activity has already
declined, although a larger sample is needed to confirm this result. We find an
average 16% efficiency for converting the mechanical energy of stellar feedback
to thermal, soft X-ray emission on the 25 Myr timescale, somewhat higher than
simulations predict. The outflows have likely been sustained for timescales
comparable to the duration of the starbursts (i.e., 100's Myr), after taking
into account the time for the development and cessation of the wind. The wind
timescales imply that material is driven to larger distances in the
circumgalactic medium than estimated by assuming short, 5-10 Myr starburst
durations, and that less material is recycled back to the host galaxy on short
timescales. In the detected outflows, the expelled hot gas shows various
morphologies which are not consistent with a simple biconical outflow
structure. The sample and analysis are part of a larger program, the STARBurst
IRregular Dwarf Survey (STARBIRDS), aimed at understanding the lifecycle and
impact of starburst activity in low-mass systems.
"
"  We present a compact current sensor based on a superconducting microwave
lumped-element resonator with a nanowire kinetic inductor, operating at 4.2 K.
The sensor is suitable for multiplexed readout in GHz range of large-format
arrays of cryogenic detectors. The device consists of a lumped-element resonant
circuit, fabricated from a single 4-nm-thick superconducting layer of niobium
nitride. Thus, the fabrication and operation is significantly simplified in
comparison to state-of-the-art approaches. Because the resonant circuit is
inductively coupled to the feed line the current to be measured can directly be
injected without having the need of an impedance matching circuit, reducing the
system complexity. With the proof-of-concept device we measured a current noise
floor {\delta}Imin of 10 pA/Hz1/2 at 10 kHz. Furthermore, we demonstrate the
ability of our sensor to amplify a pulsed response of a superconducting
nanowire single-photon detector using a GHz-range carrier for effective
frequency-division multiplexing.
"
"  To understand the evolution of extinction curve, we calculate the dust
evolution in a galaxy using smoothed particle hydrodynamics simulations
incorporating stellar dust production, dust destruction in supernova shocks,
grain growth by accretion and coagulation, and grain disruption by shattering.
The dust species are separated into carbonaceous dust and silicate. The
evolution of grain size distribution is considered by dividing grain population
into large and small gains, which allows us to estimate extinction curves. We
examine the dependence of extinction curves on the position, gas density, and
metallicity in the galaxy, and find that extinction curves are flat at $t
\lesssim 0.3$ Gyr because stellar dust production dominates the total dust
abundance. The 2175 \AA\ bump and far-ultraviolet (FUV) rise become prominent
after dust growth by accretion. At $t \gtrsim 3$ Gyr, shattering works
efficiently in the outer disc and low density regions, so extinction curves
show a very strong 2175 \AA\ bump and steep FUV rise. The extinction curves at
$t\gtrsim 3$ Gyr are consistent with the Milky Way extinction curve, which
implies that we successfully included the necessary dust processes in the
model. The outer disc component caused by stellar feedback has an extinction
curves with a weaker 2175 \AA\ bump and flatter FUV slope. The strong
contribution of carbonaceous dust tends to underproduce the FUV rise in the
Small Magellanic Cloud extinction curve, which supports selective loss of small
carbonaceous dust in the galaxy. The snapshot at young ages also explain the
extinction curves in high-redshift quasars.
"
"  Recent work using plasmonic nanosensors in a clinically relevant detection
assay reports extreme sensitivity based upon a mechanism termed 'inverse
sensitivity', whereby reduction of substrate concentration increases reaction
rate, even at the single-molecule limit. This near-homoeopathic mechanism
contradicts the law of mass action. The assay involves deposition of silver
atoms upon gold nanostars, changing their absorption spectrum. Multiple
additional aspects of the assay appear to be incompatible with settled chemical
knowledge, in particular the detection of tiny numbers of silver atoms on a
background of the classic 'silver mirror reaction'. Finally, it is estimated
here that the reported spectral changes require some 2.5E11 times more silver
atoms than are likely to be produced. It is suggested that alternative
explanations must be sought for the original observations.
"
"  Quantum Moves is a citizen science game that investigates the ability of
humans to solve complex physics challenges that are intractable for computers.
During the launch of Quantum Moves in April 2016 the game's leaderboard
function broke down resulting in a ""no leaderboard"" game experience for some
players for a couple of days (though their scores were still displayed). The
subsequent quick fix of an all-time Top 5 leaderboard, and the following
long-term implementation of a personalized relative-position (infinite)
leaderboard provided us with a unique opportunity to compare and investigate
the effect of different leaderboard implementations on player performance in a
points-driven citizen science game.
All three conditions were live sequentially during the game's initial influx
of more than 150.000 players that stemmed from global press attention on
Quantum Moves due the publication of a Nature paper about the use of Quantum
Moves in solving a specific quantum physics problem. Thus, it has been possible
to compare the three conditions and their influence on the performance (defined
as a player's quality of game play related to a high-score) of over 4500 new
players. These 4500 odd players in our three leaderboard-conditions have a
similar demographic background based upon the time-window over which the
implementations occurred and controlled against Player ID tags. Our results
placed Condition 1 experience over condition 3 and in some cases even over
condition 2 which goes against the general assumption that leaderboards enhance
gameplay and its subsequent overuse as a an oft-relied upon element that
designers slap onto a game to enhance said appeal. Our study thus questions the
use of leaderboards as general performance enhancers in gamification contexts
and brings some empirical rigor to an often under-reported but overused
phenomenon.
"
"  In seismic monitoring one is usually interested in the response of a changing
target zone, embedded in a static inhomogeneous medium. We introduce an
efficient method which predicts reflection responses at the earth's surface for
different target-zone scenarios, from a single reflection response at the
surface and a model of the changing target zone. The proposed process consists
of two main steps. In the first step, the response of the original target zone
is removed from the reflection response, using the Marchenko method. In the
second step, the modelled response of a new target zone is inserted between the
overburden and underburden responses. The method fully accounts for all orders
of multiple scattering and, in the elastodynamic case, for wave conversion. For
monitoring purposes, only the second step needs to be repeated for each
target-zone model. Since the target zone covers only a small part of the entire
medium, the proposed method is much more efficient than repeated modelling of
the entire reflection response.
"
"  Two procedures for checking Bayesian models are compared using a simple test
problem based on the local Hubble expansion. Over four orders of magnitude,
p-values derived from a global goodness-of-fit criterion for posterior
probability density functions (Lucy 2017) agree closely with posterior
predictive p-values. The former can therefore serve as an effective proxy for
the difficult-to-calculate posterior predictive p-values.
"
"  Disordered many-particle hyperuniform systems are exotic amorphous states of
matter that lie between crystals and liquids. Hyperuniform systems have
attracted recent attention because they are endowed with novel transport and
optical properties. Recently, the hyperuniformity concept has been generalized
to characterize scalar fields, two-phase media and random vector fields. In
this paper, we devise methods to explicitly construct hyperuniform scalar
fields. We investigate explicitly spatial patterns generated from Gaussian
random fields, which have been used to model the microwave background radiation
and heterogeneous materials, the Cahn-Hilliard equation for spinodal
decomposition, and Swift-Hohenberg equations that have been used to model
emergent pattern formation, including Rayleigh-B{\' e}nard convection. We show
that the Gaussian random scalar fields can be constructed to be hyperuniform.
We also numerically study the time evolution of spinodal decomposition patterns
and demonstrate that these patterns are hyperuniform in the scaling regime.
Moreover, we find that labyrinth-like patterns generated by the Swift-Hohenberg
equation are effectively hyperuniform. We show that thresholding a hyperuniform
Gaussian random field to produce a two-phase random medium tends to destroy the
hyperuniformity of the progenitor scalar field. We then propose guidelines to
achieve effectively hyperuniform two-phase media derived from thresholded
non-Gaussian fields. Our investigation paves the way for new research
directions to characterize the large-structure spatial patterns that arise in
physics, chemistry, biology and ecology. Moreover, our theoretical results are
expected to guide experimentalists to synthesize new classes of hyperuniform
materials with novel physical properties via coarsening processes and using
state-of-the-art techniques, such as stereolithography and 3D printing.
"
"  We present new viscosity measurements of a synthetic silicate system
considered an analogue for the lava erupted on the surface of Mercury. In
particular, we focus on the northern volcanic plains (NVP), which correspond to
the largest lava flows on Mercury and possibly in the Solar System.
High-temperature viscosity measurements were performed at both superliquidus
(up to 1736 K) and subliquidus conditions (1569-1502 K) to constrain the
viscosity variations as a function of crystallinity (from 0 to 28\%) and shear
rate (from 0.1 to 5 s 1). Melt viscosity shows moderate variations (4-16 Pa s)
in the temperature range of 1736-1600 K. Experiments performed below the
liquidus temperature show an increase in viscosity as shear rate decreases from
5 to 0.1 s 1, resulting in a shear thinning behavior, with a decrease in
viscosity of 1 log unit. The low viscosity of the studied composition may
explain the ability of NVP lavas to cover long distances, on the order of
hundreds of kilometers in a turbulent flow regime. Using our experimental data
we estimate that lava flows with thickness of 1, 5, and 10 m are likely to have
velocities of 4.8, 6.5, and 7.2 m/s, respectively, on a 5 degree ground slope.
Numerical modeling incorporating both the heat loss of the lavas and its
possible crystallization during emplacement allows us to infer that high
effusion rates (>10,000 m3/s) are necessary to cover the large distances
indicated by satellite data from the MErcury Surface, Space ENvironment,
GEochemistry, and Ranging spacecraft.
"
"  We implemented various DFT+U schemes, including the ACBN0 self-consistent
density-functional version of the DFT+U method [Phys. Rev. X 5, 011006 (2015)]
within the massively parallel real-space time-dependent density functional
theory (TDDFT) code Octopus. We further extended the method to the case of the
calculation of response functions with real-time TDDFT+U and to the description
of non-collinear spin systems. The implementation is tested by investigating
the ground-state and optical properties of various transition metal oxides,
bulk topological insulators, and molecules. Our results are found to be in good
agreement with previously published results for both the electronic band
structure and structural properties. The self consistent calculated values of U
and J are also in good agreement with the values commonly used in the
literature. We found that the time-dependent extension of the self-consistent
DFT+U method yields improved optical properties when compared to the empirical
TDDFT+U scheme. This work thus opens a different theoretical framework to
address the non equilibrium properties of correlated systems.
"
"  Tests on $B-L$ symmetry breaking models are important probes to search for
new physics. One proposed model with $\Delta(B-L)=2$ involves the oscillations
of a neutron to an antineutron. In this paper a new limit on this process is
derived for the data acquired from all three operational phases of the Sudbury
Neutrino Observatory experiment. The search was concentrated in oscillations
occurring within the deuteron, and 23 events are observed against a background
expectation of 30.5 events. These translate to a lower limit on the nuclear
lifetime of $1.48\times 10^{31}$ years at 90% confidence level (CL) when no
restriction is placed on the signal likelihood space (unbounded).
Alternatively, a lower limit on the nuclear lifetime was found to be
$1.18\times 10^{31}$ years at 90% CL when the signal was forced into a positive
likelihood space (bounded). Values for the free oscillation time derived from
various models are also provided in this article. This is the first search for
neutron-antineutron oscillation with the deuteron as a target.
"
"  The geologic activity at Enceladus's south pole remains unexplained, though
tidal deformations are probably the ultimate cause. Recent gravity and
libration data indicate that Enceladus's icy crust floats on a global ocean, is
rather thin, and has a strongly non-uniform thickness. Tidal effects are
enhanced by crustal thinning at the south pole, so that realistic models of
tidal tectonics and dissipation should take into account the lateral variations
of shell structure. I construct here the theory of non-uniform viscoelastic
thin shells, allowing for depth-dependent rheology and large lateral variations
of shell thickness and rheology. Coupling to tides yields two 2D linear partial
differential equations of the 4th order on the sphere which take into account
self-gravity, density stratification below the shell, and core viscoelasticity.
If the shell is laterally uniform, the solution agrees with analytical formulas
for tidal Love numbers; errors on displacements and stresses are less than 5%
and 15%, respectively, if the thickness is less than 10% of the radius. If the
shell is non-uniform, the tidal thin shell equations are solved as a system of
coupled linear equations in a spherical harmonic basis. Compared to finite
element models, thin shell predictions are similar for the deformations due to
Enceladus's pressurized ocean, but differ for the tides of Ganymede. If
Enceladus's shell is conductive with isostatic thickness variations, surface
stresses are approximately inversely proportional to the local shell thickness.
The radial tide is only moderately enhanced at the south pole. The combination
of crustal thinning and convection below the poles can amplify south polar
stresses by a factor of 10, but it cannot explain the apparent time lag between
the maximum plume brightness and the opening of tiger stripes. In a second
paper, I will study tidal dissipation in a non-uniform crust.
"
"  The present paper reports on our effort to characterize vortical interactions
in complex fluid flows through the use of network analysis. In particular, we
examine the vortex interactions in two-dimensional decaying isotropic
turbulence and find that the vortical interaction network can be characterized
by a weighted scale-free network. It is found that the turbulent flow network
retains its scale-free behavior until the characteristic value of circulation
reaches a critical value. Furthermore, we show that the two-dimensional
turbulence network is resilient against random perturbations but can be greatly
influenced when forcing is focused towards the vortical structures that are
categorized as network hubs. These findings can serve as a network-analytic
foundation to examine complex geophysical and thin-film flows and take
advantage of the rapidly growing field of network theory, which complements
ongoing turbulence research based on vortex dynamics, hydrodynamic stability,
and statistics. While additional work is essential to extend the mathematical
tools from network analysis to extract deeper physical insights of turbulence,
an understanding of turbulence based on the interaction-based network-theoretic
framework presents a promising alternative in turbulence modeling and control
efforts.
"
"  We developed control and visualization programs, YUI and HANA, for High-
Resolution Chopper spectrometer (HRC) installed at BL12 in MLF, J-PARC. YUI is
a comprehensive program to control DAQ-middleware, the accessories, and sample
environment devices. HANA is a program for the data transformation and
visualization of inelastic neutron scattering spectra. In this paper, we
describe the basic system structures and unique functions of these programs
from the viewpoint of users.
"
"  Cryo-electron microscopy provides 2-D projection images of the 3-D electron
scattering intensity of many instances of the particle under study (e.g., a
virus). Both symmetry (rotational point groups) and heterogeneity are important
aspects of biological particles and both aspects can be combined by describing
the electron scattering intensity of the particle as a stochastic process with
a symmetric probability law and therefore symmetric moments. A maximum
likelihood estimator implemented by an expectation-maximization algorithm is
described which estimates the unknown statistics of the electron scattering
intensity stochastic process from images of instances of the particle. The
algorithm is demonstrated on the bacteriophage HK97 and the virus N$\omega$V.
The results are contrasted with existing algorithms which assume that each
instance of the particle has the symmetry rather than the less restrictive
assumption that the probability law has the symmetry.
"
"  I investigate the nightly mean emission height and width of the OH*(3-1)
layer by comparing nightly mean temperatures measured by the ground-based
spectrometer GRIPS 9 and the Na lidar at ALOMAR. The data set contains 42
coincident measurements between November 2010 and February 2014, when GRIPS 9
was in operation at the ALOMAR observatory (69.3$^\circ$N, 16.0$^\circ$E) in
northern Norway. To closely resemble the mean temperature measured by GRIPS 9,
I weight each nightly mean temperature profile measured by the lidar using
Gaussian distributions with 40 different centre altitudes and 40 different full
widths at half maximum. In principle, one can thus determine the altitude and
width of an airglow layer by finding the minimum temperature difference between
the two instruments. On most nights, several combinations of centre altitude
and width yield a temperature difference of $\pm$2 K. The generally assumed
altitude of 87 km and width of 8 km is never an unambiguous, good solution for
any of the measurements. Even for a fixed width of $\sim$8.4 km, one can
sometimes find several centre altitudes that yield equally good temperature
agreement. Weighted temperatures measured by lidar are not suitable to
determine unambiguously the emission height and width of an airglow layer.
However, when actual altitude and width data are lacking, a comparison with
lidars can provide an estimate of how representative a measured rotational
temperature is of an assumed altitude and width. I found the rotational
temperature to represent the temperature at the commonly assumed altitude of
87.4 km and width of 8.4 km to within $\pm$16 K, on average. This is not a
measurement uncertainty.
"
"  The spinel/perovskite heterointerface $\gamma$-Al$_2$O$_3$/SrTiO$_3$ hosts a
two-dimensional electron system (2DES) with electron mobilities exceeding those
in its all-perovskite counterpart LaAlO$_3$/SrTiO$_3$ by more than an order of
magnitude despite the abundance of oxygen vacancies which act as electron
donors as well as scattering sites. By means of resonant soft x-ray
photoemission spectroscopy and \textit{ab initio} calculations we reveal the
presence of a sharply localized type of oxygen vacancies at the very interface
due to the local breaking of the perovskite symmetry. We explain the
extraordinarily high mobilities by reduced scattering resulting from the
preferential formation of interfacial oxygen vacancies and spatial separation
of the resulting 2DES in deeper SrTiO$_3$ layers. Our findings comply with
transport studies and pave the way towards defect engineering at interfaces of
oxides with different crystal structures.
"
"  Most optical and IR spectra are now acquired using detectors with
finite-width pixels in a square array. This paper examines the effects of such
pixellation, using computed simulations to illustrate the effects which most
concern the astronomer end-user. Coarse sampling increases the random noise
errors in wavelength by typically 10 - 20% at 2 pixels/FWHM, but with wide
variation depending on the functional form of the instrumental Line Spread
Function (LSF) and on the pixel phase. Line widths are even more strongly
affected at low sampling frequencies. However, the noise in fitted peak
amplitudes is minimally affected. Pixellation has a substantial but complex
effect on the ability to see a relative minimum between two closely-spaced
peaks (or relative maximum between two absorption lines). The consistent scale
of resolving power presented by Robertson (2013) is extended to cover
pixellated spectra. The systematic bias errors in wavelength introduced by
pixellation are examined. While they may be negligible for smooth well-sampled
symmetric LSFs, they are very sensitive to asymmetry and high spatial frequency
substructure. The Modulation Transfer Function for sampled data is shown to
give a useful indication of the extent of improperly sampled signal in an LSF.
The common maxim that 2 pixels/FWHM is the Nyquist limit is incorrect and most
LSFs will exhibit some aliasing at this sample frequency. While 2 pixels/FWHM
is often an acceptable minimum for moderate signal/noise work, it is preferable
to carry out simulations for any actual or proposed LSF to find the effects of
sampling frequency. Where end-users have a choice of sampling frequencies,
through on-chip binning and/or spectrograph configurations, the instrument user
manual should include an examination of their effects. (Abridged)
"
"  Previous secondary eclipse observations of the hot Jupiter Qatar-1b in the Ks
band suggest that it may have an unusually high day side temperature,
indicative of minimal heat redistribution. There have also been indications
that the orbit may be slightly eccentric, possibly forced by another planet in
the system. We investigate the day side temperature and orbital eccentricity
using secondary eclipse observations with Spitzer. We observed the secondary
eclipse with Spitzer/IRAC in subarray mode, in both 3.6 and 4.5 micron
wavelengths. We used pixel-level decorrelation to correct for Spitzer's
intra-pixel sensitivity variations and thereby obtain accurate eclipse depths
and central phases. Our 3.6 micron eclipse depth is 0.149 +/- 0.051% and the
4.5 micron depth is 0.273 +/- 0.049%. Fitting a blackbody planet to our data
and two recent Ks band eclipse depths indicates a brightness temperature of
1506 +/- 71K. Comparison to model atmospheres for the planet indicates that its
degree of longitudinal heat redistribution is intermediate between fully
uniform and day side only. The day side temperature of the planet is unlikely
to be as high (1885K) as indicated by the ground-based eclipses in the Ks band,
unless the planet's emergent spectrum deviates strongly from model atmosphere
predictions. The average central phase for our Spitzer eclipses is 0.4984 +/-
0.0017, yielding e cos(omega) = -0.0028 +/- 0.0027. Our results are consistent
with a circular orbit, and we constrain e cos(omega) much more strongly than
has been possible with previous observations.
"
"  Using a combination of analytic and numerical methods, we study the
polarizability of a (non-interacting) Anderson insulator in one, two, and three
dimensions and demonstrate that, in a wide range of parameters, it scales
proportionally to the square of the localization length, contrary to earlier
claims based on the effective-medium approximation. We further analyze the
effect of electron-electron interactions on the dielectric constant in
quasi-1D, quasi-2D and 3D materials with large localization length, including
both Coulomb repulsion and phonon-mediated attraction. The phonon-mediated
attraction (in the pseudogapped state on the insulating side of the
Superconductor-Insulator Transition) produces a correction to the dielectric
constant, which may be detected from a linear response of a dielectric constant
to an external magnetic field.
"
"  Magnetic fields are ubiquitous in the Universe. Extragalactic disks, halos
and clusters have consistently been shown, via diffuse radio-synchrotron
emission and Faraday rotation measurements, to exhibit magnetic field strengths
ranging from a few nG to tens of $\mu$G. The energy density of these fields is
typically comparable to the energy density of the fluid motions of the plasma
in which they are embedded, making magnetic fields essential players in the
dynamics of the luminous matter. The standard theoretical model for the origin
of these strong magnetic fields is through the amplification of tiny seed
fields via turbulent dynamo to the level consistent with current observations.
Here we demonstrate, using laser-produced colliding plasma flows, that
turbulence is indeed capable of rapidly amplifying seed fields to near
equipartition with the turbulent fluid motions. These results support the
notion that turbulent dynamo is a viable mechanism responsible for the observed
present-day magnetization of the Universe.
"
"  The multiple colliding laser pulse concept formulated in Ref. [1] is
beneficial for achieving an extremely high amplitude of coherent
electromagnetic field. Since the topology of electric and magnetic fields
oscillating in time of multiple colliding laser pulses is far from trivial and
the radiation friction effects are significant in the high field limit, the
dynamics of charged particles interacting with the multiple colliding laser
pulses demonstrates remarkable features corresponding to random walk
trajectories, limit circles, attractors, regular patterns and Levy flights.
Under extremely high intensity conditions the nonlinear dissipation mechanism
stabilizes the particle motion resulting in the charged particle trajectory
being located within narrow regions and in the occurrence of a new class of
regular patterns made by the particle ensembles.
"
"  Water and hydroxyl, once thought to be found only in the primitive airless
bodies that formed beyond roughly 2.5-3 AU, have recently been detected on the
Moon and Vesta, which both have surfaces dominated by evolved, non-primitive
compositions. In both these cases, the water/OH is thought to be exogenic,
either brought in via impacts with comets or hydrated asteroids or created via
solar wind interactions with silicates in the regolith or both. Such exogenic
processes should also be occurring on other airless body surfaces. To test this
hypothesis, we used the NASA Infrared Telescope Facility (IRTF) to measure
reflectance spectra (2.0 to 4.1 {\mu}m) of two large near-Earth asteroids
(NEAs) with compositions generally interpreted as anhydrous: 433 Eros and 1036
Ganymed. OH is detected on both of these bodies in the form of absorption
features near 3 {\mu}m. The spectra contain a component of thermal emission at
longer wavelengths, from which we estimate thermal of 167+/- 98 J m-2s-1/2K-1
for Eros (consistent with previous estimates) and 214+/- 80 J m-2s-1/2K-1 for
Ganymed, the first reported measurement of thermal inertia for this object.
These observations demonstrate that processes responsible for water/OH creation
on large airless bodies also act on much smaller bodies.
"
"  We propose a general framework for studying jump-diffusion systems driven by
both Gaussian noise and a jump process with state-dependent intensity. Of
particular natural interest are the jump locations: the system evaluated at the
jump times. However, the state-dependence of the jump rate provides direct
coupling between the diffusion and jump components, making disentangling the
two to study individually difficult. We provide an iterative map formulation of
the sequence of distributions of jump locations. Computation of these
distributions allows for the extraction of the interjump time statistics. These
quantities reveal a relationship between the long-time distribution of jump
location and the stationary density of the full process. We provide a few
examples to demonstrate the analytical and numerical tools stemming from the
results proposed in the paper, including an application that shows a
non-monotonic dependence on the strength of diffusion.
"
"  We have recently suggested that dust growth in the cold gas phase dominates
the dust abundance in elliptical galaxies while dust is efficiently destroyed
in the hot X-ray emitting plasma (hot gas). In order to understand the dust
evolution in elliptical galaxies, we construct a simple model that includes
dust growth in the cold gas and dust destruction in the hot gas. We also take
into account the effect of mass exchange between these two gas components
induced by active galactic nucleus (AGN) feedback. We survey reasonable ranges
of the relevant parameters in the model and find that AGN feedback cycles
actually produce a variety in cold gas mass and dust-to-gas ratio. By comparing
with an observational sample of nearby elliptical galaxies, we find that,
although the dust-to-gas ratio varies by an order of magnitude in our model,
the entire range of the observed dust-to-gas ratios is difficult to be
reproduced under a single parameter set. Variation of the dust growth
efficiency is the most probable solution to explain the large variety in
dust-to-gas ratio of the observational sample. Therefore, dust growth can play
a central role in creating the variation in dust-to-gas ratio through the AGN
feedback cycle and through the variation in dust growth efficiency.
"
"  We report the results of the 2dF-VST ATLAS Cold Spot galaxy redshift survey
(2CSz) based on imaging from VST ATLAS and spectroscopy from 2dF AAOmega over
the core of the CMB Cold Spot. We sparsely surveyed the inner 5$^{\circ}$
radius of the Cold Spot to a limit of $i_{AB} \le 19.2$, sampling $\sim7000$
galaxies at $z<0.4$. We have found voids at $z=$ 0.14, 0.26 and 0.30 but they
are interspersed with small over-densities and the scale of these voids is
insufficient to explain the Cold Spot through the $\Lambda$CDM ISW effect.
Combining with previous data out to $z\sim1$, we conclude that the CMB Cold
Spot could not have been imprinted by a void confined to the inner core of the
Cold Spot. Additionally we find that our 'control' field GAMA G23 shows a
similarity in its galaxy redshift distribution to the Cold Spot. Since the GAMA
G23 line-of-sight shows no evidence of a CMB temperature decrement we conclude
that the Cold Spot may have a primordial origin rather than being due to
line-of-sight effects.
"
"  We construct a toy a model which demonstrates that large field single scalar
inflation can produce an arbitrarily small scalar to tensor ratio in the window
of e-foldings recoverable from CMB experiments. This is done by generalizing
the $\alpha$-attractor models to allow the potential to approach a constant as
rapidly as we desire for super-planckian field values. This implies that a
non-detection of r alone can never rule out entirely the theory of large field
inflation.
"
"  We present the procedure to build and validate the bright-star masks for the
Hyper-Suprime-Cam Strategic Subaru Proposal (HSC-SSP) survey. To identify and
mask the saturated stars in the full HSC-SSP footprint, we rely on the Gaia and
Tycho-2 star catalogues. We first assemble a pure star catalogue down to
$G_{\rm Gaia} < 18$ after removing $\sim1.5\%$ of sources that appear extended
in the Sloan Digital Sky Survey (SDSS). We perform visual inspection on the
early data from the S16A internal release of HSC-SSP, finding that our star
catalogue is $99.2\%$ pure down to $G_{\rm Gaia} < 18$. Second, we build the
mask regions in an automated way using stacked detected source measurements
around bright stars binned per $G_{\rm Gaia}$ magnitude. Finally, we validate
those masks from visual inspection and comparison with the literature of galaxy
number counts and angular two-point correlation functions. This version
(Arcturus) supersedes the previous version (Sirius) used in the S16A internal
and DR1 public releases. We publicly release the full masks and tools to flag
objects in the entire footprint of the planned HSC-SSP observations at this
address: this ftp URL.
"
"  Advanced satellite-based frequency transfers by TWCP and IPPP have been
performed between NICT and KRISS. We confirm that the disagreement between them
is less than 1x10^{-16} at an averaging time of several days. Additionally, an
intercontinental frequency ratio measurement of Sr and Yb optical lattice
clocks was directly performed by TWCP. We achieved an uncertainty at the
mid-10^{-16} level after a total measurement time of 12 hours. The frequency
ratio was consistent with the recently reported values within the uncertainty.
"
"  The Atacama Millimeter/submillimeter Array (ALMA) Phasing Project (APP) has
developed and deployed the hardware and software necessary to coherently sum
the signals of individual ALMA antennas and record the aggregate sum in Very
Long Baseline Interferometry (VLBI) Data Exchange Format. These beamforming
capabilities allow the ALMA array to collectively function as the equivalent of
a single large aperture and participate in global VLBI arrays. The inclusion of
phased ALMA in current VLBI networks operating at (sub)millimeter wavelengths
provides an order of magnitude improvement in sensitivity, as well as
enhancements in u-v coverage and north-south angular resolution. The
availability of a phased ALMA enables a wide range of new ultra-high angular
resolution science applications, including the resolution of supermassive black
holes on event horizon scales and studies of the launch and collimation of
astrophysical jets. It also provides a high-sensitivity aperture that may be
used for investigations such as pulsar searches at high frequencies. This paper
provides an overview of the ALMA Phasing System design, implementation, and
performance characteristics.
"
"  We simulate a rotating 2D BEC to study the melting of a vortex lattice in
presence of random impurities. Impurities are introduced either through a
protocol in which vortex lattice is produced in an impurity potential or first
creating the vortex lattice in the absence of random pinning and then cranking
up the (co-rotating) impurity potential. We find that for a fixed strength,
pinning of vortices at randomly distributed impurities leads to the new states
of vortex lattice. It is unearthed that the vortex lattice follow a two-step
melting via loss of positional and orientational order. Also, the comparisons
between the states obtained in two protocols show that the vortex lattice
states are metastable states when impurities are introduced after the formation
of an ordered vortex lattice. We also show the existence of metastable states
which depend on the history of how the vortex lattice is created.
"
"  The formalism of the reduced density matrix is pursued in both length and
velocity gauges of the perturbation to the crystal Hamiltonian. The covariant
derivative is introduced as a convenient representation of the position
operator. This allow us to write compact expressions for the reduced density
matrix in any order of the perturbation which simplifies the calculations of
nonlinear optical responses; as an example, we compute the first and third
order contributions of the monolayer graphene. Expressions obtained in both
gauges share the same formal structure, allowing a comparison of the effects of
truncation to a finite set of bands. This truncation breaks the equivalence
between the two approaches: its proper implementation can be done directly in
the expressions derived in the length gauge, but require a revision of the
equations of motion of the reduced density matrix in the velocity gauge.
"
"  We report muon spin relaxation ($\mu$SR) measurements of optimally-doped and
overdoped Bi$_{2+x}$Sr$_{2-x}$CaCu$_2$O$_{8+\delta}$ (Bi2212) single crystals
that reveal the presence of a weak temperature-dependent quasi-static internal
magnetic field of electronic origin in the superconducting (SC) and pseudogap
(PG) phases. In both samples the internal magnetic field persists up to 160~K,
but muon diffusion prevents following the evolution of the field to higher
temperatures. We consider the evidence from our measurments in support of PG
order parameter candidates, namely, electronic loop currents and
magnetoelectric quadrupoles.
"
"  Colloidal migration in temperature gradient is referred to as thermophoresis.
In contrast to particles with spherical shape, we show that elongated colloids
may have a thermophoretic response that varies with the colloid orientation.
Remarkably, this can translate into a non-vanishing thermophoretic force in the
direction perpendicular to the temperature gradient. Oppositely to the friction
force, the thermophoretic force of a rod oriented with the temperature gradient
can be larger or smaller than when oriented perpendicular to it. The precise
anisotropic thermophoretic behavior clearly depends on the colloidal rod aspect
ratio, and also on its surface details, which provides an interesting
tunability to the devices constructed based on this principle. By means of
mesoscale hydrodynamic simulations, we characterize this effect for different
types of rod-like colloids.
"
"  Growth, electronic and magnetic properties of $\gamma'$-Fe$_{4}$N atomic
layers on Cu(001) are studied by scanning tunneling microscopy/spectroscopy and
x-ray absorption spectroscopy/magnetic circular dichroism. A continuous film of
ordered trilayer $\gamma'$-Fe$_{4}$N is obtained by Fe deposition under N$_{2}$
atmosphere onto monolayer Fe$_{2}$N/Cu(001), while the repetition of a
bombardment with 0.5 keV N$^{+}$ ions during growth cycles results in imperfect
bilayer $\gamma'$-Fe$_{4}$N. The increase in the sample thickness causes the
change of the surface electronic structure, as well as the enhancement in the
spin magnetic moment of Fe atoms reaching $\sim$ 1.4 $\mu_{\mathrm B}$/atom in
the trilayer sample. The observed thickness-dependent properties of the system
are well interpreted by layer-resolved density of states calculated using first
principles, which demonstrates the strongly layer-dependent electronic states
within each surface, subsurface, and interfacial plane of the
$\gamma'$-Fe$_{4}$N atomic layers on Cu(001).
"
"  Multilevel converters have found many applications within renewable energy
systems thanks to their unique capability of generating multiple voltage
levels. However, these converters need multiple DC sources and the voltage
balancing over capacitors for these systems is cumbersome. In this work, a new
grid-tie multicell inverter with high level of safety has been designed,
engineered and optimized for integrating energy storage devices to the electric
grid. The multilevel converter proposed in this work is capable of maintaining
the flying capacitors voltage in the desired value. The solar cells are the
primary energy sources for proposed inverter where the maximum power density is
obtained. Finally, the performance of the inverter and its control method
simulated using PSCAD/EMTDC software package and good agreement achieved with
experimental data.
"
"  In this paper, theoretical and numerical studies of perfect/nearly-perfect
conversion of a plane wave into a surface wave are presented. The problem of
determining the electromagnetic properties of an inhomogeneous lossless
boundary which would fully transform an incident plane wave into a surface wave
propagating along the boundary is considered. An approximate field solution
which produces a slowly growing surface wave and satisfies the energy
conservation law is discussed and numerically demonstrated. The results of the
study are of great importance for the future development of such devices as
perfect leaky-wave antennas and can potentially lead to many novel
applications.
"
"  The distribution of N/O abundance ratios calculated by the detailed modelling
of different galaxy spectra at z<4 is investigated. Supernova (SN) and long
gamma-ray-burst (LGRB) host galaxies cover different redshift domains. N/O in
SN hosts increases due to secondary N production towards low z (0.01)
accompanying the growing trend of active galaxies (AGN, LINER). N/O in LGRB
hosts decreases rapidly between z>1 and z ~0.1 following the N/H trend and
reach the characteristic N/O ratios calculated for the HII regions in local and
nearby galaxies. The few short period GRB (SGRB) hosts included in the galaxy
sample show N/H <0.04 solar and O/H solar. They seem to continue the low bound
N/H trend of SN hosts at z<0.3. The distribution of N/O as function of
metallicity for SN and LGRB hosts is compared with star chemical evolution
models. The results show that several LGRB hosts can be explained by star
multi-bursting models when 12+log(O/H) <8.5, while some objects follow the
trend of continuous star formation models. N/O in SN hosts at log(O/H)+12 <8.5
are not well explained by stellar chemical evolution models calculated for
starburst galaxies. At 12+log(O/H) >8.5 many different objects are nested close
to O/H solar with N/O ranging between the maximum corresponding to starburst
galaxies and AGN and the minimum corresponding to HII regions and SGRB.
"
"  In this work, we study the nonlinear traveling waves in density stratified
fluids with depth varying shear currents. Beginning the formulation of the
water-wave problem due to [1], we extend the work of [4] and [18] to examine
the interface between two fluids of differing densities and varying linear
shear. We derive as systems of equations depending only on variables at the
interface, and numerically solve for periodic traveling wave solutions using
numerical continuation. Here we consider only branches which bifurcate from
solutions where there is no slip in the tangential velocity at the interface
for the trivial flow. The spectral stability of these solutions is then
determined using a numerical Fourier-Floquet technique. We find that the
strength of the linear shear in each fluid impacts the stability of the
corresponding traveling wave solutions. Specifically, opposing shears may
amplify or suppress instabilities.
"
"  Internal gravity waves play a primary role in geophysical fluids: they
contribute significantly to mixing in the ocean and they redistribute energy
and momentum in the middle atmosphere. Until recently, most studies were
focused on plane wave solutions. However, these solutions are not a
satisfactory description of most geophysical manifestations of internal gravity
waves, and it is now recognized that internal wave beams with a confined
profile are ubiquitous in the geophysical context.
We will discuss the reason for the ubiquity of wave beams in stratified
fluids, related to the fact that they are solutions of the nonlinear governing
equations. We will focus more specifically on situations with a constant
buoyancy frequency. Moreover, in light of recent experimental and analytical
studies of internal gravity beams, it is timely to discuss the two main
mechanisms of instability for those beams. i) The Triadic Resonant Instability
generating two secondary wave beams. ii) The streaming instability
corresponding to the spontaneous generation of a mean flow.
"
"  Silicon-vacancy color centers in nanodiamonds are promising as fluorescent
labels for biological applications, with a narrow, non-bleaching emission line
at 738\,nm. Two-photon excitation of this fluorescence offers the possibility
of low-background detection at significant tissue depth with high
three-dimensional spatial resolution. We have measured the two-photon
fluorescence cross section of a negatively-charged silicon vacancy (SiV$^-$) in
ion-implanted bulk diamond to be $0.74(19) \times 10^{-50}{\rm cm^4\;s/photon}$
at an excitation wavelength of 1040\,nm. In comparison to the diamond nitrogen
vacancy (NV) center, the expected detection threshold of a two-photon excited
SiV center is more than an order of magnitude lower, largely due to its much
narrower linewidth. We also present measurements of two- and three-photon
excitation spectra, finding an increase in the two-photon cross section with
decreasing wavelength, and discuss the physical interpretation of the spectra
in the context of existing models of the SiV energy-level structure.
"
"  We compare the results of the semi-classical (SC) and quantum-mechanical (QM)
formalisms for angular-momentum changing transitions in Rydberg atom collisions
given by Vrinceanu & Flannery, J. Phys. B 34, L1 (2001), and Vrinceanu, Onofrio
& Sadeghpour, ApJ 747, 56 (2012), with those of the SC formalism using a
modified Monte Carlo realization. We find that this revised SC formalism agrees
well with the QM results. This provides further evidence that the rates derived
from the QM treatment are appropriate to be used when modelling recombination
through Rydberg cascades, an important process in understanding the state of
material in the early universe. The rates for $\Delta\ell=\pm1$ derived from
the QM formalism diverge when integrated to sufficiently large impact
parameter, $b$. Further to the empirical limits to the $b$ integration
suggested by Pengelly & Seaton, MNRAS 127, 165 (1964), we suggest that the
fundamental issue causing this divergence in the theory is that it does not
fully cater for the finite time taken for such distant collisions to complete.
"
"  Measurements of root-zone soil moisture across spatial scales of tens to
thousands of meters have been a challenge for many decades. The mobile
application of Cosmic-Ray Neutron Sensing (CRNS) is a promising approach to
measure field soil moisture non-invasively by surveying large regions with a
ground-based vehicle. Recently, concerns have been raised about a potentially
biasing influence of local structures and roads. We employed neutron transport
simulations and dedicated experiments to quantify the influence of different
road types on the CRNS measurement. We found that the presence of roads
introduces a bias in the CRNS estimation of field soil moisture compared to
non-road scenarios. However, this effect becomes insignificant at distances
beyond a few meters from the road. Measurements from the road could
overestimate the field value by up to 40 % depending on road material, width,
and the surrounding field water content. The bias could be successfully removed
with an analytical correction function that accounts for these parameters.
Additionally, an empirical approach is proposed that can be used on-the-fly
without prior knowledge of field soil moisture. Tests at different study sites
demonstrated good agreement between road-effect corrected measurements and
field soil moisture observations. However, if knowledge about the road
characteristics is missing, any measurements on the road could substantially
reduce the accuracy of this method. Our results constitute a practical
advancement of the mobile CRNS methodology, which is important for providing
unbiased estimates of field-scale soil moisture to support applications in
hydrology, remote sensing, and agriculture.
"
"  The idea of combining different two-dimensional (2D) crystals in van der
Waals heterostructures (vdWHs) has led to a new paradigm for band structure
engineering with atomic precision. Due to the weak interlayer couplings, the
band structures of the individual 2D crystals are largely preserved upon
formation of the heterostructure. However, regardless of the details of the
interlayer hybridisation, the size of the 2D crystal band gaps are always
reduced due to the enhanced dielectric screening provided by the surrounding
layers. The effect can be on the order of electron volts, but its precise
magnitude is non-trivial to predict because of the non-local nature of the
screening in quasi-2D materials, and it is not captured by effective
single-particle methods such as density functional theory. Here we present an
efficient and general method for calculating the band gap renormalization of a
2D material embedded in an arbitrary vdWH. The method evaluates the change in
the GW self-energy of the 2D material from the change in the screened Coulomb
interaction. The latter is obtained using the quantum-electrostatic
heterostructure (QEH) model. We benchmark the G$\Delta$W method against full
first-principles GW calculations and use it to unravel the importance of
screening-induced band structure renormalisation in various vdWHs. A main
result is the observation that the size of the band gap reduction of a given 2D
material when inserted into a heterostructure scales inversely with the
polarisability of the 2D material. Our work demonstrates that dielectric
engineering \emph{via} van der Waals heterostructuring represents a promising
strategy for tailoring the band structure of 2D materials.
"
"  The IllustrisTNG project is a new suite of cosmological
magneto-hydrodynamical simulations of galaxy formation performed with the Arepo
code and updated models for feedback physics. Here we introduce the first two
simulations of the series, TNG100 and TNG300, and quantify the stellar mass
content of about 4000 massive galaxy groups and clusters ($10^{13} \leq M_{\rm
200c}/M_{\rm sun} \leq 10^{15}$) at recent times ($z \leq 1$). The richest
clusters have half of their total stellar mass bound to satellite galaxies,
with the other half being associated with the central galaxy and the diffuse
intra-cluster light. The exact ICL fraction depends sensitively on the
definition of a central galaxy's mass and varies in our most massive clusters
between 20 to 40% of the total stellar mass. Haloes of $5\times 10^{14}M_{\rm
sun}$ and above have more diffuse stellar mass outside 100 kpc than within 100
kpc, with power-law slopes of the radial mass density distribution as shallow
as the dark matter's ( $-3.5 < \alpha_{\rm 3D} < -3$). Total halo mass is a
very good predictor of stellar mass, and vice versa: at $z=0$, the 3D stellar
mass measured within 30 kpc scales as $\propto (M_{\rm 500c})^{0.49}$ with a
$\sim 0.12$ dex scatter. This is possibly too steep in comparison to the
available observational constraints, even though the abundance of TNG less
massive galaxies ($< 10^{11}M_{\rm sun}$ in stars) is in good agreement with
the measured galaxy stellar mass functions at recent epochs. The 3D sizes of
massive galaxies fall too on a tight ($\sim$0.16 dex scatter) power-law
relation with halo mass, with $r^{\rm stars}_{\rm 0.5} \propto (M_{\rm
500c})^{0.53}$. Even more fundamentally, halo mass alone is a good predictor
for the whole stellar mass profiles beyond the inner few kpc, and we show how
on average these can be precisely recovered given a single mass measurement of
the galaxy or its halo.
"
"  Gallium arsenide (GaAs) is the widest used second generation semiconductor
with a direct band gap and increasingly used as nanofilms. However, the
magnetic properties of GaAs nanofilms have never been studied. Here we find by
comprehensive density functional theory calculations that GaAs nanofilms
cleaved along the <111> and <100> directions become intrinsically metallic
films with strong surface magnetism and magnetoelectric (ME) effect. The
surface magnetism and electrical conductivity are realized via a combined
effect of transferring charge induced by spontaneous electric-polarization
through the film thickness and spin-polarized surface states. The surface
magnetism of <111> nanofilms can be significantly and linearly tuned by
vertically applied electric field, endowing the nanofilms unexpectedly high ME
coefficients, which are tens of times higher than those of ferromagnetic metals
and transition metal oxides.
"
"  In this paper we use detailed Monte Carlo simulations to demonstrate that
liquid xenon (LXe) can be used to build a Cherenkov-based TOF-PET, with an
intrinsic coincidence resolving time (CRT) in the vicinity of 10 ps. This
extraordinary performance is due to three facts: a) the abundant emission of
Cherenkov photons by liquid xenon; b) the fact that LXe is transparent to
Cherenkov light; and c) the fact that the fastest photons in LXe have
wavelengths higher than 300 nm, therefore making it possible to separate the
detection of scintillation and Cherenkov light. The CRT in a Cherenkov LXe
TOF-PET detector is, therefore, dominated by the resolution (time jitter)
introduced by the photosensors and the electronics. However, we show that for
sufficiently fast photosensors (e.g, an overall 40 ps jitter, which can be
achieved by current micro-channel plate photomultipliers) the overall CRT
varies between 30 and 55 ps, depending of the detection efficiency. This is
still one order of magnitude better than commercial CRT devices and improves by
a factor 3 the best CRT obtained with small laboratory prototypes.
"
"  We present the first CMB power spectra from numerical simulations of the
global O(N) linear $\sigma$-model with N = 2,3, which have global strings and
monopoles as topological defects. In order to compute the CMB power spectra we
compute the unequal time correlators (UETCs) of the energy-momentum tensor,
showing that they fall off at high wave number faster than naive estimates
based on the geometry of the defects, indicating non-trivial
(anti-)correlations between the defects and the surrounding Goldstone boson
field. We obtain source functions for Einstein-Boltzmann solvers from the
UETCs, using a recent method that improves the modelling at the radiation-
matter transition. We show that the interpolation function that mimics the
transition is similar to other defect models, but not identical, confirming the
non-universality of the interpolation function. The CMB power spectra for
global strings and monopoles have the same overall shape as those obtained
using the non-linear $\sigma$-model approximation, which is well captured by a
large-N calculation. However, the amplitudes are larger than the large-N
calculation predict, and in the case of global strings much larger: a factor of
20 at the peak. Finally we compare the CMB power spectra with the latest CMB
data to put limits on the allowed contribution to the temperature power
spectrum at multipole $\ell$ = 10 of 1.7% for global strings and 2.4% for
global monopoles. These limits correspond to symmetry-breaking scales of
2.9x1015 GeV (6.3x1014 GeV with the expected logarithmic scaling of the
effective string tension between the simulation time and decoupling) and
6.4x1015 GeV respectively. The bound on global strings is a significant one for
the ultra-light axion scenario with axion masses ma 10-28 eV. These upper
limits indicate that gravitational wave from global topological defects will
not be observable at the GW observatory LISA.
"
"  We report on the result of a campaign to monitor 25 HATSouth candidates using
the K2 space telescope during Campaign 7 of the K2 mission. We discover
HATS-36b (EPIC 215969174b), a hot Jupiter with a mass of 2.79$\pm$0.40 M$_J$
and a radius of 1.263$\pm$0.045 R$_J$ which transits a solar-type G0V star
(V=14.386) in a 4.1752d period. We also refine the properties of three
previously discovered HATSouth transiting planets (HATS-9b, HATS-11b, and
HATS-12b) and search the K2 data for TTVs and additional transiting planets in
these systems. In addition we also report on a further three systems that
remain as Jupiter-radius transiting exoplanet candidates. These candidates do
not have determined masses, however pass all of our other vetting observations.
Finally we report on the 18 candidates which we are now able to classify as
eclipsing binary or blended eclipsing binary systems based on a combination of
the HATSouth data, the K2 data, and follow-up ground-based photometry and
spectroscopy. These range in periods from 0.7 days to 16.7 days, and down to
1.5 mmag in eclipse depths. Our results show the power of combining
ground-based imaging and spectroscopy with higher precision space-based
photometry, and serve as an illustration as to what will be possible when
combining ground-based observations with TESS data.
"
"  Ultra-faint dwarf galaxies (UFDs) are the faintest known galaxies and due to
their incredibly low surface brightness, it is difficult to find them beyond
the Local Group. We report a serendipitous discovery of an UFD, Fornax UFD1, in
the outskirts of NGC 1316, a giant galaxy in the Fornax cluster. The new galaxy
is located at a projected radius of 55 kpc in the south-east of NGC 1316. This
UFD is found as a small group of resolved stars in the Hubble Space Telescope
images of a halo field of NGC 1316, obtained as part of the Carnegie-Chicago
Hubble Program. Resolved stars in this galaxy are consistent with being mostly
metal-poor red giant branch (RGB) stars. Applying the tip of the RGB method to
the mean magnitude of the two brightest RGB stars, we estimate the distance to
this galaxy, 19.0 +- 1.3 Mpc. Fornax UFD1 is probably a member of the Fornax
cluster. The color-magnitude diagram of these stars is matched by a 12 Gyr
isochrone with low metallicity ([Fe/H] ~ -2.4). Total magnitude and effective
radius of Fornax UFD1 are Mv ~ -7.6 +- 0.2 mag and r_eff = 146 +- 9 pc, which
are similar to those of Virgo UFD1 that was discovered recently in the
intracluster field of Virgo by Jang & Lee (2014).Fornax UFD1 is the most
distant known UFD that is confirmed by resolved stars. This indicates that UFDs
are ubiquitous and that more UFDs remain to be discovered in the Fornax
cluster.
"
"  We compare performances of well-known numerical time-stepping methods that
are widely used to compute solutions of the doubly-infinite
Fermi-Pasta-Ulam-Tsingou (FPUT) lattice equations. The methods are benchmarked
according to (1) their accuracy in capturing the soliton peaks and (2) in
capturing highly-oscillatory parts of the solutions of the Toda lattice
resulting from a variety of initial data. The numerical inverse scattering
transform method is used to compute a reference solution with high accuracy. We
find that benchmarking a numerical method on pure-soliton initial data can lead
one to overestimate the accuracy of the method.
"
"  Due to its wide field of view, cone-beam computed tomography (CBCT) is
plagued by large amounts of scatter, where attenuated photons hit the detector,
and corrupt the linear models used for reconstruction. Given that one can
generate a good estimate of scatter however, then image accuracy can be
retained. In the context of adaptive radiotherapy, one usually has a
low-scatter planning CT image of the same patient at an earlier time.
Correcting for scatter in the subsequent CBCT scan can either be self
consistent with the new measurements or exploit the prior image, and there are
several recent methods that report high accuracy with the latter. In this
study, we will look at the accuracy of various scatter estimation methods, how
they can be effectively incorporated into a statistical reconstruction
algorithm, along with introducing a method for matching off-line Monte-Carlo
(MC) prior estimates to the new measurements. Conclusions we draw from testing
on a neck cancer patient are: statistical reconstruction that incorporates the
scatter estimate significantly outperforms analytic and iterative methods with
pre-correction; and although the most accurate scatter estimates can be made
from the MC on planning image, they only offer a slight advantage over the
measurement based scatter kernel superposition (SKS) in reconstruction error.
"
"  Finding an easy-to-build coils set has been a critical issue for stellarator
design for decades. Conventional approaches assume a toroidal ""winding""
surface. We'll investigate if the existence of winding surface unnecessarily
constrains the optimization, and a new method to design coils for stellarators
is presented. Each discrete coil is represented as an arbitrary, closed,
one-dimensional curve embedded in three-dimensional space. A target function to
be minimized that covers both physical requirements and engineering constraints
is constructed. The derivatives of the target function are calculated
analytically. A numerical code, named FOCUS, has been developed. Applications
to a simple configuration, the W7-X, and LHD plasmas are presented.
"
"  Aims: In this paper we focus on the occurrence of glycolaldehyde (HCOCH2OH)
in young solar analogs by performing the first homogeneous and unbiased study
of this molecule in the Class 0 protostars of the nearby Perseus star forming
region. Methods: We obtained sub-arcsec angular resolution maps at 1.3mm and
1.4mm of glycolaldehyde emission lines using the IRAM Plateau de Bure (PdB)
interferometer in the framework of the CALYPSO IRAM large program. Results:
Glycolaldehyde has been detected towards 3 Class 0 and 1 Class I protostars out
of the 13 continuum sources targeted in Perseus: NGC1333-IRAS2A1,
NGC1333-IRAS4A2, NGC1333-IRAS4B1, and SVS13-A. The NGC1333 star forming region
looks particularly glycolaldehyde rich, with a rate of occurrence up to 60%.
The glycolaldehyde spatial distribution overlaps with the continuum one,
tracing the inner 100 au around the protostar. A large number of lines (up to
18), with upper-level energies Eu from 37 K up to 375 K has been detected. We
derived column densities > 10^15 cm^-2 and rotational temperatures Trot between
115 K and 236 K, imaging for the first time hot-corinos around NGC1333-IRAS4B1
and SVS13-A. Conclusions: In multiple systems glycolaldehyde emission is
detected only in one component. The case of the SVS13-A+B and IRAS4-A1+A2
systems support that the detection of glycolaldehyde (at least in the present
Perseus sample) indicates older protostars (i.e. SVS13-A and IRAS4-A2), evolved
enough to develop the hot-corino region (i.e. 100 K in the inner 100 au).
However, only two systems do not allow us to firmly conclude whether the
primary factor leading to the detection of glycolaldehyde emission is the
environments hosting the protostars, evolution (e.g. low value of Lsubmm/Lint),
or accretion luminosity (high Lint).
"
"  The heavyweight stellar initial mass function (IMF) observed in the cores of
massive early-type galaxies (ETGs) has been linked to formation of their cores
in an initial swiftly-quenched rapid starburst. However, the outskirts of ETGs
are thought to be assembled via the slow accumulation of smaller systems in
which the star formation is less extreme; this suggests the form of the IMF
should exhibit a radial trend in ETGs. Here we report radial stellar population
gradients out to the half-light radii of a sample of eight nearby ETGs.
Spatially resolved spectroscopy at 0.8-1.35{\mu}m from the VLT's KMOS
instrument was used to measure radial trends in the strengths of a variety of
IMF-sensitive absorption features (including some which are previously
unexplored). We find weak or no radial variation in some of these which, given
a radial IMF trend, ought to vary measurably, e.g. for the Wing-Ford band we
measure a gradient of +0.06$\pm$0.04 per decade in radius.
Using stellar population models to fit stacked and individual spectra, we
infer that the measured radial changes in absorption feature strengths are
primarily accounted for by abundance gradients which are fairly consistent
across our sample (e.g. we derive an average [Na/H] gradient of
-0.53$\pm$0.07). The inferred contribution of dwarf stars to the total light
typically corresponds to a bottom heavy IMF, but we find no evidence for radial
IMF variations in the majority of our sample galaxies.
"
"  (Abridged) The formation of large-scale (hundreds to few thousands of AU)
bipolar structures in the circumstellar envelopes (CSEs) of post-Asymptotic
Giant Branch (post-AGB) stars is poorly understood. The shape of these
structures, traced by emission from fast molecular outflows, suggests that the
dynamics at the innermost regions of these CSEs does not depend only on the
energy of the radiation field of the central star. Deep into the Water
Fountains is an observational project based on the results of programs carried
out with three telescope facilities: The Karl G. Jansky Very Large Array
(JVLA), The Australia Telescope Compact Array (ATCA), and the Very Large
Telescope (SINFONI-VLT). Here we report the results of the observations towards
the WF nebula IRAS 18043$-$2116: Detection of radio continuum emission in the
frequency range 1.5GHz - 8.0GHz; H$_{2}$O maser spectral features and radio
continuum emission detected at 22GHz, and H$_{2}$ ro-vibrational emission lines
detected at the near infrared. The high-velocity H$_{2}$O maser spectral
features, and the shock-excited H$_{2}$ emission detected could be produced in
molecular layers which are swept up as a consequence of the propagation of a
jet-driven wind. Using the derived H$_{2}$ column density, we estimated a
molecular mass-loss rate of the order of $10^{-9}$M$_{\odot}$yr$^{-1}$. On the
other hand, if the radio continuum flux detected is generated as a consequence
of the propagation of a thermal radio jet, the mass-loss rate associated to the
outflowing ionized material is of the order of 10$^{-5}$M$_{\odot}$yr$^{-1}$.
The presence of a rotating disk could be a plausible explanation for the
mass-loss rates estimated.
"
"  The formation of vortices is usually considered to be the main mechanism of
angular momentum disposal in superfluids. Recently, it was predicted that a
superfluid can acquire angular momentum via an alternative, microscopic route
-- namely, through interaction with rotating impurities, forming so-called
`angulon quasiparticles' [Phys. Rev. Lett. 114, 203001 (2015)]. The angulon
instabilities correspond to transfer of a small number of angular momentum
quanta from the impurity to the superfluid, as opposed to vortex instabilities,
where angular momentum is quantized in units of $\hbar$ per atom. Furthermore,
since conventional impurities (such as molecules) represent three-dimensional
(3D) rotors, the angular momentum transferred is intrinsically 3D as well, as
opposed to a merely planar rotation which is inherent to vortices. Herein we
show that the angulon theory can explain the anomalous broadening of the
spectroscopic lines observed for CH$_3$ and NH$_3$ molecules in superfluid
helium nanodroplets, thereby providing a fingerprint of the emerging angulon
instabilities in experiment.
"
"  Security-Constrained Unit Commitment (SCUC) is one of the most significant
problems in secure and optimal operation of modern electricity markets. New
sources of uncertainties such as wind speed volatility and price-sensitive
loads impose additional challenges to this large-scale problem. This paper
proposes a new Stochastic SCUC using point estimation method to model the power
system uncertainties more efficiently. Conventional scenario-based Stochastic
SCUC approaches consider the Mont Carlo method; which presents additional
computational burdens to this large-scale problem. In this paper we use point
estimation instead of scenario generating to detract computational burdens of
the problem. The proposed approach is implemented on a six-bus system and on a
modified IEEE 118-bus system with 94 uncertain variables. The efficacy of
proposed algorithm is confirmed, especially in the last case with notable
reduction in computational burden without considerable loss of precision.
"
"  We theoretically study a one-dimensional (1D) mutually incommensurate
bichromatic lattice system which has been implemented in ultracold atoms to
study quantum localization. It has been universally believed that the
tight-binding version of this bichromatic incommensurate system is represented
by the well-known Aubry-Andre model. Here we establish that this belief is
incorrect and that the Aubry-Andre model description, which applies only in the
extreme tight-binding limit of very deep primary lattice potential, generically
breaks down near the localization transition due to the unavoidable appearance
of single-particle mobility edges (SPME). In fact, we show that the 1D
bichromatic incommensurate potential system manifests generic mobility edges
which disappear in the tight-binding limit, leading to the well-studied
Aubry-Andre physics. We carry out an extensive study of the localization
properties of the 1D incommensurate optical lattice without making any
tight-binding approximation. We find that, for the full lattice system, an
intermediate phase between completely localized and completely delocalized
regions appears due to the existence of the SPME, making the system
qualitatively distinct from the Aubry-Andre prediction. Using the Wegner flow
approach, we show that the SPME in the real lattice system can be attributed to
significant corrections of higher-order harmonics in the lattice potential
which are absent in the strict tight-binding limit. We calculate the dynamical
consequences of the intermediate phase in detail to guide future experimental
investigations for the observation of 1D SPME and the associated intermediate
phase. We consider effects of interaction numerically, and conjecture the
stability of SPME to weak interaction effects, thus leading to the exciting
possibility of an experimentally viable nonergodic extended phase in
interacting 1D optical lattices.
"
"  Random geometric graphs consist of randomly distributed nodes (points), with
pairs of nodes within a given mutual distance linked. In the usual model the
distribution of nodes is uniform on a square, and in the limit of infinitely
many nodes and shrinking linking range, the number of isolated nodes is Poisson
distributed, and the probability of no isolated nodes is equal to the
probability the whole graph is connected. Here we examine these properties for
several self-similar node distributions, including smooth and fractal, uniform
and nonuniform, and finitely ramified or otherwise. We show that nonuniformity
can break the Poisson distribution property, but it strengthens the link
between isolation and connectivity. It also stretches out the connectivity
transition. Finite ramification is another mechanism for lack of connectivity.
The same considerations apply to fractal distributions as smooth, with some
technical differences in evaluation of the integrals and analytical arguments.
"
"  This article explains phase noise, jitter, and some slower phenomena in
digital integrated circuits, focusing on high-demanding, noise-critical
applications. We introduce the concept of phase type and time type phase noise.
The rules for scaling the noise with frequency are chiefly determined by the
spectral properties of these two basic types, by the aliasing phenomenon, and
by the input and output circuits. Then, we discuss the parameter extraction
from experimental data and we report on the measured phase noise in some
selected devices of different node size and complexity. We observed flicker
noise between -80 and -130 dBrad^2/Hz at 1 Hz offset, and white noise down to
-165 dBrad^2/Hz in some fortunate cases and using the appropriate tricks. It
turns out that flicker noise is proportional to the reciprocal of the volume of
the transistor. This unpleasant conclusion is supported by a gedanken
experiment. Further experiments provide understanding on: (i) the interplay
between noise sources in the internal PLL, often present in FPGAs; (ii) the
chattering phenomenon, which consists in multiple bouncing at transitions; and
(iii) thermal time constants, and their effect on phase wander and on the Allan
variance.
"
"  A novel low cost, near equi-atomic alloy comprising of Al, Cu, Fe and Mn is
synthesized using arc-melting technique. The cast alloy possesses a dendritic
microstructure where the dendrites consist of disordered FCC and ordered FCC
phases. The inter-dendritic region is comprised of ordered FCC phase and
spinodally decomposed BCC phases. A Cu segregation is observed in the
inter-dendritic region while dendritic region is rich in Fe. The bulk hardness
of the alloy is ~ 380 HV, indicating significant yield strength.
"
"  Project 8 is a tritium endpoint neutrino mass experiment utilizing a phased
program to achieve sensitivity to the range of neutrino masses allowed by the
inverted mass hierarchy. The Cyclotron Radiation Emission Spectroscopy (CRES)
technique is employed to measure the differential energy spectrum of decay
electrons with high precision. We present an overview of the Project 8
experimental program, from first demonstration of the CRES technique to
ultimate sensitivity with an atomic tritium source. We highlight recent
advances in preparation for the first measurement of the continuous tritium
spectrum with CRES.
"
"  X-ray observations of two metal-deficient luminous compact galaxies (LCG)
(SHOC~486 and SDSS J084220.94+115000.2) with properties similar to the
so-called Green Pea galaxies were obtained using the {\emph{Chandra X-ray
Observatory}}. Green Pea galaxies are relatively small, compact (a few kpc
across) galaxies that get their green color from strong [OIII]$\lambda$5007\AA\
emission, an indicator of intense, recent star formation. These two galaxies
were predicted to have the highest observed count rates, using the X-ray
luminosity -- star formation rate ($L_X$--SFR) relation for X-ray binaries,
from a statistically complete sample drawn from optical criteria. We determine
the X-ray luminosity relative to star-formation rate and metallicity for these
two galaxies. Neither exhibit any evidence of active galactic nuclei and we
suspect the X-ray emission originates from unresolved populations of high mass
X-ray binaries. We discuss the $L_X$--SFR--metallicity plane for star-forming
galaxies and show that the two LCGs are consistent with the prediction of this
relation. This is the first detection of Green Pea analogs in X-rays.
"
"  CoRoT-9b is one of the rare long-period (P=95.3 days) transiting giant
planets with a measured mass known to date. We present a new analysis of the
CoRoT-9 system based on five years of radial-velocity (RV) monitoring with
HARPS and three new space-based transits observed with CoRoT and Spitzer.
Combining our new data with already published measurements we redetermine the
CoRoT-9 system parameters and find good agreement with the published values. We
uncover a higher significance for the small but non-zero eccentricity of
CoRoT-9b ($e=0.133^{+0.042}_{-0.037}$) and find no evidence for additional
planets in the system. We use simulations of planet-planet scattering to show
that the eccentricity of CoRoT-9b may have been generated by an instability in
which a $\sim 50~M_\oplus$ planet was ejected from the system. This scattering
would not have produced a spin-orbit misalignment, so we predict that CoRoT-9b
orbit should lie within a few degrees of the initial plane of the
protoplanetary disk. As a consequence, any significant stellar obliquity would
indicate that the disk was primordially tilted.
"
"  We show that the uniformly accelerated reference systems proposed by Einstein
when introducing acceleration in the theory of relativity are Fermi-Walker
coordinate systems. We then consider more general accelerated motions and, on
the one hand we obtain Thomas precession and, on the other, we prove that the
only accelerated reference systems that at any time admit an instantaneously
comoving inertial system belong necessarily to the Fermi-Walker class.
"
"  We propose a novel numerical approach for the optimal design of wide-area
heterogeneous electromagnetic metasurfaces beyond the conventionally used
unit-cell approximation. The proposed method exploits the combination of
Rigorous Coupled Wave Analysis (RCWA) and global optimization techniques (two
evolutionary algorithms namely the Genetic Algorithm (GA) and a modified form
of the Artificial Bee Colony (ABC with memetic search phase method) are
considered). As a specific example, we consider the design of beam deflectors
using all-dielectric nanoantennae for operation in the visible wavelength
region; beam deflectors can serve as building blocks for other more complicated
devices like metalenses. Compared to previous reports using local optimization
approaches our approach improves device efficiency; transmission efficiency is
especially improved for wide deflection angle beam deflectors. The ABC method
with memetic search phase is also an improvement over the more commonly used GA
as it reaches similar efficiency levels with upto 35% reduction in computation
time. The method described here is of interest for the rapid design of a wide
variety of electromagnetic metasurfaces irrespective of their operational
wavelength.
"
"  We present a novel time- and phase-resolved, background-free scheme to study
the extreme ultraviolet dipole emission of a bound electronic wavepacket,
without the use of any extreme ultraviolet exciting pulse. Using multiphoton
transitions, we populate a superposition of quantum states which coherently
emit extreme ultraviolet radiation through free induction decay. This emission
is probed and controlled, both in amplitude and phase, by a time-delayed
infrared femtosecond pulse. We directly measure the laser-induced dephasing of
the emission by using a simple heterodyne detection scheme based on two-source
interferometry. This technique provides rich information about the interplay
between the laser field and the Coulombic potential on the excited electron
dynamics. Its background-free nature enables us to use a large range of gas
pressures and to reveal the influence of collisions in the relaxation process.
"
"  Effective gauge fields have allowed the emulation of matter under strong
magnetic fields leading to the realization of Harper-Hofstadter, Haldane
models, and led to demonstrations of one-way waveguides and topologically
protected edge states. Central to these discoveries is the chirality induced by
time-symmetry breaking. Due to the discovery of quantum search algorithms based
on walks on graphs, recent work has discovered new implications the effect of
time-reversal symmetry breaking has on the transport of quantum states and has
brought with it a host of new experimental implementations. We provide a full
classification of the unitary operators defining quantum processes which break
time-reversal symmetry in their induced transition properties between basis
elements in a preferred site-basis. Our results are furthermore proven in terms
of the geometry of the corresponding Hamiltonian support graph and hence
provide a topological classification. A quantum process of this type is
necessarily time-symmetric for any choice of time-independent Hamiltonian if
and only if the underlying support graph is bipartite. Moreover, for
non-bipartite support, there exists a time-independent Hamiltonian with
necessarily complex edge weights that induces time-asymmetric transition
probabilities between edge(s). We further prove that certain bipartite graphs
give rise to transition probability suppression, but not broken time-reversal
symmetry. These results fill an important missing gap in understanding the role
this omnipresent effect has in quantum physics. Furthermore, through our
development of a general framework, along the way to our results we completely
characterize gauge potentials on combinatorial graphs.
"
"  We investigate the impact of choosing regressors and molecular
representations for the construction of fast machine learning (ML) models of
thirteen electronic ground-state properties of organic molecules. The
performance of each regressor/representation/property combination is assessed
using learning curves which report out-of-sample errors as a function of
training set size with up to $\sim$117k distinct molecules. Molecular
structures and properties at hybrid density functional theory (DFT) level of
theory used for training and testing come from the QM9 database [Ramakrishnan
et al, {\em Scientific Data} {\bf 1} 140022 (2014)] and include dipole moment,
polarizability, HOMO/LUMO energies and gap, electronic spatial extent, zero
point vibrational energy, enthalpies and free energies of atomization, heat
capacity and the highest fundamental vibrational frequency. Various
representations from the literature have been studied (Coulomb matrix, bag of
bonds, BAML and ECFP4, molecular graphs (MG)), as well as newly developed
distribution based variants including histograms of distances (HD), and angles
(HDA/MARAD), and dihedrals (HDAD). Regressors include linear models (Bayesian
ridge regression (BR) and linear regression with elastic net regularization
(EN)), random forest (RF), kernel ridge regression (KRR) and two types of
neural net works, graph convolutions (GC) and gated graph networks (GG). We
present numerical evidence that ML model predictions deviate from DFT less than
DFT deviates from experiment for all properties. Furthermore, our out-of-sample
prediction errors with respect to hybrid DFT reference are on par with, or
close to, chemical accuracy. Our findings suggest that ML models could be more
accurate than hybrid DFT if explicitly electron correlated quantum (or
experimental) data was available.
"
"  A comprehensive theoretical analysis of photo-induced forces in an
illuminated nanojunction, formed between an atomic force microscopy tip and a
sample, is presented. The formalism is valid within the dipolar approximation
and includes multiple scattering effects between the tip, sample and a planar
substrate through a dyadic Green's function approach. This physically intuitive
description allows a detailed look at the quantitative contribution of multiple
scattering effects to the measured photo-induced force, effects that are
typically unaccounted for in simpler analytical models. Our findings show that
the presence of the planar substrate and anisotropy of the tip have a
substantial effect on the magnitude and the spectral response of the
photo-induced force exerted on the tip. Unlike previous models, our
calculations predict photo-induced forces that are within range of
experimentally measured values in photo-induced force microscopy (PiFM)
experiments.
"
"  In Kinetic Inductance Detectors (KIDs) and other similar applications of
superconducting microresonators, both the large and small-signal behaviour of
the device may be affected by electrothermal feedback. Microwave power applied
to read out the device is absorbed by and heats the superconductor
quasiparticles, changing the superconductor conductivity and hence the readout
power absorbed in a positive or negative feedback loop. In this work, we
explore numerically the implications of an extensible theoretical model of a
generic superconducting microresonator device for a typical KID, incorporating
recent work on the power flow between superconductor quasiparticles and
phonons. This model calculates the large-signal (changes in operating point)
and small-signal behaviour of a device, allowing us to determine the effect of
electrothermal feedback on device responsivity and noise characteristics under
various operating conditions. We also investigate how thermally isolating the
device from the bath, for example by designing the device on a membrane only
connected to the bulk substrate by thin legs, affects device performance. We
find that at a typical device operating point, positive electrothermal feedback
reduces the effective thermal conductance from the superconductor
quasiparticles to the bath, and so increases responsivity to signal
(pair-breaking) power, increases noise from temperature fluctuations, and
decreases the Noise Equivalent Power (NEP). Similarly, increasing the thermal
isolation of the device while keeping the quasiparticle temperature constant
decreases the NEP, but also decreases the device response bandwidth.
"
"  Structural discrimination appears to be a persistent phenomenon in social
systems. We here outline the hypothesis that it can result from the
evolutionary dynamics of the social system itself. We study the evolutionary
dynamics of agents with neutral badges in a simple social game and find that
the badges are readily discriminated by the system although not being tied to
the payoff matrix of the game. The sole property of being distinguishable leads
to the subsequent discrimination, therefore providing a model for the emergence
and freezing of social prejudice.
"
"  Magnetic domain wall (DW) motion induced by a localized Gaussian temperature
profile is studied in a Permalloy nanostrip within the framework of the
stochastic Landau-Lifshitz-Bloch equation. The different contributions to
thermally induced DW motion, entropic torque and magnonic spin transfer torque,
are isolated and compared. The analysis of magnonic spin transfer torque
includes a description of thermally excited magnons in the sample. A third
driving force due to a thermally induced dipolar field is found and described.
Finally, thermally induced DW motion is studied under realistic conditions by
taking into account the edge roughness. The results give quantitative insights
into the different mechanisms responsible for domain wall motion in temperature
gradients and allow for comparison with experimental results.
"
"  We carried out 2.5-dimensional resistive MHD simulations to study the
formation mechanism of molecular loops observed by Fukui et al. (2006) at
Galactic central region. Since it is hard to form molecular loops by uplifting
dense molecular gas, we study the formation mechanism of molecular gas in
rising magnetic arcades. This model is based on the in-situ formation model of
solar prominences, in which prominences are formed by cooling instability in
helical magnetic flux ropes formed by imposing converging and shearing motion
at footpoints of the magnetic arch anchored to the solar surface. We extended
this model to Galactic center scale (a few hundreds pc). Numerical results
indicate that magnetic reconnection taking place in the current sheet formed
inside the rising magnetic arcade creates dense blobs confined by the rising
helical magnetic flux ropes. Thermal instability taking place in the flux ropes
forms dense molecular filaments floating at high Galactic latitude. The mass of
the filament increases with time, and can exceed 10^5 solar mass.
"
"  Kinetic equations play a major rule in modeling large systems of interacting
particles. Recently the legacy of classical kinetic theory found novel
applications in socio-economic and life sciences, where processes characterized
by large groups of agents exhibit spontaneous emergence of social structures.
Well-known examples are the formation of clusters in opinion dynamics, the
appearance of inequalities in wealth distributions, flocking and milling
behaviors in swarming models, synchronization phenomena in biological systems
and lane formation in pedestrian traffic. The construction of kinetic models
describing the above processes, however, has to face the difficulty of the lack
of fundamental principles since physical forces are replaced by empirical
social forces. These empirical forces are typically constructed with the aim to
reproduce qualitatively the observed system behaviors, like the emergence of
social structures, and are at best known in terms of statistical information of
the modeling parameters. For this reason the presence of random inputs
characterizing the parameters uncertainty should be considered as an essential
feature in the modeling process. In this survey we introduce several examples
of such kinetic models, that are mathematically described by nonlinear Vlasov
and Fokker--Planck equations, and present different numerical approaches for
uncertainty quantification which preserve the main features of the kinetic
solution.
"
"  Recent theoretical predictions of ""unprecedented proximity"" of the electronic
ground state of iridium fluorides to the SU(2) symmetric $j_{\mathrm{eff}}=1/2$
limit, relevant for superconductivity in iridates, motivated us to investigate
their crystal and electronic structure. To this aim, we performed
high-resolution x-ray powder diffraction, Ir L$_3$-edge resonant inelastic
x-ray scattering, and quantum chemical calculations on Rb$_2$[IrF$_6$] and
other iridium fluorides. Our results are consistent with the Mott insulating
scenario predicted by Birol and Haule [Phys. Rev. Lett. 114, 096403 (2015)],
but we observe a sizable deviation of the $j_{\mathrm{eff}}=1/2$ state from the
SU(2) symmetric limit. Interactions beyond the first coordination shell of
iridium are negligible, hence the iridium fluorides do not show any magnetic
ordering down to at least 20 K. A larger spin-orbit coupling in iridium
fluorides compared to oxides is ascribed to a reduction of the degree of
covalency, with consequences on the possibility to realize spin-orbit-induced
strongly correlated physics in iridium fluorides.
"
"  The unsteady characteristics of the flow over thick flatback airfoils have
been investigated by means of CFD calculations. Sandia airfoils which have 35%
maximum thickness with three different trailing edge thicknesses were selected.
The calculations provided good results compared with available experimental
data with regard to the lift curve and the impact of trailing edge thickness.
Unsteady CFD simulations revealed that the Strouhal number is found to be
independent of the lift coefficient before stall and increases with the
trailing edge. The present work shows the dependency of the Strouhal number and
the wake development on the trailing edge thickness. A recommendation of the
Strouhal number definition is given for flatback airfoils by considering the
trailing edge separation at low angle of attack. The detailed unsteady
characteristics of thick flatback airfoils are discussed more in the present
paper.
"
"  DZ Cha is a weak-lined T Tauri star (WTTS) surrounded by a bright
protoplanetary disc with evidence of inner disc clearing. Its narrow $\Ha$ line
and infrared spectral energy distribution suggest that DZ Cha may be a
photoevaporating disc. We aim to analyse the DZ Cha star + disc system to
identify the mechanism driving the evolution of this object. We have analysed
three epochs of high resolution optical spectroscopy, photometry from the UV up
to the sub-mm regime, infrared spectroscopy, and J-band imaging polarimetry
observations of DZ Cha. Combining our analysis with previous studies we find no
signatures of accretion in the $\Ha$ line profile in nine epochs covering a
time baseline of $\sim20$ years. The optical spectra are dominated by
chromospheric emission lines, but they also show emission from the forbidden
lines [SII] 4068 and [OI] 6300$\,\AA$ that indicate a disc outflow. The
polarized images reveal a dust depleted cavity of $\sim7$ au in radius and two
spiral-like features, and we derive a disc dust mass limit of
$M_\mathrm{dust}<3\MEarth$ from the sub-mm photometry. No stellar ($M_\star >
80 \MJup$) companions are detected down to $0\farcs07$ ($\sim 8$ au,
projected). The negligible accretion rate, small cavity, and forbidden line
emission strongly suggests that DZ Cha is currently at the initial stages of
disc clearing by photoevaporation. At this point the inner disc has drained and
the inner wall of the truncated outer disc is directly exposed to the stellar
radiation. We argue that other mechanisms like planet formation or binarity
cannot explain the observed properties of DZ Cha. The scarcity of objects like
this one is in line with the dispersal timescale ($\lesssim 10^5$ yr) predicted
by this theory. DZ Cha is therefore an ideal target to study the initial stages
of photoevaporation.
"
"  We construct exact solutions representing a
Friedmann-Lemaître-Robsertson-Walker (FLRW) universe in a generalized hybrid
metric-Palatini theory. By writing the gravitational action in a scalar-tensor
representation, the new solutions are obtained by either making an ansatz on
the scale factor or on the effective potential. Among other relevant results,
we show that it is possible to obtain exponentially expanding solutions for
flat universes even when the cosmology is not purely vacuum. We then derive the
classes of actions for the original theory which generate these solutions.
"
"  Twisted electromagnetic waves, of which the helical phase front is called
orbital angular momentum (OAM), have been recently explored for quantum
information, high speed communication and radar detections. In this context,
generation of high purity waves carrying OAM is of great significance and
challenge from low frequency band to optical area. Here, a novel strategy of
mode combination method is proposed to generate twisted waves with arbitrary
order of OAM index. The higher order mode of a circular horn antenna is used to
generate the twisted waves with quite high purity. The proposed strategy is
verified with theoretical analysis, numerical simulation and experiments. A
circular horn antenna operating at millimeter wave band is designed,
fabricated, and measured. Two twisted waves with OAM index of l=+1 and l=-1
with a mode purity as high as 87% are obtained. Compared with the other OAM
antennas, the antenna proposed here owns a high antenna gain (over 12 dBi) and
wide operating bandwidth (over 15%). The high mode purity, high antenna gain
and wide operating band make the antenna suitable for the twisted-wave
applications, not only in the microwave and millimeter wave band, but also in
the terahertz band.
"
"  Many state of the art methods for the thermodynamic and kinetic
characterization of large and complex biomolecular systems by simulation rely
on ensemble approaches, where data from large numbers of relatively short
trajectories are integrated. In this context, Markov state models (MSMs) are
extremely popular because they can be used to compute stationary quantities and
long-time kinetics from ensembles of short simulations, provided that these
short simulations are in ""local equilibrium"" within the MSM states. However, in
the last over 15 years since the inception of MSMs, it has been controversially
discussed and not yet been answered how deviations from local equilibrium can
be detected, whether these deviations induce a practical bias in MSM
estimation, and how to correct for them. In this paper, we address these
issues: We systematically analyze the estimation of Markov state models (MSMs)
from short non-equilibrium simulations, and we provide an expression for the
error between unbiased transition probabilities and the expected estimate from
many short simulations. We show that the unbiased MSM estimate can be obtained
even from relatively short non-equilibrium simulations in the limit of long lag
times and good discretization. Further, we exploit observable operator model
(OOM) theory to derive an unbiased estimator for the MSM transition matrix that
corrects for the effect of starting out of equilibrium, even when short lag
times are used. Finally, we show how the OOM framework can be used to estimate
the exact eigenvalues or relaxation timescales of the system without estimating
an MSM transition matrix, which allows us to practically assess the
discretization quality of the MSM. Applications to model systems and molecular
dynamics simulation data of alanine dipeptide are included for illustration.
The improved MSM estimator is implemented in PyEMMA as of version 2.3.
"
"  The LZ dark matter detector, like many other rare-event searches, will suffer
from backgrounds due to the radioactive decay of radon daughters. In order to
achieve its science goals, the concentration of radon within the xenon should
not exceed $2\mu$Bq/kg, or 20 mBq total within its 10 tonnes. The LZ
collaboration is in the midst of a program to screen all significant components
in contact with the xenon. The four institutions involved in this effort have
begun sharing two cross-calibration sources to ensure consistent measurement
results across multiple distinct devices. We present here five preliminary
screening results, some mitigation strategies that will reduce the amount of
radon produced by the most problematic components, and a summary of the current
estimate of radon emanation throughout the detector. This best estimate totals
$<17.3$ mBq, sufficiently low to meet the detector's science goals.
"
"  We present models for single-particle dispersion in vertical and horizontal
directions of stably stratified flows. The model in the vertical direction is
based on the observed Lagrangian spectrum of the vertical velocity, while the
model in the horizontal direction is a combination of a continuous-time
eddy-constrained random walk process with a contribution to transport from
horizontal winds. Transport at times larger than the Lagrangian turnover time
is not universal and dependent on these winds. The models yield results in good
agreement with direct numerical simulations of stratified turbulence, for which
single-particle dispersion differs from the well studied case of homogeneous
and isotropic turbulence.
"
"  Microcolonies are aggregates of a few dozen to a few thousand cells exhibited
by many bacteria. The formation of microcolonies is a crucial step towards the
formation of more mature bacterial communities known as biofilms, but also
marks a significant change in bacterial physiology. Within a microcolony,
bacteria forgo a single cell lifestyle for a communal lifestyle hallmarked by
high cell density and physical interactions between cells potentially altering
their behaviour. It is thus crucial to understand how initially identical
single cells start to behave differently while assembling in these tight
communities. Here we show that cells in the microcolonies formed by the human
pathogen Neisseria gonorrhoeae (Ng) present differential motility behaviors
within an hour upon colony formation. Observation of merging microcolonies and
tracking of single cells within microcolonies reveal a heterogeneous motility
behavior: cells close to the surface of the microcolony exhibit a much higher
motility compared to cells towards the center. Numerical simulations of a
biophysical model for the microcolonies at the single cell level suggest that
the emergence of differential behavior within a multicellular microcolony of
otherwise identical cells is of mechanical origin. It could suggest a route
toward further bacterial differentiation and ultimately mature biofilms.
"
"  Direct comparison of areal and profile roughness measurement values is not
advisable due to fundamental differences in the measurement techniques. However
researchers may wish to compare between laboratories with differing equipment,
or against literature values. This paper investigates how well the profile
arithmetic mean average roughness, $R_a$, approximates its areal equivalent
$S_a$. Simulated rough surfaces and samples from the ETOPO1 global relief model
were used. The mean of up to 20 $R_a$ profiles from the surface were compared
with surface $S_a$ for 100 repeats. Differences between $\bar{R_a}$ and $S_a$
fell as the number of $R_a$ values averaged increased. For simulated surfaces
mean % difference between $\bar{R_a}$ and $S_a$ was in the range 16.06% to
3.47% when only one $R_a$ profile was taken. By averaging 20 $R_a$ values mean
% difference fell to 6.60% to 0.81%. By not considering $R_a$ profiles parallel
to the main feature direction (identified visually), mean % difference was
further reduced. For ETOPO1 global relief surfaces mean % difference was in the
range 52.09% to 22.60% when only one $R_a$ value was used, and was 33.22% to
9.90% when 20 $R_a$ values were averaged. Where a surface feature direction
could be identified, accounting for reduced the difference between $\bar{R_a}$
and $S_a$ by approximately 5% points. The results suggest that taking the mean
of between 3 and 5 $R_a$ values will give a good estimate of $S_a$ on regular
or simple surfaces. However, for some complex real world surfaces discrepancy
between $\bar{R_a}$ and $S_a$ are high. Caveats including the use of filters
for areal and profile measurements, and profile alignment are discussed.
"
"  We present the VLA-COSMOS 3 GHz Large Project based on 384 hours of
observations with the Karl G. Jansky Very Large Array (VLA) at 3 GHz (10 cm)
toward the two square degree Cosmic Evolution Survey (COSMOS) field. The final
mosaic reaches a median rms of 2.3 uJy/beam over the two square degrees at an
angular resolution of 0.75"". To fully account for the spectral shape and
resolution variations across the broad (2 GHz) band, we image all data with a
multiscale, multifrequency synthesis algorithm. We present a catalog of 10,830
radio sources down to 5 sigma, out of which 67 are combined from multiple
components. Comparing the positions of our 3 GHz sources with those from the
Very Long Baseline Array (VLBA)-COSMOS survey, we estimate that the astrometry
is accurate to 0.01"" at the bright end (signal-to-noise ratio, S/N_3GHz > 20).
Survival analysis on our data combined with the VLA-COSMOS 1.4~GHz Joint
Project catalog yields an expected median radio spectral index of alpha=-0.7.
We compute completeness corrections via Monte Carlo simulations to derive the
corrected 3 GHz source counts. Our counts are in agreement with previously
derived 3 GHz counts based on single-pointing (0.087 square degrees) VLA data.
In summary, the VLA-COSMOS 3 GHz Large Project simultaneously provides the
largest and deepest radio continuum survey at high (0.75"") angular resolution
to date, bridging the gap between last-generation and next-generation surveys.
"
"  We study the ground state of a one-dimensional (1D) trapped Bose gas with two
mobile impurity particles. To investigate this set-up, we develop a variational
procedure in which the coordinates of the impurity particles are slow-like
variables. We validate our method using the exact results obtained for small
systems. Then, we discuss energies and pair densities for systems that contain
of the order of one hundred atoms. We show that bosonic non-interacting
impurities cluster. To explain this clustering, we calculate and discuss
induced impurity-impurity potentials in a harmonic trap. Further, we compute
the force between static impurities in a ring ({\it {à} la} the Casimir
force), and contrast the two effective potentials: the one obtained from the
mean-field approximation, and the one due to the one-phonon exchange. Our
formalism and findings are important for understanding (beyond the polaron
model) the physics of modern 1D cold-atom systems with more than one impurity.
"
"  The general theoretical description of the influence of oscillating
horizontal magnetic and quasimagnetic fields on the spin evolution in storage
rings is presented. Previous results are generalized to the case when both of
the horizontal components of the oscillating field are nonzero and the vector
of this field circumscribes an ellipse. General equations describing a behavior
of all components of the polarization vector are derived and the case of an
arbitrary initial polarization is considered. The derivation is fulfilled in
the case when the oscillation frequency is nonresonant. The general spin
evolution in storage rings conditioned by vertical betatron oscillations is
calculated as an example.
"
"  We demonstrate a close connection between observed field-induced
antiferromagnetic (AFM) order and quantum critical fluctuation (QCF) in the
Zn7%-doped heavy-fermion superconductor CeCoIn5. Magnetization, specific heat,
and electrical resistivity at low temperatures all show the presence of new
field-induced AFM order under the magnetic field B of 5-10 T, whose order
parameter is clearly distinguished from the low-field AFM phase observed for B
< 5 T and the superconducting phase for B < 3 T. The 4f electronic specific
heat divided by the temperature, C_e/T, exhibits -lnT dependence at B~10 T (=
B_0), and furthermore, the C_e/T data for B >= B_0 are well scaled by the
logarithmic function of B and T: ln[(B-B_0)/T^{2.7}]. These features are quite
similar to the scaling behavior found in pure CeCoIn5, strongly suggesting that
the field-induced QCF in pure CeCoIn5 originates from the hidden AFM order
parameter equivalent to high-field AFM order in Zn7%-doped CeCoIn5.
"
"  We report on the realization of a transversely loaded two-dimensional
magneto-optical trap serving as a source for cold strontium atoms. We analyze
the dependence of the source's properties on various parameters, in particular
the intensity of a pushing beam accelerating the atoms out of the source. An
atomic flux exceeding $10^9\,\mathrm{atoms/s}$ at a rather moderate oven
temperature of $500\,^\circ\mathrm{C}$ is achieved. The longitudinal velocity
of the atomic beam can be tuned over several tens of m/s by adjusting the power
of the pushing laser beam. The beam divergence is around $60$ mrad, determined
by the transverse velocity distribution of the cold atoms. The slow atom source
is used to load a three-dimensional magneto-optical trap realizing loading
rates up to $10^9\,\mathrm{atoms/s}$ without indication of saturation of the
loading rate for increasing oven temperature. The compact setup avoids
undesired effects found in alternative sources like, e.g., Zeeman slowers, such
as vacuum contamination and black-body radiation due to the hot strontium oven.
"
"  We present experimental measurements of the steady-state ion number in a
linear Paul trap (LPT) as a function of the ion-loading rate. These
measurements, taken with (a) constant Paul trap stability parameter $q$, (b)
constant radio-frequency (rf) amplitude, or (c) constant rf frequency, show
nonlinear behavior. At the loading rates achieved in this experiment, a plot of
the steady-state ion number as a function of loading rate has two regions: a
monotonic rise (region I) followed by a plateau (region II). Also described are
simulations and analytical theory which match the experimental results. Region
I is caused by rf heating and is fundamentally due to the time dependence of
the rf Paul-trap forces. We show that the time-independent pseudopotential,
frequently used in the analytical investigation of trapping experiments, cannot
explain region I, but explains the plateau in region II and can be used to
predict the steady-state ion number in that region. An important feature of our
experimental LPT is the existence of a radial cut-off $\hat R_{\rm cut}$ that
limits the ion capacity of our LPT and features prominently in the analytical
and numerical analysis of our LPT-loading results. We explain the dynamical
origin of $\hat R_{\rm cut}$ and relate it to the chaos border of the fractal
of non-escaping trajectories in our LPT. We also present an improved model of
LPT ion-loading as a function of time.
"
"  In the well known logistic map, the parameter of interest is weighted by a
coefficient that decreases linearly when this parameter increases. Since such a
linear decrease forms a specific case, we consider the more general case where
this coefficient decreases nonlinearly as in a hyperbolic tangent relaxation of
a system toward equilibrium. We show that, in this latter case, the asymptotic
values obtained via iteration of the logistic map are considerably modified. We
demonstrate that both the steepness of the nonlinear decrease as well as its
upper and lower boundaries significantly alter the bifurcation diagram. New
period doubling features and transitions to chaos appear, possibly leading to
regimes with small periods. Computations with a variety of parameter values
show that the logistic map can be significantly reordered in the case of a
nonlinear growth rate.
"
"  We apply the method of nonlinear steepest descent to compute the long-time
asymptotics of the Toda lattice with steplike initial data corresponding to a
rarefaction wave.
"
"  We report the three main ingredients to calculate three- and four-electron
integrals over Gaussian basis functions involving Gaussian geminal operators:
fundamental integrals, upper bounds, and recurrence relations. In particular,
we consider the three- and four-electron integrals that may arise in
explicitly-correlated F12 methods. A straightforward method to obtain the
fundamental integrals is given. We derive vertical, transfer and horizontal
recurrence relations to build up angular momentum over the centers. Strong,
simple and scaling-consistent upper bounds are also reported. This latest
ingredient allows to compute only the $\order{N^2}$ significant three- and
four-electron integrals, avoiding the computation of the very large number of
negligible integrals.
"
"  Strongly coupled quantum fluids are found in different forms, including
ultracold Fermi gases or tiny droplets of extremely hot Quark-Gluon Plasma.
Although the systems differ in temperature by many orders of magnitude, they
exhibit a similar almost inviscid fluid dynamical behavior. In this work, we
summarize some of the recent theoretical developments toward better
understanding this property in cold Fermi gases at and near unitarity.
"
"  From critical infrastructure, to physiology and the human brain, complex
systems rarely occur in isolation. Instead, the functioning of nodes in one
system often promotes or suppresses the functioning of nodes in another.
Despite advances in structural interdependence, modeling interdependence and
other interactions between dynamic systems has proven elusive. Here we define a
broadly applicable dynamic dependency link and develop a general framework for
interdependent and competitive interactions between general dynamic systems. We
apply our framework to studying interdependent and competitive synchronization
in multi-layer oscillator networks and cooperative/competitive contagions in an
epidemic model. Using a mean-field theory which we verify numerically, we find
explosive transitions and rich behavior which is absent in percolation models
including hysteresis, multi-stability and chaos. The framework presented here
provides a powerful new way to model and understand many of the interacting
complex systems which surround us.
"
"  Matrix Product Vectors form the appropriate framework to study and classify
one-dimensional quantum systems. In this work, we develop the structure theory
of Matrix Product Unitary operators (MPUs) which appear e.g. in the description
of time evolutions of one-dimensional systems. We prove that all MPUs have a
strict causal cone, making them Quantum Cellular Automata (QCAs), and derive a
canonical form for MPUs which relates different MPU representations of the same
unitary through a local gauge. We use this canonical form to prove an Index
Theorem for MPUs which gives the precise conditions under which two MPUs are
adiabatically connected, providing an alternative derivation to that of
[Commun. Math. Phys. 310, 419 (2012), arXiv:0910.3675] for QCAs. We also
discuss the effect of symmetries on the MPU classification. In particular, we
characterize the tensors corresponding to MPU that are invariant under
conjugation, time reversal, or transposition. In the first case, we give a full
characterization of all equivalence classes. Finally, we give several examples
of MPU possessing different symmetries.
"
"  Alastair Graham Walker Cameron was an astrophysicist and planetary scientist
of broad interests and exceptional originality. A founder of the field of
nuclear astrophysics, he developed the theoretical understanding of the
chemical elementsâ origins and made pioneering connections between the
abundances of elements in meteorites to advance the theory that the Moon
originated from a giant impact with the young Earth by an object at least the
size of Mars. Cameron was an early and persistent exploiter of computer
technology in the theoretical study of complex astronomical systemsâincluding
nuclear reactions in supernovae, the structure of neutron stars, and planetary
collisions.
"
"  We demonstrate how non-convex ""time crystal"" Lagrangians arise in the
effective description of conventional, realizable physical systems. Such
embeddings allow for the resolution of dynamical singularities that arise in
the reduced description. Sisyphus dynamics, featuring intervals of forward
motion interrupted by quick resets, is a generic consequence. Near the would-be
singularity of the time crystal, we find striking microstructure.
"
"  A new approach of solving the ill-conditioned inverse problem for analytical
continuation is proposed. The root of the problem lies in the fact that even
tiny noise of imaginary-time input data has a serious impact on the inferred
real-frequency spectra. By means of a modern regularization technique, we
eliminate redundant degrees of freedom that essentially carry the noise,
leaving only relevant information unaffected by the noise. The resultant
spectrum is represented with minimal bases and thus a stable analytical
continuation is achieved. This framework further provides a tool for analyzing
to what extent the Monte Carlo data need to be accurate to resolve details of
an expected spectral function.
"
"  We demonstrate a technique for obtaining the density of atomic vapor, by
doing a fit of the resonant absorption spectrum to a density-matrix model. In
order to demonstrate the usefulness of the technique, we apply it to absorption
in the ${\rm D_2}$ line of a Cs vapor cell at room temperature. The lineshape
of the spectrum is asymmetric due to the role of open transitions. This
asymmetry is explained in the model using transit-time relaxation as the atoms
traverse the laser beam. We also obtain the latent heat of evaporation by
studying the number density as a function of temperature close to room
temperature.
"
"  For inhomogeneous interacting electronic systems under a time-dependent
electromagnetic perturbation, we derive the linear equation for response
functions in a quantum mechanical manner. It is a natural extension of the
original semi-classical Singwi-Tosi-Land-Sjoelander (STLS) approach for an
electron gas. The factorization ansatz for the two-particle distribution is an
indispensable ingredient in the STLS approaches for determination of the
response function and the pair correlation function. In this study, we choose
an analytically solvable interacting two-electron system as the target for
which we examine the validity of the approximation. It is demonstrated that the
STLS response function reproduces well the exact one for low-energy
excitations. The interaction energy contributed from the STLS response function
is also discussed.
"
"  We examine by a perturbation method how the self-trapping of g-mode
oscillations in geometrically thin relativistic disks is affected by uniform
vertical magnetic fields. Disks which we consider are isothermal in the
vertical direction, but are truncated at a certain height by presence of hot
coronae. We find that the characteristics of self-trapping of axisymmetric
g-mode oscillations in non-magnetized disks is kept unchanged in magnetized
disks at least till a strength of the fields, depending on vertical thickness
of disks. These magnetic fields become stronger as the disk becomes thinner.
This result suggests that trapped g-mode oscillations still remain as one of
possible candidates of quasi-periodic oscillations observed in black-hole and
neutron-star X-ray binaries in the cases where vertical magnetic fields in
disks are weak.
"
"  We analyze the origins of the luminescence in germania-silica fibers with
high germanium concentration (about 30 mol. % GeO2) in the region 1-2 {\mu}m
with a laser pump at the wavelength 532 nm. We show that such fibers
demonstrate the high level of luminescence which unlikely allows the
observation of photon triplets, generated in a third-order spontaneous
parametric down-conversion process in such fibers. The only efficient approach
to the luminescence reduction is the hydrogen saturation of fiber samples,
however, even in this case the level of residual luminescence is still too high
for three-photon registration.
"
"  While the enhancement of the spin-space symmetry from the usual
$\mathrm{SU}(2)$ to $\mathrm{SU}(N)$ is promising for finding nontrivial
quantum spin liquids, its realization in magnetic materials remains
challenging. Here we propose a new mechanism by which the $\mathrm{SU}(4)$
symmetry emerges in the strong spin-orbit coupling limit. In $d^1$ transition
metal compounds with edge-sharing anion octahedra, the spin-orbit coupling
gives rise to strongly bond-dependent and apparently $\mathrm{SU}(4)$-breaking
hopping between the $J_\textrm{eff}=3/2$ quartets. However, in the honeycomb
structure, a gauge transformation maps the system to an
$\mathrm{SU}(4)$-symmetric Hubbard model. In the strong repulsion limit at
quarter filling, as realized in $\alpha$-ZrCl$_3,$ the low-energy effective
model is the $\mathrm{SU}(4)$ Heisenberg model on the honeycomb lattice, which
cannot have a trivial gapped ground state and is expected to host a gapless
spin-orbital liquid. By generalizing this model to other three-dimensional
lattices, we also propose crystalline spin-orbital liquids protected by this
emergent $\mathrm{SU}(4)$ symmetry and space group symmetries.
"
"  Optimization of the fidelity of control operations is of critical importance
in the pursuit of fault-tolerant quantum computation. We apply optimal control
techniques to demonstrate that a single drive via the cavity in circuit quantum
electrodynamics can implement a high-fidelity two-qubit all-microwave gate that
directly entangles the qubits via the mutual qubit-cavity couplings. This is
performed by driving at one of the qubits' frequencies which generates a
conditional two-qubit gate, but will also generate other spurious interactions.
These optimal control techniques are used to find pulse shapes that can perform
this two-qubit gate with high fidelity, robust against errors in the system
parameters. The simulations were all performed using experimentally relevant
parameters and constraints.
"
"  Plasmonics currently faces the problem of seemingly inevitable optical losses
occurring in the metallic components that challenges the implementation of
essentially any application. In this work we show that Ohmic losses are reduced
in certain layered metals, such as the transition metal dichalcogenide TaS$_2$,
due to an extraordinarily small density of states for scattering in the near-IR
originating from their special electronic band structure. Based on this
observation we propose a new class of band structure engineered van der Waals
layered metals composed of hexagonal transition metal chalcogenide-halide
layers with greatly suppressed intrinsic losses. Using first-principles
calculations we show that the suppression of optical losses lead to improved
performance for thin film waveguiding and transformation optics.
"
"  In the past 50 years, calorimeters have become the most important detectors
in many particle physics experiments, especially experiments in colliding-beam
accelerators at the energy frontier. In this paper, we describe and discuss a
number of common misconceptions about these detectors, as well as the
consequences of these misconceptions. We hope that it may serve as a useful
source of information for young colleagues who want to familiarize themselves
with these tricky instruments.
"
"  Recent experiments have revealed that the diffusivity of exothermic and fast
enzymes is enhanced when they are catalytically active, and different physical
mechanisms have been explored and quantified to account for this observation.
We perform measurements on the endothermic and relatively slow enzyme aldolase,
which also shows substrate-induced enhanced diffusion. We propose a new
physical paradigm, which reveals that the diffusion coefficient of a model
enzyme hydrodynamically coupled to its environment increases significantly when
undergoing changes in conformational fluctuations in a substrate-dependent
manner, and is independent of the overall turnover rate of the underlying
enzymatic reaction. Our results show that substrate-induced enhanced diffusion
of enzyme molecules can be explained within an equilibrium picture, and that
the exothermicity of the catalyzed reaction is not a necessary condition for
the observation of this phenomenon.
"
"  We propose a data-driven filtered reduced order model (DDF-ROM) framework for
the numerical simulation of fluid flows. The novel DDF-ROM framework consists
of two steps: (i) In the first step, we use explicit ROM spatial filtering of
the nonlinear PDE to construct a filtered ROM. This filtered ROM is
low-dimensional, but is not closed (because of the nonlinearity in the given
PDE). (ii) In the second step, we use data-driven modeling to close the
filtered ROM, i.e., to model the interaction between the resolved and
unresolved modes. To this end, we use a quadratic ansatz to model this
interaction and close the filtered ROM. To find the new coefficients in the
closed filtered ROM, we solve an optimization problem that minimizes the
difference between the full order model data and our ansatz. We emphasize that
the new DDF-ROM is built on general ideas of spatial filtering and optimization
and is independent of (restrictive) phenomenological arguments.
We investigate the DDF-ROM in the numerical simulation of a 2D channel flow
past a circular cylinder at Reynolds number $Re=100$. The DDF-ROM is
significantly more accurate than the standard projection ROM. Furthermore, the
computational costs of the DDF-ROM and the standard projection ROM are similar,
both costs being orders of magnitude lower than the computational cost of the
full order model. We also compare the new DDF-ROM with modern ROM closure
models in the numerical simulation of the 1D Burgers equation. The DDF-ROM is
more accurate and significantly more efficient than these ROM closure models.
"
"  We recalculate the leading relativistic corrections for the ground electronic
state of the hydrogen molecule using variational method with explicitly
correlated functions which satisfy the interelectronic cusp condition. The new
computational approach allowed for the control of the numerical precision which
reached about 8 significant digits. More importantly, the updated theoretical
energies became discrepant with the known experimental values and we conclude
that the yet unknown relativistic recoil corrections might be larger than
previously anticipated.
"
"  We present a quantitative analysis on the response of a dilute active
suspension of self-propelled rods (swimmers) in a planar channel subjected to
an imposed shear flow. To best capture the salient features of shear-induced
effects, we consider the case of an imposed Couette flow, providing a constant
shear rate across the channel. We argue that the steady-state behavior of
swimmers can be understood in the light of a population splitting phenomenon,
occurring as the shear rate exceeds a certain threshold, initiating the
reversal of swimming direction for a finite fraction of swimmers from down- to
upstream or vice versa, depending on swimmer position within the channel.
Swimmers thus split into two distinct, statistically significant and oppositely
swimming majority and minority populations. The onset of population splitting
translates into a transition from a self-propulsion-dominated regime to a
shear-dominated regime, corresponding to a unimodal-to-bimodal change in the
probability distribution function of the swimmer orientation. We present a
phase diagram in terms of the swim and flow Peclet numbers showing the
separation of these two regimes by a discontinuous transition line. Our results
shed further light on the behavior of swimmers in a shear flow and provide an
explanation for the previously reported non-monotonic behavior of the mean,
near-wall, parallel-to-flow orientation of swimmers with increasing shear
strength.
"
"  Composite materials comprised of ferroelectric nanoparticles in a dielectric
matrix are being actively investigated for a variety of functional properties
attractive for a wide range of novel electronic and energy harvesting devices.
However, the dependence of these functionalities on shapes, sizes, orientation
and mutual arrangement of ferroelectric particles is currently not fully
understood. In this study, we utilize a time-dependent Ginzburg-Landau approach
combined with coupled-physics finite-element-method based simulations to
elucidate the behavior of polarization in isolated spherical PbTiO3 or BaTiO3
nanoparticles embedded in a dielectric medium, including air. The equilibrium
polarization topology is strongly affected by particle diameter, as well as the
choice of inclusion and matrix materials, with monodomain, vortex-like and
multidomain patterns emerging for various combinations of size and materials
parameters. This leads to radically different polarization vs electric field
responses, resulting in highly tunable size-dependent dielectric properties
that should be possible to observe experimentally. Our calculations show that
there is a critical particle size below which ferroelectricity vanishes. For
the PbTiO3 particle, this size is 2 and 3.4 nm, respectively, for high- and
low-permittivity media. For the BaTiO3 particle, it is ~3.6 nm regardless of
the medium dielectric strength.
"
"  We have constructed the database of stars in the local group using the
extended version of the SAGA (Stellar Abundances for Galactic Archaeology)
database that contains stars in 24 dwarf spheroidal galaxies and ultra faint
dwarfs. The new version of the database includes more than 4500 stars in the
Milky Way, by removing the previous metallicity criterion of [Fe/H] <= -2.5,
and more than 6000 stars in the local group galaxies. We examined a validity of
using a combined data set for elemental abundances. We also checked a
consistency between the derived distances to individual stars and those to
galaxies in the literature values. Using the updated database, the
characteristics of stars in dwarf galaxies are discussed. Our statistical
analyses of alpha-element abundances show that the change of the slope of the
[alpha/Fe] relative to [Fe/H] (so-called ""knee"") occurs at [Fe/H] = -1.0+-0.1
for the Milky Way. The knee positions for selected galaxies are derived by
applying the same method. Star formation history of individual galaxies are
explored using the slope of the cumulative metallicity distribution function.
Radial gradients along the four directions are inspected in six galaxies where
we find no direction dependence of metallicity gradients along the major and
minor axes. The compilation of all the available data shows a lack of CEMP-s
population in dwarf galaxies, while there may be some CEMP-no stars at [Fe/H]
<~ -3 even in the very small sample. The inspection of the relationship between
Eu and Ba abundances confirms an anomalously Ba-rich population in Fornax,
which indicates a pre-enrichment of interstellar gas with r-process elements.
We do not find any evidence of anti-correlations in O-Na and Mg-Al abundances,
which characterises the abundance trends in the Galactic globular clusters.
"
"  General relativistic effects have long been predicted to subtly influence the
observed large-scale structure of the universe. The current generation of
galaxy redshift surveys have reached a size where detection of such effects is
becoming feasible. In this paper, we report the first detection of the redshift
asymmetry from the cross-correlation function of two galaxy populations which
is consistent with relativistic effects. The dataset is taken from the Sloan
Digital Sky Survey DR12 CMASS galaxy sample, and we detect the asymmetry at the
$2.7\sigma$ level by applying a shell-averaged estimator to the
cross-correlation function. Our measurement dominates at scales around $10$
h$^{-1}$Mpc, larger than those over which the gravitational redshift profile
has been recently measured in galaxy clusters, but smaller than scales for
which linear perturbation theory is likely to be accurate. The detection
significance varies by 0.5$\sigma$ with the details of our measurement and
tests for systematic effects. We have also devised two null tests to check for
various survey systematics and show that both results are consistent with the
null hypothesis. We measure the dipole moment of the cross-correlation
function, and from this the asymmetry is also detected, at the $2.8 \sigma$
level. The amplitude and scale-dependence of the clustering asymmetries are
approximately consistent with the expectations of General Relativity and a
biased galaxy population, within large uncertainties. We explore theoretical
predictions using numerical simulations in a companion paper.
"
"  The role of phase separation in the emergence of superconductivity in alkali
metal doped iron selenides A$_{x}$Fe$_{2-y}$Se$_{2}$ (A = K, Rb, Cs) is
revisited. High energy X-ray diffraction and Monte Carlo simulation were used
to investigate the crystal structure of quenched superconducting (SC) and
as-grown non-superconducting (NSC) K$_{x}$Fe$_{2-y}$Se$_{2}$ single crystals.
The coexistence of superlattice structures with the in-plane
$\sqrt{2}\times\sqrt{2}$ K-vacancy ordering and the $\sqrt{5}\times\sqrt{5}$
Fe-vacancy ordering were observed in SC and NSC crystals along side the
\textit{I4/mmm} Fe-vacancy free phase. Moreover, in the SC crystal an
Fe-vacancy disordered phase is additionally present. It appears at the boundary
between the \textit{I4/mmm} vacancy free phase and the \textit{I4/m} vacancy
ordered phase ($\sqrt{5}\times\sqrt{5}$). The vacancy disordered phase is most
likely the host of superconductivity.
"
"  Primordial black holes (PBHs) have long been suggested as a candidate for
making up some or all of the dark matter in the Universe. Most of the
theoretically possible mass range for PBH dark matter has been ruled out with
various null observations of expected signatures of their interaction with
standard astrophysical objects. However, current constraints are significantly
less robust in the 20 M_sun < M_PBH < 100 M_sun mass window, which has received
much attention recently, following the detection of merging black holes with
estimated masses of ~30 M_sun by LIGO and the suggestion that these could be
black holes formed in the early Universe. We consider the potential of advanced
LIGO (aLIGO) operating at design sensitivity to probe this mass range by
looking for peaks in the mass spectrum of detected events. To quantify the
background, which is due to black holes that are formed from dying stars, we
model the shape of the stellar-black-hole mass function and calibrate its
amplitude to match the O1 results. Adopting very conservative assumptions about
the PBH and stellar-black-hole merger rates, we show that ~5 years of aLIGO
data can be used to detect a contribution of >20 M_sun PBHs to dark matter down
to f_PBH<0.5 at >99.9% confidence level. Combined with other probes that
already suggest tension with f_PBH=1, the obtainable independent limits from
aLIGO will thus enable a firm test of the scenario that PBHs make up all of
dark matter.
"
"  We present K-band Multi-Object Spectrograph (KMOS) observations of 18 Red
Supergiant (RSG) stars in the Sculptor Group galaxy NGC 55. Radial velocities
are calculated and are shown to be in good agreement with previous estimates,
confirming the supergiant nature of the targets and providing the first
spectroscopically confirmed RSGs in NGC 55. Stellar parameters are estimated
for 14 targets using the $J$-band analysis technique, making use of
state-of-the-art stellar model atmospheres. The metallicities estimated confirm
the low-metallicity nature of NGC 55, in good agreement with previous studies.
This study provides an independent estimate of the metallicity gradient of NGC
55, in excellent agreement with recent results published using hot massive
stars. In addition, we calculate luminosities of our targets and compare their
distribution of effective temperatures and luminosities to other RSGs, in
different environments, estimated using the same technique.
"
"  For the gas near a solid planar wall, we propose a scaling formula for the
mean free path of a molecule as a function of the distance from the wall, under
the assumption of a uniform distribution of the incident directions of the
molecular free flight. We subsequently impose the same scaling onto the
viscosity of the gas near the wall, and compute the Navier-Stokes solution of
the velocity of a shear flow parallel to the wall. This solution exhibits the
Knudsen velocity boundary layer in agreement with the corresponding Direct
Simulation Monte Carlo computations for argon and nitrogen. We also find that
the proposed mean free path and viscosity scaling sets the second derivative of
the velocity to infinity at the wall boundary of the flow domain, which
suggests that the gas flow is formally turbulent within the Knudsen boundary
layer near the wall.
"
"  The demographics of dwarf galaxy populations have long been in tension with
predictions from the Cold Dark Matter (CDM) paradigm. If primordial density
fluctuations were scale-free as predicted, dwarf galaxies should themselves
host dark matter subhaloes, the most massive of which may have undergone star
formation resulting in dwarf galaxy groups. Ensembles of dwarf galaxies are
observed as satellites of more massive galaxies, and there is observational and
theoretical evidence to suggest that these satellites at z=0 were captured by
the massive host halo as a group. However, the evolution of dwarf galaxies is
highly susceptible to environment making these satellite groups imperfect
probes of CDM in the low mass regime. We have identified one of the clearest
examples to date of hierarchical structure formation at low masses: seven
isolated, spectroscopically confirmed groups with only dwarf galaxies as
members. Each group hosts 3-5 known members, has a baryonic mass of ~4.4 x 10^9
to 2 x 10^10 Msun, and requires a mass-to-light ratio of <100 to be
gravitationally bound. Such groups are predicted to be rare theoretically and
found to be rare observationally at the current epoch and thus provide a unique
window into the possible formation mechanism of more massive, isolated
galaxies.
"
"  We recently showed that several Local Group (LG) galaxies have much higher
radial velocities (RVs) than predicted by a 3D dynamical model of the standard
cosmological paradigm. Here, we show that 6 of these 7 galaxies define a thin
plane with root mean square thickness of only 101 kpc despite a widest extent
of nearly 3 Mpc, much larger than the conventional virial radius of the Milky
Way (MW) or M31. This plane passes within ${\sim 70}$ kpc of the MW-M31
barycentre and is oriented so the MW-M31 line is inclined by $16^\circ$ to it.
We develop a toy model to constrain the scenario whereby a past MW-M31 flyby
in Modified Newtonian Dynamics (MOND) forms tidal dwarf galaxies that settle
into the recently discovered planes of satellites around the MW and M31. The
scenario is viable only for a particular MW-M31 orbital plane. This roughly
coincides with the plane of LG dwarfs with anomalously high RVs.
Using a restricted $N$-body simulation of the LG in MOND, we show how the
once fast-moving MW and M31 gravitationally slingshot test particles outwards
at high speeds. The most distant such particles preferentially lie within the
MW-M31 orbital plane, probably because the particles ending up with the highest
RVs are those flung out almost parallel to the motion of the perturber. This
suggests a dynamical reason for our finding of a similar trend in the real LG,
something not easily explained as a chance alignment of galaxies with an
isotropic or mildly flattened distribution (probability $= {0.0015}$).
"
"  Understanding the nature of two-level tunneling defects is important for
minimizing their disruptive effects in various nano-devices. By exploiting the
resonant coupling of these defects to a superconducting qubit, one can probe
and coherently manipulate them individually. In this work we utilize a phase
qubit to induce Rabi oscillations of single tunneling defects and measure their
dephasing rates as a function of the defect's asymmetry energy, which is tuned
by an applied strain. The dephasing rates scale quadratically with the external
strain and are inversely proportional to the Rabi frequency. These results are
analyzed and explained within a model of interacting standard defects, in which
pure dephasing of coherent high-frequency (GHz) defects is caused by
interaction with incoherent low-frequency thermally excited defects.
"
"  We present a simplified description for spin-dependent electronic transport
in honeycomb-lattice structures with spin-orbit interactions, using
generalizations of the stochastic non-equilibrium model known as the totally
asymmetric simple exclusion process. Mean field theory and numerical
simulations are used to study currents, density profiles and current
polarization in quasi- one dimensional systems with open boundaries, and
externally-imposed particle injection ($\alpha$) and ejection ($\beta$) rates.
We investigate the influence of allowing for double site occupancy, according
to Pauli's exclusion principle, on the behavior of the quantities of interest.
We find that double occupancy shows strong signatures for specific combinations
of rates, namely high $\alpha$ and low $\beta$, but otherwise its effects are
quantitatively suppressed. Comments are made on the possible relevance of the
present results to experiments on suitably doped graphenelike structures.
"
"  Acoustic neutrino detection is a promising approach to extend the energy
range of neutrino telescopes to energies beyond $10^{18}$\,eV. Currently
operational and planned water-Cherenkov neutrino telescopes, most notably
KM3NeT, include acoustic sensors in addition to the optical ones. These
acoustic sensors could be used as instruments for acoustic detection, while
their main purpose is the position calibration of the detection units. In this
article, a Monte Carlo simulation chain for acoustic detectors will be
presented, covering the initial interaction of the neutrino up to the signal
classification of recorded events. The ambient and transient background in the
simulation was implemented according to data recorded by the acoustic set-up
AMADEUS inside the ANTARES detector. The effects of refraction on the neutrino
signature in the detector are studied, and a classification of the recorded
events is implemented. As bipolar waveforms similar to those of the expected
neutrino signals are also emitted from other sound sources, additional features
like the geometrical shape of the propagation have to be considered for the
signal classification. This leads to a large improvement of the background
suppression by almost two orders of magnitude, since a flat cylindrical
""pancake"" propagation pattern is a distinctive feature of neutrino signals. An
overview of the simulation chain and the signal classification will be
presented and preliminary studies of the performance of the classification will
be discussed.
"
"  We compute the polarization function in a doped three-dimensional
anisotropic-Weyl semimetal, in which the fermion energy dispersion is linear in
two components of the momenta and quadratic in the third. Through detailed
calculations, we find that the long wavelength plasmon mode depends on the
fermion density $n_e$ in the form $\Omega_{p}^{\bot}\propto n_{e}^{3/10}$
within the basal plane and behaves as $\Omega_{p}^{z}\propto n_{e}^{1/2}$ along
the third direction. This unique characteristic of the plasmon mode can be
probed by various experimental techniques, such as electron energy-loss
spectroscopy. The Debye screening at finite chemical potential and finite
temperature is also analyzed based on the polarization function.
"
"  Unsupervised machine learning via a restricted Boltzmann machine is an useful
tool in distinguishing an ordered phase from a disordered phase. Here we study
its application on the two-dimensional Ashkin-Teller model, which features a
partially ordered product phase. We train the neural network with spin
configuration data generated by Monte Carlo simulations and show that distinct
features of the product phase can be learned from non-ergodic samples resulting
from symmetry breaking. Careful analysis of the weight matrices inspires us to
define a nontrivial machine-learning motivated quantity of the product form,
which resembles the conventional product order parameter.
"
"  We study the thermodynamics of ideal Bose gas as well as the transport
properties of non interacting bosons and fermions in a one dimensional
quasi-periodic potential, namely Aubry-André (AA) model at finite
temperature. For bosons in finite size systems, the effect of quasi-periodic
potential on the crossover phenomena corresponding to Bose-Einstein
condensation (BEC), superfluidity and localization phenomena at finite
temperatures are investigated. From the ground state number fluctuation we
calculate the crossover temperature of BEC which exhibits a non monotonic
behavior with the strength of AA potential and vanishes at the self-dual
critical point following power law. Appropriate rescaling of the crossover
temperatures reveals universal behavior which is studied for different
quasi-periodicity of the AA model. Finally, we study the temperature and flux
dependence of the persistent current of fermions in presence of a
quasi-periodic potential to identify the localization at the Fermi energy from
the decay of the current.
"
"  Using a high-frequency expansion in periodically driven extended Hubbard
models, where the strengths and ranges of density-density interactions are
arbitrary, we obtain the effective interactions and bandwidth, which depend
sensitively on the polarization of the driving field. Then, we numerically
calculate modulations of correlation functions in a quarter-filled extended
Hubbard model with nearest-neighbor interactions on a triangular lattice with
trimers after monocycle pulse excitation. We discuss how the resultant
modulations are compatible with the effective interactions and bandwidth
derived above on the basis of their dependence on the polarization of
photoexcitation, which is easily accessible by experiments. Some correlation
functions after monocycle pulse excitation are consistent with the effective
interactions, which are weaker or stronger than the original ones. However, the
photoinduced enhancement of anisotropic charge correlations previously
discussed for the three-quarter-filled organic conductor
$\alpha$-(bis[ethylenedithio]-tetrathiafulvalene)$_2$I$_3$
[$\alpha$-(BEDT-TTF)$_2$I$_3$] in the metallic phase is not fully explained by
the effective interactions or bandwidth, which are derived independently of the
filling.
"
"  We define a lattice model for rock, absorbers, and gas that makes it possible
to examine the flow of gas to a complicated absorbing boundary over long
periods of time. The motivation is to deduce the geometry of the boundary from
the time history of gas absorption. We find a solution to this model using
Green's function techniques, and apply the solution to three absorbing networks
of increasing complexity.
"
"  Analyzing temperature dependent photoemission (PE) data of the ferromagnetic
Kondo-lattice (KL) system YbNiSn in the light of the Periodic Anderson model
(PAM) we show that the KL behavior is not limited to temperatures below a
temperature T_K, defined empirically from resistivity and specificic heat
measurements. As characteristic for weakly hybridized Ce and Yb systems, the PE
spectra reveal a 4f-derived Fermi level peak, which reflects contributions from
the Kondo resonance and its crystal electric field (CEF) satellites. In YbNiSn
this peak has an unusual temperature dependence: With decreasing temperature a
steady linear increase of intensity is observed which extends over a large
interval ranging from 100 K down to 1 K without showing any peculiarities in
the region of T_K ~ TC= 5.6 K. In the light of the single-impurity Anderson
model (SIAM) this intensity variation reflects a linear increase of 4f
occupancy with decreasing temperature, indicating an onset of Kondo screening
at temperatures above 100 K. Within the PAM this phenomenon could be described
by a non-Fermi liquid like T- linear damping of the self-energy which accounts
phenomenologically for the feedback from the closely spaced CEF-states.
"
"  The cosmic ray electrons measured by Voyager 1 between 3-70 MeV beyond the
heliopause have intensities several hundred times those measured at the Earth
by PAMELA at nearly the same energies. This paper compares this new V1 data
with data from the earth-orbiting PAMELA experiment up to energies greater than
10 GeV where solar modulation effects are negligible. In this energy regime we
assume the main parameters governing electron propagation are diffusion and
energy loss and we use a Monte Carlo program to describe this propagation in
the galaxy. To reproduce the new Voyager electron spectrum, which is E-1.3,
together with that measured by PAMELA which is E-3.20 above 10 GeV, we require
a diffusion coefficient which is P 0.45 at energies above 0.5 GeV changing to a
P-1.00 dependence at lower rigidities. The entire electron spectrum observed at
both V1 and PAMELA from 3 MeV to 30 GeV can then be described by a simple
source spectrum, dj/dP P-2.25, with a spectral exponent that is independent of
rigidity. The change in exponent of the measured electron spectrum from -1.3 at
low energies to 3.2 at the highest energies can be explained by galactic
propagation effects related to the changing dependence of the diffusion
coefficient below 0.5 GeV, and the increasing importance above 0.5 GV of energy
loss from synchrotron and inverse Compton radiation, which are both E2, and
which are responsible for most of the changing spectral exponent above 1.0 GV.
As a result of the P-1.00 dependence of the diffusion coefficient below 0.5
GV that is required to fit the V1 electron spectrum, there is a rapid flow of
these low energy electrons out of the galaxy. These electrons in local IG space
are unobservable to us at any wave length and therefore form a dark energy
component which is 100 times the electrons rest energy.
"
"  Transition metal oxides are promising candidates for thermoelectric
applications, because they are stable at high temperature and because strong
electronic correlations can generate large Seebeck coefficients, but their
thermoelectric power factors are limited by the low electrical conductivity. We
report transport measurements on Ca3Co4O9 films on various perovskite
substrates and show that reversible incorporation of oxygen into SrTiO3 and
LaAlO3 substrates activates a parallel conduction channel for p-type carriers,
greatly enhancing the thermoelectric performance of the film-substrate system
at temperatures above 450 °C. Thin-film structures that take advantage of
both electronic correlations and the high oxygen mobility of transition metal
oxides thus open up new perspectives for thermopower generation at high
temperature.
"
"  We analyze the motion of a rod floating in a weightless environment in space
when a force is applied at some point on the rod in a direction perpendicular
to its length. If the force applied is at the centre of mass, then the rod gets
a linear motion perpendicular to its length. However, if the same force is
applied at a point other than the centre of mass, say, near one end of the rod,
thereby giving rise to a torque, then there will also be a rotation of the rod
about its centre of mass, in addition to the motion of the centre of mass
itself. If the force applied is for a very short duration, but imparting
nevertheless a finite impulse, like in a sudden (quick) hit at one end of the
rod, then the centre of mass will move with a constant linear speed and
superimposed on it will be a rotation of the rod with constant angular speed
about the centre of mass. However, if force is applied continuously, say by
strapping a tiny rocket at one end of the rod, then the rod will spin faster
and faster about the centre of mass, with angular speed increasing linearly
with time. As the direction of the applied force, as seen by an external
(inertial) observer, will be changing continuously with the rotation of the
rod, the acceleration of the centre of mass would also be not in one fixed
direction. However, it turns out that the locus of the velocity vector of the
centre of mass will describe a Cornu spiral, with the velocity vector reaching
a final constant value with time. The mean motion of the centre of mass will be
in a straight line, with superposed initial oscillations that soon die down.
"
"  Magnetic Particle Imaging (MPI) has been shown to provide remarkable contrast
for imaging applications such as angiography, stem cell tracking, and cancer
imaging. Recently, there is growing interest in the functional imaging
capabilities of MPI, where color MPI techniques have explored separating
different nanoparticles, which could potentially be used to distinguish
nanoparticles in different states or environments. Viscosity mapping is a
promising functional imaging application for MPI, as increased viscosity levels
in vivo have been associated with numerous diseases such as hypertension,
atherosclerosis, and cancer. In this work, we propose a viscosity mapping
technique for MPI through the estimation of the relaxation time constant of the
nanoparticles. Importantly, the proposed time constant estimation scheme does
not require any prior information regarding the nanoparticles. We validate this
method with extensive experiments in an in-house magnetic particle spectroscopy
(MPS) setup at four different frequencies (between 250 Hz and 10.8 kHz) and at
three different field strengths (between 5 mT and 15 mT) for viscosities
ranging between 0.89 mPa.s to 15.33 mPa.s. Our results demonstrate the
viscosity mapping ability of MPI in the biologically relevant viscosity range.
"
"  We show that in decaying hydromagnetic turbulence with initial kinetic
helicity, a weak magnetic field eventually becomes fully helical. The sign of
magnetic helicity is opposite to that of the kinetic helicity - regardless of
whether or not the initial magnetic field was helical. The magnetic field
undergoes inverse cascading with the magnetic energy decaying approximately
like t^{-1/2}. This is even slower than in the fully helical case, where it
decays like t^{-2/3}. In this parameter range, the product of magnetic energy
and correlation length raised to a certain power slightly larger than unity, is
approximately constant. This scaling of magnetic energy persists over long time
scales. At very late times and for domain sizes large enough to accommodate the
growing spatial scales, we expect a cross-over to the t^{-2/3} decay law that
is commonly observed for fully helical magnetic fields. Regardless of the
presence or absence of initial kinetic helicity, the magnetic field experiences
exponential growth during the first few turnover times, which is suggestive of
small-scale dynamo action. Our results have applications to a wide range of
experimental dynamos and astrophysical time-dependent plasmas, including
primordial turbulence in the early universe.
"
"  We present millimetre dust emission measurements of two Lyman Break Galaxies
at z~3 and construct for the first time fully sampled infrared spectral energy
distributions (SEDs), from mid-IR to the Rayleigh-Jeans tail, of individually
detected, unlensed, UV-selected, main sequence (MS) galaxies at $z=3$. The SED
modelling of the two sources confirms previous findings, based on stacked
ensembles, of an increasing mean radiation field <U> with redshift, consistent
with a rapidly decreasing gas metallicity in z > 2 galaxies. Complementing our
study with CO[3-2] emission line observations, we measure the molecular gas
mass (M_H2) reservoir of the systems using three independent approaches: 1) CO
line observations, 2) the dust to gas mass ratio vs metallicity relation and 3)
a single band, dust emission flux on the Rayleigh-Jeans side of the SED. All
techniques return consistent M_H2 estimates within a factor of ~2 or less,
yielding gas depletion time-scales (tau_dep ~ 0.35 Gyrs) and gas-to-stellar
mass ratios (M_H2/M* ~ 0.5-1) for our z~3 massive MS galaxies. The overall
properties of our galaxies are consistent with trends and relations established
at lower redshifts, extending the apparent uniformity of star-forming galaxies
over the last 11.5 billion years.
"
"  Experiments are reported on the performance of a pitching and heaving
two-dimensional foil in a water channel in either continuous or intermittent
motion. We find that the thrust and power are independent of the mean
freestream velocity for two-fold changes in the mean velocity (four-fold in the
dynamic pressure), and for oscillations in the velocity up to 38\% of the mean,
where the oscillations are intended to mimic those of freely swimming motions
where the thrust varies during the flapping cycle. We demonstrate that the
correct velocity scale is not the flow velocity but the mean velocity of the
trailing edge. We also find little or no impact of streamwise velocity change
on the wake characteristics such as vortex organization, vortex strength, and
time-averaged velocity profile development---the wake is both qualitatively and
quantitatively unchanged. Our results suggest that constant velocity studies
can be used to make robust conclusions about swimming performance without a
need to explore the free-swimming condition.
"
"  Aims. The purpose of this paper is to detect and investigate the nature of
long-term radial velocity (RV) variations of K-type giants and to confirm
planetary companions around the stars.
Methods. We have conducted two planet search programs by precise RV
measurement using the 1.8 m telescope at Bohyunsan Optical Astronomy
Observatory (BOAO) and the 1.88 m telescope at Okayama Astrophysical
Observatory (OAO). The BOAO program searches for planets around 55 early K
giants. The OAO program is looking for 190 G-K type giants.
Results. In this paper, we report the detection of long-period RV variations
of three K giant stars, HD 40956, HD 111591, and HD 113996. We investigated the
cause of the observed RV variations and conclude the substellar companions are
most likely the cause of the RV variations. The orbital analyses yield P =
578.6 $\pm$ 3.3 d, $m$ sin $i$ = 2.7 $\pm$ 0.6 $M_{\rm{J}}$, $a$ = 1.4 $\pm$
0.1 AU for HD 40956; P = 1056.4 $\pm$ 14.3 d, $m$ sin $i$ = 4.4 $\pm$ 0.4
$M_{\rm{J}}$, $a$ = 2.5 $\pm$ 0.1 AU for HD 111591; P = 610.2 $\pm$ 3.8 d, $m$
sin $i$ = 6.3 $\pm$ 1.0 $M_{\rm{J}}$, $a$ = 1.6 $\pm$ 0.1 AU for HD 113996.
"
"  We address the computation of ground-state properties of chemical systems and
realistic materials within the auxiliary-field quantum Monte Carlo method. The
phase constraint to control the fermion phase problem requires the random walks
in Slater determinant space to be open-ended with branching. This in turn makes
it necessary to use back-propagation (BP) to compute averages and correlation
functions of operators that do not commute with the Hamiltonian. Several BP
schemes are investigated and their optimization with respect to the phaseless
constraint is considered. We propose a modified BP method for the computation
of observables in electronic systems, discuss its numerical stability and
computational complexity, and assess its performance by computing ground-state
properties for several substances, including constituents of the primordial
terrestrial atmosphere and small organic molecules.
"
"  Topological insulator surfaces in proximity to superconductors have been
proposed as a way to produce Majorana fermions in condensed matter physics. One
of the simplest proposed experiments with such a system is Majorana
interferometry. Here, we consider two possibly conflicting constraints on the
size of such an interferometer. Coupling of a Majorana mode from the edge (the
arms) of the interferometer to vortices in the centre of the device sets a
lower bound on the size of the device. On the other hand, scattering to the
usually imperfectly insulating bulk sets an upper bound. From estimates of
experimental parameters, we find that typical samples may have no size window
in which the Majorana interferometer can operate, implying that a new
generation of more highly insulating samples must be explored.
"
"  We present an analysis of the main systematic effects that could impact the
measurement of CMB polarization with the proposed CORE space mission. We employ
timeline-to-map simulations to verify that the CORE instrumental set-up and
scanning strategy allow us to measure sky polarization to a level of accuracy
adequate to the mission science goals. We also show how the CORE observations
can be processed to mitigate the level of contamination by potentially worrying
systematics, including intensity-to-polarization leakage due to bandpass
mismatch, asymmetric main beams, pointing errors and correlated noise. We use
analysis techniques that are well validated on data from current missions such
as Planck to demonstrate how the residual contamination of the measurements by
these effects can be brought to a level low enough not to hamper the scientific
capability of the mission, nor significantly increase the overall error budget.
We also present a prototype of the CORE photometric calibration pipeline, based
on that used for Planck, and discuss its robustness to systematics, showing how
CORE can achieve its calibration requirements. While a fine-grained assessment
of the impact of systematics requires a level of knowledge of the system that
can only be achieved in a future study phase, the analysis presented here
strongly suggests that the main areas of concern for the CORE mission can be
addressed using existing knowledge, techniques and algorithms.
"
"  Line-intensity mapping surveys probe large-scale structure through spatial
variations in molecular line emission from a population of unresolved
cosmological sources. Future such surveys of carbon monoxide line emission,
specifically the CO(1-0) line, face potential contamination from a disjoint
population of sources emitting in a hydrogen cyanide emission line, HCN(1-0).
This paper explores the potential range of the strength of HCN emission and its
effect on the CO auto power spectrum, using simulations with an empirical model
of the CO/HCN--halo connection. We find that effects on the observed CO power
spectrum depend on modeling assumptions but are very small for our fiducial
model based on our understanding of the galaxy--halo connection, with the bias
in overall CO detection significance due to HCN expected to be less than 1%.
"
"  Two-dimensional (spin-$2$) Affleck-Kennedy-Lieb-Tasaki (AKLT) type valence
bond solids on the square lattice are known to be symmetry protected
topological (SPT) gapped spin liquids [Shintaro Takayoshi, Pierre Pujol, and
Akihiro Tanaka Phys. Rev. B ${\bf 94}$, 235159 (2016)]. Using the projected
entangled pair state (PEPS) framework, we extend the construction of the AKLT
state to the case of $SU(3)$, relevant for cold atom systems. The entanglement
spectrum is shown to be described by an alternating $SU(3)$ chain of ""quarks""
and ""antiquarks"", subject to exponentially decaying (with distance) Heisenberg
interactions, in close similarity with its $SU(2)$ analog. We discuss the SPT
feature of the state.
"
"  ESA Gaia mission is producing the more accurate source catalogue in astronomy
up to now. That represents a challenge on the archiving area to make accessible
this information to the astronomers in an efficient way. Also, new astronomical
missions have reinforced the change on the development of archives. Archives,
as simple applications to access the data are being evolving into complex data
center structures where computing power services are available for users and
data mining tools are integrated into the server side. In the case of astronomy
science that involves the use of big catalogues, as in Gaia (or Euclid to
come), the common ways to work on the data need to be changed to a new paradigm
""move code close to the data"", what implies that data mining functionalities
are becoming a must to allow the science exploitation. To enable these
capabilities, a TAP+ interface, crossmatch capabilities, full catalogue
histograms, serialisation of intermediate results in cloud resources like
VOSpace, etc have been implemented for the Gaia DR1, to enable the exploitation
of these science resources by the community without the bottlenecks on the
connection bandwidth. We present the architecture, infrastructure and tools
already available in the Gaia Archive Data Release 1
(this http URL) and we describe capabilities and
infrastructure.
"
"  Seven of the nine known Mars Trojan asteroids belong to an orbital cluster
named after its largest member 5261 Eureka. Eureka is likely the progenitor of
the whole cluster, which formed at least 1 Gyr ago. It was suggested that the
thermal YORP effect spun-up Eureka resulting with fragments being ejected by
the rotational-fission mechanism. Eureka's spectrum exhibits a broad and deep
absorption band around 1 {\mu}m, indicating an olivine-rich composition. Here
we show evidence that the Trojan Eureka cluster progenitor could have
originated as impact debris excavated from the Martian mantle. We present new
near-infrared observations of two Trojans (311999 2007 NS2 and 385250 2001
DH47) and find that both exhibit an olivine-rich reflectance spectrum similar
to Eureka's. These measurements confirm that the progenitor of the cluster has
an achondritic composition. Olivine-rich reflectance spectra are rare amongst
asteroids but are seen around the largest basins on Mars. They are also
consistent with some Martian meteorites (e.g. Chassigny), and with the material
comprising much of the Martian mantle. Using numerical simulations, we show
that the Mars Trojans are more likely to be impact ejecta from Mars than
captured olivine-rich asteroids transported from the main belt. This result
links directly specific asteroids to debris from the forming planets.
"
"  The abundance of metals in galaxies is a key parameter which permits to
distinguish between different galaxy formation and evolution models. Most of
the metallicity determinations are based on optical line ratios. However, the
optical spectral range is subject to dust extinction and, for high-z objects (z
> 3), some of the lines used in optical metallicity diagnostics are shifted to
wavelengths not accessible to ground based observatories. For this reason, we
explore metallicity diagnostics using far-infrared (IR) line ratios which can
provide a suitable alternative in such situations. To investigate these far-IR
line ratios, we modeled the emission of a starburst with the photoionization
code CLOUDY. The most sensitive far-IR ratios to measure metallicities are the
[OIII]52$\mu$m and 88$\mu$m to [NIII]57$\mu$m ratios. We show that this ratio
produces robust metallicities in the presence of an AGN and is insensitive to
changes in the age of the ionizing stellar. Another metallicity sensitive ratio
is the [OIII]88$\mu$m/[NII]122$\mu$m ratio, although it depends on the
ionization parameter. We propose various mid- and far-IR line ratios to break
this dependency. Finally, we apply these far-IR diagnostics to a sample of 19
local ultraluminous IR galaxies (ULIRGs) observed with Herschel and Spitzer. We
find that the gas-phase metallicity in these local ULIRGs is in the range 0.7 <
Z_gas/Z_sun < 1.5, which corresponds to 8.5 < 12 + log (O/H) < 8.9. The
inferred metallicities agree well with previous estimates for local ULIRGs and
this confirms that they lie below the local mass-metallicity relation.
"
"  We show that quantum communication by means of collapse of the wave function
is possible. In this study, quantum communication does not mean quantum
teleportation or quantum cryptography, but transmission of information itself.
Because of consistency with special relativity, the possibility of the quantum
communication leads to another conclusion that the collapse of the wave
function must propagate at the speed of light or slower.
We show this requirement is consistent with nonlocality in quantum mechanics.
We also demonstrate that the Einstein-Podolsky-Rosen experiment does not
disprove our conclusion.
"
"  The effects of including the Hubbard on-site Coulombic correction to the
structural parameters and valence energy states of wurtzite ZnO was explored.
Due to the changes in the structural parameters caused by correction of
hybridization between Zn d states and O p states, suitable parameters of
Hubbard terms have to be determined for an accurate prediction of ZnO
properties. Using the LDA+${U}$ method by applying Hubbard corrections $U_p$ to
Zn 3d states and $U_p$ to O 2p states, the lattice constants were
underestimated for all tested Hubbard parameters. The combination of both $U_d$
and $U_p$ correction terms managed to widen the band gap of wurtzite ZnO to the
experimental value. Pairs of $U_p$ and $U_p$ parameters with the correct
positioning of d-band and accurate bandwidths were selected, in addition to
predicting an accurate band gap value. Inspection of vibrational properties,
however, revealed mismatches between the estimated gamma phonon frequencies and
experimental values. The selection of Hubbard terms based on electronic band
properties alone cannot ensure an accurate vibrational description in LDA+${U}$
calculation.
"
"  We present a model for the evolution of supermassive protostars from their
formation at $M_\star \simeq 0.1\,\text{M}_\odot$ until their growth to
$M_\star \simeq 10^5\,\text{M}_\odot$. To calculate the initial properties of
the object in the optically thick regime we follow two approaches: based on
idealized thermodynamic considerations, and on a more detailed one-zone model.
Both methods derive a similar value of $n_{\rm F} \simeq 2 \times 10^{17}
\,\text{cm}^{-3}$ for the density of the object when opacity becomes important,
i.e. the opacity limit. The subsequent evolution of the growing protostar is
determined by the accretion of gas onto the object and can be described by a
mass-radius relation of the form $R_\star \propto M_\star^{1/3}$ during the
early stages, and of the form $R_\star \propto M_\star^{1/2}$ when internal
luminosity becomes important. For the case of a supermassive protostar, this
implies that the radius of the star grows from $R_\star \simeq 0.65 \,{\rm AU}$
to $R_\star \simeq 250 \,{\rm AU}$ during its evolution. Finally, we use this
model to construct a sub-grid recipe for accreting sink particles in numerical
simulations. A prime ingredient thereof is a physically motivated prescription
for the accretion radius and the effective temperature of the growing protostar
embedded inside it. From the latter, we can conclude that photo-ionization
feedback can be neglected until very late in the assembly process of the
supermassive object.
"
"  Combining material informatics and high-throughput electronic structure
calculations offers the possibility of a rapid characterization of complex
magnetic materials. Here we demonstrate that datasets of electronic properties
calculated at the ab initio level can be effectively used to identify and
understand physical trends in magnetic materials, thus opening new avenues for
accelerated materials discovery. Following a data-centric approach, we utilize
a database of Heusler alloys calculated at the density functional theory level
to identify the ideal ions neighbouring Fe in the $X_2$Fe$Z$ Heusler prototype.
The hybridization of Fe with the nearest neighbour $X$ ion is found to cause
redistribution of the on-site Fe charge and a net increase of its magnetic
moment proportional to the valence of $X$. Thus, late transition metals are
ideal Fe neighbours for producing high-moment Fe-based Heusler magnets. At the
same time a thermodynamic stability analysis is found to restrict $Z$ to main
group elements. Machine learning regressors, trained to predict magnetic moment
and volume of Heusler alloys, are used to determine the magnetization for all
materials belonging to the proposed prototype. We find that Co$_2$Fe$Z$ alloys,
and in particular Co$_2$FeSi, maximize the magnetization, which reaches values
up to 1.2T. This is in good agreement with both ab initio and experimental
data. Furthermore, we identify the Cu$_2$Fe$Z$ family to be a cost-effective
materials class, offering a magnetization of approximately 0.65T.
"
"  We study the asymptotic behaviour of the solutions of the fifth Painlevé
equation as the independent variable approaches zero and infinity in the space
of initial values. We show that the limit set of each solution is compact and
connected and, moreover, that any solution with the essential singularity at
zero has an infinite number of poles and zeroes, and any solution with the
essential singularity at infinity has infinite number of poles and takes value
$1$ infinitely many times.
"
"  We suggested an algorithm for searching the recursion operators for nonlinear
integrable equations. It was observed that the recursion operator $R$ can be
represented as a ratio of the form $R=L_1^{-1}L_2$ where the linear
differential operators $L_1$ and $L_2$ are chosen in such a way that the
ordinary differential equation $(L_2-\lambda L_1)U=0$ is consistent with the
linearization of the given nonlinear integrable equation for any value of the
parameter $\lambda\in \textbf{C}$. For constructing the operator $L_1$ we use
the concept of the invariant manifold which is a generalization of the
symmetry. Then for searching $L_2$ we take an auxiliary linear equation
connected with the linearized equation by the Darboux transformation.
Connection of the invariant manifold with the Lax pairs and the
Dubrovin-Weierstrass equations is discussed.
"
"  A second derivative-based moment method is proposed for describing the
thickness and shape of the region where viscous forces are dominant in
turbulent boundary layer flows. Rather than the fixed location sublayer model
presently employed, the new method defines thickness and shape parameters that
are experimentally accessible without differentiation. It is shown
theoretically that one of the new length parameters used as a scaling parameter
is also a similarity parameter for the velocity profile. In fact, we show that
this new length scale parameter removes one of the theoretical inconsistencies
present in the traditional Prandtl Plus scalings. Furthermore, the new length
parameter and the Prandtl Plus scaling parameters perform identically when
operating on experimental datasets. This means that many of the past successes
ascribed to the Prandtl Plus scaling also apply to the new parameter set but
without one of the theoretical inconsistencies. Examples are offered to show
how the new description method is useful in exploring the actual physics of the
boundary layer.
"
"  Globular clusters (GCs) are amongst the oldest objects in the Galaxy and play
a pivotal role in deciphering its early history. We present the first
spectroscopic study of the GC ESO452-SC11 using the AAOmega spectrograph at
medium resolution. Given the sparsity of this object and high degree of
foreground contamination due to its location toward the bulge, few details are
known for this cluster: there is no consensus of its age, metallicity, or its
association with the disk or bulge. We identify 5 members based on radial
velocity, metallicity, and position within the GC. Using spectral synthesis,
accurate abundances of Fe and several $\alpha$-, Fe-peak, neutron-capture
elements (Si,Ca,Ti,Cr,Co,Ni,Sr,Eu) were measured. Two of the 5 cluster
candidates are likely non-members, as they have deviant Fe abundances and
[$\alpha$/Fe] ratios. The mean radial velocity is 19$\pm$2 km s$^{-1}$ with a
low dispersion of 2.8$\pm$3.4 km s$^{-1}$, in line with its low mass. The mean
Fe-abundance from spectral fitting is $-0.88\pm0.03$, with a spread driven by
observational errors. The $\alpha$-elements of the GC candidates are marginally
lower than expected for the bulge at similar metallicities. As spectra of
hundreds of stars were collected in a 2 degree field around ESO452-SC11,
detailed abundances in the surrounding field were measured. Most non-members
have higher [$\alpha$/Fe] ratios, typical of the nearby bulge population. Stars
with measured Fe-peak abundances show a large scatter around Solar values,
though with large uncertainties. Our study provides the first systematic
measurement of Sr in a Galactic bulge GC. The Eu and Sr abundances of the GC
candidates are consistent with a disk or bulge association. Our calculations
place ESO452 on an elliptical orbit in the central 3 kpc of the bulge. We find
no evidence of extratidal stars in our data. (Abridged)
"
"  We study the changes of opinions about vaccination together with the
evolution of a disease. In our model we consider a multiplex network consisting
of two layers. One of the layers corresponds to a social network where people
share their opinions and influence others opinions. The social model that rules
the dynamic is the M-model, which takes into account two different processes
that occurs in a society: persuasion and compromise. This two processes are
related through a parameter $r$, $r<1$ describes a moderate and committed
society, for $r>1$ the society tends to have extremist opinions, while $r=1$
represents a neutral society. This social network may be of real or virtual
contacts. On the other hand, the second layer corresponds to a network of
physical contacts where the disease spreading is described by the SIR-Model. In
this model the individuals may be in one of the following four states:
Susceptible ($S$), Infected($I$), Recovered ($R$) or Vaccinated ($V$). A
Susceptible individual can: i) get vaccinated, if his opinion in the other
layer is totally in favor of the vaccine, ii) get infected, with probability
$\beta$ if he is in contact with an infected neighbor. Those $I$ individuals
recover after a certain period $t_r=6$. Vaccinated individuals have an
extremist positive opinion that does not change. We consider that the vaccine
has a certain effectiveness $\omega$ and as a consequence vaccinated nodes can
be infected with probability $\beta (1 - \omega)$ if they are in contact with
an infected neighbor. In this case, if the infection process is successful, the
new infected individual changes his opinion from extremist positive to totally
against the vaccine. We find that depending on the trend in the opinion of the
society, which depends on $r$, different behaviors in the spread of the
epidemic occurs. An epidemic threshold was found.
"
"  We study production of primordial black holes (PBHs) during an early
matter-dominated phase. As a source of perturbations, we consider either the
inflaton field with a running spectral index or a spectator field that has a
blue spectrum and thus provides a significant contribution to the PBH
production at small scales. First, we identify the region of the parameter
space where a significant fraction of the observed dark matter can be produced,
taking into account all current PBH constraints. Then, we present constraints
on the amplitude and spectral index of the spectator field as a function of the
reheating temperature. We also derive constraints on the running of the
inflaton spectral index, ${\rm d}n/{\rm d}{\rm ln}k \lesssim -0.002$, which are
comparable to those from the Planck satellite for a scenario where the
spectator field is absent.
"
"  Fast carrier cooling is important for high power graphene based devices.
Strongly Coupled Optical Phonons (SCOPs) play a major role in the relaxation of
photoexcited carriers in graphene. Heterostructures of graphene and hexagonal
boron nitride (hBN) have shown exceptional mobility and high saturation
current, which makes them ideal for applications, but the effect of the hBN
substrate on carrier cooling mechanisms is not understood. We track the cooling
of hot photo-excited carriers in graphene-hBN heterostructures using ultrafast
pump-probe spectroscopy. We find that the carriers cool down four times faster
in the case of graphene on hBN than on a silicon oxide substrate thus
overcoming the hot phonon (HP) bottleneck that plagues cooling in graphene
devices.
"
"  The beams at the ILC produce electron positron pairs due to beam-beam
interactions. This note presents for the first time a study of these processes
in a detailed simulation, which shows that these pair background particles
appear at angles that extend to the inner layers of the detector. The full data
set of pairs produced in one bunch crossing was used to calculate the helix
tracks, which the particles form in the solenoid field of the SiD detector. The
results suggest to further study the reduction of the beam pipe radius and
therefore to either add another SiD vertex detector layer, or reduce the radius
of the existing vertex detector layers, without increasing the detector
occupancy significantly. This has to go along with additional studies whether
the improvement in physics reconstruction methods, like c-tagging, is worth the
increased background level at smaller radii.
"
"  This note is a short summary of the workshop on ""Energy and time measurements
with high-granular silicon devices"" that took place on the 13/6/16 and the
14/6/16 at DESY/Hamburg in the frame of the first AIDA-2020 Annual Meeting.
This note tries to put forward trends that could be spotted and to emphasise in
particular open issues that were addressed by the speakers.
"
"  We analyze the relation between the emission radii of twin kilohertz
quasi-periodic oscillations (kHz QPOs) and the co-rotation radii of the 12
neutron star low mass X-ray binaries (NS-LMXBs) which are simultaneously
detected with the twin kHz QPOs and NS spins. We find that the average
co-rotation radius of these sources is r_co about 32 km, and all the emission
positions of twin kHz QPOs lie inside the corotation radii, indicating that the
twin kHz QPOs are formed in the spin-up process. It is noticed that the upper
frequency of twin kHz QPOs is higher than NS spin frequency by > 10%, which may
account for a critical velocity difference between the Keplerian motion of
accretion matter and NS spin that is corresponding to the production of twin
kHz QPOs. In addition, we also find that about 83% of twin kHz QPOs cluster
around the radius range of 15-20 km, which may be affected by the hard surface
or the local strong magnetic field of NS. As a special case, SAX J1808.4-3658
shows the larger emission radii of twin kHz QPOs of r about 21-24 km, which may
be due to its low accretion rate or small measured NS mass (< 1.4 solar mass).
"
"  The transverse momentum ($p_T$) spectra from heavy-ion collisions at
intermediate momenta are described by non-extensive statistical models.
Assuming a fixed relative variance of the temperature fluctuating event by
event or alternatively a fixed mean multiplicity in a negative binomial
distribution (NBD), two different linear relations emerge between the
temperature, $T$, and the Tsallis parameter $q-1$. Our results qualitatively
agree with that of G.~Wilk. Furthermore we revisit the ""Soft+Hard"" model,
proposed recently by G.~G.~Barnaföldi \textit{et.al.}, by a $T$-independent
average $p_T^2$ assumption. Finally we compare results with those predicted by
another deformed distribution, using Kaniadakis' $\kappa$ parametrization.
"
"  Properties of galaxies like their absolute magnitude and their stellar mass
content are correlated. These correlations are tighter for close pairs of
galaxies, which is called galactic conformity. In hierarchical structure
formation scenarios, galaxies form within dark matter halos. To explain the
amplitude and the spatial range of galactic conformity two--halo terms or
assembly bias become important. With the scale dependent correlation
coefficients the amplitude and the spatial range of conformity are determined
from galaxy and halo samples. The scale dependent correlation coefficients are
introduced as a new descriptive statistic to quantify the correlations between
properties of galaxies or halos, depending on the distances to other galaxies
or halos. These scale dependent correlation coefficients can be applied to the
galaxy distribution directly. Neither a splitting of the sample into
subsamples, nor an a priori clustering is needed. This new descriptive
statistic is applied to galaxy catalogues derived from the Sloan Digital Sky
Survey III and to halo catalogues from the MultiDark simulations. In the galaxy
sample the correlations between absolute Magnitude, velocity dispersion,
ellipticity, and stellar mass content are investigated. The correlations of
mass, spin, and ellipticity are explored in the halo samples. Both for galaxies
and halos a scale dependent conformity is confirmed. Moreover the scale
dependent correlation coefficients reveal a signal of conformity out to 40Mpc
and beyond. The halo and galaxy samples show a differing amplitude and range of
conformity.
"
"  More than 23 million people are suffered by Heart failure worldwide. Despite
the modern transplant operation is well established, the lack of heart
donations becomes a big restriction on transplantation frequency. With respect
to this matter, ventricular assist devices (VADs) can play an important role in
supporting patients during waiting period and after the surgery. Moreover, it
has been shown that VADs by means of blood pump have advantages for working
under different conditions. While a lot of work has been done on modeling the
functionality of the blood pump, but quantifying uncertainties in a numerical
model is a challenging task. We consider the Polynomial Chaos (PC) method,
which is introduced by Wiener for modeling stochastic process with Gaussian
distribution. The Galerkin projection, the intrusive version of the generalized
Polynomial Chaos (gPC), has been densely studied and applied for various
problems. The intrusive Galerkin approach could represent stochastic process
directly at once with Polynomial Chaos series expansions, it would therefore
optimize the total computing effort comparing with classical non-intrusive
methods. We compared different preconditioning techniques for a steady state
simulation of a blood pump configuration in our previous work, the comparison
shows that an inexact multilevel preconditioner has a promising performance. In
this work, we show an instationary blood flow through a FDA blood pump
configuration with Galerkin Projection method, which is implemented in our open
source Finite Element library Hiflow3. Three uncertainty sources are
considered: inflow boundary condition, rotor angular speed and dynamic
viscosity, the numerical results are demonstrated with more than 30 Million
degrees of freedom by using supercomputer.
"
"  The combination of strong correlation and emergent lattice can be achieved
when quantum gases are confined in a superradiant Fabry-Perot cavity. In
addition to the discoveries of exotic phases, such as density wave ordered Mott
insulator and superfluid, a surprising kink structure is found in the slope of
the cavity strength as a function of the pumping strength. In this Letter, we
show that the appearance of such a kink is a manifestation of a liquid-gas like
transition between two superfluids with different densities. The slopes in the
immediate neighborhood of the kink become divergent at the liquid-gas critical
points and display a critical scaling law with a critical exponent 1 in the
quantum critical region. Our predictions could be tested in current
experimental set-up.
"
"  In the Alvarez-Macovski method, the line integrals of the x-ray basis set
coefficients are computed from measurements with multiple spectra. An important
question is whether the transformation from measurements to line integrals is
invertible. This paper presents a proof that for a system with two spectra and
a photon counting detector, pileup does not affect the invertibility of the
system. If the system is invertible with no pileup, it will remain invertible
with pileup although the reduced Jacobian may lead to increased noise.
"
"  Lattice Quantum Chromodynamics (Lattice QCD) is a quantum field theory on a
finite discretized space-time box so as to numerically compute the dynamics of
quarks and gluons to explore the nature of subatomic world. Solving the
equation of motion of quarks (quark solver) is the most compute-intensive part
of the lattice QCD simulations and is one of the legacy HPC applications. We
have developed a mixed-precision quark solver for a large Intel Xeon Phi (KNL)
system named ""Oakforest-PACS"", employing the $O(a)$-improved Wilson quarks as
the discretized equation of motion. The nested-BiCGSTab algorithm for the
solver was implemented and optimized using mixed-precision,
communication-computation overlapping with MPI-offloading, SIMD vectorization,
and thread stealing techniques. The solver achieved 2.6 PFLOPS in the
single-precision part on a $400^3\times 800$ lattice using 16000 MPI processes
on 8000 nodes on the system.
"
"  A new MHD solver, based on the Nektar++ spectral/hp element framework, is
presented in this paper. The velocity and electric potential quasi-static MHD
model is used. The Hartmann flow in plane channel and its stability, the
Hartmann flow in rectangular duct, and the stability of Hunt's flow are
explored as examples. Exponential convergence is achieved and the resulting
numerical values were found to have an accuracy up to $10^{-12}$ for the state
flows compared to an exact solution, and $10^{-5}$ for the stability
eigenvalues compared to independent numerical results.
"
"  We report magnetic and calorimetric measurements down to T = 1 mK on the
canonical heavy-electron metal YbRh2Si2. The data reveal the development of
nuclear antiferromagnetic order slightly above 2 mK. The latter weakens the
primary electronic antiferromagnetism, thereby paving the way for
heavy-electron superconductivity below Tc = 2 mK. Our results demonstrate that
superconductivity driven by quantum criticality is a general phenomenon.
"
"  Exoplanets smaller than Neptune are numerous, but the nature of the planet
populations in the 1-4 Earth radii range remains a mystery. The complete Kepler
sample of Q1-Q17 exoplanet candidates shows a radius gap at ~ 2 Earth radii, as
reported by us in January 2017 in LPSC conference abstract #1576 (Zeng et al.
2017). A careful analysis of Kepler host stars spectroscopy by the CKS survey
allowed Fulton et al. (2017) in March 2017 to unambiguously show this radius
gap. The cause of this gap is still under discussion (Ginzburg et al. 2017;
Lehmer & Catling 2017; Owen & Wu 2017). Here we add to our original analysis
the dependence of the radius gap on host star type.
"
"  The projection factor p is the key quantity used in the Baade-Wesselink (BW)
method for distance determination; it converts radial velocities into pulsation
velocities. Several methods are used to determine p, such as geometrical and
hydrodynamical models or the inverse BW approach when the distance is known. We
analyze new HARPS-N spectra of delta Cep to measure its cycle-averaged
atmospheric velocity gradient in order to better constrain the projection
factor. We first apply the inverse BW method to derive p directly from
observations. The projection factor can be divided into three subconcepts: (1)
a geometrical effect (p0); (2) the velocity gradient within the atmosphere
(fgrad); and (3) the relative motion of the optical pulsating photosphere with
respect to the corresponding mass elements (fo-g). We then measure the fgrad
value of delta Cep for the first time. When the HARPS-N mean cross-correlated
line-profiles are fitted with a Gaussian profile, the projection factor is
pcc-g = 1.239 +/- 0.034(stat) +/- 0.023(syst). When we consider the different
amplitudes of the radial velocity curves that are associated with 17 selected
spectral lines, we measure projection factors ranging from 1.273 to 1.329. We
find a relation between fgrad and the line depth measured when the Cepheid is
at minimum radius. This relation is consistent with that obtained from our best
hydrodynamical model of delta Cep and with our projection factor decomposition.
Using the observational values of p and fgrad found for the 17 spectral lines,
we derive a semi-theoretical value of fo-g. We alternatively obtain fo-g =
0.975+/-0.002 or 1.006+/-0.002 assuming models using radiative transfer in
plane-parallel or spherically symmetric geometries, respectively. The new
HARPS-N observations of delta Cep are consistent with our decomposition of the
projection factor.
"
"  We analyze a proprietary dataset of trades by a single asset manager,
comparing their price impact with that of the trades of the rest of the market.
In the context of a linear propagator model we find no significant difference
between the two, suggesting that both the magnitude and time dependence of
impact are universal in anonymous, electronic markets. This result is important
as optimal execution policies often rely on propagators calibrated on anonymous
data. We also find evidence that in the wake of a trade the order flow of other
market participants first adds further copy-cat trades enhancing price impact
on very short time scales. The induced order flow then quickly inverts, thereby
contributing to impact decay.
"
"  ZrSe2 is a band semiconductor studied long time ago. It has interesting
electronic properties, and because its layers structure can be intercalated
with different atoms to change some of the physical properties. In this
investigation we found that Zr deficiencies alter the semiconducting behavior
and the compound can be turned into a superconductor. In this paper we report
our studies related to this discovery. The decreasing of the number of Zr atoms
in small proportion according to the formula ZrxSe2, where x is varied from
about 8.1 to 8.6 K, changing the semiconducting behavior to a superconductor
with transition temperatures ranging between 7.8 to 8.5 K, it depending of the
deficiencies. Outside of those ranges the compound behaves as semiconducting
with the properties already known. In our experiments we found that this new
superconductor has only a very small fraction of superconducting material
determined by magnetic measurements with applied magnetic field of 10 Oe. Our
conclusions is that superconductivity is filamentary. However, in one studied
sample the fraction was about 10.2 %, whereas in others is only about 1 % or
less. We determined the superconducting characteristics; the critical fields
that indicate a type two superonductor with Ginzburg-Landau ? parameter of the
order about 2.7. The synthesis procedure is quite normal fol- lowing the
conventional solid state reaction. In this paper are included, the electronic
characteristics, transition temperature, and evolution with temperature of the
critical fields.
"
"  A large class of modified theories of gravity used as models for dark energy
predict a propagation speed for gravitational waves which can differ from the
speed of light. This difference of propagations speeds for photons and
gravitons has an impact in the emission of gravitational waves by binary
systems. Thus, we revisit the usual quadrupolar emission of binary system for
an arbitrary propagation speed of gravitational waves and obtain the
corresponding period decay formula. We then use timing data from the
Hulse-Taylor binary pulsar and obtain that the speed of gravitational waves can
only differ from the speed of light at the percentage level. This bound places
tight constraints on dark energy models featuring an anomalous propagations
speed for the gravitational waves.
"
"  3D-Polarized Light Imaging (3D-PLI) reconstructs nerve fibers in histological
brain sections by measuring their birefringence. This study investigates
another effect caused by the optical anisotropy of brain tissue -
diattenuation. Based on numerical and experimental studies and a complete
analytical description of the optical system, the diattenuation was determined
to be below 4 % in rat brain tissue. It was demonstrated that the diattenuation
effect has negligible impact on the fiber orientations derived by 3D-PLI. The
diattenuation signal, however, was found to highlight different anatomical
structures that cannot be distinguished with current imaging techniques, which
makes Diattenuation Imaging a promising extension to 3D-PLI.
"
"  The pair density wave (PDW) superconducting state has been proposed to
explain the layer- decoupling effect observed in the compound
La$_{2-x}$Ba$_x$CuO$_4$ at $x=1/8$ (Phys. Rev. Lett. 99, 127003). In this state
the superconducting order parameter is spatially modulated, in contrast with
the usual superconducting (SC) state where the order parameter is uniform. In
this work, we study the properties of the amplitude (Higgs) modes in a
unidirectional PDW state. To this end we consider a phenomenological model of
PDW type states coupled to a Fermi surface of fermionic quasiparticles. In
contrast to conventional superconductors that have a single Higgs mode,
unidirectional PDW superconductors have two Higgs modes. While in the PDW state
the Fermi surface largely remains gapless, we find that the damping of the PDW
Higgs modes into fermionic quasiparticles requires exceeding an energy
threshold. We show that this suppression of damping in the PDW state is due to
kinematics. As a result, only one of the two Higgs modes is significantly
damped. In addition, motivated by the experimental phase diagram, we discuss
the mixing of Higgs modes in the coexistence regime of the PDW and uniform SC
states. These results should be observable directly in a Raman spectroscopy, in
momentum resolved electron energy loss spectroscopy, and in resonant inelastic
X-ray scattering, thus providing evidence of the PDW states.
"
"  Let $F(x)=(f_1(x), \dots, f_m(x))$ be such that $1, f_1, \dots, f_m$ are
linearly independent polynomials with real coefficients. Based on ideas of
Bachoc, DeCorte, Oliveira and Vallentin in combination with estimating certain
oscillatory integrals with polynomial phase we will show that the independence
ratio of the Cayley graph of $\mathbb{R}^m$ with respect to the portion of the
graph of $F$ defined by $a\leq \log |s| \leq T$ is at most $O(1/(T-a))$. We
conclude that if $I \subseteq \mathbb{R}^m$ has positive upper density, then
the difference set $I-I$ contains vectors of the form $F(s)$ for an unbounded
set of values $s \in \mathbb{R}$. It follows that the Borel chromatic number of
the Cayley graph of $\mathbb{R}^m$ with respect to the set $\{ \pm F(s): s \in
\mathbb{R} \}$ is infinite. Analogous results are also proven when $\mathbb{R}$
is replaced by the field of $p$-adic numbers $\mathbb{Q}_p$. At the end, we
will also the existence of real analytic functions $f_1, \dots, f_m$, for which
the analogous statements no longer hold.
"
"  We study the cyclicity in weighted $\ell^p(\mathbb{Z})$ spaces. For $p \geq
1$ and $\beta \geq 0$, let $\ell^p\_\beta(\mathbb{Z})$ be the space of
sequences $u=(u\_n)\_{n\in \mathbb{Z}}$ such that $(u\_n |n|^{\beta})\in
\ell^p(\mathbb{Z}) $. We obtain both necessary conditions and sufficient
conditions for $u$ to be cyclic in $\ell^p\_\beta(\mathbb{Z})$, in other words,
for $ \{(u\_{n+k})\_{n \in \mathbb{Z}},~ k \in \mathbb{Z} \}$ to span a dense
subspace of $\ell^p\_\beta(\mathbb{Z})$. The conditions are given in terms of
the Hausdorff dimension and the capacity of the zero set of the Fourier
transform of $u$.
"
"  We give a new axiomatization of the N-pseudospace, studied in [2]
(Tent(2014)) and [1] (Baudisch,Martin-Pizarro,Ziegler(2014)) based on the
zigzags introduced in [2]. We also present a more detailed account of the
characterization of forking given in [2].
"
"  In this paper we deal with a robust Stackelberg strategy for the
Navier--Stokes system. The scheme is based in considering a robust control
problem for the ""follower control"" and its associated disturbance function.
Afterwards, we consider the notion of Stackelberg optimization (which is
associated to the ""leader control"") in order to deduce a local null
controllability result for the Navier--Stokes system.
"
"  In this paper, we present some extensions of interpolation between the
arithmetic-geometric means inequality. Among other inequalities, it is shown
that if $A, B, X$ are $n\times n$ matrices, then \begin{align*}
\|AXB^*\|^2\leq\|f_1(A^*A)Xg_1(B^*B)\|\,\|f_2(A^*A)Xg_2(B^*B)\|, \end{align*}
where $f_1,f_2,g_1,g_2$ are non-negative continues functions such that
$f_1(t)f_2(t)=t$ and $g_1(t)g_2(t)=t\,\,(t\geq0)$. We also obtain the
inequality \begin{align*}
\left|\left|\left|AB^*\right|\right|\right|^2\nonumber&\leq
\left|\left|\left|p(A^*A)^{\frac{m}{p}}+
(1-p)(B^*B)^{\frac{s}{1-p}}\right|\right|\right|\,\left|\left|\left|(1-p)(A^*A)^{\frac{n}{1-p}}+
p(B^*B)^{\frac{t}{p}}\right|\right|\right|, \end{align*} in which $m,n,s,t$ are
real numbers such that $m+n=s+t=1$, $|||\cdot|||$ is an arbitrary unitarily
invariant norm and $p\in[0,1]$.
"
"  We prove Riemann hypothesis, Generalized Riemann hypothesis, and Ramanujan
$\tau$-Dirichlet series hypothesis. Method is to show the convexity of function
which has zeros critical strip the same as zeta function.
"
"  We introduce multi-colour partition algebras $P_{n,m}(\delta_0, ...,
\delta_{m-1})$, which are generalization of both bubble algebras and partition
algebras, then define the bubble algebra $T_{n,m}(\delta_0, ..., \delta_{m-1})$
as a sub-algebra of the algebra $P_{n,m}(\delta_0, ..., \delta_{m-1})$. We
present general techniques to determine the structure of the bubble algebra
over the complex field in the non-semisimple case.
"
"  Knot Floer homology is an invariant for knots discovered by the authors and,
independently, Jacob Rasmussen. The discovery of this invariant grew naturally
out of studying how a certain three-manifold invariant, Heegaard Floer
homology, changes as the three-manifold undergoes Dehn surgery along a knot.
Since its original definition, thanks to the contributions of many researchers,
knot Floer homology has emerged as a useful tool for studying knots in its own
right. We give here a few selected highlights of this theory, and then move on
to some new algebraic developments in the computation of knot Floer homology.
"
"  We prove new exact formulas for the generalized sum-of-divisors functions.
The formulas for $\sigma_{\alpha}(x)$ when $\alpha \in \mathbb{C}$ is fixed and
$x \geq 1$ involves a finite sum over all of the prime factors $n \leq x$ and
terms involving the $r$-order harmonic number sequences. The generalized
harmonic number sequences correspond to the partial sums of the Riemann zeta
function when $r > 1$ and are related to the generalized Bernoulli numbers when
$r \leq 0$ is integer-valued. A key part of our expansions of the Lambert
series generating functions for the generalized divisor functions is formed by
taking logarithmic derivatives of the cyclotomic polynomials, $\Phi_n(q)$,
which completely factorize the Lambert series terms $(1-q^n)^{-1}$ into
irreducible polynomials in $q$. We also consider applications of our new
results to asymptotic approximations for sums over these divisor functions and
to the forms of perfect numbers defined by the special case of the divisor
function, $\sigma(n)$, when $\alpha := 1$.
Keywords: divisor function; sum-of-divisors function; Lambert series; perfect
number.
MSC (2010): 30B50; 11N64; 11B83
"
"  This paper provides some explicit formulas related to addition theorems for
elliptic integrals $\int_0^x dt/R(t)$, where $R(t)$ is the square root from a
polynomial of degree 4. These integrals are related to complex elliptic genera
and are motivated by Euler's addition theorem for elliptic integrals of the
first kind.
"
"  The aim of this paper is to introduce the notion of fantastic deductive
systems on generalizations of fuzzy structures, and to emphasize their role in
the probability theory on these algebras. We give a characterization of
commutative pseudo-BE algebras and we generalize an axiom system consisting of
four identities to the case of commutative pseudo-BE algebras. We define the
fantastic deductive systems of pseudo-BE algebras and we investigate their
properties. It is proved that, if a pseudo-BE(A) algebra $A$ is commutative,
then all deductive systems of $A$ are fantastic. Moreover, we generalize the
notions of measures, state-measures and measure-morphisms to the case of
pseudo-BE algebras and we also prove that there is a one-to-one correspondence
between the set of all Bosbach states on a bounded pseudo-BE algebra and the
set of its state-measures. The notions of internal states and state-morphism
operators on pseudo-BCK algebras are extended to the case of pseudo-BE algebras
and we also prove that any type II state operator on a pseudo-BE algebra is a
state-morphism operator on it. The notions of pseudo-valuation and commutative
pseudo-valuation on pseudo-BE algebras are defined and investigated. For the
case of commutative pseudo-BE algebras we prove that the two kind of
pseudo-valuations coincide. Characterizations of pseudo-valuations and
commutative pseudo-valuations are given. We show that the kernel of a Bosbach
state (state-morphism, measure, type II state operator, pseudo-valuation) is a
fantastic deductive system.
"
"  The linear fractional map $ f(z) = \frac{az+ b}{cz + d} $ on the Riemann
sphere with complex coefficients $ ad-bc \neq 0 $ is called Möbius map. If $
f $ satisfies $ ad-bc=1 $ and $ -2<a+d<2 $, then $ f $ is called
$\textit{elliptic}$ Möbius map. Let $ \{ b_n \}_{n \in \mathbb{N}_0} $ be the
solution of the elliptic Möbius difference equation $ b_{n+1} = f(b_n) $ for
every $ n \in \mathbb{N}_0 $. Then the sequence $ \{ b_n \}_{n \in
\mathbb{N}_0} $ has no Hyers-Ulam stability.
"
"  Differential calculus on Euclidean spaces has many generalisations. In
particular, on a set $X$, a diffeological structure is given by maps from open
subsets of Euclidean spaces to $X$, a differential structure is given by maps
from $X$ to $\mathbb{R}$, and a Frölicher structure is given by maps from
$\mathbb{R}$ to $X$ as well as maps from $X$ to $\mathbb{R}$. We illustrate the
relations between these structures through examples.
"
"  Let $P$ be a finite $p$-group and $p$ be an odd prime. Let
$\mathcal{A}_p(P)_{\geq2}$ be a poset consisting of elementary abelian
subgroups of rank at least 2. If the derived subgroup $P'\cong C_p\times C_p$,
then the spheres occurring in $\mathcal{A}_p(P)_{\geq2}$ all have the same
dimension.
"
"  We study the attractors of a class of holomorphic systems with an
irrationally indifferent fixed point. We prove a trichotomy for the topology of
the attractor based on the arithmetic of the rotation number at the fixed
point. That is, the attractor is either a Jordan curve, a one-sided hairy
circle, or a Cantor bouquet. This has a number of remarkable corollaries on a
conjecture of M. Herman about the optimal arithmetic condition for the
existence of a critical point on the boundary of the Siegel disk, and a
conjecture of A. Douady on the topology of the boundary of Siegel disks.
Combined with earlier results on the topic, this completes the topological
description of the behaviors of typical orbits near such fixed points, when the
rotation number is of high type.
"
"  In this paper, we consider solving a class of convex optimization problem
which minimizes the sum of three convex functions $f(x)+g(x)+h(Bx)$, where
$f(x)$ is differentiable with a Lipschitz continuous gradient, $g(x)$ and
$h(x)$ have a closed-form expression of their proximity operators and $B$ is a
bounded linear operator. This type of optimization problem has wide application
in signal recovery and image processing. To make full use of the
differentiability function in the optimization problem, we take advantage of
two operator splitting methods: the forward-backward splitting method and the
three operator splitting method. In the iteration scheme derived from the two
operator splitting methods, we need to compute the proximity operator of $g+h
\circ B$ and $h \circ B$, respectively. Although these proximity operators do
not have a closed-form solution in general, they can be solved very
efficiently. We mainly employ two different approaches to solve these proximity
operators: one is dual and the other is primal-dual. Following this way, we
fortunately find that three existing iterative algorithms including Condat and
Vu algorithm, primal-dual fixed point (PDFP) algorithm and primal-dual three
operator (PD3O) algorithm are a special case of our proposed iterative
algorithms. Moreover, we discover a new kind of iterative algorithm to solve
the considered optimization problem, which is not covered by the existing ones.
Under mild conditions, we prove the convergence of the proposed iterative
algorithms. Numerical experiments applied on fused Lasso problem, constrained
total variation regularization in computed tomography (CT) image reconstruction
and low-rank total variation image super-resolution problem demonstrate the
effectiveness and efficiency of the proposed iterative algorithms.
"
"  Let $A= \Lambda \oplus C$ be a trivial extension algebra. The aim of this
paper is to establish formulas for the projective dimension and the injective
dimension for a certain class of $A$-modules which is expressed by using the
derived functors $- \otimes^{\mathbb{L}}_{\Lambda}C$ and
$\mathbb{R}\text{Hom}_{\Lambda}(C, -)$. Consequently, we obtain formulas for
the global dimension of $A$, which gives a modern expression of the classical
formula for the global dimension by Palmer-Roos and Löfwall that is written
in complicated classical derived functors.
The main application of the formulas is to give a necessary and sufficient
condition for $A$ to be an Iwanaga-Gorenstein algebra.
We also give a description of the kernel $\text{Ker} \varpi$ of the canonical
functor $\varpi: \mathsf{D}^{\mathrm{b}}(\text{mod} \Lambda) \to
\text{Sing}^{\mathbb{Z}} A$ in the case $\text{pd} C < \infty$.
"
"  We explore lattice structures on integer binary relations (i.e. binary
relations on the set $\{1, 2, \dots, n\}$ for a fixed integer $n$) and on
integer posets (i.e. partial orders on the set $\{1, 2, \dots, n\}$ for a fixed
integer $n$). We first observe that the weak order on the symmetric group
naturally extends to a lattice structure on all integer binary relations. We
then show that the subposet of this weak order induced by integer posets
defines as well a lattice. We finally study the subposets of this weak order
induced by specific families of integer posets corresponding to the elements,
the intervals, and the faces of the permutahedron, the associahedron, and some
recent generalizations of those.
"
"  Manifolds with infinite cylindrical ends have continuous spectrum of
increasing multiplicity as energy grows, and in general embedded resonances and
eigenvalues can accumulate at infinity. However, we prove that if geodesic
trapping is sufficiently mild, then such an accumulation is ruled out, and
moreover the cutoff resolvent is uniformly bounded at high energies. We obtain
as a corollary the existence of resonance free regions near the continuous
spectrum.
We also obtain improved estimates when the resolvent is cut off away from
part of the trapping, and along the way we prove some resolvent estimates for
repulsive potentials on the half line which may be of independent interest.
"
"  We give a construction of a real number that is normal to all integer bases
and continued fraction normal. The computation of the first n digits of its
continued fraction expansion performs in the order of n^4 mathematical
operations. The construction works by defining successive refinements of
appropriate subintervals to achieve, in the limit, simple normality to all
integer bases and continued fraction normality. The main diffculty is to
control the length of these subintervals. To achieve this we adapt and combine
known metric theorems on continued fractions and on expansions in integers
bases.
"
"  A strong interaction is known to exist between edge-colored graphs (which
encode PL pseudo-manifolds of arbitrary dimension) and random tensor models (as
a possible approach to the study of Quantum Gravity). The key tool is the {\it
G-degree} of the involved graphs, which drives the {\it $1/N$ expansion} in the
tensor models context. In the present paper - by making use of combinatorial
properties concerning Hamiltonian decompositions of the complete graph - we
prove that, in any even dimension $d\ge 4$, the G-degree of all bipartite
graphs, as well as of all (bipartite or non-bipartite) graphs representing
singular manifolds, is an integer multiple of $(d-1)!$. As a consequence, in
even dimension, the terms of the $1/N$ expansion corresponding to odd powers of
$1/N$ are null in the complex context, and do not involve colored graphs
representing singular manifolds in the real context.
In particular, in the 4-dimensional case, where the G-degree is shown to
depend only on the regular genera with respect to an arbitrary pair of
""associated"" cyclic permutations, several results are obtained, relating the
G-degree or the regular genus of 5-colored graphs and the Euler characteristic
of the associated PL 4-manifolds.
"
"  We study a class of flat bundles, of finite rank $N$, which arise naturally
from the Donaldson-Thomas theory of a Calabi-Yau threefold $X$ via the notion
of a variation of BPS structure. We prove that in a large $N$ limit their flat
sections converge to the solutions to certain infinite dimensional
Riemann-Hilbert problems recently found by Bridgeland. In particular this
implies an expression for the positive degree, genus $0$ Gopakumar-Vafa
contribution to the Gromov-Witten partition function of $X$ in terms of
solutions to confluent hypergeometric differential equations.
"
"  Smooth backfitting has proven to have a number of theoretical and practical
advantages in structured regression. Smooth backfitting projects the data down
onto the structured space of interest providing a direct link between data and
estimator. This paper introduces the ideas of smooth backfitting to survival
analysis in a proportional hazard model, where we assume an underlying
conditional hazard with multiplicative components. We develop asymptotic theory
for the estimator and we use the smooth backfitter in a practical application,
where we extend recent advances of in-sample forecasting methodology by
allowing more information to be incorporated, while still obeying the
structured requirements of in-sample forecasting.
"
"  Suppose that the inverse image of the zero vector by a continuous map
$f:{\mathbb R}^n\to{\mathbb R}^q$ has an isolated point $P$. There is a local
obstruction to removing this isolated zero by a small perturbation,
generalizing the notion of index for vector fields, the $q=n$ case. The
existence of a continuous map $g$ which approximates $f$ but is nonvanishing
near $P$ is equivalent to a topological property we call ""locally inessential,""
and for dimensions $n$, $q$ where $\pi_{n-1}(S^{q-1})$ is trivial, every
isolated zero is locally inessential. We consider the problem of constructing
such an approximation $g$, and show that there exists a continuous homotopy
from $f$ to $g$ through locally nonvanishing maps. If $f$ is a semialgebraic
map, then there exists such a homotopy which is also semialgebraic. For $q=2$
and $f$ real analytic with a locally inessential isolated zero, there exists a
Hölder continuous homotopy $F(x,t)$ which, for $(x,t)\ne(P,0)$, is real
analytic and nonvanishing. The existence of a smooth homotopy, given a smooth
map $f$, is stated as an open question.
"
"  We prove that certain coinduced actions for an inclusion of finitely
generated commensurated subgroups with relative one end are continuous cocycle
superrigid actions. We also show the necessity for the relative end assumption.
"
"  In this paper we consider a degenerate pseudoparabolic equation for the
wetting saturation of an unsaturated two-phase flow in porous media with
dynamic capillary pressure-saturation relationship where the relaxation
parameter depends on the saturation. Following the approach given in [12] the
existence of a weak solution is proved using Galerkin approximation and
regularization techniques. A priori estimates needed for passing to the limit
when the regularization parameter goes to zero are obtained by using
appropriate test-functions, motivated by the fact that considered PDE allows a
natural generalization of the classical Kullback entropy. Finally, a special
care was given in obtaining an estimate of the mixed derivative term by
combining the information from the capillary pressure with obtained a priori
estimates on the saturation.
"
"  We study the problem of matrix estimation and matrix completion under a
general framework. This framework includes several important models as special
cases such as the gaussian mixture model, mixed membership model, bi-clustering
model and dictionary learning. We consider the optimal convergence rates in a
minimax sense for estimation of the signal matrix under the Frobenius norm and
under the spectral norm. As a consequence of our general result we obtain
minimax optimal rates of convergence for various special models.
"
"  Let $G$ be a finite solvable or symmetric group and let $B$ be a $2$-block of
$G$. We construct a canonical correspondence between the irreducible characters
of height zero in $B$ and those in its Brauer first main correspondent. For
symmetric groups our bijection is compatible with restriction of characters.
"
"  We give a classification of semisimple and separable algebras in a
multi-fusion category over an arbitrary field in analogy to Wedderben-Artin
theorem in classical algebras. It turns out that, if the multi-fusion category
admits a semisimple Drinfeld center, the only obstruction to the separability
of a semisimple algebra arises from inseparable field extensions as in
classical algebras. Among others, we show that a division algebra is separable
if and only if it has a nonvanishing dimension.
"
"  We discuss various characterizations of synthetic upper Ricci bounds for
metric measure spaces in terms of heat flow, entropy and optimal transport. In
particular, we present a characterization in terms of semiconcavity of the
entropy along certain Wasserstein geodesics which is stable under convergence
of mm-spaces. And we prove that a related characterization is equivalent to an
asymptotic lower bound on the growth of the Wasseretein distance between heat
flows. For weighted Riemannian manifolds, the crucial result will be a precise
uniform two-sided bound for \begin{eqnarray*}\frac{d}{dt}\Big|_{t=0}W\big(\hat
P_t\delta_x,\hat P_t\delta_y\big)\end{eqnarray*} in terms of the mean value of
the Bakry-Emery Ricci tensor $\mathrm{Ric}+\mathrm{Hess}\, f$ along the
minimizing geodesic from $x$ to $y$ and an explicit correction term depending
on the bound for the curvature along this curve.
"
"  We introduce a condition on Garside groups that we call Dehornoy structure.
An iteration of such a structure leads to a left order on the group. We show
conditions for a Garside group to admit a Dehornoy structure, and we apply
these criteria to prove that the Artin groups of type A and I 2 (m), m $\ge$ 4,
have Dehornoy structures. We show that the left orders on the Artin groups of
type A obtained from their Dehornoy structures are the Dehornoy orders. In the
case of the Artin groups of type I 2 (m), m $\ge$ 4, we show that the left
orders derived from their Dehornoy structures coincide with the orders obtained
from embeddings of the groups into braid groups. 20F36
"
"  The main aim of this paper is to study the Lipschitz continuity of certain
$(K, K')$-quasiconformal mappings with respect to the distance ratio metric,
and the Lipschitz continuity of the solution of a quasilinear differential
equation with respect to the distance ratio metric.
"
"  We propose an estimation method for the conditional mode when the
conditioning variable is high-dimensional. In the proposed method, we first
estimate the conditional density by solving quantile regressions multiple
times. We then estimate the conditional mode by finding the maximum of the
estimated conditional density. The proposed method has two advantages in that
it is computationally stable because it has no initial parameter dependencies,
and it is statistically efficient with a fast convergence rate. Synthetic and
real-world data experiments demonstrate the better performance of the proposed
method compared to other existing ones.
"
"  The mean field variational Bayes method is becoming increasingly popular in
statistics and machine learning. Its iterative Coordinate Ascent Variational
Inference algorithm has been widely applied to large scale Bayesian inference.
See Blei et al. (2017) for a recent comprehensive review. Despite the
popularity of the mean field method there exist remarkably little fundamental
theoretical justifications. To the best of our knowledge, the iterative
algorithm has never been investigated for any high dimensional and complex
model. In this paper, we study the mean field method for community detection
under the Stochastic Block Model. For an iterative Batch Coordinate Ascent
Variational Inference algorithm, we show that it has a linear convergence rate
and converges to the minimax rate within $\log n$ iterations. This complements
the results of Bickel et al. (2013) which studied the global minimum of the
mean field variational Bayes and obtained asymptotic normal estimation of
global model parameters. In addition, we obtain similar optimality results for
Gibbs sampling and an iterative procedure to calculate maximum likelihood
estimation, which can be of independent interest.
"
"  We propose to estimate a metamodel and the sensitivity indices of a complex
model m in the Gaussian regression framework. Our approach combines methods for
sensitivity analysis of complex models and statistical tools for sparse
non-parametric estimation in multivariate Gaussian regression model. It rests
on the construction of a metamodel for aproximating the Hoeffding-Sobol
decomposition of m. This metamodel belongs to a reproducing kernel Hilbert
space constructed as a direct sum of Hilbert spaces leading to a functional
ANOVA decomposition. The estimation of the metamodel is carried out via a
penalized least-squares minimization allowing to select the subsets of
variables that contribute to predict the output. It allows to estimate the
sensitivity indices of m. We establish an oracle-type inequality for the risk
of the estimator, describe the procedure for estimating the metamodel and the
sensitivity indices, and assess the performances of the procedure via a
simulation study.
"
"  In this article, the notion of bi-monotonic independence is introduced as an
extension of monotonic independence to the two-faced framework for a family of
pairs of algebras in a non-commutative space. The associated cumulants are
defined and a moment-cumulant formula is derived in the bi-monotonic setting.
In general the bi-monotonic product of states is not a state and the
bi-monotonic convolution of probability measures on the plane is not a
probability measure. This provides an additional example of how positivity need
not be preserved under conditional bi-free convolutions.
"
"  Topological cyclic homology is a refinement of Connes--Tsygan's cyclic
homology which was introduced by Bökstedt--Hsiang--Madsen in 1993 as an
approximation to algebraic $K$-theory. There is a trace map from algebraic
$K$-theory to topological cyclic homology, and a theorem of
Dundas--Goodwillie--McCarthy asserts that this induces an equivalence of
relative theories for nilpotent immersions, which gives a way for computing
$K$-theory in various situations. The construction of topological cyclic
homology is based on genuine equivariant homotopy theory, the use of explicit
point-set models, and the elaborate notion of a cyclotomic spectrum.
The goal of this paper is to revisit this theory using only
homotopy-invariant notions. In particular, we give a new construction of
topological cyclic homology. This is based on a new definition of the
$\infty$-category of cyclotomic spectra: We define a cyclotomic spectrum to be
a spectrum $X$ with $S^1$-action (in the most naive sense) together with
$S^1$-equivariant maps $\varphi_p: X\to X^{tC_p}$ for all primes $p$. Here
$X^{tC_p}=\mathrm{cofib}(\mathrm{Nm}: X_{hC_p}\to X^{hC_p})$ is the Tate
construction. On bounded below spectra, we prove that this agrees with previous
definitions. As a consequence, we obtain a new and simple formula for
topological cyclic homology.
In order to construct the maps $\varphi_p: X\to X^{tC_p}$ in the example of
topological Hochschild homology we introduce and study Tate diagonals for
spectra and Frobenius homomorphisms of commutative ring spectra. In particular
we prove a version of the Segal conjecture for the Tate diagonals and relate
these Frobenius homomorphisms to power operations.
"
"  In this paper we present results on dynamic multivariate scalar risk
measures, which arise in markets with transaction costs and systemic risk. Dual
representations of such risk measures are presented. These are then used to
obtain the main results of this paper on time consistency; namely, an
equivalent recursive formulation of multivariate scalar risk measures to
multiportfolio time consistency. We are motivated to study time consistency of
multivariate scalar risk measures as the superhedging risk measure in markets
with transaction costs (with a single eligible asset) (Jouini and Kallal
(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy
the usual scalar concept of time consistency. In fact, as demonstrated in
(Feinstein and Rudloff (2018)), scalar risk measures with the same
scalarization weight at all times would not be time consistent in general. The
deduced recursive relation for the scalarizations of multiportfolio time
consistent set-valued risk measures provided in this paper requires
consideration of the entire family of scalarizations. In this way we develop a
direct notion of a ""moving scalarization"" for scalar time consistency that
corroborates recent research on scalarizations of dynamic multi-objective
problems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2018)).
"
"  Associated varieties of vertex algebras are analogue of the associated
varieties of primitive ideals of the universal enveloping algebras of
semisimple Lie algebras. They not only capture some of the important properties
of vertex algebras but also have interesting relationship with the Higgs
branches of four-dimensional $N=2$ superconformal field theories (SCFTs). As a
consequence, one can deduce the modular invariance of Schur indices of 4d $N=2$
SCFTs from the theory of vertex algebras.
"
"  Due to the possible lack of primal-dual-type error bounds, the superlinear
convergence for the Karush-Kuhn-Tucker (KKT) residues of the sequence generated
by augmented Lagrangian method (ALM) for solving convex composite conic
programming (CCCP) has long been an outstanding open question. In this paper,
we aim to resolve this issue by first conducting convergence rate analysis for
the ALM with Rockafellar's stopping criteria under only a mild quadratic growth
condition on the dual of CCCP. More importantly, by further assuming that the
Robinson constraint qualification holds, we establish the R-superlinear
convergence of the KKT residues of the iterative sequence under
easy-to-implement stopping criteria {for} the augmented Lagrangian subproblems.
Equipped with this discovery, we gain insightful interpretations on the
impressive numerical performance of several recently developed semismooth
Newton-CG based ALM solvers for solving linear and convex quadratic
semidefinite programming.
"
"  We propose a nonlinear Discrete Duality Finite Volume scheme to approximate
the solutions of drift diffusion equations. The scheme is built to preserve at
the discrete level even on severely distorted meshes the energy / energy
dissipation relation. This relation is of paramount importance to capture the
long-time behavior of the problem in an accurate way. To enforce it, the linear
convection diffusion equation is rewritten in a nonlinear form before being
discretized. We establish the existence of positive solutions to the scheme.
Based on compactness arguments, the convergence of the approximate solution
towards a weak solution is established. Finally, we provide numerical evidences
of the good behavior of the scheme when the discretization parameters tend to 0
and when time goes to infinity.
"
"  We analyse a multilevel Monte Carlo method for the approximation of
distribution functions of univariate random variables. Since, by assumption,
the target distribution is not known explicitly, approximations have to be
used. We provide an asymptotic analysis of the error and the cost of the
algorithm. Furthermore we construct an adaptive version of the algorithm that
does not require any a priori knowledge on weak or strong convergence rates. We
apply the adaptive algorithm to smooth path-independent and path-dependent
functionals and to stopped exit times of SDEs.
"
"  This paper investigates the problem of detecting relevant change points in
the mean vector, say $\mu_t =(\mu_{1,t},\ldots ,\mu_{d,t})^T$ of a high
dimensional time series $(Z_t)_{t\in \mathbb{Z}}$.
While the recent literature on testing for change points in this context
considers hypotheses for the equality of the means $\mu_h^{(1)}$ and
$\mu_h^{(2)}$ before and after the change points in the different components,
we are interested in a null hypothesis of the form $$ H_0: |\mu^{(1)}_{h} -
\mu^{(2)}_{h} | \leq \Delta_h ~~~\mbox{ for all } ~~h=1,\ldots ,d $$ where
$\Delta_1, \ldots , \Delta_d$ are given thresholds for which a smaller
difference of the means in the $h$-th component is considered to be
non-relevant.
We propose a new test for this problem based on the maximum of squared and
integrated CUSUM statistics and investigate its properties as the sample size
$n$ and the dimension $d$ both converge to infinity. In particular, using
Gaussian approximations for the maximum of a large number of dependent random
variables, we show that on certain points of the boundary of the null
hypothesis a standardised version of the maximum converges weakly to a Gumbel
distribution.
"
"  We introduce a general methodology for post hoc inference in a large-scale
multiple testing framework. The approach is called ""user-agnostic"" in the sense
that the statistical guarantee on the number of correct rejections holds for
any set of candidate items selected by the user (after having seen the data).
This task is investigated by defining a suitable criterion, named the
joint-family-wise-error rate (JER for short). We propose several procedures for
controlling the JER, with a special focus on incorporating dependencies while
adapting to the unknown quantity of signal (via a step-down approach). We show
that our proposed setting incorporates as particular cases a version of the
higher criticism as well as the closed testing based approach of Goeman and
Solari (2011). Our theoretical statements are supported by numerical
experiments.
"
"  Here, we suggest a method to represent general directed uniform and
non-uniform hypergraphs by different connectivity tensors. We show many results
on spectral properties of undirected hypergraphs also hold for general directed
uniform hypergraphs. Our representation of a connectivity tensor will be very
useful for the further development in spectral theory of directed hypergraphs.
At the end, we have also introduced the concept of weak* irreducible
hypermatrix to better explain connectivity of a directed hypergraph.
"
"  We introduce uncertainty regions to perform inference on partial correlations
when data are missing not at random. These uncertainty regions are shown to
have a desired asymptotic coverage. Their finite sample performance is
illustrated via simulations and real data example.
"
"  The Sturm-Liouville operator with singular potentials on the lasso graph is
considered. We suppose that the potential is known a priori on the boundary
edge, and recover the potential on the loop from a part of the spectrum and
some additional data. We prove the uniqueness theorem and provide a
constructive algorithm for the solution of this partial inverse problem.
"
"  An infinitely smooth convex body in $\mathbb R^n$ is called polynomially
integrable of degree $N$ if its parallel section functions are polynomials of
degree $N$. We prove that the only smooth convex bodies with this property in
odd dimensions are ellipsoids, if $N\ge n-1$. This is in contrast with the case
of even dimensions and the case of odd dimensions with $N<n-1$, where such
bodies do not exist, as it was recently shown by Agranovsky.
"
"  In this series of papers, we develop the theory of a class of locally compact
quantum groupoids, which is motivated by the purely algebraic notion of weak
multiplier Hopf algebras. In this Part I, we provide motivation and formulate
the definition in the C*-algebra framework. Existence of a certain canonical
idempotent element is required and it plays a fundamental role, including the
establishment of the coassociativity of the comultiplication. This class
contains locally compact quantum groups as a subclass.
"
"  We define and study a numerical-range analogue of the notion of spectral set.
Among the results obtained are a positivity criterion and a dilation theorem,
analogous to those already known for spectral sets. An important difference
from the classical definition is the role played in the new definition by the
base point. We present some examples to illustrate this aspect.
"
"  The nonlinear Klein-Gordon (NLKG) equation on a manifold $M$ in the
nonrelativistic limit, namely as the speed of light $c$ tends to infinity, is
considered. In particular, a higher-order normalized approximation of NLKG
(which corresponds to the NLS at order $r=1$) is constructed, and when $M$ is a
smooth compact manifold or $\mathbb{R}^d$ it is proved that the solution of the
approximating equation approximates the solution of the NLKG locally uniformly
in time. When $M=\mathbb{R}^d$, $d \geq 3$, it is proved that solutions of the
linearized order $r$ normalized equation approximate solutions of linear
Klein-Gordon equation up to times of order $\mathcal{O}(c^{2(r-1)})$ for any
$r>1$.
"
"  We compute the Hochschild cohomology ring of the algebras $A= k\langle X,
Y\rangle/ (X^a, XY-qYX, Y^a)$ over a field $k$ where $a\geq 2$ and where $q\in
k$ is a primitive $a$-th root of unity. We find the the dimension of
$\mathrm{HH}^n(A)$ and show that it is independent of $a$. We compute
explicitly the ring structure of the even part of the Hochschild cohomology
modulo homogeneous nilpotent elements.
"
"  We provide complete source code for a front-end GUI and its back-end
counterpart for a stock market visualization tool. It is built based on the
""functional visualization"" concept we discuss, whereby functionality is not
sacrificed for fancy graphics. The GUI, among other things, displays a
color-coded signal (computed by the back-end code) based on how ""out-of-whack""
each stock is trading compared with its peers (""mean-reversion""), and the most
sizable changes in the signal (""momentum""). The GUI also allows to efficiently
filter/tier stocks by various parameters (e.g., sector, exchange, signal,
liquidity, market cap) and functionally display them. The tool can be run as a
web-based or local application.
"
"  The pioneering work of Brezis-Merle [7], Li-Shafrir [27], Li [26] and
Bartolucci-Tarantello [4] showed that any sequence of blow up solutions for
(singular) mean field equations of Liouville type must exhibit a ""mass
concentration"" property. A typical situation of blow-up occurs when we let the
singular (vortex) points involved in the equation (see (1.1) below) collapse
together. However in this case Lin-Tarantello in [30] pointed out that the
phenomenon: ""bubbling implies mass concentration"" might not occur and new
scenarios open for investigation. In this paper, we present two explicit
examples which illustrate (with mathematical rigor) how a ""non-concentration""
situation does happen and its new features. Among other facts, we show that in
certain situations, the collapsing rate of the singularities can be used as
blow up parameter to describe the bubbling properties of the solution-sequence.
In this way we are able to establish accurate estimates around the blow-up
points which we hope to use towards a degree counting formula for the shadow
system (1.34) below.
"
"  In this note we determine all possible dominations between different products
of manifolds, when none of the factors of the codomain is dominated by
products. As a consequence, we determine the finiteness of every
product-associated functorial semi-norm on the fundamental classes of the
aforementioned products. These results give partial answers to questions of M.
Gromov.
"
"  Consider a space X with the singular locus, Z=Sing(X), of positive dimension.
Suppose both Z and X are locally complete intersections. The transversal type
of X along Z is generically constant but at some points of Z it degenerates. We
introduce (under certain conditions) the discriminant of the transversal type,
a subscheme of Z, that reflects these degenerations whenever the generic
transversal type is `ordinary'.
The scheme structure of this discriminant is imposed by various compatibility
properties and is often non-reduced. We establish the basic properties of this
discriminant: it is a Cartier divisor in Z, functorial under base change, flat
under some deformations of (X,Z), and compatible with pullback under some
morphisms, etc.
Furthermore, we study the local geometry of this discriminant, e.g. we
compute its multiplicity at a point, and we obtain the resolution of its
structure sheaf (as module on Z) and study the locally defining equation.
"
"  Efficiency of the error control of numerical solutions of partial
differential equations entirely depends on the two factors: accuracy of an a
posteriori error majorant and the computational cost of its evaluation for some
test function/vector-function plus the cost of the latter. In the paper,
consistency of an a posteriori bound implies that it is the same in the order
with the respective unimprovable a priori bound. Therefore, it is the basic
characteristic related to the first factor. The paper is dedicated to the
elliptic diffusion-reaction equations. We present a guaranteed robust a
posteriori error majorant effective at any nonnegative constant reaction
coefficient (r.c.). For a wide range of finite element solutions on a
quasiuniform meshes the majorant is consistent. For big values of r.c. the
majorant coincides with the majorant of Aubin (1972), which, as it is known,
for not big r.c. ($<ch^{-2}$) is inconsistent and loses its sense at r.c.
approaching zero. Our majorant improves also some other majorants derived for
the Poisson and reaction-diffusion equations.
"
"  This paper finds near equilibrium prices for electricity markets with
nonconvexities due to binary variables, in order to reduce the market
participants' opportunity costs, such as generators' unrecovered costs. The
opportunity cost is defined as the difference between the profit when the
instructions of the market operator are followed and when the market
participants can freely make their own decisions based on the market prices. We
use the minimum complementarity approximation to the minimum total opportunity
cost (MTOC) model, from previous research, with tests on a much more realistic
unit commitment (UC) model than in previous research, including features such
as reserve requirements, ramping constraints, and minimum up and down times.
The developed model incorporates flexible price responsive demand, as in
previous research, but since not all demand is price responsive, we consider
the more realistic case that total demand is a mixture of fixed and flexible.
Another improvement over previous MTOC research is computational: whereas the
previous research had nonconvex terms among the objective function's continuous
variables, we convert the objective to an equivalent form that contains only
linear and convex quadratic terms in the continuous variables. We compare the
unit commitment model with the standard social welfare optimization version of
UC, in a series of sensitivity analyses, varying flexible demand to represent
varying degrees of future penetration of electric vehicles and smart
appliances, different ratios of generation availability, and different values
of transmission line capacities to consider possible congestion. The minimum
total opportunity cost and social welfare solutions are mostly very close in
different scenarios, except in some extreme cases.
"
"  The least square Monte Carlo (LSM) algorithm proposed by Longstaff and
Schwartz [2001] is the most widely used method for pricing options with early
exercise features. The LSM estimator contains look-ahead bias, and the
conventional technique of removing it necessitates an independent set of
simulations. This study proposes a new approach for efficiently eliminating
look-ahead bias by using the leave-one-out method, a well-known
cross-validation technique for machine learning applications. The leave-one-out
LSM (LOOLSM) method is illustrated with examples, including multi-asset options
whose LSM price is biased high. The asymptotic behavior of look-ahead bias is
also discussed with the LOOLSM approach.
"
"  We study a nonlocal Venttsel' problem in a non-convex bounded domain with a
Koch-type boundary. Regularity results of the strict solution are proved in
weighted Sobolev spaces. The numerical approximation of the problem is carried
out and optimal a priori error estimates are obtained.
"
"  Testing whether a probability distribution is compatible with a given
Bayesian network is a fundamental task in the field of causal inference, where
Bayesian networks model causal relations. Here we consider the class of causal
structures where all correlations between observed quantities are solely due to
the influence from latent variables. We show that each model of this type
imposes a certain signature on the observable covariance matrix in terms of a
particular decomposition into positive semidefinite components. This signature,
and thus the underlying hypothetical latent structure, can be tested in a
computationally efficient manner via semidefinite programming. This stands in
stark contrast with the algebraic geometric tools required if the full
observable probability distribution is taken into account. The semidefinite
test is compared with tests based on entropic inequalities.
"
"  The Peierls-Nabarro (PN) model for dislocations is a hybrid model that
incorporates the atomistic information of the dislocation core structure into
the continuum theory. In this paper, we study the convergence from a full
atomistic model to the PN model with $\gamma$-surface for the dislocation in a
bilayer system (e.g. bilayer graphene). We prove that the displacement field of
and the total energy of the dislocation solution of the PN model are
asymptotically close to those of the full atomistic model. Our work can be
considered as a generalization of the analysis of the convergence from
atomistic model to Cauchy-Born rule for crystals without defects in the
literature.
"
"  We consider partial torsion fields (fields generated by a root of a division
polynomial) for elliptic curves. By analysing the reduction properties of
elliptic curves, and applying the Montes Algorithm, we obtain information about
the ring of integers. In particular, for the partial $3$-torsion fields for a
certain one-parameter family of non-CM elliptic curves, we describe a power
basis. As a result, we show that the one-parameter family of quartic $S_4$
fields given by $T^4 - 6T^2 - \alpha T - 3$ for $\alpha \in \mathbb{Z}$ such
that $\alpha \pm 8$ are squarefree, are monogenic.
"
"  We prove an identity relating the product of two opposite Schubert varieties
in the (equivariant) quantum K-theory ring of a cominuscule flag variety to the
minimal degree of a rational curve connecting the Schubert varieties. We deduce
that the sum of the structure constants associated to any product of Schubert
classes is equal to one. Equivalently, the sheaf Euler characteristic map
extends to a ring homomorphism defined on the quantum K-theory ring.
"
"  We give a new proof of the strong Arnold conjecture for $1$-periodic
solutions of Hamiltonian systems on tori, that was first shown by C. Conley and
E. Zehnder in 1983. Our proof uses other methods and is shorter than the
previous one. We first show that the $E$-cohomological Conley index, that was
introduced by the first author recently, has a natural module structure. This
yields a new cup-length and a lower bound for the number of critical points of
functionals. Then an existence result for the $E$-cohomological Conley index,
which applies to the setting of the Arnold conjecture, paves the way to a new
proof of it on tori.
"
"  We introduce and study the inhomogeneous exponential jump model - an
integrable stochastic interacting particle system on the continuous half line
evolving in continuous time. An important feature of the system is the presence
of arbitrary spatial inhomogeneity on the half line which does not break the
integrability. We completely characterize the macroscopic limit shape and
asymptotic fluctuations of the height function (= integrated current) in the
model. In particular, we explain how the presence of inhomogeneity may lead to
macroscopic phase transitions in the limit shape such as shocks or traffic
jams. Away from these singularities the asymptotic fluctuations of the height
function around its macroscopic limit shape are governed by the GUE Tracy-Widom
distribution. A surprising result is that while the limit shape is
discontinuous at a traffic jam caused by a macroscopic slowdown in the
inhomogeneity, fluctuations on both sides of such a traffic jam still have the
GUE Tracy-Widom distribution (but with different non-universal normalizations).
The integrability of the model comes from the fact that it is a degeneration
of the inhomogeneous stochastic higher spin six vertex models studied earlier
in arXiv:1601.05770 [math.PR]. Our results on fluctuations are obtained via an
asymptotic analysis of Fredholm determinantal formulas arising from contour
integral expressions for the q-moments in the stochastic higher spin six vertex
model. We also discuss ""product-form"" translation invariant stationary
distributions of the exponential jump model which lead to an alternative
hydrodynamic-type heuristic derivation of the macroscopic limit shape.
"
"  A system modeling bacteriophage treatments with coinfections in a noisy
context is analyzed. We prove that in a small noise regime, the system
converges in the long term to a bacteria free equilibrium. Moreover, we compare
the treatment with coinfection with the treatment without coinfection, showing
how the coinfection affects the dose of bacteriophages that is needed to
eliminate the bacteria and the velocity of convergence to the free bacteria
equilibrium.
"
"  The virtual unknotting number of a virtual knot is the minimal number of
crossing changes that makes the virtual knot to be the unknot, which is defined
only for virtual knots virtually homotopic to the unknot. We focus on the
virtual knot obtained from the standard (p,q)-torus knot diagram by replacing
all crossings on one overstrand into virtual crossings and prove that its
virtual unknotting number is equal to the unknotting number of the
$(p,q)$-torus knot, i.e. it is (p-1)(q-1)/2.
"
"  We explore the Hunters and Rabbits game on the hypercube. In the process, we
find the solution for all classes of graphs with an isoperimetric nesting
property and find the exact hunter number of $Q^n$ to be
$1+\sum\limits_{i=0}^{n-2} \binom{i}{\lfloor i/2 \rfloor}$. In addition, we
extend results to the situation where we allow the rabbit to not move between
shots.
"
"  We consider a point cloud $X_n := \{ x_1, \dots, x_n \}$ uniformly
distributed on the flat torus $\mathbb{T}^d : = \mathbb{R}^d / \mathbb{Z}^d $,
and construct a geometric graph on the cloud by connecting points that are
within distance $\epsilon$ of each other. We let $\mathcal{P}(X_n)$ be the
space of probability measures on $X_n$ and endow it with a discrete Wasserstein
distance $W_n$ as introduced independently by Maas and Zhou et al. for general
finite Markov chains. We show that as long as $\epsilon= \epsilon_n$ decays
towards zero slower than an explicit rate depending on the level of uniformity
of $X_n$, then the space $(\mathcal{P}(X_n), W_n)$ converges in the
Gromov-Hausdorff sense towards the space of probability measures on
$\mathbb{T}^d$ endowed with the Wasserstein distance.
"
"  We modify the definable ultrapower construction of Kanovei and Shelah (2004)
to develop a ZF-definable extension of the continuum with transfer provable
using countable choice only, with an additional mild hypothesis on
well-ordering implying properness. Under the same assumptions, we also prove
the existence of a definable, proper elementary extension of the standard
superstructure over the reals.
Keywords: definability; hyperreal; superstructure; elementary embedding.
"
"  Kriging based on Gaussian random fields is widely used in reconstructing
unknown functions. The kriging method has pointwise predictive distributions
which are computationally simple. However, in many applications one would like
to predict for a range of untried points simultaneously. In this work we obtain
some error bounds for the (simple) kriging predictor under the uniform metric.
It works for a scattered set of input points in an arbitrary dimension, and
also covers the case where the covariance function of the Gaussian process is
misspecified. These results lead to a better understanding of the rate of
convergence of kriging under the Gaussian or the Matérn correlation
functions, the relationship between space-filling designs and kriging models,
and the robustness of the Matérn correlation functions.
"
"  We study the long-range, long-time behavior of the reactive-telegraph
equation and a related reactive-kinetic model. The two problems are equivalent
in one spatial dimension. We point out that the reactive-telegraph equation,
meant to model a population density, does not preserve positivity in higher
dimensions. In view of this, in dimensions larger than one, we consider a
reactive-kinetic model and investigate the long-range, long-time limit of the
solutions. We provide a general characterization of the speed of propagation
and we compute it explicitly in one and two dimensions. We show that a phase
transition between parabolic and hyperbolic behavior takes place only in one
dimension. Finally, we investigate the hydrodynamic limit of the limiting
problem.
"
"  We study abelian varieties and K3 surfaces with complex multiplication
defined over number fields of fixed degree. We show that these varieties fall
into finitely many isomorphism classes over an algebraic closure of the field
of rational numbers. As an application we confirm finiteness conjectures of
Shafarevich and Coleman in the CM case. In addition we prove the uniform
boundedness of the Galois invariant subgroup of the geometric Brauer group for
forms of a smooth projective variety satisfying the integral Mumford--Tate
conjecture. When applied to K3 surfaces, this affirms a conjecture of
Várilly-Alvarado in the CM case.
"
"  The present paper generalises the results of Ray and Buchstaber-Ray,
Buchstaber-Panov-Ray in unitary cobordism theory. I prove that any class $x\in
\Omega^{*}_{U}$ of the unitary cobordism ring contains a quasitoric totally
normally and tangentially split manifold.
"
"  In this article, we consider Cayley deformations of a compact complex surface
in a Calabi--Yau four-fold. We will study complex deformations of compact
complex submanifolds of Calabi--Yau manifolds with a view to explaining why
complex and Cayley deformations of a compact complex surface are the same. We
in fact prove that the moduli space of complex deformations of any compact
complex embedded submanifold of a Calabi--Yau manifold is a smooth manifold.
"
"  We study the smooth structure of convex functions by generalizing a powerful
concept so-called self-concordance introduced by Nesterov and Nemirovskii in
the early 1990s to a broader class of convex functions, which we call
generalized self-concordant functions. This notion allows us to develop a
unified framework for designing Newton-type methods to solve convex optimiza-
tion problems. The proposed theory provides a mathematical tool to analyze both
local and global convergence of Newton-type methods without imposing
unverifiable assumptions as long as the un- derlying functionals fall into our
generalized self-concordant function class. First, we introduce the class of
generalized self-concordant functions, which covers standard self-concordant
functions as a special case. Next, we establish several properties and key
estimates of this function class, which can be used to design numerical
methods. Then, we apply this theory to develop several Newton-type methods for
solving a class of smooth convex optimization problems involving the
generalized self- concordant functions. We provide an explicit step-size for
the damped-step Newton-type scheme which can guarantee a global convergence
without performing any globalization strategy. We also prove a local quadratic
convergence of this method and its full-step variant without requiring the
Lipschitz continuity of the objective Hessian. Then, we extend our result to
develop proximal Newton-type methods for a class of composite convex
minimization problems involving generalized self-concordant functions. We also
achieve both global and local convergence without additional assumption.
Finally, we verify our theoretical results via several numerical examples, and
compare them with existing methods.
"
"  In this paper, we study joint functional calculus for commuting $n$-tuple of
Ritt operators. We provide an equivalent characterisation of boundedness for
joint functional calculus for Ritt operators on $L^p$-spaces, $1< p<\infty$. We
also investigate joint similarity problem and joint bounded functional calculus
on non-commutative $L^p$-spaces for $n$-tuple of Ritt operators. We get our
results by proving a suitable multivariable transfer principle between
sectorial and Ritt operators as well as an appropriate joint dilation result in
a general setting.
"
"  It is well known, thanks to Lax-Wendroff theorem, that the local conservation
of a numerical scheme for a conservative hyperbolic system is a simple and
systematic way to guarantee that, if stable, a scheme will provide a sequence
of solutions that will converge to a weak solution of the continuous problem.
In [1], it is shown that a nonconservative scheme will not provide a good
solution. The question of using, nevertheless, a nonconservative formulation of
the system and getting the correct solution has been a long-standing debate. In
this paper, we show how get a relevant weak solution from a pressure-based
formulation of the Euler equations of fluid mechanics. This is useful when
dealing with nonlinear equations of state because it is easier to compute the
internal energy from the pressure than the opposite. This makes it possible to
get oscillation free solutions, contrarily to classical conservative methods.
An extension to multiphase flows is also discussed, as well as a
multidimensional extension.
"
"  This article is concerned with the asymptotic behavior of certain sequences
of ideals in rings of prime characteristic. These sequences, which we call
$p$-families of ideals, are ubiquitous in prime characteristic commutative
algebra (e.g., they occur naturally in the theories of tight closure,
Hilbert-Kunz multiplicity, and $F$-signature). We associate to each $p$-family
of ideals an object in Euclidean space that is analogous to the Newton-Okounkov
body of a graded family of ideals, which we call a $p$-body. Generalizing the
methods used to establish volume formulas for the Hilbert-Kunz multiplicity and
$F$-signature of semigroup rings, we relate the volume of a $p$-body to a
certain asymptotic invariant determined by the corresponding $p$-family of
ideals. We apply these methods to obtain new existence results for limits in
positive characteristic, an analogue of the Brunn-Minkowski theorem for
Hilbert-Kunz multiplicity, and a uniformity result concerning the positivity of
a $p$-family.
"
"  We show that 2-dimensional systolic complexes are quasi-isometric to quadric
complexes with flat intervals. We use this fact along with the weight function
of Brodzki, Campbell, Guentner, Niblo and Wright to prove that 2-dimensional
systolic complexes satisfy Property A.
"
"  We show that the Galois cohomology groups of $p$-adic representations of a
direct power of $\operatorname{Gal}(\overline{\mathbb{Q}_p}/\mathbb{Q}_p)$ can
be computed via the generalization of Herr's complex to multivariable
$(\varphi,\Gamma)$-modules. Using Tate duality and a pairing for multivariable
$(\varphi,\Gamma)$-modules we extend this to analogues of the Iwasawa
cohomology. We show that all $p$-adic representations of a direct power of
$\operatorname{Gal}(\overline{\mathbb{Q}_p}/\mathbb{Q}_p)$ are overconvergent
and, moreover, passing to overconvergent multivariable
$(\varphi,\Gamma)$-modules is an equivalence of categories. Finally, we prove
that the overconvergent Herr complex also computes the Galois cohomology
groups.
"
"  This paper is a continuation of \ct{cmf16} where an efficient algorithm for
computing the maximal eigenpair was introduced first for tridiagonal matrices
and then extended to the irreducible matrices with nonnegative off-diagonal
elements. This paper introduces two global algorithms for computing the maximal
eigenpair in a rather general setup, including even a class of real (with some
negative off-diagonal elements) or complex matrices.
"
"  We proposed a new penalized method in this paper to solve sparse Poisson
Regression problems. Being different from $\ell_1$ penalized log-likelihood
estimation, our new method can be viewed as penalized weighted score function
method. We show that under mild conditions, our estimator is $\ell_1$
consistent and the tuning parameter can be pre-specified, which shares the same
good property of the square-root Lasso.
"
"  We consider Jacobi matrices $J$ whose parameters have the power asymptotics
$\rho_n=n^{\beta_1} \left( x_0 + \frac{x_1}{n} + {\rm
O}(n^{-1-\epsilon})\right)$ and $q_n=n^{\beta_2} \left( y_0 + \frac{y_1}{n} +
{\rm O}(n^{-1-\epsilon})\right)$ for the off-diagonal and diagonal,
respectively. We show that for $\beta_1 > \beta_2$, or $\beta_1=\beta_2$ and
$2x_0 > |y_0|$, the matrix $J$ is in the limit circle case and the convergence
exponent of its spectrum is $1/\beta_1$. Moreover, we obtain upper and lower
bounds for the upper density of the spectrum. When the parameters of the matrix
$J$ have a power asymptotic with one more term, we characterise the occurrence
of the limit circle case completely (including the exceptional case $\lim_{n\to
\infty} |q_n|\big/ \rho_n = 2$) and determine the convergence exponent in
almost all cases.
"
"  Seidel introduced the notion of a Fukaya category `relative to an ample
divisor', explained that it is a deformation of the Fukaya category of the
affine variety that is the complement of the divisor, and showed how the
relevant deformation theory is controlled by the symplectic cohomology of the
complement. We elaborate on Seidel's definition of the relative Fukaya
category, and give a criterion under which the deformation is versal.
"
"  Given any polynomial $p$ in $C[X]$, we show that the set of irreducible
matrices satisfying $p(A)=0$ is finite. In the specific case $p(X)=X^2-nX$, we
count the number of irreducible matrices in this set and analyze the arising
sequences and their asymptotics. Such matrices turn out to be related to
generalized compositions and generalized partitions.
"
"  The paper solves the problem of optimal portfolio choice when the parameters
of the asset returns distribution, like the mean vector and the covariance
matrix are unknown and have to be estimated by using historical data of the
asset returns. The new approach employs the Bayesian posterior predictive
distribution which is the distribution of the future realization of the asset
returns given the observable sample. The parameters of the posterior predictive
distributions are functions of the observed data values and, consequently, the
solution of the optimization problem is expressed in terms of data only and
does not depend on unknown quantities. In contrast, the optimization problem of
the traditional approach is based on unknown quantities which are estimated in
the second step leading to a suboptimal solution. We also derive a very useful
stochastic representation of the posterior predictive distribution whose
application leads not only to the solution of the considered optimization
problem, but provides the posterior predictive distribution of the optimal
portfolio return used to construct a prediction interval. A Bayesian efficient
frontier, a set of optimal portfolios obtained by employing the posterior
predictive distribution, is constructed as well. Theoretically and using real
data we show that the Bayesian efficient frontier outperforms the sample
efficient frontier, a common estimator of the set of optimal portfolios known
to be overoptimistic.
"
"  We show that the Verdier quotients can be realized as subfactors by the
homotopy theory of additive categories with suspensions developed in
\cite{ZWLi2, ZWLi3}. As applications, we develop the homotopy theory of Nakaoka
twin cotorsion pairs of triangulated categories and prove that Iyama-Yoshino
triangulated subfactors are Verdier quotients under suitable conditions.
"
"  We prove that the Grothendieck rings of category $\mathcal{C}^{(t)}_Q$ over
quantum affine algebras $U_q'(\g^{(t)})$ $(t=1,2)$ associated to each Dynkin
quiver $Q$ of finite type $A_{2n-1}$ (resp. $D_{n+1}$) is isomorphic to one of
category $\mathcal{C}_{\mQ}$ over the Langlands dual $U_q'({^L}\g^{(2)})$ of
$U_q'(\g^{(2)})$ associated to any twisted adapted class $[\mQ]$ of $A_{2n-1}$
(resp. $D_{n+1}$). This results provide partial answers of conjectures of
Frenkel-Hernandez on Langlands duality for finite-dimensional representation of
quantum affine algebras.
"
"  The present paper is devoted to the description of finite-dimensional
semisimple Leibniz algebras over complex numbers, their derivations and
automorphisms.
"
"  In this paper, we consider the nonlinear inhomogeneous compressible elastic
waves in three spatial dimensions when the density is a small disturbance
around a constant state. In homogeneous case, the almost global existence was
established by Klainerman-Sideris [1996_CPAM], and global existence was built
by Agemi [2000_Invent. Math.] and Sideris [1996_Invent. Math., 2000_Ann. Math.]
independently. Here we establish the corresponding almost global and global
existence theory in the inhomogeneous case.
"
"  Emil Artin defined a zeta function for algebraic curves over finite fields
and made a conjecture about them analogous to the famous Riemann hypothesis.
This and other conjectures about these zeta functions would come to be called
the Weil conjectures, which were proved by Weil for curves and later, by
Deligne for varieties over finite fields. Much work was done in the search for
a proof of these conjectures, including the development in algebraic geometry
of a Weil cohomology theory for these varieties, which uses the Frobenius
operator on a finite field. The zeta function is then expressed as a
determinant, allowing the properties of the function to relate to those of the
operator. The search for a suitable cohomology theory and associated operator
to prove the Riemann hypothesis is still on. In this paper, we study the
properties of the derivative operator $D = \frac{d}{dz}$ on a particular
weighted Bergman space of entire functions. The operator $D$ can be naturally
viewed as the `infinitesimal shift of the complex plane'. Furthermore, this
operator is meant to be the replacement for the Frobenius operator in the
general case and is used to construct an operator associated to any suitable
meromorphic function. We then show that the meromorphic function can be
recovered by using a regularized determinant involving the above operator. This
is illustrated in some important special cases: rational functions, zeta
functions of curves over finite fields, the Riemann zeta function, and
culminating in a quantized version of the Hadamard factorization theorem that
applies to any entire function of finite order. Our construction is motivated
in part by [23] on the infinitesimal shift of the real line, as well as by
earlier work of Deninger [10] on cohomology in number theory and a conjectural
`fractal cohomology theory' envisioned in [25] and [28].
"
"  In this paper we deal with Seifert fibre spaces, which are compact
3-manifolds admitting a foliation by circles. We give a combinatorial
description for these manifolds in all the possible cases: orientable,
non-orientable, closed, with boundary. Moreover, we compute a potentially sharp
upper bound for their complexity in terms of the invariants of the
combinatorial description, extending to the non-orientable case results by
Fominykh and Wiest for the orientable case with boundary and by Martelli and
Petronio for the closed orientable case.
"
"  Instanton partition functions of $\mathcal{N}=1$ 5d Super Yang-Mills reduced
on $S^1$ can be engineered in type IIB string theory from the $(p,q)$-branes
web diagram. To this diagram is superimposed a web of representations of the
Ding-Iohara-Miki (DIM) algebra that acts on the partition function. In this
correspondence, each segment is associated to a representation, and the
(topological string) vertex is identified with the intertwiner operator
constructed by Awata, Feigin and Shiraishi. We define a new intertwiner acting
on the representation spaces of levels $(1,n)\otimes(0,m)\to(1,n+m)$, thereby
generalizing to higher rank $m$ the original construction. It allows us to use
a folded version of the usual $(p,q)$-web diagram, bringing great
simplifications to actual computations. As a result, the characterization of
Gaiotto states and vertical intertwiners, previously obtained by some of the
authors, is uplifted to operator relations acting in the Fock space of
horizontal representations. We further develop a method to build qq-characters
of linear quivers based on the horizontal action of DIM elements. While
fundamental qq-characters can be built using the coproduct, higher ones require
the introduction of a (quantum) Weyl reflection acting on tensor products of
DIM generators.
"
"  The computation of the Noether numbers of all groups of order less than
thirty-two is completed. It turns out that for these groups in non-modular
characteristic the Noether number is attained on a multiplicity free
representation, it is strictly monotone on subgroups and factor groups, and it
does not depend on the characteristic. Algorithms are developed and used to
determine the small and large Davenport constants of these groups. For each of
these groups the Noether number is greater than the small Davenport constant,
whereas the first example of a group whose Noether number exceeds the large
Davenport constant is found, answering partially a question posed by
Geroldinger and Grynkiewicz.
"
"  Given a real number $ \beta > 1$, we study the associated $ (-\beta)$-shift
introduced by S. Ito and T. Sadahiro. We compares some aspects of the
$(-\beta)$-shift to the $\beta$-shift. When the expansion in base $ -\beta $ of
$ -\frac{\beta}{\beta+1} $ is periodic with odd period or when $ \beta $ is
strictly less than the golden ratio, the $ (-\beta)$-shift, as defined by S.
Ito and T. Sadahiro cannot be coded because its language is not transitive.
This intransitivity of words explains the existence of gaps in the interval. We
observe that an intransitive word appears in the $(-\beta)$-expansion of a real
number taken in the gap. Furthermore, we determine the Zeta function
$\zeta_{-\beta}$ of the $(-\beta)$-transformation and the associated
lap-counting function $L_{T_{-\beta}}$. These two functions are related by $
\zeta_{-\beta}=(1-z^2)L_{T_{-\beta}}$. We observe some similarities with the
zeta function of the $\beta$-transformation. The function $\zeta_{-\beta}$ is
meromorphic in the unit disk, is holomorphic in the open disk $ \{z |z| <
\frac{1}{\beta} \}$, has a simple pole at $ \frac{1}{\beta}$ and no other
singularities $ z $ such that $\|z| = \frac{1}{\beta}$. We also note an
influence of gaps ($\beta$ less than the golden ratio) on the zeta function. In
factors of the denominator of $\zeta_{-\beta}$, the coefficients count the
words generating gaps.
"
"  In order to find a way of measuring the degree of incompleteness of an
incomplete financial market, the rank of the vector price process of the traded
assets and the dimension of the associated acceptance set are introduced. We
show that they are equal and state a variety of consequences.
"
"  We prove the existence of singular harmonic ${\bf Z}_2$ spinors on
$3$-manifolds with $b_1 > 1$. The proof relies on a wall-crossing formula for
solutions to the Seiberg-Witten equation with two spinors. The existence of
singular harmonic ${\bf Z}_2$ spinors and the shape of our wall-crossing
formula shed new light on recent observations made by Joyce regarding Donaldson
and Segal's proposal for counting $G_2$-instantons.
"
"  Matrix divisors are introduced in the work by A.Weil (1938) which is
considered as a starting point of the theory of holomorphic vector bundles on
Riemann surfaces. In this theory matrix divisors play the role similar to the
role of usual divisors in the theory of line bundles. Moreover, they provide
explicit coordinates (Tyurin parameters) in an open subset of the moduli space
of stable vector bundles. These coordinates turned out to be helpful in
integration of soliton equations.
We would like to gain attention to one more relationship between matrix
divisors of vector G-bundles (where G is a complex semi-simple Lie group) and
the theory of integrable systems, namely to the relationship with Lax operator
algebras. The result we obtain can be briefly formulated as follows: the moduli
space of matrix divisors with certain discrete invariants and fixed support is
a homogeneous space. Its tangent space at the unit is naturally isomorphic to
the quotient space of M-operators by L-operators, both spaces essentially
defined by the same invariants (the result goes back to Krichever, 2001). We
give one more description of the same space in terms of root systems.
"
"  Kristensen and Mele (2011) developed a new approach to obtain closed-form
approximations to continuous-time derivatives pricing models. The approach uses
a power series expansion of the pricing bias between an intractable model and
some known auxiliary model. Since the resulting approximation formula has
closed-form it is straightforward to obtain approximations of greeks. In this
thesis I will introduce Kristensen and Mele's methods and apply it to a variety
of stochastic volatility models of European style options as well as a model
for commodity futures. The focus of this thesis is the effect of different
model choices and different model parameter values on the numerical stability
of Kristensen and Mele's approximation.
"
"  We study Leinster's notion of magnitude for a compact metric space. For a
smooth, compact domain $X\subset \mathbb{R}^{2m-1}$, we find geometric
significance in the function $\mathcal{M}_X(R) = \mathrm{mag}(R\cdot X)$. The
function $\mathcal{M}_X$ extends from the positive half-line to a meromorphic
function in the complex plane. Its poles are generalized scattering resonances.
In the semiclassical limit $R \to \infty$, $\mathcal{M}_X$ admits an asymptotic
expansion. The three leading terms of $\mathcal{M}_X$ at $R=+\infty$ are
proportional to the volume, surface area and integral of the mean curvature. In
particular, for convex $X$ the leading terms are proportional to the intrinsic
volumes, and we obtain an asymptotic variant of the convex magnitude conjecture
by Leinster and Willerton, with corrected coefficients.
"
"  In this paper, we show that the edge connectivity of a distance-regular
digraph $\Gamma$ with valency $k$ is $k$ and for $k>2$, any minimum edge cut of
$\Gamma$ is the set of all edges going into (or coming out of) a single vertex.
Moreover we show that the same result holds for strongly regular digraphs.
These results extend the same known results for undirected case with quite
different proofs.
"
"  In this work, we prove that the growth of the Artin conductor is at most,
exponential in the degree of the character.
"
"  In this short paper we generalise a theorem due to Kani and Rosen on
decomposition of Jacobian varieties of Riemann surfaces with group action. This
generalisation extends the set of Jacobians for which it is possible to obtain
an isogeny decomposition where all the factors are Jacobians.
"
"  We construct optimal designs for group testing experiments where the goal is
to estimate the prevalence of a trait by using a test with uncertain
sensitivity and specificity. Using optimal design theory for approximate
designs, we show that the most efficient design for simultaneously estimating
the prevalence, sensitivity and specificity requires three different group
sizes with equal frequencies. However, if estimating prevalence as accurately
as possible is the only focus, the optimal strategy is to have three group
sizes with unequal frequencies. On the basis of a chlamydia study in the
U.S.A., we compare performances of competing designs and provide insights into
how the unknown sensitivity and specificity of the test affect the performance
of the prevalence estimator. We demonstrate that the locally D- and Ds-optimal
designs proposed have high efficiencies even when the prespecified values of
the parameters are moderately misspecified.
"
"  A generalization of the Emden-Fowler equation is presented and its solutions
are investigated. This paper is devoted to asymptotic behavior of its
solutions. The procedure is entirely based on a previous paper by the author.
"
"  Quantum technology is increasingly relying on specialised statistical
inference methods for analysing quantum measurement data. This motivates the
development of ""quantum statistics"", a field that is shaping up at the overlap
of quantum physics and ""classical"" statistics. One of the less investigated
topics to date is that of statistical inference for infinite dimensional
quantum systems, which can be seen as quantum counterpart of non-parametric
statistics. In this paper we analyse the asymptotic theory of quantum
statistical models consisting of ensembles of quantum systems which are
identically prepared in a pure state. In the limit of large ensembles we
establish the local asymptotic equivalence (LAE) of this i.i.d. model to a
quantum Gaussian white noise model. We use the LAE result in order to establish
minimax rates for the estimation of pure states belonging to Hermite-Sobolev
classes of wave functions. Moreover, for quadratic functional estimation of the
same states we note an elbow effect in the rates, whereas for testing a pure
state a sharp parametric rate is attained over the nonparametric
Hermite-Sobolev class.
"
"  Devaney and Krych showed that for the exponential family $\lambda e^z$, where
$0<\lambda <1/e$, the Julia set consists of uncountably many pairwise disjoint
simple curves tending to $\infty$. Viana proved that these curves are smooth.
In this article we consider a quasiregular counterpart of the exponential map,
the so-called Zorich maps, and generalize Viana's result to these maps.
"
"  In this paper we introduce new modules over the ring of ponderation
functions, so we recover old results in harmonic analysis from the side of ring
theory.
Moreover, we prove that Laplace transform, Fourier transform and Hankel
transform generate some kind of modules over the ring of ponderation functions.
"
"  This paper studies the eigenvalue problem on $\mathbb{R}^d$ for a class of
second order, elliptic operators of the form $\mathscr{L} =
a^{ij}\partial_{x_i}\partial_{x_j} + b^{i}\partial_{x_i} + f$, associated with
non-degenerate diffusions. We show that strict monotonicity of the principal
eigenvalue of the operator with respect to the potential function $f$ fully
characterizes the ergodic properties of the associated ground state diffusion,
and the unicity of the ground state, and we present a comprehensive study of
the eigenvalue problem from this point of view. This allows us to extend or
strengthen various results in the literature for a class of viscous
Hamilton-Jacobi equations of ergodic type with smooth coefficients to equations
with measurable drift and potential. In addition, we establish the strong
duality for the equivalent infinite dimensional linear programming formulation
of these ergodic control problems. We also apply these results to the study of
the infinite horizon risk-sensitive control problem for diffusions, and
establish existence of optimal Markov controls, verification of optimality
results, and the continuity of the controlled principal eigenvalue with respect
to stationary Markov controls.
"
"  We consider a classical risk process with arrival of claims following a
stationary Hawkes process. We study the asymptotic regime when the premium rate
and the baseline intensity of the claims arrival process are large, and claim
size is small. The main goal of this article is to establish a diffusion
approximation by verifying a functional central limit theorem of this model and
to compute both the finite-time and infinite-time horizon ruin probabilities.
Numerical results will also be given.
"
"  A common approach to analyzing categorical correlated time series data is to
fit a generalized linear model (GLM) with past data as covariate inputs. There
remain challenges to conducting inference for short time series length. By
treating the historical data as covariate inputs, standard errors of estimates
of GLM parameters computed using the empirical Fisher information do not fully
account the auto-correlation in the data. To overcome this serious limitation,
we derive the exact conditional Fisher information matrix of a general logistic
autoregressive model with endogenous covariates for any series length $T$.
Moreover, we also develop an iterative computational formula that allows for
relatively easy implementation of the proposed estimator. Our simulation
studies show that confidence intervals derived using the exact Fisher
information matrix tend to be narrower than those utilizing the empirical
Fisher information matrix while maintaining type I error rates at or below
nominal levels. Further, we establish that the exact Fisher information matrix
approaches, as T tends to infinity, the asymptotic Fisher information matrix
previously derived for binary time series data. The developed exact conditional
Fisher information matrix is applied to time-series data on respiratory rate
among a cohort of expectant mothers where it is found to provide narrower
confidence intervals for functionals of scientific interest and lead to greater
statistical power when compared to the empirical Fisher information matrix.
"
"  In this paper, we study inhomogeneous Diophantine approximation with rational
numbers of reduced form. The central object to study is the set $W(f,\theta)$
as follows, \begin{eqnarray*} \left\{x\in [0,1]:\left
|x-\frac{m+\theta(n)}{n}\right|<\frac{f(n)}{n}\text{ for infinitely many
coprime pairs of numbers } m,n\right\}, \end{eqnarray*} where
$\{f(n)\}_{n\in\mathbb{N}}$ and $\{\theta(n)\}_{n\in\mathbb{N}}$ are sequences
of real numbers in $[0,1/2]$. We will completely determine the Hausdorff
dimension of $W(f,\theta)$ in terms of $f$ and $\theta$. As a by-product, we
also obtain a new sufficient condition for $W(f,\theta)$ to have full Lebesgue
measure and this result is closely related to the study of \ds with extra
conditions.
"
"  Kustaanheimo-Stiefel (KS) transformation depends on the choice of some
preferred direction in the Cartesian 3D space. This choice, seldom explicitly
mentioned, amounts typically to the direction of the first or the third
coordinate axis in celestial mechanics and atomic physics, respectively. The
present work develops a canonical KS transformation with an arbitrary preferred
direction, indicated by what we call a defining vector. Using a mix of vector
and quaternion algebra, we formulate the transformation in a reference frame
independent manner. The link between the oscillator and Keplerian first
integrals is given. As an example of the present formulation, the Keplerian
motion in a rotating frame is re-investigated.
"
"  In this paper, a projected primal-dual gradient flow of augmented Lagrangian
is presented to solve convex optimization problems that are not necessarily
strictly convex. The optimization variables are restricted by a convex set with
computable projection operation on its tangent cone as well as equality
constraints. As a supplement of the analysis in
\cite{niederlander2016distributed}, we show that the projected dynamical system
converges to one of the saddle points and hence finding an optimal solution.
Moreover, the problem of distributedly maximizing the algebraic connectivity of
an undirected network by optimizing the port gains of each nodes (base
stations) is considered. The original semi-definite programming (SDP) problem
is relaxed into a nonlinear programming (NP) problem that will be solved by the
aforementioned projected dynamical system. Numerical examples show the
convergence of the aforementioned algorithm to one of the optimal solutions.
The effect of the relaxation is illustrated empirically with numerical
examples. A methodology is presented so that the number of iterations needed to
reach the equilibrium is suppressed. Complexity per iteration of the algorithm
is illustrated with numerical examples.
"
"  Although there is a significant literature on the asymptotic theory of Bayes
factor, the set-ups considered are usually specialized and often involves
independent and identically distributed data. Even in such specialized cases,
mostly weak consistency results are available. In this article, for the first
time ever, we derive the almost sure convergence theory of Bayes factor in the
general set-up that includes even dependent data and misspecified models.
Somewhat surprisingly, the key to the proof of such a general theory is a
simple application of a result of Shalizi (2009) to a well-known identity
satisfied by the Bayes factor.
"
"  This paper provides a general and abstract approach to approximate ergodic
regimes of Markov and Feller processes. More precisely, we show that the
recursive algorithm presented in Lamberton & Pages (2002) and based on
simulation algorithms of stochastic schemes with decreasing step can be used to
build invariant measures for general Markov and Feller processes. We also
propose applications in three different configurations: Approximation of Markov
switching Brownian diffusion ergodic regimes using Euler scheme, approximation
of Markov Brownian diffusion ergodic regimes with Milstein scheme and
approximation of general diffusions with jump components ergodic regimes.
"
"  We exhibit the first explicit examples of Salem sets in $\mathbb{Q}_p$ of
every dimension $0 < \alpha < 1$ by showing that certain sets of
well-approximable $p$-adic numbers are Salem sets. We construct measures
supported on these sets that satisfy essentially optimal Fourier decay and
upper regularity conditions, and we observe that these conditions imply that
the measures satisfy strong Fourier restriction inequalities. We also partially
generalize our results to higher dimensions. Our results extend theorems of
Kaufman, Papadimitropoulos, and Hambrook from the real to the $p$-adic setting.
"
"  The use of Kalman filtering, as well as its nonlinear extensions, for the
estimation of system variables and parameters has played a pivotal role in many
fields of scientific inquiry where observations of the system are restricted to
a subset of variables. However in the case of censored observations, where
measurements of the system beyond a certain detection point are impossible, the
estimation problem is complicated. Without appropriate consideration, censored
observations can lead to inaccurate estimates. Motivated by the work of [1], we
develop a modified version of the extended Kalman filter to handle the case of
censored observations in nonlinear systems. We validate this methodology in a
simple oscillator system first, showing its ability to accurately reconstruct
state variables and track system parameters when observations are censored.
Finally, we utilize the nonlinear censored filter to analyze censored datasets
from patients with hepatitis C and human immunodeficiency virus.
"
"  This paper presents an extension of a recently developed high order finite
difference method for the wave equation on a grid with non-conforming
interfaces. The stability proof of the existing methods relies on the
interpolation operators being norm-contracting, which is satisfied by the
second and fourth order operators, but not by the sixth order operator. We
construct new penalty terms to impose interface conditions such that the
stability proof does not require the norm-contracting condition. As a
consequence, the sixth order accurate scheme is also provably stable. Numerical
experiments demonstrate the improved stability and accuracy property.
"
"  In their work on a sharp compactness theorem for the Yamabe problem, Khuri,
Marques and Schoen apply a refined blow-up analysis (what we call `second order
blow-up argument' in this article) to obtain highly accurate approximate
solutions for the Yamabe equation. As for the conformal scalar curvature
equation on S^n with n > 3, we examine the second order blow-up argument and
obtain refined estimate for a blow-up sequence near a simple blow-up point. The
estimate involves local effect from the Taylor expansion of the scalar
curvature function, global effect from other blow-up points, and the balance
formula as expressed in the Pohozaev identity in an essential way.
"
"  This paper studies a \textit{partial functional partially linear single-index
model} that consists of a functional linear component as well as a linear
single-index component. This model generalizes many well-known existing models
and is suitable for more complicated data structures. However, its estimation
inherits the difficulties and complexities from both components and makes it a
challenging problem, which calls for new methodology. We propose a novel
profile B-spline method to estimate the parameters by approximating the unknown
nonparametric link function in the single-index component part with B-spline,
while the linear slope function in the functional component part is estimated
by the functional principal component basis. The consistency and asymptotic
normality of the parametric estimators are derived, and the global convergence
of the proposed estimator of the linear slope function is also established.
More excitingly, the latter convergence is optimal in the minimax sense. A
two-stage procedure is implemented to estimate the nonparametric link function,
and the resulting estimator possesses the optimal global rate of convergence.
Furthermore, the convergence rate of the mean squared prediction error for a
predictor is also obtained. Empirical properties of the proposed procedures are
studied through Monte Carlo simulations. A real data example is also analyzed
to illustrate the power and flexibility of the proposed methodology.
"
"  We show that the classical equivalence between the BMO norm and the $L^2$
norm of a lacunary Fourier series has an analogue on any discrete group $G$
equipped with a conditionally negative function.
"
"  In this paper, we construct an equivariant coarse homology theory with values
in the category of non-commutative motives of Blumberg, Gepner and Tabuada,
with coefficients in any small additive category. Equivariant coarse K-theory
is obtained from the latter by passing to global sections. The present
construction extends joint work of the first named author with Engel,
Kasprowski and Winges by promoting codomain of the equivariant coarse
K-homology functor to non-commutative motives.
"
"  The paper deals with a construction of a separating system of rational
invariants for finite dimensional generic algebras. In the process of dealing
an approach to a rough classification of finite dimensional algebras is offered
by attaching them some quadratic forms.
"
"  It is shown that for controlled Moran constructions in $\mathbb{R}$,
including the (sub) self-similar and more generally, (sub) self-conformal sets,
the quasi-Assouad dimension coincides with the upper box dimension. This can be
extended to some special classes of self-similar sets in higher dimensions. The
connections between quasi-Assouad dimension and tangents are studied. We show
that sets with decreasing gaps have quasi-Assouad dimension $0$ or $1$ and we
exhibit an example of a set in the plane whose quasi-Assouad dimension is
smaller than that of its projection onto the $x$-axis, showing that
quasi-Assouad dimension may increase under Lipschitz mappings.
"
"  We prove an explicit formula for the first non-zero entry in the n-th row of
the graded Betti table of an n-dimensional projective toric variety associated
to a normal polytope with at least one interior lattice point. This applies to
Veronese embeddings of projective space where we prove a special case of a
conjecture of Ein and Lazarsfeld. We also prove an explicit formula for the
entire n-th row when the interior of the polytope is one-dimensional. All
results are valid over an arbitrary field k.
"
"  We will characterize topologically conjugate two-sided topological Markov
shifts $(\bar{X}_A,\bar{\sigma}_A)$ in terms of the associated asymptotic
Ruelle $C^*$-algebras ${\mathcal{R}}_A$ with its commutative $C^*$-subalgebras
$C(\bar{X}_A)$ and the canonical circle actions. We will also show that
extended Ruelle algebras ${\widetilde{\mathcal{R}}}_A$, which are purely
infinite version of the asymptotic Ruelle algebras, with its commutative
$C^*$-subalgebras $C(\bar{X}_A)$ and the canonical torus actions $\gamma^A$ are
complete invariants for topological conjugacy of two-sided topological Markov
shifts. We then have a computable topological conjugacy invariant, written in
terms of the underlying matrix, of a two-sided topological Markov shift by
using K-theory of the extended Ruelle algebra. The diagonal action of
$\gamma^A$ has a unique KMS-state on ${\widetilde{\mathcal{R}}}_A$, which is an
extension of the Parry measure on $\bar{X}_A$.
"
"  The main aim of this survey paper is to gather together some results
concerning the Calabi type duality discovered by Hojoo Lee between certain
families of (spacelike) graphs with constant mean curvature in Riemannian and
Lorentzian homogeneous 3-manifolds with isometry group of dimension 4. The
duality is conformal and swaps mean curvature and bundle curvature, and we will
revisit it by giving a more general statement in terms of conformal immersions.
This will show that some features in the theory of surfaces with mean curvature
$\frac{1}{2}$ in $\mathbb{H}^2\times\mathbb{R}$ or minimal surfaces in the
Heisenberg space have nice geometric interpretations in terms of their dual
Lorentzian counterparts. We will briefly discuss some applications such as
gradient estimates for entire minimal graphs in Heisenberg space or the
existence of complete spacelike surfaces, and we will also give an uniform
treatment to the behavior of the duality with respect to ambient isometries.
Finally, some open questions are posed in the last section.
"
"  Let $\Omega:=\left( a,b\right) \subset\mathbb{R}$, $m\in L^{1}\left(
\Omega\right) $ and $\lambda>0$ be a real parameter. Let $\mathcal{L}$ be the
differential operator given by $\mathcal{L}u:=-\phi\left( u^{\prime}\right)
^{\prime}+r\left( x\right) \phi\left( u\right) $, where $\phi
:\mathbb{R\rightarrow R}$ is an odd increasing homeomorphism and $0\leq r\in
L^{1}\left( \Omega\right) $. We study the existence of positive solutions for
problems of the form $\mathcal{L}u=\lambda m\left( x\right) f\left( u\right)$
in $\Omega,$ $u=0$ on $\partial\Omega$, where $f:\left[ 0,\infty\right)
\rightarrow\left[ 0,\infty\right) $ is a continuos function which is, roughly
speaking, sublinear with respect to $\phi$. Our approach combines the sub and
supersolution method with some estimates on related nonlinear problems. We
point out that our results are new even in the cases $r\equiv0$ and/or
$m\geq0$.
"
"  Estimating the tail index parameter is one of the primal objectives in
extreme value theory. For heavy-tailed distributions the Hill estimator is the
most popular way to estimate the tail index parameter. Improving the Hill
estimator was aimed by recent works with different methods, for example by
using bootstrap, or Kolmogorov-Smirnov metric. These methods are asymptotically
consistent, but for tail index $\xi >1$ and smaller sample sizes the estimation
fails to approach the theoretical value for realistic sample sizes. In this
paper, we introduce a new empirical method, which can estimate high tail index
parameters well and might also be useful for relatively small sample sizes.
"
"  In this paper we study a special case of the completion of cusp
Kähler-Einstein metric on the regular part of varieties by taking the
continuity method proposed by La Nave and Tian. The differential geometric and
algebro-geometric properties of the noncollapsing limit in the continuity
method with cusp singularities will be investigated.
"
"  For $d\geq1$, we study the simplicial structure of the chain complex
associated to the higher order Hochschild homology over the $d$-sphere. We
discuss $H_\bullet^{S^d}(A,M)$ by way of a bar-like resolution
$\mathcal{B}^d(A)$ in the context of simplicial modules. Besides the general
case, we give explicit detail corresponding to $S^3$. We also present a
description of what can replace these bar-like resolutions in order to aid with
computation. The cohomology version can be done following a similar
construction, of which we make mention.
"
"  We consider a class of variational problems for densities that repel each
other at distance. Typical examples are given by the Dirichlet functional and
the Rayleigh functional \[
D(\mathbf{u}) = \sum_{i=1}^k \int_{\Omega} |\nabla u_i|^2 \quad \text{or}
\quad R(\mathbf{u}) = \sum_{i=1}^k \frac{\int_{\Omega} |\nabla
u_i|^2}{\int_{\Omega} u_i^2} \] minimized in the class of
$H^1(\Omega,\mathbb{R}^k)$ functions attaining some boundary conditions on
$\partial \Omega$, and subjected to the constraint \[
\mathrm{dist} (\{u_i > 0\}, \{u_j > 0\}) \ge 1 \qquad \forall i \neq j. \]
For these problems, we investigate the optimal regularity of the solutions,
prove a free-boundary condition, and derive some preliminary results
characterizing the free boundary $\partial \{\sum_{i=1}^k u_i > 0\}$.
"
"  A flexible approach for modeling both dynamic event counting and dynamic
link-based networks based on counting processes is proposed, and estimation in
these models is studied. We consider nonparametric likelihood based estimation
of parameter functions via kernel smoothing. The asymptotic behavior of these
estimators is rigorously analyzed by allowing the number of nodes to tend to
infinity. The finite sample performance of the estimators is illustrated
through an empirical analysis of bike share data.
"
"  A general conjecture is stated on the cone of automorphic vector bundles
admitting nonzero global sections on schemes endowed with a smooth, surjective
morphism to a stack of $G$-zips of connected-Hodge-type; such schemes should
include all Hodge-type Shimura varieties with hyperspecial level. We prove our
conjecture for groups of type $A_1^n$, $C_2$ and $\mathbf F_p$-split groups of
type $A_2$ (this includes all Hilbert-Blumenthal varieties and should also
apply to Siegel modular threefolds and Picard modular surfaces). An example is
given to show that our conjecture can fail for zip data not of
connected-Hodge-type.
"
"  We establish that every $K$-quasiconformal mapping $w$ of the unit ball $\IB$
onto a $C^2$-Jordan domain $\Omega$ is Hölder continuous with constant
$\alpha= 2-\frac{n}{p}$, provided that its weak Laplacean $\Delta w$ is in $
L^p(\IB)$ for some $n/2<p<n$. In particular it is Hölder continuous for every
$0<\alpha<1$ provided that $\Delta w\in L^n(\IB)$.
"
"  We prove that averaging operators are uniformly bounded on $L^1$ for all
geometrically doubling metric measure spaces, with bounds independent of the
measure. From this result, the $L^1$ convergence of averages as $r \to 0$
immediately follows.
"
"  This note corrects conditions in Proposition 3.4 and Theorem 5.2(ii) and
comments on imprecisions in Propositions 4.2 and 4.4 in Fissler and Ziegel
(2016).
"
"  We investigate spectral properties of the tensor products of two quantum
channels defined on matrix algebras. This leads to the important question of
when an arbitrary subalgebra can split into the tensor product of two
subalgebras. We show that for two unital quantum channels $\mathcal{E}_1$ and
$\mathcal{E}_2$ the multiplicative domain of
$\mathcal{E}_1\otimes\mathcal{E}_2$ splits into the tensor product of the
individual multiplicative domains. Consequently, we fully describe the fixed
points and peripheral eigen operators of the tensor product of channels.
Through a structure theorem of maximal unital proper $^*$-subalgebras (MUPSA)
of a matrix algebra we provide a non-trivial upper bound of the 'multiplicative
index' of a unital channel which was recently introduced. This bound gives a
criteria on when a channel cannot be factored into a product of two different
channels. We construct examples of channels which can not be realized as a
tensor product of two channels in any way. With these techniques and results,
we found some applications in quantum error correction.
"
"  We prove that if a smooth projective algebraic variety of dimension less or
equal to three has a unit type integral $K$-motive, then its integral Chow
motive is of Lefschetz type. As a consequence, the integral Chow motive is of
Lefschetz type for a smooth projective variety of dimension less or equal to
three that admits a full exceptional collection.
"
"  Demand response (DR) is a cost-effective and environmentally friendly
approach for mitigating the uncertainties in renewable energy integration by
taking advantage of the flexibility of customers' demands. However, existing DR
programs suffer from either low participation due to strict commitment
requirements or not being reliable in voluntary programs. In addition, the
capacity planning for energy storage/reserves is traditionally done separately
from the demand response program design, which incurs inefficiencies. Moreover,
customers often face high uncertainties in their costs in providing demand
response, which is not well studied in literature.
This paper first models the problem of joint capacity planning and demand
response program design by a stochastic optimization problem, which
incorporates the uncertainties from renewable energy generation, customer power
demands, as well as the customers' costs in providing DR. We propose online DR
control policies based on the optimal structures of the offline solution. A
distributed algorithm is then developed for implementing the control policies
without efficiency loss. We further offer enhanced policy design by allowing
flexibilities into the commitment level. We perform real world trace based
numerical simulations. Results demonstrate that the proposed algorithms can
achieve near optimal social costs, and significant social cost savings compared
to baseline methods.
"
"  There is widespread sentiment that it is not possible to effectively utilize
fast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy
ball) for the purposes of stochastic optimization due to their instability and
error accumulation, a notion made precise in d'Aspremont 2008 and Devolder,
Glineur, and Nesterov 2014. This work considers these issues for the special
case of stochastic approximation for the least squares regression problem, and
our main result refutes the conventional wisdom by showing that acceleration
can be made robust to statistical errors. In particular, this work introduces
an accelerated stochastic gradient method that provably achieves the minimax
optimal statistical risk faster than stochastic gradient descent. Critical to
the analysis is a sharp characterization of accelerated stochastic gradient
descent as a stochastic process. We hope this characterization gives insights
towards the broader question of designing simple and effective accelerated
stochastic methods for more general convex and non-convex optimization
problems.
"
"  Application of fuzzy support vector machine in stock price forecast. Support
vector machine is a new type of machine learning method proposed in 1990s. It
can deal with classification and regression problems very successfully. Due to
the excellent learning performance of support vector machine, the technology
has become a hot research topic in the field of machine learning, and it has
been successfully applied in many fields. However, as a new technology, there
are many limitations to support vector machines. There is a large amount of
fuzzy information in the objective world. If the training of support vector
machine contains noise and fuzzy information, the performance of the support
vector machine will become very weak and powerless. As the complexity of many
factors influence the stock price prediction, the prediction results of
traditional support vector machine cannot meet people with precision, this
study improved the traditional support vector machine fuzzy prediction
algorithm is proposed to improve the new model precision. NASDAQ Stock Market,
Standard & Poor's (S&P) Stock market are considered. Novel advanced- fuzzy
support vector machine (NA-FSVM) is the proposed methodology.
"
"  The goal of the paper is to study the angle between two curves in the
framework of metric (and metric measure) spaces. More precisely, we give a new
notion of angle between two curves in a metric space. Such a notion has a
natural interplay with optimal transportation and is particularly well suited
for metric measure spaces satisfying the curvature-dimension condition. Indeed
one of the main results is the validity of the cosine formula on $RCD^{*}(K,N)$
metric measure spaces. As a consequence, the new introduced notions are
compatible with the corresponding classical ones for Riemannian manifolds,
Ricci limit spaces and Alexandrov spaces.
"
"  In mathematical physics, the space-fractional diffusion equations are of
particular interest in the studies of physical phenomena modelled by Lévy
processes, which are sometimes called super-diffusion equations. In this
article, we develop the differential quadrature (DQ) methods for solving the 2D
space-fractional diffusion equations on irregular domains. The methods in
presence reduce the original equation into a set of ordinary differential
equations (ODEs) by introducing valid DQ formulations to fractional directional
derivatives based on the functional values at scattered nodal points on problem
domain. The required weighted coefficients are calculated by using radial basis
functions (RBFs) as trial functions, and the resultant ODEs are discretized by
the Crank-Nicolson scheme. The main advantages of our methods lie in their
flexibility and applicability to arbitrary domains. A series of illustrated
examples are finally provided to support these points.
"
"  We derive some Positivstellensatzë for noncommutative rational expressions
from the Positivstellensatzë for noncommutative polynomials. Specifically, we
show that if a noncommutative rational expression is positive on a polynomially
convex set, then there is an algebraic certificate witnessing that fact. As in
the case of noncommutative polynomials, our results are nicer when we
additionally assume positivity on a convex set-- that is, we obtain a so-called
""perfect Positivstellensatz"" on convex sets.
"
"  We study the minus order on the algebra of bounded linear operators on a
Hilbert space. By giving a characterization in terms of range additivity, we
show that the intrinsic nature of the minus order is algebraic. Applications to
generalized inverses of the sum of two operators, to systems of operator
equations and to optimization problems are also presented.
"
"  A ballean (or coarse structure) is a set endowed with some family of subsets,
the balls, is such a way that balleans with corresponding morphisms can be
considered as asymptotic counterparts of uniform topological spaces. For a
ballean $\mathcal{B}$ on a set $X$, the hyperballean $\mathcal{B}^{\flat}$ is a
ballean naturally defined on the set $X^{\flat}$ of all bounded subsets of $X$.
We describe all balleans with hyperballeans of bounded geometry and analyze the
structure of these hyperballeans.
"
"  In this article, we investigate large sample properties of model selection
procedures in a general Bayesian framework when a closed form expression of the
marginal likelihood function is not available or a local asymptotic quadratic
approximation of the log-likelihood function does not exist. Under appropriate
identifiability assumptions on the true model, we provide sufficient conditions
for a Bayesian model selection procedure to be consistent and obey the Occam's
razor phenomenon, i.e., the probability of selecting the ""smallest"" model that
contains the truth tends to one as the sample size goes to infinity. In order
to show that a Bayesian model selection procedure selects the smallest model
containing the truth, we impose a prior anti-concentration condition, requiring
the prior mass assigned by large models to a neighborhood of the truth to be
sufficiently small. In a more general setting where the strong model
identifiability assumption may not hold, we introduce the notion of local
Bayesian complexity and develop oracle inequalities for Bayesian model
selection procedures. Our Bayesian oracle inequality characterizes a trade-off
between the approximation error and a Bayesian characterization of the local
complexity of the model, illustrating the adaptive nature of averaging-based
Bayesian procedures towards achieving an optimal rate of posterior convergence.
Specific applications of the model selection theory are discussed in the
context of high-dimensional nonparametric regression and density regression
where the regression function or the conditional density is assumed to depend
on a fixed subset of predictors. As a result of independent interest, we
propose a general technique for obtaining upper bounds of certain small ball
probability of stationary Gaussian processes.
"
"  The dynamics along the particle trajectories for the 3D axisymmetric Euler
equations are considered. It is shown that if the inflow is rapidly increasing
(pushy) in time, the corresponding laminar profile of the incompressible Euler
flow is not (in some sense) stable provided that the swirling component is not
zero. It is also shown that if the vorticity on the axis is not zero (with some
extra assumptions), then there is no steady flow. We can rephrase these
instability to an instantaneous blow-up. In the proof, Frenet-Serret formulas
and orthonormal moving frame are essentially used.
"
"  We use the Maximum $q$-log-likelihood estimation for Least informative
distributions (LID) in order to estimate the parameters in probability density
functions (PDFs) efficiently and robustly when data include outlier(s). LIDs
are derived by using convex combinations of two PDFs,
$f_\epsilon=(1-\epsilon)f_0+\epsilon f_1$. A convex combination of two PDFs is
considered as a contamination $f_1$ as outlier(s) to underlying $f_0$
distributions and $f_\epsilon$ is a contaminated distribution. The optimal
criterion is obtained by minimizing the change of Maximum q-log-likelihood
function when the data have slightly more contamination. In this paper, we make
a comparison among ordinary Maximum likelihood, Maximum q-likelihood
estimations, LIDs based on $\log_q$ and Huber M-estimation. Akaike and Bayesian
information criterions (AIC and BIC) based on $\log_q$ and LID are proposed to
assess the fitting performance of functions. Real data sets are applied to test
the fitting performance of estimating functions that include shape, scale and
location parameters.
"
"  Unwanted variation, including hidden confounding, is a well-known problem in
many fields, particularly large-scale gene expression studies. Recent proposals
to use control genes --- genes assumed to be unassociated with the covariates
of interest --- have led to new methods to deal with this problem. Going by the
moniker Removing Unwanted Variation (RUV), there are many versions --- RUV1,
RUV2, RUV4, RUVinv, RUVrinv, RUVfun. In this paper, we introduce a general
framework, RUV*, that both unites and generalizes these approaches. This
unifying framework helps clarify connections between existing methods. In
particular we provide conditions under which RUV2 and RUV4 are equivalent. The
RUV* framework also preserves an advantage of RUV approaches --- their
modularity --- which facilitates the development of novel methods based on
existing matrix imputation algorithms. We illustrate this by implementing RUVB,
a version of RUV* based on Bayesian factor analysis. In realistic simulations
based on real data we found that RUVB is competitive with existing methods in
terms of both power and calibration, although we also highlight the challenges
of providing consistently reliable calibration among data sets.
"
"  Let $(x_n)_{n=1}^{\infty}$ be a sequence on the torus $\mathbb{T}$
(normalized to length 1). We show that if there exists a sequence of positive
real numbers $(t_n)_{n=1}^{\infty}$ converging to 0 such that $$ \lim_{N
\rightarrow \infty}{\frac{1}{N^2} \sum_{m,n = 1}^{N}{\frac{1}{\sqrt{t_N}}
\exp{\left(- \frac{1}{t_N} (x_m - x_n)^2 \right)}}}
= \sqrt{\pi},$$ then $(x_n)_{n=1}^{\infty}$ is uniformly distributed. This is
especially interesting when $t_N \sim N^{-2}$ since the size of the sum is then
essentially determined exclusively by local gaps at scale $\sim N^{-1}$. This
can be used to show equidistribution of sequences with Poissonian pair
correlation, which recovers a recent result of Aistleitner, Lachmann &
Pausinger and Grepstad & Larcher. The general form of the result is proven on
arbitrary compact manifolds $(M,g)$ where the role of the exponential function
is played by the heat kernel $e^{t\Delta}$: for all $x_1, \dots, x_N \in M$ and
all $t>0$ $$ \frac{1}{N^2}\sum_{m,n=1}^{N}{[e^{t\Delta}\delta_{x_m}](x_n)} \geq
\frac{1}{vol(M)}$$ and equality is attained as $N \rightarrow \infty$ if and
only if $(x_n)_{n=1}^{\infty}$ equidistributes.
"
"  We prove a recognition principle for motivic infinite P1-loop spaces over a
perfect field. This is achieved by developing a theory of framed motivic
spaces, which is a motivic analogue of the theory of E-infinity-spaces. A
framed motivic space is a motivic space equipped with transfers along finite
syntomic morphisms with trivialized cotangent complex in K-theory. Our main
result is that grouplike framed motivic spaces are equivalent to the full
subcategory of motivic spectra generated under colimits by suspension spectra.
As a consequence, we deduce some representability results for suspension
spectra of smooth varieties, and in particular for the motivic sphere spectrum,
in terms of Hilbert schemes of points in affine spaces.
"
"  Based on properties of n-subharmonic functions we show that a complete,
noncompact, properly embedded hypersurface with nonnegative Ricci curvature in
hyperbolic space has an asymptotic boundary at infinity of at most two points.
Moreover, the presence of two points in the asymptotic boundary is a rigidity
condition that forces the hypersurface to be an equidistant hypersurface about
a geodesic line in hyperbolic space. This gives an affirmative answer to the
question raised by Alexander and Currier in 1990.
"
"  We propose a simple approach which, given distributed computing resources,
can nearly achieve the accuracy of $k$-NN prediction, while matching (or
improving) the faster prediction time of $1$-NN. The approach consists of
aggregating denoised $1$-NN predictors over a small number of distributed
subsamples. We show, both theoretically and experimentally, that small
subsample sizes suffice to attain similar performance as $k$-NN, without
sacrificing the computational efficiency of $1$-NN.
"
"  There is a forgetful functor from the category of generalized effect algebras
to the category of effect algebras. We prove that this functor is a right
adjoint and that the corresponding left adjoint is the well-known unitization
construction by Hedlíková and Pulmannová. Moreover, this adjunction is
monadic.
"
"  We evaluate integrals of certain polynomials over spheres and balls in real
or complex spaces. We also promote the use of the Pochhammer symbol which gives
the values of our integrals in compact forms.
"
"  We find boundaries of Borel-Serre compactifications of locally symmetric
spaces, for which any filling is incompressible. We prove this result by
showing that these boundaries have small singular models and using these models
to obstruct compressions. We also show that small singular models of boundaries
obstruct $S^1$-actions (and more generally homotopically trivial $\mathbb
Z/p$-actions) on interiors of aspherical fillings. We use this to bound the
symmetry of complete Riemannian metrics on such interiors in terms of the
fundamental group. We also use small singular models to simplify the proofs of
some already known theorems about moduli spaces (the minimal orbifold theorem
and a topological analogue of Royden's theorem).
"
"  Let $k$ be an algebraically closed field of any characteristic. We apply the
Hamburger-Noether process of successive quadratic transformations to show the
equivalence of two definitions of the {\L}ojasiewicz exponent
$\mathfrak{L}(\mathfrak{a})$ of an ideal $\mathfrak{a}\subset k[[x,y]]$.
"
"  We establish the sharp growth rate, in terms of cardinality, of the $L^p$
norms of the maximal Hilbert transform $H_\Omega$ along finite subsets of a
finite order lacunary set of directions $\Omega \subset \mathbb R^3$, answering
a question of Parcet and Rogers in dimension $n=3$. Our result is the first
sharp estimate for maximal directional singular integrals in dimensions greater
than 2.
The proof relies on a representation of the maximal directional Hilbert
transform in terms of a model maximal operator associated to compositions of
two-dimensional angular multipliers, as well as on the usage of weighted norm
inequalities, and their extrapolation, in the directional setting.
"
"  Let $M$ be a compact oriented three-manifold whose interior is hyperbolic of
finite volume. We prove a variation formula for the volume on the variety of
representations of $M$ in $\operatorname{SL}_n(\mathbb C)$. Our proof follows
the strategy of Reznikov's rigidity when $M$ is closed, in particular we use
Fuks' approach to variations by means of Lie algebra cohomology. When $n=2$, we
get back Hodgson's formula for variation of volume on the space of hyperbolic
Dehn fillings. Our formula also yields the variation of volume on the space of
decorated triangulations obtained by Bergeron-Falbel-Guillou and
Dimofte-Gabella-Goncharov.
"
"  We examine volume computation of general-dimensional polytopes and more
general convex bodies, defined as the intersection of a simplex by a family of
parallel hyperplanes, and another family of parallel hyperplanes or a family of
concentric ellipsoids. Such convex bodies appear in modeling and predicting
financial crises. The impact of crises on the economy (labor, income, etc.)
makes its detection of prime interest. Certain features of dependencies in the
markets clearly identify times of turmoil. We describe the relationship between
asset characteristics by means of a copula; each characteristic is either a
linear or quadratic form of the portfolio components, hence the copula can be
constructed by computing volumes of convex bodies. We design and implement
practical algorithms in the exact and approximate setting, we experimentally
juxtapose them and study the tradeoff of exactness and accuracy for speed. We
analyze the following methods in order of increasing generality: rejection
sampling relying on uniformly sampling the simplex, which is the fastest
approach, but inaccurate for small volumes; exact formulae based on the
computation of integrals of probability distribution functions; an optimized
Lawrence sign decomposition method, since the polytopes at hand are shown to be
simple; Markov chain Monte Carlo algorithms using random walks based on the
hit-and-run paradigm generalized to nonlinear convex bodies and relying on new
methods for computing a ball enclosed; the latter is experimentally extended to
non-convex bodies with very encouraging results. Our C++ software, based on
CGAL and Eigen and available on github, is shown to be very effective in up to
100 dimensions. Our results offer novel, effective means of computing portfolio
dependencies and an indicator of financial crises, which is shown to correctly
identify past crises.
"
"  We analyze the clustering problem through a flexible probabilistic model that
aims to identify an optimal partition on the sample X 1 , ..., X n. We perform
exact clustering with high probability using a convex semidefinite estimator
that interprets as a corrected, relaxed version of K-means. The estimator is
analyzed through a non-asymptotic framework and showed to be optimal or
near-optimal in recovering the partition. Furthermore, its performances are
shown to be adaptive to the problem's effective dimension, as well as to K the
unknown number of groups in this partition. We illustrate the method's
performances in comparison to other classical clustering algorithms with
numerical experiments on simulated data.
"
"  In this paper we consider the problem of finding periodic solutions of
certain Euler-Lagrange equations, which include, among others, equations
involving the $p$-Laplace and, more generality, the $(p,q)$-Laplace operator.
We employ the direct method of the calculus of variations in the framework of
anisotropic Orlicz-Sobolev spaces. These spaces appear to be useful in
formulating a unified theory of existence for the type of problem considered.
"
"  We show that for each positive integer $k$ there exist right-angled Artin
groups containing free-by-cyclic subgroups whose monodromy automorphisms grow
as $n^k$. As a consequence we produce examples of right-angled Artin groups
containing finitely presented subgroups whose Dehn functions grow as $n^{k+2}$.
"
"  Many real world practical problems can be formulated as
$\ell_{0}$-minimization problems with nonnegativity constraints, which seek the
sparsest nonnegative signals to underdetermined linear systems. They have been
widely applied in signal and image processing, machine learning, pattern
recognition and computer vision. Unfortunately, this $\ell_{0}$-minimization
problem with nonnegativity constraint is computational and NP-hard because of
the discrete and discontinuous nature of the $\ell_{0}$-norm. In this paper, we
replace the $\ell_{0}$-norm with a non-convex fraction function, and study the
minimization problem of this non-convex fraction function in recovering the
sparse nonnegative signals from an underdetermined linear system. Firstly, we
discuss the equivalence between $(P_{0}^{\geq})$ and $(FP_{a}^{\geq})$, and the
equivalence between $(FP_{a}^{\geq})$ and $(FP_{a,\lambda}^{\geq})$. It is
proved that the optimal solution of the problem $(P_{0}^{\geq})$ could be
approximately obtained by solving the regularization problem
$(FP_{a,\lambda}^{\geq})$ if some specific conditions satisfied. Secondly, we
propose a nonnegative iterative thresholding algorithm to solve the
regularization problem $(FP_{a,\lambda}^{\geq})$ for all $a>0$. Finally, some
numerical experiments on sparse nonnegative siganl recovery problems show that
our method performs effective in finding sparse nonnegative signals compared
with the linear programming.
"
"  In this paper we develop a way of obtaining Green's functions for partial
differential equations with linear involutions by reducing the equation to a
higher-order PDE without involutions. The developed theory is applied to a
model of heat transfer in a conducting plate which is bent in half.
"
"  It was recently shown that the phase retrieval imaging of a sample can be
modeled as a simple convolution process. Sometimes, such a convolution depends
on physical parameters of the sample which are difficult to estimate a priori.
In this case, a blind choice for those parameters usually lead to wrong
results, e.g., in posterior image segmentation processing. In this manuscript,
we propose a simple connection between phase-retrieval algorithms and
optimization strategies, which lead us to ways of numerically determining the
physical parameters
"
"  The purpose of the present paper is to investigate a fusion rule algebra
arising from irreducible characters of a compact group $G$ and a closed
subgroup $G_0$ of $G$ with finite index. The convolution of this fusion rule
algebra is introduced by inducing irreducible representations of $G_0$ to $G$
and by restricting irreducible representations of $G$ to $G_0$.
"
"  In this paper we study moment sequences of matrix-valued measures on compact
intervals. A complete parametrization of such sequences is obtained via a
symmetric version of matricial canonical moments. Furthermore, distinguished
extensions of finite moment sequences are characterized in this framework. The
results are applied to the underlying matrix-valued measures, generalizing some
results from the scalar theory of canonical moments.
"
"  The main purpose of this paper is to study multi-parameter singular integral
operators which commute with Zygmund dilations. We introduce a class of
singular integral operators associated with Zygmund dilations and show the
boundedness for these operators on $L^p, 1<p<\infty$, which covers those
studied by Ricci--Stein \cite{RS} and Nagel--Wainger \cite{NW}
"
"  Let $Di\langle X\rangle$ be the free dialgebra over a field generated by a
set $X$. Let $S$ be a monic subset of $Di\langle X\rangle$. A
Composition-Diamond lemma for dialgebras is firstly established by Bokut, Chen
and Liu in 2010 \cite{Di} which claims that if (i) $S$ is a
Gröbner-Shirshov basis in $Di\langle X\rangle$, then (ii) the set of
$S$-irreducible words is a linear basis of the quotient dialgebra $Di\langle X
\mid S \rangle$, but not conversely. Such a lemma based on a fixed ordering on
normal diwords of $Di\langle X\rangle$ and special definition of composition
trivial modulo $S$. In this paper, by introducing an arbitrary monomial-center
ordering and the usual definition of composition trivial modulo $S$, we give a
new Composition-Diamond lemma for dialgebras which makes the conditions (i) and
(ii) equivalent. We show that every ideal of $Di\langle X\rangle$ has a unique
reduced Gröbner-Shirshov basis. The new lemma is more useful and convenient
than the one in \cite{Di}. As applications, we give a method to find normal
forms of elements of an arbitrary disemigroup, in particular, A.V. Zhuchok's
(2010) and Y.V. Zhuchok's (2015) normal forms of the free commutative
disemigroups and the free abelian disemigroups, and normal forms of the free
left (right) commutative disemigroups.
"
"  In this article, we study the Kapustin-Witten equations on a closed,
simply-connected, four-manifold. We using a compactness theorem due to Taubes
to prove that if $(A,\phi)$ is a solution of Kapustin-Witten equations and the
connection $A$ is closed to a $generic$ ASD connection $A_{\infty}$, then
$(A,\phi)$ must be a trivial solution. We also prove that the moduli space of
the solutions of Kapustin-Witten equations is non-connected if the connections
on the compactification of moduli space of ASD connections are all $generic$.
As one application, we extend the ideas of Kapustin-Witten equations to other
equations on gauge theory-- Hitchin-Simpson equations and Vafa-Witten on
compact Kähler surface with a Kähler metric $g$.
"
"  Let $G$ be a finite group with the property that if $a,b$ are powers of
$\delta_1^*$-commutators such that $(|a|,|b|)=1$, then $|ab|=|a||b|$. We show
that $\gamma_{\infty}(G)$ is nilpotent.
"
"  We study an unbiased estimator for the density of a sum of random variables
that are simulated from a computer model. A numerical study on examples with
copula dependence is conducted where the proposed estimator performs favourably
in terms of variance compared to other unbiased estimators. We provide
applications and extensions to the estimation of marginal densities in Bayesian
statistics and to the estimation of the density of sums of random variables
under Gaussian copula dependence.
"
"  We introduce a class of fixed points of primitive morphisms among aperiodic
binary generalized pseudostandard words. We conjecture that this class contains
all fixed points of primitive morphisms among aperiodic binary generalized
pseudostandard words that are not standard Sturmian words.
"
"  R. Beheshti showed that, for a smooth Fano hypersurface $X$ of degree $\leq
8$ over the complex number field $\mathbb{C}$, the dimension of the space of
lines lying in $X$ is equal to the expected dimension. We study the space of
conics on $X$. In this case, if $X$ contains some linear subvariety, then the
dimension of the space can be larger than the expected dimension. In this
paper, we show that, for a smooth Fano hypersurface $X$ of degree $\leq 6$ over
$\mathbb{C}$, and for an irreducible component $R$ of the space of conics lying
in $X$, if the $2$-plane spanned by a general conic of $R$ is not contained in
$X$, then the dimension of $R$ is equal to the expected dimension.
"
"  The structure of a certain subgroup $S$ of the automorphism group of a
partially commutative group (RAAG) $G$ is described in detail: namely the
subgroup generated by inversions and elementary transvections. We define
admissible subsets of the generators of $G$, and show that $S$ is the subgroup
of automorphisms which fix all subgroups $\langle Y\rangle$ of $G$, for all
admissible subsets $Y$. A decomposition of $S$ as an iterated tower of
semi-direct products in given and the structure of the factors of this
decomposition described. The construction allows a presentation of $S$ to be
computed, from the commutation graph of $G$.
"
"  By excluding some regions, in which each eigenvalue of a matrix is not
contained, from the \alpha\beta-type eigenvalue inclusion region provided by
Huang et al.(Electronic Journal of Linear Algebra, 15 (2006) 215-224), a new
eigenvalue inclusion region is given. And it is proved that the new region is
contained in the \alpha\beta-type eigenvalue inclusion region.
"
"  We consider the problem of nonparametric regression under shape constraints.
The main examples include isotonic regression (with respect to any partial
order), unimodal/convex regression, additive shape-restricted regression, and
constrained single index model. We review some of the theoretical properties of
the least squares estimator (LSE) in these problems, emphasizing on the
adaptive nature of the LSE. In particular, we study the behavior of the risk of
the LSE, and its pointwise limiting distribution theory, with special emphasis
to isotonic regression. We survey various methods for constructing pointwise
confidence intervals around these shape-restricted functions. We also briefly
discuss the computation of the LSE and indicate some open research problems and
future directions.
"
"  We study local asymptotic normality of M-estimates of convex minimization in
an infinite dimensional parameter space. The objective function of M-estimates
is not necessary differentiable and is possibly subject to convex constraints.
In the above circumstance, narrow convergence with respect to uniform
convergence fails to hold, because of the strength of it's topology. A new
approach we propose to the lack-of-uniform-convergence is based on
Mosco-convergence that is weaker topology than uniform convergence. By applying
narrow convergence with respect to Mosco topology, we develop an
infinite-dimensional version of the convexity argument and provide a proof of a
local asymptotic normality. Our new technique also provides a proof of an
asymptotic distribution of the likelihood ratio test statistic defined on real
separable Hilbert spaces.
"
"  The Hamiltonian of the quantum Calogero-Sutherland model of $N$ identical
particles on the circle with $1/r^{2}$ interactions has eigenfunctions
consisting of Jack polynomials times the base state. By use of the generalized
Jack polynomials taking values in modules of the symmetric group and the matrix
solution of a system of linear differential equations one constructs novel
eigenfunctions of the Hamiltonian. Like the usual wavefunctions each
eigenfunction determines a symmetric probability density on the $N$-torus. The
construction applies to any irreducible representation of the symmetric group.
The methods depend on the theory of generalized Jack polynomials due to
Griffeth, and the Yang-Baxter graph approach of Luque and the author.
"
"  We define the {\it Wirtinger number} of a link, an invariant closely related
to the meridional rank. The Wirtinger number is the minimum number of
generators of the fundamental group of the link complement over all meridional
presentations in which every relation is an iterated Wirtinger relation arising
in a diagram. We prove that the Wirtinger number of a link equals its bridge
number. This equality can be viewed as establishing a weak version of Cappell
and Shaneson's Meridional Rank Conjecture, and suggests a new approach to this
conjecture. Our result also leads to a combinatorial technique for obtaining
strong upper bounds on bridge numbers. This technique has so far allowed us to
add the bridge numbers of approximately 50,000 prime knots of up to 14
crossings to the knot table. As another application, we use the Wirtinger
number to show there exists a universal constant $C$ with the property that the
hyperbolic volume of a prime alternating link $L$ is bounded below by $C$ times
the bridge number of $L$.
"
"  It is shown that the Orlik-Terao algebra is graded isomorphic to the special
fiber of the ideal $I$ generated by the $(n-1)$-fold products of the members of
a central arrangement of size $n$. This momentum is carried over to the Rees
algebra (blowup) of $I$ and it is shown that this algebra is of fiber-type and
Cohen-Macaulay. It follows by a result of Simis-Vasconcelos that the special
fiber of $I$ is Cohen-Macaulay, thus giving another proof of a result of
Proudfoot-Speyer about the Cohen-Macauleyness of the Orlik-Terao algebra.
"
"  In hybrid normed ideal perturbations of $n$-tuples of operators, the normed
ideal is allowed to vary with the component operators. We begin extending to
this setting the machinery we developed for normed ideal perturbations based on
the modulus of quasicentral approximation and an adaptation of our
non-commutative generalization of the Weyl--von~Neumann theorem. For commuting
$n$-tuples of hermitian operators, the modulus of quasicentral approximation
remains essentially the same when $\cC_n^-$ is replaced by a hybrid $n$-tuple
$\cC_{p_1,\dots}^-,\dots,\cC^-_{p_n}$, $p_1^{-1} + \dots + p_n^{-1} = 1$. The
proof involves singular integrals of mixed homogeneity.
"
"  This paper is concerned with radially symmetric solutions of systems of the
form \[ u_t = -\nabla V(u) + \Delta_x u \] where space variable $x$ and and
state-parameter $u$ are multidimensional, and the potential $V$ is coercive at
infinity. For such systems, under generic assumptions on the potential, the
asymptotic behaviour of solutions ""stable at infinity"", that is approaching a
spatially homogeneous equilibrium when $|x|$ approaches $+\infty$, is
investigated. It is proved that every such solutions approaches a stacked
family of radially symmetric bistable fronts travelling to infinity. This
behaviour is similar to the one of bistable solutions for gradient systems in
one unbounded spatial dimension, described in a companion paper. It is expected
(but unfortunately not proved at this stage) that behind these travelling
fronts the solution again behaves as in the one-dimensional case (that is, the
time derivative approaches zero and the solution approaches a pattern of
stationary solutions).
"
"  Let $G$ be a simple and finite graph without isolated vertices. In this paper
we study forcing sets (zero forcing sets) which induce a subgraph of $G$
without isolated vertices. Such a set is called a total forcing set, introduced
and first studied by Davila \cite{Davila}. The minimum cardinality of a total
forcing set in $G$ is the total forcing number of $G$, denoted $F_t(G)$. We
study basic properties of $F_t(G)$, relate $F_t(G)$ to various domination
parameters, and establish $NP$-completeness of the associated decision problem
for $F_t(G)$. We also prove that if $G$ is a connected graph of order $n \ge 3$
and maximum degree $\Delta$, then $F_t(G) \le ( \frac{\Delta}{\Delta +1} ) n$,
with equality if and only if $G$ is a complete graph $K_{\Delta + 1}$.
"
"  We study decay of small solutions of the Born-Infeld equation in 1+1
dimensions, a quasilinear scalar field equation modeling nonlinear
electromagnetism, as well as branes in String theory and minimal surfaces in
Minkowski space-times. From the work of Whitham, it is well-known that there is
no decay because of arbitrary solutions traveling to the speed of light just as
linear wave equation. However, even if there is no global decay in 1+1
dimensions, we are able to show that all globally small $H^{s+1}\times H^s$,
$s>\frac12$ solutions do decay to the zero background state in space, inside a
strictly proper subset of the light cone. We prove this result by constructing
a Virial identity related to a momentum law, in the spirit of works
\cite{KMM,KMM1}, as well as a Lyapunov functional that controls the $\dot H^1
\times L^2$ energy.
"
"  This paper establishes the almost sure convergence and asymptotic normality
of levels and differenced quasi maximum-likelihood (QML) estimators of dynamic
panel data models. The QML estimators are robust with respect to initial
conditions, conditional and time-series heteroskedasticity, and
misspecification of the log-likelihood. The paper also provides an ECME
algorithm for calculating levels QML estimates. Finally, it uses Monte Carlo
experiments to compare the finite sample performance of levels and differenced
QML estimators, the differenced GMM estimator, and the system GMM estimator. In
these experiments the QML estimators usually have smaller --- typically
substantially smaller --- bias and root mean squared errors than the panel data
GMM estimators.
"
"  We present a simple result that allows us to evaluate the asymptotic order of
the remainder of a partial asymptotic expansion of the quantile function $h(u)$
as $u\to 0^+$ or $1^-$. This is focussed on important univariate distributions
when $h(\cdot)$ has no simple closed form, with a view to assessing asymptotic
rate of decay to zero of tail dependence in the context of bivariate copulas.
The Introduction motivates the study in terms of the standard Normal. The
Normal, Skew-Normal and Gamma are used as initial examples. Finally, we discuss
approximation to the lower quantile of the Variance-Gamma and Skew-Slash
distributions.
"
"  In this paper, we deal with the null controllability of a population dynamics
model with an interior degenerate diffusion. To this end, we proved first a new
Carleman estimate for the full adjoint system and afterwards we deduce a
suitable observability inequality which will be needed to establish the
existence of a control acting on a subset of the space which lead the
population to extinction in a finite time.
"
"  We consider the problem of estimating the joint distribution $P$ of $n$
independent random variables within the Bayes paradigm from a non-asymptotic
point of view. Assuming that $P$ admits some density $s$ with respect to a
given reference measure, we consider a density model $\overline S$ for $s$ that
we endow with a prior distribution $\pi$ (with support $\overline S$) and we
build a robust alternative to the classical Bayes posterior distribution which
possesses similar concentration properties around $s$ whenever it belongs to
the model $\overline S$. Furthermore, in density estimation, the Hellinger
distance between the classical and the robust posterior distributions tends to
0, as the number of observations tends to infinity, under suitable assumptions
on the model and the prior, provided that the model $\overline S$ contains the
true density $s$. However, unlike what happens with the classical Bayes
posterior distribution, we show that the concentration properties of this new
posterior distribution are still preserved in the case of a misspecification of
the model, that is when $s$ does not belong to $\overline S$ but is close
enough to it with respect to the Hellinger distance.
"
"  This paper deals with the asymptotic behavior of solutions to the delayed
monostable equation: $(*)$ $u_{t}(t,x) = u_{xx}(t,x) - u(t,x) + g(u(t-h,x)),$
$x \in \mathbb{R},\ t >0,$ where $h>0$ and the reaction term $g: \mathbb{R}_+
\to \mathbb{R}_+$ has exactly two fixed points (zero and $\kappa >0$). Under
certain condition on the derivative of $g$ at $\kappa$, the global stability of
fast wavefronts is proved. Also, the stability of the $leading \ edge$ of
semi-wavefronts for $(*)$ with $g$ satisfying $g(u)\leq g'(0)u, u\in\R_+,$ is
established
"
"  We prove that the generalised Fibonacci group F(r,n) is infinite for (r,n) in
{(7 + 5k,5), (8 + 5k,5)} where k is greater than or equal to 0. This together
with previously known results yields a complete classification of the finite
F(r,n), a problem that has its origins in a question by J H Conway in 1965. The
method is to show that a related relative presentation is aspherical from which
it can be deduced that the groups are infinite.
"
"  A Rota--Baxter operator is an algebraic abstraction of integration, which is
the typical example of a weight zero Rota-Baxter operator. We show that
studying the modules over the polynomial Rota--Baxter algebra $(k[x],P)$ is
equivalent to studying the modules over the Jordan plane, and we generalize the
direct decomposability results for the $(k[x],P)$-modules in [Iy] from
algebraically closed fields of characteristic zero to fields of characteristic
zero. Furthermore, we provide a classification of Rota--Baxter modules up to
isomorphism based on indecomposable $k[x]$-modules.
"
"  We obtain the rigorous uniform asymptotics of a particular integral where a
stationary point is close to an endpoint. There exists a general method
introduced by Bleistein for obtaining uniform asymptotics in this situation.
However, this method does not provide rigorous estimates for the error. Indeed,
the method of Bleistein starts with a change of variables, which implies that
the parameter governing how close the stationary point is to the endpoint
appears in several parts of the integrand, and this means that one cannot
obtain general error bounds. By adapting the above method to our particular
integral, we obtain rigorous uniform leading-order asymptotics. We also give a
rigorous derivation of the asymptotics to all orders of the same integral; the
novelty of this second approach is that it does not involve a global change of
variables.
"
"  Recently, B.-Y. Chen and O. J. Garay studied pointwise slant submanifolds of
almost Hermitian manifolds. By using this notion, we investigate pointwise
semi-slant submanifolds and their warped products in Sasakian manifolds. We
give non-trivial examples of such submanifolds and obtain several fundamental
results, including a characterization for warped product pointwise semi-slant
submanifolds of Sasakian manifolds.
"
"  This paper is concerned with optimal control problems for systems governed by
mean-field stochastic differential equation, in which the control enters both
the drift and the diffusion coefficient. We prove that the relaxed state
process, associated with measure valued controls, is governed by an orthogonal
martingale measure rather that a Brownian motion. In particular, we show by a
counter example that replacing the drift and diffusion coefficient by their
relaxed counterparts does not define a true relaxed control problem. We
establish the existence of an optimal relaxed control, which can be
approximated by a sequence of strict controls. Moreover under some convexity
conditions, we show that the optimal control is realized by a strict control.
"
"  We show that every uniformly recurrent subgroup of a locally compact group is
the family of stabilizers of a minimal action on a compact space. More
generally, every closed invariant subset of the Chabauty space is the family of
stabilizers of an action on a compact space on which the stabilizer map is
continuous everywhere. This answers a question of Glasner and Weiss. We also
introduce the notion of a universal minimal flow relative to a uniformly
recurrent subgroup and prove its existence and uniqueness.
"
"  Excitation of waves in a three-layer acoustic wavegide is studied. The wave
field is presented as a sum of integrals. The summation is held over all
waveguide modes. The integration is performed over the temporal frequency axis.
The dispersion diagram of the waveguide is analytically continued, and the
integral is transformed by deformation of the integration contour into the
domain of complex frequencies. As the result, the expression for the fast
components of the signal (i.e. for the transient fields) is simplified.
The structure of the Riemann surface of the dispersion diagram of the
waveguide is studied. For this, a family of auxiliary problems indexed by the
parameters describing the links between layers is introduced. The family
depends on the linking parameters analytically, and the limiting case of weak
links can be solved analytically.
"
"  In this article we introduce a simple dynamical system called symplectic
billiards. As opposed to usual/Birkhoff billiards, where length is the
generating function, for symplectic billiards symplectic area is the generating
function. We explore basic properties and exhibit several similarities, but
also differences of symplectic billiards to Birkhoff billiards.
"
"  A filling Dehn surface in a $3$-manifold $M$ is a generically immersed
surface in $M$ that induces a cellular decomposition of $M$. Given a tame link
$L$ in $M$ there is a filling Dehn sphere of $M$ that ""trivializes""
(\emph{diametrically splits}) it. This allows to construct filling Dehn
surfaces in the coverings of $M$ branched over $L$. It is shown that one of the
simplest filling Dehn spheres of $S^3$ (Banchoff's sphere) diametrically splits
the trefoil knot. Filling Dehn spheres, and their Johansson diagrams, are
constructed for the coverings of $S^3$ branched over the trefoil. The
construction is explained in detail. Johansson diagrams for generic cyclic
coverings and for the simplest locally cyclic and irregular ones are
constructed explicitly, providing new proofs of known results about cyclic
coverings and the $3$-fold irregular covering over the trefoil.
"
"  We show that under a low complexity condition on the gradient of a
Hamiltonian, Gibbs distributions on the Boolean hypercube are approximate
mixtures of product measures whose probability vectors are critical points of
an associated mean-field functional. This extends a previous work by the first
author. As an application, we demonstrate how this framework helps characterize
both Ising models satisfying a mean-field condition and the conditional
distributions which arise in the emerging theory of nonlinear large deviations,
both in the dense case and in the polynomially-sparse case.
"
"  Under very general conditions it is shown that if $A$ is a uniform algebra
generated by real-analytic functions, then either $A$ consists of all
continuous functions or else there exists a disc on which every function in $A$
is holomorphic. This strengthens several earlier results concerning uniform
algebras generated by real-analytic functions.
"
"  This paper is intended to be a further step through our Killing spinor
programme started with Class. Quantum Grav. \textbf{32}, 175007 (2015), and we
will advance our programme in accordance with the road map recently given in
arXiv:1611.04424v2. In the latter reference many open problems were declared,
one of which contained the uncovered relations between specific spinors in
spacetime represented by an arrow diagram built upon them. This work deals with
one of the arrows with almost all of its details and ends up with an important
physical interpretation of this setup in terms of the quantum electrodynamical
pair annihilation process. This method will shed light on the classification of
pseudo-Riemannian manifolds admitting twistors in connection with the
classification problem related to Killing spinors. Many physical
interpretations are given during the text some of which include dynamics of
brane immersions, quantum field theoretical considerations and black hole
evaporation.
"
"  We investigate the universal cover of a topological group that is not
necessarily connected. Its existence as a topological group is governed by a
Taylor cocycle, an obstruction in 3-cohomology. Alternatively, it always exists
as a topological 2-group. The splitness of this 2-group is also governed by an
obstruction in 3-cohomology, a Sinh cocycle. We give explicit formulas for both
obstructions and show that they are inverse of each other.
"
"  An edited version is given of the text of Gödel's unpublished manuscript of
the notes for a course in basic logic he delivered at the University of Notre
Dame in 1939. Gödel's notes deal with what is today considered as important
logical problems par excellence, completeness, decidability, independence of
axioms, and with natural deduction too, which was all still a novelty at the
time the course was delivered. Full of regards towards beginners, the notes are
not excessively formalistic. Gödel presumably intended them just for himself,
and they are full of abbreviations. This together with some other matters (like
two versions of the same topic, and guessing the right order of the pages)
required additional effort to obtain a readable edited version. Because of the
quality of the material provided by Gödel, including also important
philosophical points, this effort should however be worthwhile. The edited
version of the text is accompanied by another version, called the source
version, which is quite close to Gödel's manuscript. It is meant to be a
record of the editorial interventions involved in producing the edited version
(in particular, how the abbreviations were disabridged), and a justification of
that later version.
"
"  Minimax lower bounds are pessimistic in nature: for any given estimator,
minimax lower bounds yield the existence of a worst-case target vector
$\beta^*_{worst}$ for which the prediction error of the given estimator is
bounded from below. However, minimax lower bounds shed no light on the
prediction error of the given estimator for target vectors different than
$\beta^*_{worst}$. A characterization of the prediction error of any convex
regularized least-squares is given. This characterization provide both a lower
bound and an upper bound on the prediction error. This produces lower bounds
that are applicable for any target vector and not only for a single, worst-case
$\beta^*_{worst}$. Finally, these lower and upper bounds on the prediction
error are applied to the Lasso is sparse linear regression. We obtain a lower
bound involving the compatibility constant for any tuning parameter, matching
upper and lower bounds for the universal choice of the tuning parameter, and a
lower bound for the Lasso with small tuning parameter.
"
"  Z^d-extensions of probability-preserving dynamical systems are themselves
dynamical systems preserving an infinite measure, and generalize random walks.
Using the method of moments, we prove a generalized central limit theorem for
additive functionals of the extension of integral zero, under spectral
assumptions. As a corollary, we get the fact that Green-Kubo's formula is
invariant under induction. This allows us to relate the hitting probability of
sites with the symmetrized potential kernel, giving an alternative proof and
generalizing a theorem of Spitzer. Finally, this relation is used to improve in
turn the asumptions of the generalized central limit theorem. Applications to
Lorentz gases in finite horizon and to the geodesic flow on abelian covers of
compact manifolds of negative curvature are discussed.
"
"  An ADE Dynkin diagram gives rise to a family of algebraic curves. In this
paper, we use arithmetic invariant theory to study the integral points of the
curves associated to the exceptional diagrams $E_6, E_7$, $E_8$. These curves
are non-hyperelliptic of genus 3 or 4. We prove that a positive proportion of
each family consists of curves with integral points everywhere locally but no
integral points globally.
"
"  Symmetric nonnegative matrix factorization (SymNMF) has important
applications in data analytics problems such as document clustering, community
detection and image segmentation. In this paper, we propose a novel nonconvex
variable splitting method for solving SymNMF. The proposed algorithm is
guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points of the
nonconvex SymNMF problem. Furthermore, it achieves a global sublinear
convergence rate. We also show that the algorithm can be efficiently
implemented in parallel. Further, sufficient conditions are provided which
guarantee the global and local optimality of the obtained solutions. Extensive
numerical results performed on both synthetic and real data sets suggest that
the proposed algorithm converges quickly to a local minimum solution.
"
"  Dimer algebras arise from a particular type of quiver gauge theory. However,
part of the input to such a theory is the gauge group, and this choice may
impose additional constraints on the algebra. If the gauge group of a dimer
theory is abelian, then the algebra that arises is not actually the dimer
algebra itself, but a particular quotient we introduce called the 'homotopy
algebra'. We show that a homotopy algebra $\Lambda$ on a torus is a dimer
algebra if and only if it is noetherian, and otherwise $\Lambda$ is the
quotient of a dimer algebra by homotopy relations. Stated in physics terms, a
dimer theory is superconformal if and only if the corresponding dimer and
homotopy algebras coincide. We also give an explicit description of the center
of a homotopy algebra in terms of a special subset of its perfect matchings. In
our proofs we introduce formalized notions of Higgsing and the mesonic chiral
ring from quiver gauge theory.
"
"  The subject is traces of Sobolev spaces with mixed Lebesgue norms on
Euclidean space. Specifically, restrictions to the hyperplanes given by the
first and last coordinates are applied to functions belonging to
quasi-homogeneous, mixed-norm Lizorkin--Triebel spaces; Sobolev spaces are
obtained from these as special cases. Spaces admitting traces in the
distribution sense are characterised except for the borderline cases; these are
also covered in case of the first variable. With respect to the first variable
the trace spaces are proved to be mixed-norm Lizorkin--Triebel spaces with a
specific sum exponent. For the last variable they are similarly defined Besov
spaces. The treatment includes continuous right-inverses and higher order
traces. The results rely on a sequence version of Nikolskij's inequality,
Marschall's inequality for pseudo-differential operators (and Fourier
multiplier assertions), as well as dyadic ball criteria.
"
"  Models with many signals, high-dimensional models, often impose structures on
the signal strengths. The common assumption is that only a few signals are
strong and most of the signals are zero or close (collectively) to zero.
However, such a requirement might not be valid in many real-life applications.
In this article, we are interested in conducting large-scale inference in
models that might have signals of mixed strengths. The key challenge is that
the signals that are not under testing might be collectively non-negligible
(although individually small) and cannot be accurately learned. This article
develops a new class of tests that arise from a moment matching formulation. A
virtue of these moment-matching statistics is their ability to borrow strength
across features, adapt to the sparsity size and exert adjustment for testing
growing number of hypothesis. GRoup-level Inference of Parameter, GRIP, test
harvests effective sparsity structures with hypothesis formulation for an
efficient multiple testing procedure. Simulated data showcase that GRIPs error
control is far better than the alternative methods. We develop a minimax
theory, demonstrating optimality of GRIP for a broad range of models, including
those where the model is a mixture of a sparse and high-dimensional dense
signals.
"
"  It is shown that for a solvable subgroup $G$ of an almost simple group $S$
which socle is isomorphic to $A_n$ $ (n\ge5)$ there are $x,y,z,t \in S$ such
that $G \cap G^x \cap G^y \cap G^z \cap G^t =1.$
"
"  Motivated by truncated EM method introduced by Mao (2015), a new explicit
numerical method named modified truncated Euler-Maruyama method is developed in
this paper. Strong convergence rates of the given numerical scheme to the exact
solutions to stochastic differential equations are investigated under given
conditions in this paper. Compared with truncated EM method, the given
numerical simulation strongly converges to the exact solution at fixed time $T$
and over a time interval $[0,T]$ under weaker sufficient conditions. Meanwhile,
the convergence rates are also obtained for both cases. Two examples are
provided to support our conclusions.
"
"  This expository paper is concerned with the properties of proper holomorphic
mappings between domains in complex affine spaces. We discuss some of the main
geometric methods of this theory, such as the Reflection Principle, the scaling
method, and the Kobayashi-Royden metric. We sketch the proofs of certain
principal results and discuss some recent achievements. Several open problems
are also stated.
"
"  Dual spectral computed tomography (DSCT) can achieve energy- and
material-selective images, and has a superior distinguishability of some
materials than conventional single spectral computed tomography (SSCT).
However, the decomposition process is illposed, which is sensitive with noise,
thus the quality of decomposed images are usually degraded, especially the
signal-to-noise ratio (SNR) is much lower than single spectra based directly
reconstructions. In this work, we first establish a local linear relationship
between dual spectra based decomposed results and single spectra based directly
reconstructed images. Then, based on this constraint, we propose an
optimization model for DSCT and develop a guided image filtering based
iterative solution method. Both simulated and real experiments are provided to
validate the effectiveness of the proposed approach.
"
"  We propose a method for variable selection in discriminant analysis with
mixed categorical and continuous variables. This method is based on a criterion
that permits to reduce the variable selection problem to a problem of
estimating suitable permutation and dimensionality. Then, estimators for these
parameters are proposed and the resulting method for selecting variables is
shown to be consistent. A simulation study that permits to study several
poperties of the proposed approach and to compare it with an existing method is
given.
"
"  A high order wavelet integral collocation method (WICM) is developed for
general nonlinear boundary value problems in physics. This method is
established based on Coiflet approximation of multiple integrals of interval
bounded functions combined with an accurate and adjustable boundary extension
technique. The convergence order of this approximation has been proven to be N
as long as the Coiflet with N-1 vanishing moment is adopted, which can be any
positive even integers. Before the conventional collocation method is applied
to the general problems, the original differential equation is changed into its
equivalent form by denoting derivatives of the unknown function as new
functions and constructing relations between the low and high order
derivatives. For the linear cases, error analysis has proven that the proposed
WICM is order N, and condition numbers of relevant matrices are almost
independent of the number of collocation points. Numerical examples of a wide
range of nonlinear differential equations in physics demonstrate that accuracy
of the proposed WICM is even greater than N, and most interestingly, such
accuracy is independent of the order of the differential equation to be solved.
Comparison to existing numerical methods further justifies the accuracy and
efficiency of the proposed method.
"
"  We study the Gevrey character of a natural parameterization of one
dimensional invariant manifolds associated to a parabolic direction of fixed
points of analytic maps, that is, a direction associated with an eigenvalue
equal to $1$. We show that, under general hypotheses, these invariant manifolds
are Gevrey with type related to some explicit constants. We provide examples of
the optimality of our results as well as some applications to celestial
mechanics, namely, the Sitnikov problem and the restricted planar three body
problem.
"
"  A meticulous assessment of the risk of extreme environmental events is of
great necessity for populations, civil authorities as well as the
insurance/reinsurance industry. Koch (2017, 2018) introduced a concept of
spatial risk measure and a related set of axioms which are well-suited to
analyse and quantify the risk due to events having a spatial extent, precisely
such as natural disasters. In this paper, we first carry out a detailed study
of the correlation (and covariance) structure of powers of the Smith and
Brown-Resnick max-stable random fields. Then, using the latter results, we
thoroughly investigate spatial risk measures associated with variance and
induced by powers of max-stable random fields. In addition, we show that
spatial risk measures associated with several classical risk measures and
induced by such cost fields satisfy (at least) part of the previously mentioned
axioms under appropriate conditions on the max-stable fields. Considering such
cost fields is particularly relevant when studying the impact of extreme wind
speeds on buildings and infrastructure.
"
"  This paper deals with the homogenization problem of one-dimensional
pseudo-elliptic equations with a rapidly varying random potential. The main
purpose is to characterize the homogenization error (random fluctuations),
i.e., the difference between the random solution and the homogenized solution,
which strongly depends on the autocovariance property of the underlying random
potential. It is well known that when the random potential has short-range
dependence, the rescaled homogenization error converges in distribution to a
stochastic integral with respect to standard Brownian motion. Here, we are
interested in potentials with long-range dependence and we prove convergence to
stochastic integrals with respect to Hermite process.
"
"  Traditional centralized energy systems have the disadvantages of difficult
management and insufficient incentives. Blockchain is an emerging technology,
which can be utilized in energy systems to enhance their management and
control. Integrating token economy and blockchain technology, token economic
systems in energy possess the characteristics of strong incentives and low
cost, facilitating integrating renewable energy and demand side management, and
providing guarantees for improving energy efficiency and reducing emission.
This article describes the concept and functionality of token economics, and
then analyzes the feasibility of applying token economics in the energy
systems, and finally discuss the applications of token economics with an
example in integrated energy systems.
"
"  Let $(\mathbb{X} , d, \mu )$ be a proper metric measure space and let $\Omega
\subset \mathbb{X}$ be a bounded domain. For each $x\in \Omega$, we choose a
radius $0< \varrho (x) \leq \mathrm{dist}(x, \partial \Omega ) $ and let $B_x$
be the closed ball centered at $x$ with radius $\varrho (x)$. If $\alpha \in
\mathbb{R}$, consider the following operator in $C( \overline{\Omega} )$, $$
\mathcal{T}_{\alpha}u(x)=\frac{\alpha}{2}\left(\sup_{B_x } u+\inf_{B_x }
u\right)+(1-\alpha)\,\frac{1}{\mu(B_x)}\int_{B_x}\hspace{-0.1cm} u\ d\mu. $$
Under appropriate assumptions on $\alpha$, $\mathbb{X}$, $\mu$ and the radius
function $\varrho$ we show that solutions $u\in C( \overline{\Omega} )$ of the
functional equation $\mathcal{T}_{\alpha}u = u$ satisfy a local Hölder or
Lipschitz condition in $\Omega$. The motivation comes from the so called
$p$-harmonious functions in euclidean domains.
"
"  We consider Friedlander's wave equation in two space dimensions in the
half-space x > 0 with the boundary condition u(x,y,t)=0 when x=0. For a
Gaussian beam w(x,y,t;k) concentrated on a ray path that is tangent to x=0 at
(x,y,t)=(0,0,0) we calculate the ""reflected"" wave z(x,y,t;k) in t > 0 such that
w(x,y,t;k)+z(x,y,t;k) satisfies Friedlander's wave equation and vanishes on
x=0. These computations are done to leading order in k on the ray path. The
interaction of beams with boundaries has been studied for non-tangential beams
and for beams gliding along the boundary. We find that the amplitude of the
solution on the central ray for large k after leaving the boundary is very
nearly one half of that of the incoming beam.
"
"  In this paper we study the following multi-parameter variant of the
celebrated Falconer distance problem. Given ${\textbf{d}}=(d_1,d_2, \dots,
d_{\ell})\in \mathbb{N}^{\ell}$ with $d_1+d_2+\dots+d_{\ell}=d$ and $E
\subseteq \mathbb{R}^d$, we define $$ \Delta_{\textbf{d}}(E) = \left\{
\left(|x^{(1)}-y^{(1)}|,\ldots,|x^{(\ell)}-y^{(\ell)}|\right) : x,y \in E
\right\} \subseteq \mathbb{R}^{\ell}, $$ where for $x\in \mathbb{R}^d$ we write
$x=\left( x^{(1)},\dots, x^{(\ell)} \right)$ with $x^{(i)} \in
\mathbb{R}^{d_i}$.
We ask how large does the Hausdorff dimension of $E$ need to be to ensure
that the $\ell$-dimensional Lebesgue measure of $\Delta_{\textbf{d}}(E)$ is
positive? We prove that if $2 \leq d_i$ for $1 \leq i \leq \ell$, then the
conclusion holds provided $$ \dim(E)>d-\frac{\min d_i}{2}+\frac{1}{3}.$$ We
also note that, by previous constructions, the conclusion does not in general
hold if $$\dim(E)<d-\frac{\min d_i}{2}.$$ A group action derivation of a
suitable Mattila integral plays an important role in the argument.
"
"  In this paper we prove a global result for the Schrödinger map problem with
initial data with small Besov norm at critical regularity.
"
"  We give a complete description of the congruence lattices of the following
finite diagram monoids: the partition monoid, the planar partition monoid, the
Brauer monoid, the Jones monoid (also known as the Temperley-Lieb monoid), the
Motzkin monoid, and the partial Brauer monoid. All the congruences under
discussion arise as special instances of a new construction, involving an ideal
I, a retraction I->M onto the minimal ideal, a congruence on M, and a normal
subgroup of a maximal subgroup outside I.
"
"  Let $\sf X$ be a symplectic orbifold groupoid with $\sf S$ being a symplectic
sub-orbifold groupoid, and $\sf X_{\mathfrak a}$ be the weight-$\mathfrak a$
blowup of $\sf X$ along $\sf S$ with $\sf Z$ being the corresponding
exceptional divisor. We show that there is a weighted blowup correspondence
between some certain absolute orbifold Gromov--Witten invariants of $\sf X$
relative to $\sf S$ and some certain relative orbifold Gromov--Witten
invariants of the pair $(\sf X_{\mathfrak a}|Z)$. As an application, we prove
that the symplectic uniruledness of symplectic orbifold groupoids is a weighted
blowup invariant.
"
"  This paper is focused on dimension-free PAC-Bayesian bounds, under weak
polynomial moment assumptions, allowing for heavy tailed sample distributions.
It covers the estimation of the mean of a vector or a matrix, with applications
to least squares linear regression. Special efforts are devoted to the
estimation of Gram matrices, due to their prominent role in high-dimension data
analysis.
"
"  The paper surveys topological problems relevant to the motion planning
problem of robotics and includes some new results and constructions. First we
analyse the notion of topological complexity of configuration spaces which is
responsible for discontinuities in algorithms for robot navigation. Then we
present explicit motion planning algorithms for coordinated collision free
control of many particles moving in Euclidean spaces or on graphs. These
algorithms are optimal in the sense that they have minimal number of regions of
continuity. Moreover, we describe in full detail the topology of configuration
spaces of two particles on a tree and use it to construct some top-dimensional
cohomology classes in configuration spaces of n particles on a tree.
"
"  The GIKN construction was introduced by Gorodetski, Ilyashenko, Kleptsyn, and
Nalsky in [Functional Analysis and its Applications, 39 (2005), 21--30]. It
gives a nonhyperbolic ergodic measure which is a weak$^*$ limit of a special
sequence of measures supported on periodic orbits. This method was later
adapted by numerous authors and provided examples of nonhyperbolic invariant
measures in various settings. We prove that the result of the GIKN construction
is always a loosely Kronecker measure in the sense of Ornstein, Rudolph, and
Weiss (equivalently, standard measure in the sense of Katok, another name is
loosely Bernoulli measure with zero entropy). For a proof we introduce and
study the Feldman-Katok pseudometric $\bar{F_{K}}$. The pseudodistance
$\bar{F_{K}}$ is a topological counterpart of the $\bar f$ metric for
finite-state stationary stochastic processes introduced by Feldman and,
independently, by Katok, later developed by Ornstein, Rudolph, and Weiss. We
show that every measure given by the GIKN construction is the
$\bar{F_{K}}$-limit of a sequence of periodic measures. On the other hand we
prove that a measure which is the $\bar{F_{K}}$-limit of a sequence of ergodic
measures is ergodic and its entropy is smaller or equal than the lower limit of
entropies of measures in the sequence. Furthermore we demonstrate that
$\bar{F_{K}}$-Cauchy sequence of periodic measures tends in the weak$^*$
topology either to a periodic measure or to a loosely Kronecker measure.
"
"  This article sets forth results on the existence, a priori estimates and
boundedness of positive solutions of a singular quasilinear systems of elliptic
equations involving variable exponents. The approach is based on Schauder's
fixed point Theorem. A Moser iteration procedure is also obtained for singular
cooperative systems involving variable exponents establishing a priori
estimates and boundedness of solutions.
"
"  In this paper we characterize planar central configurations in terms of a
sectional curvature value of the Jacobi-Maupertuis metric. This
characterization works for the $N$-body problem with general masses and any
$1/r^{\alpha}$ potential with $\alpha> 0$. We also observe dynamical
consequences of these curvature values for relative equilibrium solutions.
These curvature methods work well for strong forces ($\alpha \ge 2$).
"
"  Subordinate diffusions are constructed by time changing diffusion processes
with an independent Lévy subordinator. This is a rich family of Markovian
jump processes which exhibit a variety of jump behavior and have found many
applications. This paper studies parametric inference of discretely observed
ergodic subordinate diffusions. We solve the identifiability problem for these
processes using spectral theory and propose a two-step estimation procedure
based on estimating functions. In the first step, we use an estimating function
that only involves diffusion parameters. In the second step, a martingale
estimating function based on eigenvalues and eigenfunctions of the subordinate
diffusion is used to estimate the parameters of the Lévy subordinator and
the problem of how to choose the weighting matrix is solved. When the
eigenpairs do not have analytical expressions, we apply the constant
perturbation method with high order corrections to calculate them numerically
and the martingale estimating function can be computed efficiently. Consistency
and asymptotic normality of our estimator are established considering the
effect of numerical approximation. Through numerical examples, we show that our
method is both computationally and statistically efficient. A subordinate
diffusion model for VIX (CBOE volatility index) is developed which provides
good fit to the data.
"
"  In the setting of finite type invariants for null-homologous knots in
rational homology 3-spheres with respect to null Lagrangian-preserving
surgeries, there are two candidates to be universal invariants, defined
respectively by Kricker and Lescop. In a previous paper, the second author
defined maps between spaces of Jacobi diagrams. Injectivity for these maps
would imply that Kricker and Lescop invariants are indeed universal invariants;
this would prove in particular that these two invariants are equivalent. In the
present paper, we investigate the injectivity status of these maps for degree 2
invariants, in the case of knots whose Blanchfield modules are direct sums of
isomorphic Blanchfield modules of Q-- dimension two. We prove that they are
always injective except in one case, for which we determine explicitly the
kernel.
"
"  We adapt the well-known spectral decimation technique for computing spectra
of Laplacians on certain symmetric self-similar sets to the case of magnetic
Schrodinger operators and work through this method completely for the diamond
lattice fractal. This connects results of physicists from the 1980's, who used
similar techniques to compute spectra of sequences of magnetic operators on
graph approximations to fractals but did not verify existence of a limiting
fractal operator, to recent work describing magnetic operators on fractals via
functional analytic techniques.
"
"  We develop a calculus for diagrams of knotted objects. We define Arrow
presentations, which encode the crossing informations of a diagram into arrows
in a way somewhat similar to Gauss diagrams, and more generally w-tree
presentations, which can be seen as `higher order Gauss diagrams'. This Arrow
calculus is used to develop an analogue of Habiro's clasper theory for welded
knotted objects, which contain classical link diagrams as a subset. This
provides a 'realization' of Polyak's algebra of arrow diagrams at the welded
level, and leads to a characterization of finite type invariants of welded
knots and long knots. As a corollary, we recover several topological results
due to K. Habiro and A. Shima and to T. Watanabe on knotted surfaces in
4-space. We also classify welded string links up to homotopy, thus recovering a
result of the first author with B. Audoux, P. Bellingeri and E. Wagner.
"
"  Schmidt's game, and other similar intersection games have played an important
role in recent years in applications to number theory, dynamics, and
Diophantine approximation theory. These games are real games, that is, games in
which the players make moves from a complete separable metric space. The
determinacy of these games trivially follows from the axiom of determinacy for
real games, $\mathsf{AD}_\mathbb{R}$, which is a much stronger axiom than that
asserting all integer games are determined, $\mathsf{AD}$. One of our main
results is a general theorem which under the hypothesis $\mathsf{AD}$ implies
the determinacy of intersection games which have a property allowing strategies
to be simplified. In particular, we show that Schmidt's $(\alpha,\beta,\rho)$
game on $\mathbb{R}$ is determined from $\mathsf{AD}$ alone, but on
$\mathbb{R}^n$ for $n \geq 3$ we show that $\mathsf{AD}$ does not imply the
determinacy of this game. We also prove several other results specifically
related to the determinacy of Schmidt's game. These results highlight the
obstacles in obtaining the determinacy of Schmidt's game from $\mathsf{AD}$.
"
"  This paper is mainly inspired by the conjecture about the existence of bound
states for magnetic Neumann Laplacians on planar wedges of any aperture
$\phi\in (0,\pi)$. So far, a proof was only obtained for apertures
$\phi\lesssim 0.511\pi$. The conviction in the validity of this conjecture for
apertures $\phi\gtrsim 0.511\pi$ mainly relied on numerical computations. In
this paper we succeed to prove the existence of bound states for any aperture
$\phi \lesssim 0.583\pi$ using a variational argument with suitably chosen test
functions. Employing some more involved test functions and combining a
variational argument with computer-assistance, we extend this interval up to
any aperture $\phi \lesssim 0.595\pi$. Moreover, we analyse the same question
for closely related problems concerning magnetic Robin Laplacians on wedges and
for magnetic Schrödinger operators in the plane with $\delta$-interactions
supported on broken lines.
"
"  Last decade witnesses significant methodological and theoretical advances in
estimating large precision matrices. In particular, there are scientific
applications such as longitudinal data, meteorology and spectroscopy in which
the ordering of the variables can be interpreted through a bandable structure
on the Cholesky factor of the precision matrix. However, the minimax theory has
still been largely unknown, as opposed to the well established minimax results
over the corresponding bandable covariance matrices. In this paper, we focus on
two commonly used types of parameter spaces, and develop the optimal rates of
convergence under both the operator norm and the Frobenius norm. A striking
phenomenon is found: two types of parameter spaces are fundamentally different
under the operator norm but enjoy the same rate optimality under the Frobenius
norm, which is in sharp contrast to the equivalence of corresponding two types
of bandable covariance matrices under both norms. This fundamental difference
is established by carefully constructing the corresponding minimax lower
bounds. Two new estimation procedures are developed: for the operator norm, our
optimal procedure is based on a novel local cropping estimator targeting on all
principle submatrices of the precision matrix while for the Frobenius norm, our
optimal procedure relies on a delicate regression-based block-thresholding
rule. We further establish rate optimality in the nonparanormal model.
Numerical studies are carried out to confirm our theoretical findings.
"
"  For a second order operator on a compact manifold satisfying the strong
Hörmander condition, we give a bound for the spectral gap analogous to the
Lichnerowicz estimate for the Laplacian of a Riemannian manifold. We consider a
wide class of such operators which includes horizontal lifts of the Laplacian
on Riemannian submersions with minimal leaves.
"
"  We prove that for any dimension function $h$ with $h \prec x^d$ and for any
countable set of linear patterns, there exists a compact set $E$ with
$\mathcal{H}^h(E)>0$ avoiding all the given patterns. We also give several
applications and recover results of Keleti, Maga, and Máthé.
"
"  As a counterpart of the classical Yamabe problem, a fractional Yamabe flow
has been introduced by Jin and Xiong (2014) on the sphere. Here we pursue its
study in the context of general compact smooth manifolds with positive
fractional curvature. First, we prove that the flow is locally well posed in
the weak sense on any compact manifold. If the manifold is locally conformally
flat with positive Yamabe invariant, we also prove that the flow is smooth and
converges to a constant scalar curvature metric. We provide different proofs
using extension properties introduced by Chang and González (2011) for the
conformally covariant fractional order operators.
"
"  We provide an explicit presentation of an infinite hyperbolic Kazhdan group
with $4$ generators and $16$ relators of length at most $73$. That group acts
properly and cocompactly on a hyperbolic triangle building of type $(3,4,4)$.
We also point out a variation of the construction that yields examples of
lattices in $\tilde A_2$-buildings admitting non-Desarguesian residues of
arbitrary prime power order.
"
"  Bayesian hierarchical models are increasingly popular for realistic modelling
and analysis of complex data. This trend is accompanied by the need for
flexible, general, and computationally efficient methods for model criticism
and conflict detection. Usually, a Bayesian hierarchical model incorporates a
grouping of the individual data points, for example individuals in repeated
measurement data. In such cases, the following question arises: Are any of the
groups ""outliers"", or in conflict with the remaining groups? Existing general
approaches aiming to answer such questions tend to be extremely computationally
demanding when model fitting is based on MCMC. We show how group-level model
criticism and conflict detection can be done quickly and accurately through
integrated nested Laplace approximations (INLA). The new method is implemented
as a part of the open source R-INLA package for Bayesian computing
(this http URL).
"
"  We consider a Kepler problem in dimension two or three, with a time-dependent
$T$-periodic perturbation. We prove that for any prescribed positive integer
$N$, there exist at least $N$ periodic solutions (with period $T$) as long as
the perturbation is small enough. Here the solutions are understood in a
general sense as they can have collisions. The concept of generalized solutions
is defined intrinsically and it coincides with the notion obtained in Celestial
Mechanics via the theory of regularization of collisions.
"
"  Knaster continua and solenoids are well-known examples of indecomposable
continua whose composants (maximal arcwise-connected subsets) are one-to-one
images of lines. We show that essentially all non-trivial one-to-one composant
images of (half-)lines are indecomposable. And if $f$ is a one-to-one mapping
of $[0,\infty)$ or $(-\infty,\infty)$, then there is an indecomposable
continuum of which $X:=$ran$(f)$ is a composant if and only if $f$ maps all
final or initial segments densely and every non-closed sequence of arcs in $X$
has a convergent subsequence in the hyperspace $K(X)\cup \{X\}$. We also prove
the existence of composant-preserving embeddings in Euclidean $3$-space.
Accompanying the proofs are illustrations and examples.
"
"  We introduce a class of theories called metastable, including the theory of
algebraically closed valued fields (ACVF) as a motivating example. The key
local notion is that of definable types dominated by their stable part. A
theory is metastable (over a sort $\Gamma$) if every type over a sufficiently
rich base structure can be viewed as part of a $\Gamma$-parametrized family of
stably dominated types. We initiate a study of definable groups in metastable
theories of finite rank. Groups with a stably dominated generic type are shown
to have a canonical stable quotient. Abelian groups are shown to be
decomposable into a part coming from $\Gamma$, and a definable direct limit
system of groups with stably dominated generic. In the case of ACVF, among
definable subgroups of affine algebraic groups, we characterize the groups with
stably dominated generics in terms of group schemes over the valuation ring.
Finally, we classify all fields definable in ACVF.
"
"  We prove, by a computer aided proof, the existence of noise induced order in
the model of chaotic chemical reactions where it was first discovered
numerically by Matsumoto and Tsuda in 1983. We prove that in this random
dynamical system the increase in amplitude of the noise causes the Lyapunov
exponent to decrease from positive to negative, stabilizing the system. The
method used is based on a certified approximation of the stationary measure in
the $L^{1}$ norm. This is done by an efficient algorithm which is general
enough to be adapted to any piecewise differentiable dynamical system on the
interval with additive noise. We also prove that the stationary measure of the
system and its Lyapunov exponent have a Lipschitz stability under several kinds
of perturbation of the noise and of the system itself. The Lipschitz constants
of this stability result are also estimated explicitly.
"
"  Extensions and generalizations of Alzer's inequality; which is of Wirtinger
type are proved. As applications, sharp trapezoid type inequality and sharp
bound for the geometric mean are deduced.
"
"  Let M be ternary, homogeneous and simple. We prove that if M is finitely
constrained, then it is supersimple with finite SU-rank and dependence is
$k$-trivial for some $k < \omega$ and for finite sets of real elements. Now
suppose that, in addition, M is supersimple with SU-rank 1. If M is finitely
constrained then algebraic closure in M is trivial. We also find connections
between the nature of the constraints of M, the nature of the amalgamations
allowed by the age of M, and the nature of definable equivalence relations. A
key method of proof is to ""extract"" constraints (of M) from instances of
dividing and from definable equivalence relations. Finally, we give new
examples, including an uncountable family, of ternary homogeneous supersimple
structures of SU-rank 1.
"
"  Let $\mathbb{K}$ be the algebraic closure of a finite field $\mathbb{F}_q$ of
odd characteristic $p$. For a positive integer $m$ prime to $p$, let
$F=\mathbb{K}(x,y)$ be the transcendency degree $1$ function field defined by
$y^q+y=x^m+x^{-m}$. Let $t=x^{m(q-1)}$ and $H=\mathbb{K}(t)$. The extension
$F|H$ is a non-Galois extension. Let $K$ be the Galois closure of $F$ with
respect to $H$. By a result of Stichtenoth, $K$ has genus $g(K)=(qm-1)(q-1)$,
$p$-rank (Hasse-Witt invariant) $\gamma(K)=(q-1)^2$ and a
$\mathbb{K}$-automorphism group of order at least $2q^2m(q-1)$. In this paper
we prove that this subgroup is the full $\mathbb{K}$-automorphism group of $K$;
more precisely $Aut_{\mathbb {K}}(K)=Q\rtimes D$ where $Q$ is an elementary
abelian $p$-group of order $q^2$ and $D$ has a index $2$ cyclic subgroup of
order $m(q-1)$. In particular, $\sqrt{m}|Aut_{\mathbb{K}}(K)|> g(K)^{3/2}$, and
if $K$ is ordinary (i.e. $g(K)=\gamma(K)$) then
$|Aut_{\mathbb{K}}(K)|>g^{3/2}$. On the other hand, if $G$ is a solvable
subgroup of the $\mathbb{K}$-automorphism group of an ordinary, transcendency
degree $1$ function field $L$ of genus $g(L)\geq 2$ defined over $\mathbb{K}$,
then by a result due to Korchmáros and Montanucci, $|Aut_{\mathbb{K}}(K)|\le
34 (g(L)+1)^{3/2}<68\sqrt{2}g(L)^{3/2}$. This shows that $K$ hits this bound up
to the constant $68\sqrt{2}$.
Since $Aut_{\mathbb{K}}(K)$ has several subgroups, the fixed subfield $F^N$
of such a subgroup $N$ may happen to have many automorphisms provided that the
normalizer of $N$ in $Aut_{\mathbb{K}}(K)$ is large enough. This possibility is
worked out for subgroups of $Q$.
"
"  Let $K(B_{\ell_p^n},B_{\ell_q^n}) $ be the $n$-dimensional $(p,q)$-Bohr
radius for holomorphic functions on $\mathbb C^n$. That is,
$K(B_{\ell_p^n},B_{\ell_q^n}) $ denotes the greatest constant $r\geq 0$ such
that for every entire function $f(z)=\sum_{\alpha} c_{\alpha} z^{\alpha}$ in
$n$-complex variables, we have the following (mixed) Bohr-type inequality
$$\sup_{z \in r \cdot B_{\ell_q^n}} \sum_{\alpha} | c_{\alpha} z^{\alpha} |
\leq \sup_{z \in B_{\ell_p^n}} | f(z) |,$$ where $B_{\ell_r^n}$ denotes the
closed unit ball of the $n$-dimensional sequence space $\ell_r^n$.
For every $1 \leq p, q \leq \infty$, we exhibit the exact asymptotic growth
of the $(p,q)$-Bohr radius as $n$ (the number of variables) goes to infinity.
"
"  We consider the stochastic damped Navier-Stokes equations in $\mathbb R^d$
($d=2,3$), assuming as in our previous work [4] that the covariance of the
noise is not too regular, so Itô calculus cannot be applied in the space of
finite energy vector fields. We prove the existence of an invariant measure
when $d=2$ and of a stationary solution when $d=3$.
"
"  A compact circle-packing $P$ of the Euclidean plane is a set of circles which
bound mutually disjoint open discs with the property that, for every circle
$S\in P$, there exists a maximal indexed set $\{A_{0},\ldots,A_{n-1}\}\subseteq
P$ so that, for every $i\in\{0,\ldots,n-1\}$, the circle $A_{i}$ is tangent to
both circles $S$ and $A_{i+1\mod n}$ .
We show that there exist at most $11462$ pairs $(r,s)$ with $0<s<r<1$ for
which there exist a compact circle-packing of the plane consisting of circles
with radii $s$, $r$ and $1$.
We discuss computing the exact values of such $0<s<r<1$ as roots of
polynomials and exhibit a selection of compact circle-packings consisting of
circles of three radii. We also discuss the apparent infeasibility of computing
all these values on contemporary consumer hardware.
"
"  This paper considers the optimal modification of the likelihood ratio test
(LRT) for the equality of two high-dimensional covariance matrices. The
classical LRT is not well defined when the dimensions are larger than or equal
to one of the sample sizes. In this paper, an optimally modified test that
works well in cases where the dimensions may be larger than the sample sizes is
proposed. In addition, the test is established under the weakest conditions on
the moments and the dimensions of the samples. We also present weakly
consistent estimators of the fourth moments, which are necessary for the
proposed test, when they are not equal to 3. From the simulation results and
real data analysis, we find that the performances of the proposed statistics
are robust against affine transformations.
"
"  Given a connected real Lie group and a contractible homogeneous proper
$G$--space $X$ furnished with a $G$--invariant volume form, a real valued
volume can be assigned to any representation $\rho\colon \pi_1(M)\to G$ for any
oriented closed smooth manifold $M$ of the same dimension as $X$. Suppose that
$G$ contains a closed and cocompact semisimple subgroup, it is shown in this
paper that the set of volumes is finite for any given $M$. From a perspective
of model geometries, examples are investigated and applications with mapping
degrees are discussed.
"
"  This paper is a continuation of the second author's previous work. We
investigate the isoperimetric problem in the 2-dimensional Finsler space form
$(F_B, B^2(1))$ with $k=0$ by using the Holmes-Thompson area and prove that the
circle centered the origin achieves the local maximum area of the isoperimetric
problem.
"
"  In this paper, we investigate the cooling-off effect (opposite to the magnet
effect) from two aspects. Firstly, from the viewpoint of dynamics, we study the
existence of the cooling-off effect by following the dynamical evolution of
some financial variables over a period of time before the stock price hits its
limit. Secondly, from the probability perspective, we investigate, with the
logit model, the existence of the cooling-off effect through analyzing the
high-frequency data of all A-share common stocks traded on the Shanghai Stock
Exchange and the Shenzhen Stock Exchange from 2000 to 2011 and inspecting the
trading period from the opening phase prior to the moment that the stock price
hits its limits. A comparison is made of the properties between up-limit hits
and down-limit hits, and the possible difference will also be compared between
bullish and bearish market state by dividing the whole period into three
alternating bullish periods and three bearish periods. We find that the
cooling-off effect emerges for both up-limit hits and down-limit hits, and the
cooling-off effect of the down-limit hits is stronger than that of the up-limit
hits. The difference of the cooling-off effect between bullish period and
bearish period is quite modest. Moreover, we examine the sub-optimal orders
effect, and infer that the professional individual investors and institutional
investors play a positive role in the cooling-off effects. All these findings
indicate that the price limit trading rule exerts a positive effect on
maintaining the stability of the Chinese stock markets.
"
"  The transition mechanism of jump processes between two different subsets in
state space reveals important dynamical information of the processes and
therefore has attracted considerable attention in the past years. In this
paper, we study the first passage path ensemble of both discrete-time and
continuous-time jump processes on a finite state space. The main approach is to
divide each first passage path into nonreactive and reactive segments and to
study them separately. The analysis can be applied to jump processes which are
non-ergodic, as well as continuous-time jump processes where the waiting time
distributions are non-exponential. In the particular case that the jump
processes are both Markovian and ergodic, our analysis elucidates the relations
between the study of the first passage paths and the study of the transition
paths in transition path theory. We provide algorithms to numerically compute
statistics of the first passage path ensemble. The computational complexity of
these algorithms scales with the complexity of solving a linear system, for
which efficient methods are available. Several examples demonstrate the wide
applicability of the derived results across research areas.
"
"  Speckle reduction is a longstanding topic in synthetic aperture radar (SAR)
imaging. Since most current and planned SAR imaging satellites operate in
polarimetric, interferometric or tomographic modes, SAR images are
multi-channel and speckle reduction techniques must jointly process all
channels to recover polarimetric and interferometric information. The
distinctive nature of SAR signal (complex-valued, corrupted by multiplicative
fluctuations) calls for the development of specialized methods for speckle
reduction. Image denoising is a very active topic in image processing with a
wide variety of approaches and many denoising algorithms available, almost
always designed for additive Gaussian noise suppression. This paper proposes a
general scheme, called MuLoG (MUlti-channel LOgarithm with Gaussian denoising),
to include such Gaussian denoisers within a multi-channel SAR speckle reduction
technique. A new family of speckle reduction algorithms can thus be obtained,
benefiting from the ongoing progress in Gaussian denoising, and offering
several speckle reduction results often displaying method-specific artifacts
that can be dismissed by comparison between results.
"
"  We define the Radon transform functor for sheaves and prove that it is an
equivalence after suitable microlocal localizations. As a result, the sheaf
category associated to a Legendrian is invariant under the Radon transform. We
also manage to place the Radon transform and other transforms in microlocal
sheaf theory altogether in a diagram.
"
"  We propose foundations for a synthetic theory of $(\infty,1)$-categories
within homotopy type theory. We axiomatize a directed interval type, then
define higher simplices from it and use them to probe the internal categorical
structures of arbitrary types. We define Segal types, in which binary
composites exist uniquely up to homotopy; this automatically ensures
composition is coherently associative and unital at all dimensions. We define
Rezk types, in which the categorical isomorphisms are additionally equivalent
to the type-theoretic identities - a ""local univalence"" condition. And we
define covariant fibrations, which are type families varying functorially over
a Segal type, and prove a ""dependent Yoneda lemma"" that can be viewed as a
directed form of the usual elimination rule for identity types. We conclude by
studying homotopically correct adjunctions between Segal types, and showing
that for a functor between Rezk types to have an adjoint is a mere proposition.
To make the bookkeeping in such proofs manageable, we use a three-layered
type theory with shapes, whose contexts are extended by polytopes within
directed cubes, which can be abstracted over using ""extension types"" that
generalize the path-types of cubical type theory. In an appendix, we describe
the motivating semantics in the Reedy model structure on bisimplicial sets, in
which our Segal and Rezk types correspond to Segal spaces and complete Segal
spaces.
"
"  In this article, we give the explicit solutions to the Laplace equations
associated to the Dirac operator, Euler operator and the harmonic oscillator in
R.
"
"  Let $q$ be a power of a prime $p$ and let $U(q)$ be a Sylow $p$-subgroup of a
finite Chevalley group $G(q)$ defined over the field with $q$ elements. We
first give a parametrization of the set $\text{Irr}(U(q))$ of irreducible
characters of $U(q)$ when $G(q)$ is of type $\mathrm{G}_2$. This is uniform for
primes $p \ge 5$, while the bad primes $p=2$ and $p=3$ have to be considered
separately. We then use this result and the contribution of several authors to
show a general result, namely that if $G(q)$ is any finite Chevalley group with
$p$ a bad prime, then there exists a character $\chi \in \text{Irr}(U(q))$ such
that $\chi(1)=q^n/p$ for some $n \in \mathbb{Z}_{\ge_0}$. In particular, for
each $G(q)$ and every bad prime $p$, we construct a family of characters of
such degree as inflation followed by an induction of linear characters of an
abelian subquotient $V(q)$ of $U(q)$.
"
"  In this paper we define a notion of calibration for an equivalent approach to
the classical Steiner problem in a covering space setting and we give some
explicit examples. Moreover we introduce the notion of calibration in families:
the idea is to divide the set of competitors in a suitable way, defining an
appropriate (and weaker) notion of calibration. Then, calibrating the candidate
minimizers in each family and comparing their perimeter, it is possible to find
the minimizers of the minimization problem. Thanks to this procedure we prove
the minimality of the Steiner configurations spanning the vertices of a regular
hexagon and of a regular pentagon.
"
"  In the first part of this paper we will prove the Voevodsky's nilpotence
conjecture for smooth cubic fourfolds and ordinary generic Gushel-Mukai
fourfolds. Then, making use of noncommutative motives, we will prove the
Voevodsky's nilpotence conjecture for generic Gushel-Mukai fourfolds containing
a $\tau$-plane $\G(2,3)$ and for ordinary Gushel-Mukai fourfolds containing a
quintic del Pezzo surface.
"
"  In this note, we expand on some technical issues raised in \cite{PPV} by the
authors, as well as providing a friendly introduction to and summary of our
previous work. We construct a set of heterotic string compactifications to 0+1
dimensions intimately related to the Monstrous moonshine module of Frenkel,
Lepowsky, and Meurman (and orbifolds thereof). Using this model, we review our
physical interpretation of the genus zero property of Monstrous moonshine.
Furthermore, we show that the space of (second-quantized) BPS-states forms a
module over the Monstrous Lie algebras $\mathfrak{m}_g$---some of the first and
most prominent examples of Generalized Kac-Moody algebras---constructed by
Borcherds and Carnahan. In particular, we clarify the structure of the module
present in the second-quantized string theory. We also sketch a proof of our
methods in the language of vertex operator algebras, for the interested
mathematician.
"
"  There is a significant literature on methods for incorporating knowledge into
multiple testing procedures so as to improve their power and precision. Some
common forms of prior knowledge include (a) beliefs about which hypotheses are
null, modeled by non-uniform prior weights; (b) differing importances of
hypotheses, modeled by differing penalties for false discoveries; (c) multiple
arbitrary partitions of the hypotheses into (possibly overlapping) groups; and
(d) knowledge of independence, positive or arbitrary dependence between
hypotheses or groups, suggesting the use of more aggressive or conservative
procedures. We present a unified algorithmic framework called p-filter for
global null testing and false discovery rate (FDR) control that allows the
scientist to incorporate all four types of prior knowledge (a)-(d)
simultaneously, recovering a variety of known algorithms as special cases.
"
"  In the present work weighted area integral means $M_{p,\varphi}(f;{\mathrm
{Im}}z)$ are studied and it is proved that the function $y\to \log
M_{p,\varphi}(f;y)$ is convex in the case when $f$ belongs to a Hardy space on
the upper half-plane.
"
"  Let $\mathfrak{g}$ be a hyperbolic Kac-Moody algebra of rank $2$, and set
$\lambda: = \Lambda_1 - \Lambda_2$, where $\Lambda_1, \Lambda_2$ are the
fundamental weights for $\mathfrak{g}$; note that $\lambda$ is neither dominant
nor antidominant. Let $\mathbb{B}(\lambda)$ be the crystal of all
Lakshmibai-Seshadri paths of shape $\lambda$. We prove that (the crystal graph
of) $\mathbb{B}(\lambda)$ is connected. Furthermore, we give an explicit
description of Lakshmibai-Seshadri paths of shape $\lambda$.
"
"  Affine policies (or control) are widely used as a solution approach in
dynamic optimization where computing an optimal adjustable solution is usually
intractable. While the worst case performance of affine policies can be
significantly bad, the empirical performance is observed to be near-optimal for
a large class of problem instances. For instance, in the two-stage dynamic
robust optimization problem with linear covering constraints and uncertain
right hand side, the worst-case approximation bound for affine policies is
$O(\sqrt m)$ that is also tight (see Bertsimas and Goyal (2012)), whereas
observed empirical performance is near-optimal. In this paper, we aim to
address this stark-contrast between the worst-case and the empirical
performance of affine policies. In particular, we show that affine policies
give a good approximation for the two-stage adjustable robust optimization
problem with high probability on random instances where the constraint
coefficients are generated i.i.d. from a large class of distributions; thereby,
providing a theoretical justification of the observed empirical performance. On
the other hand, we also present a distribution such that the performance bound
for affine policies on instances generated according to that distribution is
$\Omega(\sqrt m)$ with high probability; however, the constraint coefficients
are not i.i.d.. This demonstrates that the empirical performance of affine
policies can depend on the generative model for instances.
"
"  We give the sharp conditions for boundedness of Hausdorff operators on
certain modulation and Wiener amalgam spaces.
"
"  We prove the superhedging duality for a discrete-time financial market with
proportional transaction costs under portfolio constraints and model
uncertainty. Frictions are modeled through solvency cones as in the original
model of [Kabanov, Y., Hedging and liquidation under transaction costs in
currency markets. Fin. Stoch., 3(2):237-248, 1999] adapted to the quasi-sure
setup of [Bouchard, B. and Nutz, M., Arbitrage and duality in nondominated
discrete-time models. Ann. Appl. Probab., 25(2):823-859, 2015]. Our results
hold under the condition of No Strict Arbitrage and under the efficient
friction hypothesis.
"
"  In 2016 we proved that for every symmetric, repetition invariant and Jensen
concave mean $\mathscr{M}$ the Kedlaya-type inequality $$
\mathscr{A}\big(x_1,\mathscr{M}(x_1,x_2),\ldots,\mathscr{M}(x_1,\ldots,x_n)\big)\le
\mathscr{M} \big(x_1,
\mathscr{A}(x_1,x_2),\ldots,\mathscr{A}(x_1,\ldots,x_n)\big) $$ holds for an
arbitrary $(x_n)$ ($\mathscr{A}$ stands for the arithmetic mean). We are going
to prove the weighted counterpart of this inequality. More precisely, if
$(x_n)$ is a vector with corresponding (non-normalized) weights $(\lambda_n)$
and $\mathscr{M}_{i=1}^n(x_i,\lambda_i)$ denotes the weighted mean then, under
analogous conditions on $\mathscr{M}$, the inequality $$ \mathscr{A}_{i=1}^n
\big(\mathscr{M}_{j=1}^i (x_j,\lambda_j),\:\lambda_i\big) \le
\mathscr{M}_{i=1}^n \big(\mathscr{A}_{j=1}^i (x_j,\lambda_j),\:\lambda_i\big)
$$ holds for every $(x_n)$ and $(\lambda_n)$ such that the sequence
$(\frac{\lambda_k}{\lambda_1+\cdots+\lambda_k})$ is decreasing.
"
"  We present a sufficient condition for irreducibility of forcing algebras and
study the (non)-reducedness phenomenon. Furthermore, we prove a criterion for
normality for forcing algebras over a polynomial base ring with coefficients in
a perfect field. This gives a geometrical normality criterion for algebraic
(forcing) varieties over algebraically closed fields. Besides, we examine in
detail an specific (enlightening) example with several forcing equations.
Finally, we compute explicitly the normalization of a particular forcing
algebra by means of finding explicitly the generators of the ideal defining it
as an affine ring.
"
"  This paper addresses a task allocation problem for a large-scale robotic
swarm, namely swarm distribution guidance problem. Unlike most of the existing
frameworks handling this problem, the proposed framework suggests utilising
local information available to generate its time-varying stochastic policies.
As each agent requires only local consistency on information with neighbouring
agents, rather than the global consistency, the proposed framework offers
various advantages, e.g., a shorter timescale for using new information and
potential to incorporate an asynchronous decision-making process. We perform
theoretical analysis on the properties of the proposed framework. From the
analysis, it is proved that the framework can guarantee the convergence to the
desired density distribution even using local information while maintaining
advantages of global-information-based approaches. The design requirements for
these advantages are explicitly listed in this paper. This paper also provides
specific examples of how to implement the framework developed. The results of
numerical experiments confirm the effectiveness and comparability of the
proposed framework, compared with the global-information-based framework.
"
"  Let $f\colon M \to M$ be a uniformly quasiregular self-mapping of a compact,
connected, and oriented Riemannian $n$-manifold $M$ without boundary, $n\ge 2$.
We show that, for $k \in \{0,\ldots, n\}$, the induced homomorphism $f^* \colon
H^k(M;\mathbb{R}) \to H^k(M;\mathbb{R})$, where $H^k(M;\mathbb{R})$ is the
$k$:th singular cohomology of $M$, is complex diagonalizable and the
eigenvalues of $f^*$ have modulus $(\mathrm{deg}\ f)^{k/n}$. As an application,
we obtain a degree restriction for uniformly quasiregular self-mappings of
closed manifolds. In the proof of the main theorem, we use a Sobolev--de Rham
cohomology based on conformally invariant differential forms and an induced
push-forward operator.
"
"  Our method of density elimination is generalized to the non-commutative
substructural logic GpsUL*. Then the standard completeness of GpsUL* follows as
a lemma by virtue of previous work by Metcalfe and Montagna. This result shows
that GpsUL* is the logic of pseudo-uninorms and their residua and answered the
question posed by Prof. Metcalfe, Olivetti, Gabbay and Tsinakis.
"
"  In this article we use linear algebra to improve the computational time for
the obtaining of Green's functions of linear differential equations with
reflection (DER). This is achieved by decomposing both the `reduced' equation
(the ODE associated to a given DER) and the corresponding two-point boundary
conditions.
"
"  Batyrev constructed a family of Calabi-Yau hypersurfaces dual to the first
Chern class in toric Fano varieties. Using this construction, we introduce a
family of Calabi-Yau manifolds whose SU-bordism classes generate the special
unitary bordism ring
$\varOmega^{SU}\otimes\mathbb{Z}[\frac{1}{2}]\cong\mathbb{Z}[\frac{1}{2}][y_{i}\colon
i\ge 2]$. We also describe explicit Calabi-Yau representatives for
multiplicative generators of the SU-bordism ring in low dimensions.
"
"  We show that every periodic virtual knot can be realized as the closure of a
periodic virtual braid and use this to study the Alexander invariants of
periodic virtual knots. If $K$ is a $q$-periodic and almost classical knot, we
show that its quotient knot $K_*$ is also almost classical, and in the case
$q=p^r$ is a prime power, we establish an analogue of Murasugi's congruence
relating the Alexander polynomials of $K$ and $K_*$ over the integers modulo
$p$. This result is applied to the problem of determining the possible periods
of a virtual knot $K$. One consequence is that if $K$ is an almost classical
knot with a nontrivial Alexander polynomial, then it is $p$-periodic for only
finitely many primes $p$. Combined with parity and Manturov projection, our
methods provide conditions that a general virtual knot must satisfy in order to
be $q$-periodic.
"
"  We establish the monotonicity property for the mass of non-pluripolar
products on compact Kahler manifolds, and we initiate the study of complex
Monge-Ampere type equations with prescribed singularity type. Using the
variational method of Berman-Boucksom-Guedj-Zeriahi we prove existence and
uniqueness of solutions with small unbounded locus. We give applications to
Kahler-Einstein metrics with prescribed singularity, and we show that the
log-concavity property holds for non-pluripolar products with small unbounded
locus.
"
"  This paper is a continuation of arXiv:1405.1707. We present certain new
applications and generalizations of the free field realization of the twisted
Heisenberg-Virasoro algebra ${\mathcal H}$ at level zero.
We find explicit formulas for singular vectors in certain Verma modules. A
free field realization of self-dual modules for ${\mathcal H}$ is presented by
combining a bosonic construction of Whittaker modules from arXiv:1409.5354 with
a construction of logarithmic modules for vertex algebras. As an application,
we prove that there exists a non-split self-extension of irreducible self-dual
module which is a logarithmic module of rank two.
We construct a large family of logarithmic modules containing different types
of highest weight modules as subquotients. We believe that these logarithmic
modules are related with projective covers of irreducible modules in a suitable
category of ${\mathcal H}$-modules.
"
"  In this paper we systematically explore questions of succinctness in modal
logics employed in spatial reasoning. We show that the closure operator,
despite being less expressive, is exponentially more succinct than the
limit-point operator, and that the $\mu$-calculus is exponentially more
succinct than the equally-expressive tangled limit operator. These results hold
for any class of spaces containing at least one crowded metric space or
containing all spaces based on ordinals below $\omega^\omega$, with the usual
limit operator. We also show that these results continue to hold even if we
enrich the less succinct language with the universal modality.
"
"  The Dominative $p$-Laplace Operator is introduced. This operator is a
relative to the $p$-Laplacian, but with the distinguishing property of being
sublinear. It explains the superposition principle in the $p$-Laplace Equation.
"
"  Elliptically contoured distributions generalize the multivariate normal
distributions in such a way that the density generators need not be
exponential. However, as the name suggests, elliptically contoured
distributions remain to be restricted in that the similar density contours
ought to be elliptical. Kamiya, Takemura and Kuriki [Star-shaped distributions
and their generalizations, Journal of Statistical Planning and Inference 138
(2008), 3429--3447] proposed star-shaped distributions, for which the density
contours are allowed to be boundaries of arbitrary similar star-shaped sets. In
the present paper, we propose a nonparametric estimator of the shape of the
density contours of star-shaped distributions, and prove its strong consistency
with respect to the Hausdorff distance. We illustrate our estimator by
simulation.
"
"  The MDL two-part coding $ \textit{index of resolvability} $ provides a
finite-sample upper bound on the statistical risk of penalized likelihood
estimators over countable models. However, the bound does not apply to
unpenalized maximum likelihood estimation or procedures with exceedingly small
penalties. In this paper, we point out a more general inequality that holds for
arbitrary penalties. In addition, this approach makes it possible to derive
exact risk bounds of order $1/n$ for iid parametric models, which improves on
the order $(\log n)/n$ resolvability bounds. We conclude by discussing
implications for adaptive estimation.
"
"  We construct an explicit projective bimodule resolution for the Leavitt path
algebra of a row-finite quiver. We prove that the Leavitt path algebra of a
row-countable quiver has Hochschild cohomolgical dimension at most one, that
is, it is quasi-free in the sense of Cuntz-Quillen. The construction of the
resolution relies on an explicit derivation of the Leavitt path algebra.
"
"  In this paper, we establish the Carleman estimates for forward and backward
stochastic fourth order Schrödinger equations, on basis of which, we can
obtain the observability, unique continuation property and the exact
controllability for the forward and backward stochastic fourth order
Schrödinger equations.
"
"  In this paper, we consider numerical approximations of a hydrodynamically
coupled phase field diblock copolymer model, in which the free energy contains
a kinetic potential, a gradient entropy, a Ginzburg-Landau double well
potential, and a long range nonlocal type potential. We develop a set of second
order time marching schemes for this system using the ""Invariant Energy
Quadratization"" approach for the double well potential, the projection method
for the Navier-Stokes equation, and a subtle implicit-explicit treatment for
the stress and convective term. The resulting schemes are linear and lead to
symmetric positive definite systems at each time step, thus they can be
efficiently solved. We further prove that these schemes are unconditionally
energy stable. Various numerical experiments are performed to validate the
accuracy and energy stability of the proposed schemes.
"
"  Several Fourier transformations of functions of one and two variables are
evaluated and then used to derive some integral and series identities. It is
shown that certain two- dimensional Mordell integrals factorize into product of
two integrals and that the square of the absolute value of the Mordell integral
can be reduced to a single one-dimensional integral. Some connections to
elliptic functions and lattice sums are discussed.
"
"  Shunt FACTS devices, such as, a Static Var Compensator (SVC), are capable of
providing local reactive power compensation. They are widely used in the
network to reduce the real power loss and improve the voltage profile. This
paper proposes a planning model based on mixed integer conic programming (MICP)
to optimally allocate SVCs in the transmission network considering load
uncertainty. The load uncertainties are represented by a number of scenarios.
Reformulation and linearization techniques are utilized to transform the
original non-convex model into a convex second order cone programming (SOCP)
model. Numerical case studies based on the IEEE 30-bus system demonstrate the
effectiveness of the proposed planning model.
"
"  In this paper we obtain a description of the Grothendieck group of complex
vector bundles over the classifying space of a p-local finite group in terms of
representation rings of subgroups of its Sylow. We also prove a stable elements
formula for generalized cohomological invariants of p-local finite groups,
which is used to show the existence of unitary embeddings of p-local finite
groups. Finally, we show that the augmentation map for the cochains of the
classifying space of a p-local finite group is Gorenstein in the sense of
Dwyer-Greenlees-Iyengar and obtain some consequences about the cohomology ring
of these classifying spaces.
"
"  The space of based loops in $SL_n(\mathbb{C})$, also known as the affine
Grassmannian of $SL_n(\mathbb{C})$, admits an $\mathbb{E}_2$ or fusion product.
Work of Mitchell and Richter proves that this based loop space stably splits as
an infinite wedge sum. We prove that the Mitchell--Richter splitting is
coherently multiplicative, but not $\mathbb{E}_2$. Nonetheless, we show that
the splitting becomes $\mathbb{E}_2$ after base-change to complex cobordism.
Our proof of the $\mathbb{A}_\infty$ splitting involves on the one hand an
analysis of the multiplicative properties of Weiss calculus, and on the other a
use of Beilinson--Drinfeld Grassmannians to verify a conjecture of Mahowald and
Richter. Other results are obtained by explicit, obstruction-theoretic
computations.
"
"  We study Le Potier's strange duality conjecture on a rational surface. We
focus on the strange duality map $SD_{c_n^r,L}$ which involves the moduli space
of rank $r$ sheaves with trivial first Chern class and second Chern class $n$,
and the moduli space of 1-dimensional sheaves with determinant $L$ and Euler
characteristic 0. We show there is an exact sequence relating the map
$SD_{c_r^r,L}$ to $SD_{c^{r-1}_{r},L}$ and $SD_{c_r^r,L\otimes K_X}$ for all
$r\geq1$ under some conditions on $X$ and $L$ which applies to a large number
of cases on $\p^2$ or Hirzebruch surfaces . Also on $\mathbb{P}^2$ we show that
for any $r>0$, $SD_{c^r_r,dH}$ is an isomorphism for $d=1,2$, injective for
$d=3$ and moreover $SD_{c_3^3,rH}$ and $SD_{c_3^2,rH}$ are injective. At the
end we prove that the map $SD_{c_n^2,L}$ ($n\geq2$) is an isomorphism for
$X=\mathbb{P}^2$ or Fano rational ruled surfaces and $g_L=3$, and hence so is
$SD_{c_3^3,L}$ as a corollary of our main result.
"
"  Sylvester factor, an essential part of the asymptotic formula of Hardy and
Littlewood which is the extended Goldbach conjecture, regarded as strongly
multiplicative arithmetic function, has several remarkable properties.
"
"  Inspired by mirror symmetry, we investigate some differential geometric
aspects of the space of Bridgeland stability conditions on a Calabi-Yau
triangulated category. The aim is to develop theory of Weil-Petersson geometry
on the stringy Kähler moduli space. A few basic examples are studied. In
particular, we identify our Weil-Petersson metric with the Bergman metric on a
Siegel modular variety in the case of the self-product of an elliptic curve.
"
"  In this paper we study the asymptotic behavior of second-order uniformly
elliptic operators on weighted Riemannian manifolds. We appeal to the notion of
\mbox{$H$-convergence} introduced by Murat and Tartar. In our main result we
establish an \mbox{$H$-compactness} result that applies to elliptic operators
with measurable, uniformly elliptic coefficients on weighted Riemannian
manifolds. We further discuss the special case of ""locally periodic""
coefficients and study the asymptotic behavior of the Laplace-Beltrami operator
on families of weighted manifolds obtained from a reference manifold by a
conformal (rapidly oscillating) change of metrics.
"
"  Here we construct the conformal mappings with the help of continuous
fractions approximations. These approximations converge to the algebraic roots
$\sqrt[N]{z}$ for $N \in \mathbb{N}$ and $z$ from the right half-plane of the
complex plane. We estimate both the convergence rate and the compact set of
convergence. Also we give the examples that illustrate the introduced technique
of a conformal mapping construction.
"
"  Let K be a field and denote by K[t], the polynomial ring with coefficients in
K. Set A = K[f1,. .. , fs], with f1,. .. , fs $\in$ K[t]. We give a procedure
to calculate the monoid of degrees of the K algebra M = F1A + $\times$ $\times$
$\times$ + FrA with F1,. .. , Fr $\in$ K[t]. We show some applications to the
problem of the classification of plane polynomial curves (that is, plane
algebraic curves parametrized by polynomials) with respect to some oh their
invariants, using the module of K{ä}hler differentials.
"
"  Measurements on a subset of the boundary are common in electrical impedance
tomography, especially any electrode model can be interpreted as a partial
boundary problem. The information obtained is different to full-boundary
measurements as modeled by the ideal continuum model. In this study we discuss
an approach to approximate full-boundary data from partial-boundary
measurements that is based on the knowledge of the involved projections. The
approximate full-boundary data can then be obtained as the solution of a
suitable optimization problem on the coefficients of the Neumann-to-Dirichlet
map. By this procedure we are able to improve the reconstruction quality of
continuum model based algorithms, in particular we present the effectiveness
with a D-bar method. Reconstructions are presented for noisy simulated and real
measurement data.
"
"  We show that for any positive integer k, the k-th nonzero eigenvalue of the
Laplace-Beltrami operator on the two-dimensional sphere endowed with a
Riemannian metric of unit area, is maximized in the limit by a sequence of
metrics converging to a union of k touching identical round spheres. This
proves a conjecture posed by the second author in 2002 and yields a sharp
isoperimetric inequality for all nonzero eigenvalues of the Laplacian on a
sphere. Earlier, the result was known only for k=1 (J.Hersch, 1970), k=2
(N.Nadirashvili, 2002; R.Petrides, 2014) and k=3 (N.Nadirashvili and Y.Sire,
2017). In particular, we argue that for any k>=2, the supremum of the k-th
nonzero eigenvalue on a sphere of unit area is not attained in the class of
Riemannin metrics which are smooth outsitde a finite set of conical
singularities. The proof uses certain properties of harmonic maps between
spheres, the key new ingredient being a bound on the harmonic degree of a
harmonic map into a sphere obtained by N. Ejiri.
"
"  We review different constructions of the supersymmetry subalgebras of the
chiral de Rham complex on special holonomy manifolds. We describe the
difference between the holomorphic-anti-holomorphic sectors based on a local
free ghost system vs the decomposition in left-right sectors from a local
Boson-Fermion system. We describe the topological twist in the case of $G_2$
and $Spin_7$ manifolds. We describe the construction of these algebras as
quantum Hamiltonian reduction of Lie superalgebras at the minimal or
superprincipal nilpotent.
"
"  I present a discussion of the hierarchy of Toda flows that gives center stage
to the associated cocycles and the maps they induce on the $m$ functions. In
the second part, these ideas are then applied to canonical systems; an
important feature of this discussion will be my proposal that the role of the
shift on Jacobi matrices should now be taken over by the more general class of
twisted shifts.
"
"  A vector bundle E on a projective variety X is called finite if it satisfies
a nontrivial polynomial equation with integral coefficients. A theorem of Nori
implies that E is finite if and only if the pullback of E to some finite etale
Galois covering of X is trivial. We prove the same statement when X is a
compact complex manifold admitting a Gauduchon astheno-Kahler metric.
"
"  We formalize the arithmetic topology, i.e. a relationship between knots and
primes. Namely, using the notion of a cluster C*-algebra we construct a functor
from the category of 3-dimensional manifolds M to a category of algebraic
number fields K, such that the prime ideals (ideals, resp.) in the ring of
integers of K correspond to knots (links, resp.) in M. It is proved that the
functor realizes all axioms of the arithmetic topology conjectured in the
1960's by Manin, Mazur and Mumford.
"
"  We introduce the notion of a ""crystallographic sphere packing,"" defined to be
one whose limit set is that of a geometrically finite hyperbolic reflection
group in one higher dimension. We exhibit for the first time an infinite family
of conformally-inequivalent such with all radii being reciprocals of integers.
We then prove a result in the opposite direction: the ""superintegral"" ones
exist only in finitely many ""commensurability classes,"" all in dimensions below
30.
"
"  The paper analyzes special cyclic Jacobi methods for symmetric matrices of
order $4$. Only those cyclic pivot strategies that enable full parallelization
of the method are considered. These strategies, unlike the serial pivot
strategies, can force the method to be very slow or very fast within one cycle,
depending on the underlying matrix. Hence, for the global convergence proof one
has to consider two or three adjacent cycles. It is proved that for any
symmetric matrix $A$ of order~$4$ the inequality
$S(A^{[2]})\leq(1-10^{-5})S(A)$ holds, where $A^{[2]}$ results from $A$ by
applying two cycles of a particular parallel method. Here $S(A)$ stands for the
Frobenius norm of the strictly upper-triangular part of $A$. The result holds
for two special parallel strategies and implies the global convergence of the
method under all possible fully parallel strategies. It is also proved that for
every $\epsilon>0$ and $n\geq4$ there exist a symmetric matrix $A(\epsilon)$ of
order $n$ and a cyclic strategy, such that upon completion of the first cycle
of the appropriate Jacobi method the inequality $S(A^{[1]})>
(1-\epsilon)S(A(\epsilon))$ holds.
"
"  For an affine toric variety $\mathrm{Spec}(A)$, we give a convex geometric
description of the Hodge decomposition of its Hochschild cohomology. Under
certain assumptions we compute the dimensions of the Hodge summands
$T^1_{(i)}(A)$, generalizing the existing results about the Andre-Quillen
cohomology group $T^1_{(1)}(A)$. We prove that every Poisson structure on a
possibly singular affine toric variety can be quantized in the sense of
deformation quantization.
"
"  For a signed cyclic graph G, we can construct a unique virtual link L by
taking the medial construction and convert 4-valent vertices of the medial
graph to crossings according to the signs. If a virtual link can occur in this
way then we say that the virtual link is graphical. In the article we shall
prove that a virtual link L is graphical if and only if it is checkerboard
colorable. On the other hand, we introduce a polynomial F[G] for signed cyclic
graphs, which is defined via a deletion-marking recursion. We shall establish
the relationship between F[G] of a signed cyclic graph G and the bracket
polynomial of one of the virtual link diagrams associated with G. Finally we
give a spanning subgraph expansion for F[G].
"
"  We study the boundary behavior of the so-called ring $Q$-mappings obtained as
a natural generalization of mappings with bounded distortion. We establish a
series of conditions imposed on a function $Q(x)$ for the continuous extension
of given mappings with respect to prime ends in domains with regular boundaries
in metric spaces.
"
"  In this paper, we assume that all isoparametric submanifolds have flat
section. The main purpose of this paper is to prove that, if a full irreducible
complete isoparametric submanifold of codimension greater than one in a
symmetric space of non-compact type admits a reflective focal submanifold and
if it of real analytic, then it is a principal orbit of a Hermann type action
on the symmetric space. A hyperpolar action on a symmetric space of non-compact
type admits a reflective singular orbit if and only if it is a Hermann type
action. Hence is not extra the assumption that the isoparametric submanifold
admits a reflective focal submanifold. Also, we prove that, if a full
irreducible complete isoparametric submanifold of codimension greater than one
in a symmetric space of non-compact type satisfies some additional conditions,
then it is a principal orbit of the isotropy action of the symmetric space,
where we need not impose that the submanifold is of real analytic. We use the
building theory in the proof.
"
"  Antiunitary representations of Lie groups take values in the group of unitary
and antiunitary operators on a Hilbert space H. In quantum physics, antiunitary
operators implement time inversion or a PCT symmetry, and in the modular theory
of operator algebras they arise as modular conjugations from cyclic separating
vectors of von Neumann algebras. We survey some of the key concepts at the
borderline between the theory of local observables (Quantum Field Theory (QFT)
in the sense of Araki--Haag--Kastler) and modular theory of operator algebras
from the perspective of antiunitary group representations. Here a central point
is to encode modular objects in standard subspaces V in H which in turn are in
one-to-one correspondence with antiunitary representations of the
multiplicative group R^x. Half-sided modular inclusions and modular
intersections of standard subspaces correspond to antiunitary representations
of Aff(R), and these provide the basic building blocks for a general theory
started in the 90s with the ground breaking work of Borchers and Wiesbrock and
developed in various directions in the QFT context. The emphasis of these notes
lies on the translation between configurations of standard subspaces as they
arise in the context of modular localization developed by Brunetti, Guido and
Longo, and the more classical context of von Neumann algebras with cyclic
separating vectors. Our main point is that configurations of standard subspaces
can be studied from the perspective of antiunitary Lie group representations
and the geometry of the corresponding spaces, which are often fiber bundles
over ordered symmetric spaces. We expect this perspective to provide new and
systematic insight into the much richer configurations of nets of local
observables in QFT.
"
"  We study XXZ spin systems on general graphs. In particular, we describe the
formation of droplet states near the bottom of the spectrum in the Ising phase
of the model, where the Z-term dominates the XX-term. As key tools we use
particle number conservation of XXZ systems and symmetric products of graphs
with their associated adjacency matrices and Laplacians. Of particular interest
to us are strips and multi-dimensional Euclidean lattices, for which we discuss
the existence of spectral gaps above the droplet regime. We also prove a
Combes-Thomas bound which shows that the eigenstates in the droplet regime are
exponentially small perturbations of strict (classical) droplets.
"
"  There is growing interest in estimating and analyzing heterogeneous treatment
effects in experimental and observational studies. We describe a number of
meta-algorithms that can take advantage of any supervised learning or
regression method in machine learning and statistics to estimate the
Conditional Average Treatment Effect (CATE) function. Meta-algorithms build on
base algorithms---such as Random Forests (RF), Bayesian Additive Regression
Trees (BART) or neural networks---to estimate the CATE, a function that the
base algorithms are not designed to estimate directly. We introduce a new
meta-algorithm, the X-learner, that is provably efficient when the number of
units in one treatment group is much larger than in the other, and can exploit
structural properties of the CATE function. For example, if the CATE function
is linear and the response functions in treatment and control are Lipschitz
continuous, the X-learner can still achieve the parametric rate under
regularity conditions. We then introduce versions of the X-learner that use RF
and BART as base learners. In extensive simulation studies, the X-learner
performs favorably, although none of the meta-learners is uniformly the best.
In two persuasion field experiments from political science, we demonstrate how
our new X-learner can be used to target treatment regimes and to shed light on
underlying mechanisms. A software package is provided that implements our
methods.
"
"  We consider the problem of the combinatorial computation of the first Chern
class of a circle bundle. N.Mnev found such a formula in terms of canonical
shellings. It represents certain invariant of a triangulation computed by
analyzing cyclic word in 3-character alphabet associated to the bundle. This
curvature is a kind of discretization of Konstevich's curvature differential
2-form.
We find a new expression of Mnev's curvature by counting triangles in a
cyclic word. Our formula is different from that of Mnev. In particular, it is
cyclically invariant by its very form. We present also some sample computations
of this invariant and also provide a small Mathematica code for the computation
of this invariant.
"
"  We consider the estimation accuracy of individual strength parameters of a
Thurstone choice model when each input observation consists of a choice of one
item from a set of two or more items (so called top-1 lists). This model
accommodates the well-known choice models such as the Luce choice model for
comparison sets of two or more items and the Bradley-Terry model for pair
comparisons.
We provide a tight characterization of the mean squared error of the maximum
likelihood parameter estimator. We also provide similar characterizations for
parameter estimators defined by a rank-breaking method, which amounts to
deducing one or more pair comparisons from a comparison of two or more items,
assuming independence of these pair comparisons, and maximizing a likelihood
function derived under these assumptions. We also consider a related binary
classification problem where each individual parameter takes value from a set
of two possible values and the goal is to correctly classify all items within a
prescribed classification error.
"
"  In this paper we prove the uniqueness and radial symmetry of minimizers for
variational problems that model several phenomena. The uniqueness is a
consequence of the convexity of the functional. The main technique is Fourier
transform of tempered distributions.
"
"  We consider complements of standard Seifert surfaces of special alternating
links. On these handlebodies, we use Honda's method to enumerate those tight
contact structures whose dividing sets are isotopic to the link, and find their
number to be the leading coefficient of the Alexander polynomial. The Euler
classes of the contact structures are identified with hypertrees in a certain
hypergraph. Using earlier work, this establishes a connection between contact
topology and the Homfly polynomial. We also show that the contact invariants of
our tight contact structures form a basis for sutured Floer homology. Finally,
we relate our methods and results to Kauffman's formal knot theory.
"
"  We compute the modular transformation formula of the characters for a certain
family of (finitely or uncountably many) simple modules over the simple
$\mathcal{N}=2$ vertex operator superalgebra of central charge
$c_{p,p'}=3\left(1-\frac{2p'}{p}\right),$ where $(p,p')$ is a pair of coprime
positive integers such that $p\geq2$. When $p'=1$, the formula coincides with
that of the $\mathcal{N}=2$ unitary minimal series found by F. Ravanini and
S.-K. Yang. In addition, we study the properties of the corresponding ""modular
$S$-matrix"", which is no longer a matrix if $p'\geq2$.
"
"  We combine conditions found in [Wh] with results from [MPR] to show that
quasi-isometries between uniformly discrete bounded geometry spaces that
satisfy linear isoperimetric inequalities are within bounded distance to
bilipschitz equivalences. We apply this result to regularly branching trees and
hyperbolic fillings of metric spaces.
"
"  We prove that if $p \equiv 4,7 \pmod{9}$ is prime and $3$ is not a cube
modulo $p$, then both of the equations $x^3+y^3=p$ and $x^3+y^3=p^2$ have a
solution with $x,y \in \mathbb{Q}$.
"
"  As a first approach to the study of systems coupling finite and infinite
dimensional natures, this article addresses the stability of a system of
ordinary differential equations coupled with a classic heat equation using a
Lyapunov functional technique. Inspired from recent developments in the area of
time delay systems, a new methodology to study the stability of such a class of
distributed parameter systems is presented here. The idea is to use a
polynomial approximation of the infinite dimensional state of the heat equation
in order to build an enriched energy functional. A well known efficient
integral inequality (Bessel inequality) will allow to obtain stability
conditions expressed in terms of linear matrix inequalities. We will eventually
test our approach on academic examples in order to illustrate the efficiency of
our theoretical results.
"
"  Fix a quadratic order over the ring of integers. An embedding of the
quadratic order into a quaternionic order naturally gives an integral binary
hermitian form over the quadratic order. We show that, in certain cases, this
correspondence is a discriminant preserving bijection between the isomorphism
classes of embeddings and integral binary hermitian forms.
"
"  The paper is focused on the problem of estimating the probability $p$ of
individual contaminated sample, under group testing. The precision of the
estimator is given by the probability of proportional closeness, a concept
defined in the Introduction. Two-stage and sequential sampling procedures are
characterized. An adaptive procedure is examined.
"
"  We demonstrate that a prior influence on the posterior distribution of
covariance matrix vanishes as sample size grows. The assumptions on a prior are
explicit and mild. The results are valid for a finite sample and admit the
dimension $p$ growing with the sample size $n$. We exploit the described fact
to derive the finite sample Bernstein - von Mises theorem for functionals of
covariance matrix (e.g. eigenvalues) and to find the posterior distribution of
the Frobenius distance between spectral projector and empirical spectral
projector. This can be useful for constructing sharp confidence sets for the
true value of the functional or for the true spectral projector.
"
"  In this paper we study solutions, possibly unbounded and sign-changing, of
the following problem:
-\D_{\lambda} u=|x|_{\lambda}^a |u|^{p-1}u, in R^n,\;n\geq 1,\; p>1, and a
\geq 0, where \D_{\lambda} is a strongly degenerate elliptic operator, the
functions \lambda=(\lambda_1, ..., \lambda_k) : R^n \rightarrow R^k, satisfies
some certain conditions, and |.|_{\lambda} the homogeneous norm associated to
the \D_{\lambda}-Laplacian.
We prove various Liouville-type theorems for smooth solutions under the
assumption that they are stable or stable outside a compact set of R^n. First,
we establish the standard integralestimates via stability property to derive
the nonexistence results for stable solutions. Next, by mean of the Pohozaev
identity, we deduce the Liouville-type theorem for solutions stable outside a
compact set.
"
"  The question of selecting the ""best"" amongst different choices is a common
problem in statistics. In drug development, our motivating setting, the
question becomes, for example: what is the dose that gives me a pre-specified
risk of toxicity or which treatment gives the best response rate. Motivated by
a recent development in the weighted information measures theory, we propose an
experimental design based on a simple and intuitive criterion which governs arm
selection in the experiment with multinomial outcomes. The criterion leads to
accurate arm selection without any parametric or monotonicity assumption. The
asymptotic properties of the design are studied for different allocation rules
and the small sample size behaviour is evaluated in simulations in the context
of Phase I and Phase II clinical trials with binary endpoints. We compare the
proposed design to currently used alternatives and discuss its practical
implementation.
"
"  Using the language of Riordan arrays, we study a one-parameter family of
orthogonal polynomials that we call the restricted Chebyshev-Boubaker
polynomials. We characterize these polynomials in terms of the three term
recurrences that they satisfy, and we study certain central sequences defined
by their coefficient arrays. We give an integral representation for their
moments, and we show that the Hankel transforms of these moments have a simple
form. We show that the (sequence) Hankel transform of the row sums of the
corresponding moment matrix is defined by a family of polynomials closely
related to the Chebyshev polynomials of the second kind, and that these row
sums are in fact the moments of another family of orthogonal polynomials.
"
"  In portfolio analysis, the traditional approach of replacing population
moments with sample counterparts may lead to suboptimal portfolio choices. I
show that optimal portfolio weights can be estimated using a machine learning
(ML) framework, where the outcome to be predicted is a constant and the vector
of explanatory variables is the asset returns. It follows that ML specifically
targets estimation risk when estimating portfolio weights, and that
""off-the-shelf"" ML algorithms can be used to estimate the optimal portfolio in
the presence of parameter uncertainty. The framework nests the traditional
approach and recently proposed shrinkage approaches as special cases. By
relying on results from the ML literature, I derive new insights for existing
approaches and propose new estimation methods. Based on simulation studies and
several datasets, I find that ML significantly reduces estimation risk compared
to both the traditional approach and the equal weight strategy.
"
"  We consider two stage estimation with a non-parametric first stage and a
generalized method of moments second stage, in a simpler setting than
(Chernozhukov et al. 2016). We give an alternative proof of the theorem given
in (Chernozhukov et al. 2016) that orthogonal second stage moments, sample
splitting and $n^{1/4}$-consistency of the first stage, imply
$\sqrt{n}$-consistency and asymptotic normality of second stage estimates. Our
proof is for a variant of their estimator, which is based on the empirical
version of the moment condition (Z-estimator), rather than a minimization of a
norm of the empirical vector of moments (M-estimator). This note is meant
primarily for expository purposes, rather than as a new technical contribution.
"
"  This paper shows that the Conditional Quantile Treatment Effect on the
Treated can be identified using a combination of (i) a conditional
Distributional Difference in Differences assumption and (ii) an assumption on
the conditional dependence between the change in untreated potential outcomes
and the initial level of untreated potential outcomes for the treated group.
The second assumption recovers the unknown dependence from the observed
dependence for the untreated group. We also consider estimation and inference
in the case where all of the covariates are discrete. We propose a uniform
inference procedure based on the exchangeable bootstrap and show its validity.
We conclude the paper by estimating the effect of state-level changes in the
minimum wage on the distribution of earnings for subgroups defined by race,
gender, and education.
"
"  Models involving branched structures are employed to describe several
supply-demand systems such as the structure of the nerves of a leaf, the system
of roots of a tree and the nervous or cardiovascular systems. Given a flow
(traffic path) that transports a given measure $\mu^-$ onto a target measure
$\mu^+$, along a 1-dimensional network, the transportation cost per unit length
is supposed in these models to be proportional to a concave power $\alpha \in
(0,1)$ of the intensity of the flow.
In this paper we address an open problem in the book ""Optimal transportation
networks"" by Bernot, Caselles and Morel and we improve the stability for
optimal traffic paths in the Euclidean space $\mathbb{R}^d$, with respect to
variations of the given measures $(\mu^-,\mu^+)$, which was known up to now
only for $\alpha>1-\frac1d$. We prove it for exponents $\alpha>1-\frac1{d-1}$
(in particular, for every $\alpha \in (0,1)$ when $d=2$), for a fairly large
class of measures $\mu^+$ and $\mu^-$.
"
"  We consider co-rotational wave maps from the $(1+d)$-dimensional Minkowski
space into the $d$-sphere for $d\geq 3$ odd. This is an energy-supercritical
model which is known to exhibit finite-time blowup via self-similar solutions.
Based on a method developed by the second author and Schörkhuber, we prove
the asymptotic nonlinear stability of the ""ground-state"" self-similar solution.
"
"  Satellite conjunction analysis is the assessment of collision risk during a
close encounter between a satellite and another object in orbit. A
counterintuitive phenomenon has emerged in the conjunction analysis literature:
probability dilution, in which lower quality data paradoxically appear to
reduce the risk of collision. We show that probability dilution is a symptom of
a fundamental deficiency in epistemic probability distributions. In
probabilistic representations of statistical inference, there are always false
propositions that have a high probability of being assigned a high degree of
belief. We call this deficiency false confidence. In satellite conjunction
analysis, it results in a severe and persistent underestimation of collision
risk exposure.
We introduce the Martin--Liu validity criterion as a benchmark by which to
identify statistical methods that are free from false confidence. If expressed
using belief functions, such inferences will necessarily be non-additive. In
satellite conjunction analysis, we show that $K \sigma$ uncertainty ellipsoids
satisfy the validity criterion. Performing collision avoidance maneuvers based
on ellipsoid overlap will ensure that collision risk is capped at the
user-specified level. Further, this investigation into satellite conjunction
analysis provides a template for recognizing and resolving false confidence
issues as they occur in other problems of statistical inference.
"
"  In the present paper, new classes of wavelet functions are presented in the
framework of Clifford analysis. Firstly, some classes of orthogonal polynomials
are provided based on 2-parameters weight functions. Such classes englobe the
well known ones of Jacobi and Gegenbauer polynomials when relaxing one of the
parameters. The discovered polynomial sets are next applied to introduce new
wavelet functions. Reconstruction formula as well as Fourier-Plancherel rules
have been proved.
"
"  Let $P_1,\dots, P_n$ and $Q_1,\dots, Q_n$ be convex polytopes in
$\mathbb{R}^n$ such that $P_i\subset Q_i$. It is well-known that the mixed
volume has the monotonicity property: $V(P_1,\dots,P_n)\leq V(Q_1,\dots,Q_n)$.
We give two criteria for when this inequality is strict in terms of essential
collections of faces as well as mixed polyhedral subdivisions. This geometric
result allows us to characterize sparse polynomial systems with Newton
polytopes $P_1,\dots,P_n$ whose number of isolated solutions equals the
normalized volume of the convex hull of $P_1\cup\dots\cup P_n$. In addition, we
obtain an analog of Cramer's rule for sparse polynomial systems.
"
"  Consider a coloring of a graph such that each vertex is assigned a fraction
of each color, with the total amount of colors at each vertex summing to $1$.
We define the fractional defect of a vertex $v$ to be the sum of the overlaps
with each neighbor of $v$, and the fractional defect of the graph to be the
maximum of the defects over all vertices. Note that this coincides with the
usual definition of defect if every vertex is monochromatic. We provide results
on the minimum fractional defect of $2$-colorings of some graphs.
"
"  In several literatures, the authors give a new thinking of measurement theory
system based on error non-classification philosophy, which completely
overthrows the existing measurement concept system of precision, trueness and
accuracy. In this paper, by focusing on the issues of error's regularities and
effect characteristics, the authors will do a thematic interpretation, and
prove that the error's regularities actually come from different cognitive
perspectives, are also unable to be used for classifying errors, and that the
error's effect characteristics actually depend on artificial condition rules of
repeated measurement, and are still unable to be used for classifying errors.
Thus, from the perspectives of error's regularities and effect characteristics,
the existing error classification philosophy is still incorrect; and an
uncertainty concept system, which must be interpreted by the error
non-classification philosophy, naturally becomes the only way out of
measurement theory.
"
"  We address the question concerning the birational geometry of the strata of
holomorphic and quadratic differentials. We show strata of holomorphic and
quadratic differentials to be uniruled in small genus by constructing rational
curves via pencils on K3 and del Pezzo surfaces respectively. Restricting to
genus $3\leq g\leq6$, we construct projective bundles over a rational varieties
that dominate the holomorphic strata with length at most $g-1$, hence showing
in addition that these strata are unirational.
"
"  We show that the distribution of symmetry of a naturally reductive nilpotent
Lie group coincides with the invariant distribution induced by the set of fixed
vectors of the isotropy. This extends a known result on compact naturally
reductive spaces. We also address the study of the quotient by the foliation of
symmetry.
"
"  We develop refined Strichartz estimates at $L^2$ regularity for a class of
time-dependent Schrödinger operators. Such refinements begin to
characterize the near-optimizers of the Strichartz estimate, and play a pivotal
part in the global theory of mass-critical NLS. On one hand, the harmonic
analysis is quite subtle in the $L^2$-critical setting due to an enormous group
of symmetries, while on the other hand, the spacetime Fourier analysis employed
by the existing approaches to the constant-coefficient equation are not adapted
to nontranslation-invariant situations, especially with potentials as large as
those considered in this article.
Using phase space techniques, we reduce to proving certain analogues of
(adjoint) bilinear Fourier restriction estimates. Then we extend Tao's bilinear
restriction estimate for paraboloids to more general Schrödinger operators.
As a particular application, the resulting inverse Strichartz theorem and
profile decompositions constitute a key harmonic analysis input for studying
large data solutions to the $L^2$-critical NLS with a harmonic oscillator
potential in dimensions $\ge 2$. This article builds on recent work of Killip,
Visan, and the author in one space dimension.
"
"  Let $S=\{x_1,x_2,\dots,x_n\}$ be a set of distinct positive integers, and let
$f$ be an arithmetical function. The GCD matrix $(S)_f$ on $S$ associated with
$f$ is defined as the $n\times n$ matrix having $f$ evaluated at the greatest
common divisor of $x_i$ and $x_j$ as its $ij$ entry. The LCM matrix $[S]_f$ is
defined similarly. We consider inertia, positive definiteness and $\ell_p$ norm
of GCD and LCM matrices and their unitary analogs. Proofs are based on matrix
factorizations and convolutions of arithmetical functions.
"
"  Suppose $\Omega, A \subseteq \RR\setminus\Set{0}$ are two sets, both of mixed
sign, that $\Omega$ is Lebesgue measurable and $A$ is a discrete set. We study
the problem of when $A \cdot \Omega$ is a (multiplicative) tiling of the real
line, that is when almost every real number can be uniquely written as a
product $a\cdot \omega$, with $a \in A$, $\omega \in \Omega$. We study both the
structure of the set of multiples $A$ and the structure of the tile $\Omega$.
We prove strong results in both cases. These results are somewhat analogous to
the known results about the structure of translational tiling of the real line.
There is, however, an extra layer of complexity due to the presence of sign in
the sets $A$ and $\Omega$, which makes multiplicative tiling roughly equivalent
to translational tiling on the larger group $\ZZ_2 \times \RR$.
"
"  Modern investigation in economics and in other sciences requires the ability
to store, share, and replicate results and methods of experiments that are
often multidisciplinary and yield a massive amount of data. Given the
increasing complexity and growing interaction across diverse bodies of
knowledge it is becoming imperative to define a platform to properly support
collaborative research and track origin, accuracy and use of data. This paper
starts by defining a set of methods leveraging scientific principles and
advocating the importance of those methods in multidisciplinary, computer
intensive fields like computational finance. The next part of this paper
defines a class of systems called scientific support systems, vis-a-vis usages
in other research fields such as bioinformatics, physics and engineering. We
outline a basic set of fundamental concepts, and list our goals and motivation
for leveraging such systems to enable large-scale investigation, ""crowd powered
science"", in economics. The core of this paper provides an outline of FRACTI in
five steps. First we present definitions related to scientific support systems
intrinsic to finance and describe common characteristics of financial use
cases. The second step concentrates on what can be exchanged through the
definition of shareable entities called contributions. The third step is the
description of a classification system for building blocks of the conceptual
framework, called facets. The fourth step introduces the meta-model that will
enable provenance tracking and representation of data fragments and simulation.
Finally we describe intended cases of use to highlight main strengths of
FRACTI: application of the scientific method for investigation in computational
finance, large-scale collaboration and simulation.
"
"  Given a field $F$ of $\operatorname{char}(F)=2$, we define $u^n(F)$ to be the
maximal dimension of an anisotropic form in $I_q^n F$. For $n=1$ it recaptures
the definition of $u(F)$. We study the relations between this value and the
symbol length of $H_2^n(F)$, denoted by $sl_2^n(F)$. We show for any $n \geq 2$
that if $2^n \leq u^n(F) \leq u^2(F) < \infty$ then $sl_2^n(F) \leq
\prod_{i=2}^n (\frac{u^i(F)}{2}+1-2^{i-1})$. As a result, if $u(F)$ is finite
then $sl_2^n(F)$ is finite for any $n$, a fact which was previously proven when
$\operatorname{char}(F) \neq 2$ by Saltman and Krashen. We also show that if
$sl_2^n(F)=1$ then $u^n(F)$ is either $2^n$ or $2^{n+1}$.
"
"  The truncated Fourier operator $\mathscr{F}_{\mathbb{R^{+}}}$, $$
(\mathscr{F}_{\mathbb{R^{+}}}x)(t)=\frac{1}{\sqrt{2\pi}}
\int\limits_{\mathbb{R^{+}}}x(\xi)e^{it\xi}\,d\xi\,,\ \ \
t\in{}{\mathbb{R^{+}}}, $$ is studied. The operator
$\mathscr{F}_{\mathbb{R^{+}}}$ is considered as an operator acting in the space
$L^2(\mathbb{R^{+}})$. The functional model for the operator
$\mathscr{F}_{\mathbb{R^{+}}}$ is constructed. This functional model is the
multiplication operator on the appropriate $2\times2$ matrix function acting in
the space $L^2(\mathbb{R^{+}})\oplus{}L^2(\mathbb{R^{+}})$. Using this
functional model, the spectrum of the operator $\mathscr{F}_{\mathbb{R^{+}}}$
is found. The resolvent of the operator $\mathscr{F}_{\mathbb{R^{+}}}$ is
estimated near its spectrum.
"
"  In this paper we establish square-function estimates on the double and single
layer potentials with rough inputs for divergence form elliptic operators, of
arbitrary even order 2m, with variable t-independent coefficients in the upper
half-space.
"
"  The aim of this paper is to study a poset isomorphism between two support
$\tau$-tilting posets. We take several algebraic information from combinatorial
properties of support $\tau$-tilting posets. As an application, we treat a
certain class of basic algebras which contains preprojective algebras of type
$A$, Nakayama algebras, and generalized Brauer tree algebras. We provide a
necessary condition for that an algebra $\Lambda$ share the same support
$\tau$-tilting poset with a given algebra $\Gamma$ in this class. Furthermore,
we see that this necessary condition is also a sufficient condition if $\Gamma$
is either a preprojective algebra of type $A$, a Nakayama algebra, or a
generalized Brauer tree algebra.
"
"  We are concerned with the inverse scattering problem of recovering an
inhomogeneous medium by the associated acoustic wave measurement. We prove that
under certain assumptions, a single far-field pattern determines the values of
a perturbation to the refractive index on the corners of its support. These
assumptions are satisfied for example in the low acoustic frequency regime. As
a consequence if the perturbation is piecewise constant with either a
polyhedral nest geometry or a known polyhedral cell geometry, such as a pixel
or voxel array, we establish the injectivity of the perturbation to far-field
map given a fixed incident wave. This is the first unique determinancy result
of its type in the literature, and all of the existing results essentially make
use of infinitely many measurements.
"
"  In this paper, we further develop the theory of complete mixability and joint
mixability for some distribution families. We generalize a result of
Rüschendorf and Uckelmann (2002) related to complete mixability of continuous
distribution function having a symmetric and unimodal density. Two different
proofs to a result of Wang and Wang (2016) which related to the joint
mixability of elliptical distributions with the same characteristic generator
are present. We solve the Open Problem 7 in Wang (2015) by constructing a
bimodal-symmetric distribution. The joint mixability of slash-elliptical
distributions and skew-elliptical distributions is studied and the extension to
multivariate distributions is also investigated.
"
"  Statistical inference for exponential-family models of random graphs with
dependent edges is challenging. We stress the importance of additional
structure and show that additional structure facilitates statistical inference.
A simple example of a random graph with additional structure is a random graph
with neighborhoods and local dependence within neighborhoods. We develop the
first concentration and consistency results for maximum likelihood and
$M$-estimators of a wide range of canonical and curved exponential-family
models of random graphs with local dependence. All results are non-asymptotic
and applicable to random graphs with finite populations of nodes, although
asymptotic consistency results can be obtained as well. In addition, we show
that additional structure can facilitate subgraph-to-graph estimation, and
present concentration results for subgraph-to-graph estimators. As an
application, we consider popular curved exponential-family models of random
graphs, with local dependence induced by transitivity and parameter vectors
whose dimensions depend on the number of nodes.
"
"  In this paper, we study the compressibility of random processes and fields,
called generalized Lévy processes, that are solutions of stochastic
differential equations driven by $d$-dimensional periodic Lévy white noises.
Our results are based on the estimation of the Besov regularity of Lévy white
noises and generalized Lévy processes. We show in particular that
non-Gaussian generalized Lévy processes are more compressible in a wavelet
basis than the corresponding Gaussian processes, in the sense that their
$n$-term approximation error decays faster. We quantify this compressibility in
terms of the Blumenthal-Getoor index of the underlying Lévy white noise.
"
"  Space-filling designs are popular choices for computer experiments. A sliced
design is a design that can be partitioned into several subdesigns. We propose
a new type of sliced space-filling design called sliced rotated sphere packing
designs. Their full designs and subdesigns are rotated sphere packing designs.
They are constructed by rescaling, rotating, translating and extracting the
points from a sliced lattice. We provide two fast algorithms to generate such
designs. Furthermore, we propose a strategy to use sliced rotated sphere
packing designs adaptively. Under this strategy, initial runs are uniformly
distributed in the design space, follow-up runs are added by incorporating
information gained from initial runs, and the combined design is space-filling
for any local region. Examples are given to illustrate its potential
application.
"
"  The article addresses a long-standing open problem on the justification of
using variational Bayes methods for parameter estimation. We provide general
conditions for obtaining optimal risk bounds for point estimates acquired from
mean-field variational Bayesian inference. The conditions pertain to the
existence of certain test functions for the distance metric on the parameter
space and minimal assumptions on the prior. A general recipe for verification
of the conditions is outlined which is broadly applicable to existing Bayesian
models with or without latent variables. As illustrations, specific
applications to Latent Dirichlet Allocation and Gaussian mixture models are
discussed.
"
"  This paper is a comprehensive introduction to the results of [7]. It grew as
an expanded version of a talk given at INdAM Meeting Complex and Symplectic
Geometry, held at Cortona in June 12-18, 2016. It deals with the construction
of the Teichmüller space of a smooth compact manifold M (that is the space of
isomorphism classes of complex structures on M) in arbitrary dimension. The
main problem is that, whenever we leave the world of surfaces, the
Teichmüller space is no more a complex manifold or an analytic space but an
analytic Artin stack. We explain how to construct explicitly an atlas for this
stack using ideas coming from foliation theory. Throughout the article, we use
the case of $\mathbb{S}^3\times\mathbb{S}^1$ as a recurrent example.
"
"  The almost sure Hausdorff dimension of the limsup set of randomly distributed
rectangles in a product of Ahlfors regular metric spaces is computed in terms
of the singular value function of the rectangles.
"
"  Let $V_1,V_2,V_3$ be a triple of even dimensional vector spaces over a number
field $F$ equipped with nondegenerate quadratic forms
$\mathcal{Q}_1,\mathcal{Q}_2,\mathcal{Q}_3$, respectively. Let \begin{align*} Y
\subset \prod_{i=1}V_i \end{align*} be the closed subscheme consisting of
$(v_1,v_2,v_3)$ on which
$\mathcal{Q}_1(v_1)=\mathcal{Q}_2(v_2)=\mathcal{Q}_3(v_3)$. Motivated by
conjectures of Braverman and Kazhdan and related work of Lafforgue, Ngô, and
Sakellaridis we prove an analogue of the Poisson summation formula for certain
functions on this space.
"
"  Let $f$ be a Lipschitz map from a subset $A$ of a stratified group to a
Banach homogeneous group. We show that directional derivatives of $f$ act as
homogeneous homomorphisms at density points of $A$ outside a $\sigma$-porous
set. At density points of $A$ we establish a pointwise characterization of
differentiability in terms of directional derivatives. We use these new results
to obtain an alternate proof of almost everywhere differentiability of
Lipschitz maps from subsets of stratified groups to Banach homogeneous groups
satisfying a suitably weakened Radon-Nikodym property. As a consequence we also
get an alternative proof of Pansu's Theorem.
"
"  We study which algebras have tilting modules that are both generated and
cogenerated by projective-injective modules. Crawley-Boevey and Sauter have
shown that Auslander algebras have such tilting modules; and for algebras of
global dimension $2$, Auslander algebras are classified by the existence of
such tilting modules.
In this paper, we show that the existence of such a tilting module is
equivalent to the algebra having dominant dimension at least $2$, independent
of its global dimension. In general such a tilting module is not necessarily
cotilting. Here, we show that the algebras which have a tilting-cotilting
module generated-cogenerated by projective-injective modules are precisely
$1$-Auslander-Gorenstein algebras.
When considering such a tilting module, without the assumption that it is
cotilting, we study the global dimension of its endomorphism algebra, and
discuss a connection with the Finitistic Dimension Conjecture. Furthermore, as
special cases, we show that triangular matrix algebras obtained from Auslander
algebras and certain injective modules, have such a tilting module. We also
give a description of which Nakayama algebras have such a tilting module.
"
"  Recently, the authors and de Wolff introduced the imaginary projection of a
polynomial $f\in\mathbb{C}[\mathbf{z}]$ as the projection of the variety of $f$
onto its imaginary part, $\mathcal{I}(f) \ = \ \{\text{Im}(\mathbf{z}) \, : \,
\mathbf{z} \in \mathcal{V}(f) \}$. Since a polynomial $f$ is stable if and only
if $\mathcal{I}(f) \cap \mathbb{R}_{>0}^n \ = \ \emptyset$, the notion offers a
novel geometric view underlying stability questions of polynomials. In this
article, we study the relation between the imaginary projections and
hyperbolicity cones, where the latter ones are only defined for homogeneous
polynomials. Building upon this, for homogeneous polynomials we provide a tight
upper bound for the number of components in the complement $\mathcal{I}(f)^{c}$
and thus for the number of hyperbolicity cones of $f$. And we show that for $n
\ge 2$, a polynomial $f$ in $n$ variables can have an arbitrarily high number
of strictly convex and bounded components in $\mathcal{I}(f)^{c}$.
"
"  Community identification in a network is an important problem in fields such
as social science, neuroscience, and genetics. Over the past decade, stochastic
block models (SBMs) have emerged as a popular statistical framework for this
problem. However, SBMs have an important limitation in that they are suited
only for networks with unweighted edges; in various scientific applications,
disregarding the edge weights may result in a loss of valuable information. We
study a weighted generalization of the SBM, in which observations are collected
in the form of a weighted adjacency matrix and the weight of each edge is
generated independently from an unknown probability density determined by the
community membership of its endpoints. We characterize the optimal rate of
misclustering error of the weighted SBM in terms of the Renyi divergence of
order 1/2 between the weight distributions of within-community and
between-community edges, substantially generalizing existing results for
unweighted SBMs. Furthermore, we present a computationally tractable algorithm
based on discretization that achieves the optimal error rate. Our method is
adaptive in the sense that the algorithm, without assuming knowledge of the
weight densities, performs as well as the best algorithm that knows the weight
densities.
"
"  This work considers a stochastic Nash game in which each player solves a
parameterized stochastic optimization problem. In deterministic regimes,
best-response schemes have been shown to be convergent under a suitable
spectral property associated with the proximal best-response map. However, a
direct application of this scheme to stochastic settings requires obtaining
exact solutions to stochastic optimization at each iteration. Instead, we
propose an inexact generalization in which an inexact solution is computed via
an increasing number of projected stochastic gradient steps. Based on this
framework, we present three inexact best-response schemes: (i) First, we
propose a synchronous scheme where all players simultaneously update their
strategies; (ii) Subsequently, we extend this to a randomized setting where a
subset of players is randomly chosen to their update strategies while the
others keep their strategies invariant; (iii) Finally, we propose an
asynchronous scheme, where each player determines its own update frequency and
may use outdated rival-specific data in updating its strategy. Under a suitable
contractive property of the proximal best-response map, we derive a.s.
convergence of the iterates for (i) and (ii) and mean-convergence for (i) --
(iii). In addition, we show that for (i) -- (iii), the iterates converge to the
unique equilibrium in mean at a prescribed linear rate. Finally, we establish
the overall iteration complexity in terms of projected stochastic gradient
steps for computing an $\epsilon-$Nash equilibrium and in all settings, the
iteration complexity is ${\cal O}(1/\epsilon^{2(1+c) + \delta})$ where $c = 0$
in the context of (i) and represents the positive cost of randomization (in
(ii)) and asynchronicity and delay (in (iii)). The schemes are further extended
to linear and quadratic recourse-based stochastic Nash games.
"
"  Penalty-based variable selection methods are powerful in selecting relevant
covariates and estimating coefficients simultaneously. However, variable
selection could fail to be consistent when covariates are highly correlated.
The partial correlation approach has been adopted to solve the problem with
correlated covariates. Nevertheless, the restrictive range of partial
correlation is not effective for capturing signal strength for relevant
covariates. In this paper, we propose a new Semi-standard PArtial Covariance
(SPAC) which is able to reduce correlation effects from other predictors while
incorporating the magnitude of coefficients. The proposed SPAC variable
selection facilitates choosing covariates which have direct association with
the response variable, via utilizing dependency among covariates. We show that
the proposed method with the Lasso penalty (SPAC-Lasso) enjoys strong sign
consistency in both finite-dimensional and high-dimensional settings under
regularity conditions. Simulation studies and the `HapMap' gene data
application show that the proposed method outperforms the traditional Lasso,
adaptive Lasso, SCAD, and Peter-Clark-simple (PC-simple) methods for highly
correlated predictors.
"
"  We consider composite-composite testing problems for the expectation in the
Gaussian sequence model where the null hypothesis corresponds to a convex
subset $\mathcal{C}$ of $\mathbb{R}^d$. We adopt a minimax point of view and
our primary objective is to describe the smallest Euclidean distance between
the null and alternative hypotheses such that there is a test with small total
error probability. In particular, we focus on the dependence of this distance
on the dimension $d$ and the sample size/variance parameter $n$ giving rise to
the minimax separation rate. In this paper we discuss lower and upper bounds on
this rate for different smooth and non- smooth choices for $\mathcal{C}$.
"
"  In this paper we investigate the metric properties of quadrics and cones of
the $n$-dimensional Euclidean space. As applications of our formulas we give a
more detailed description of the construction of Chasles and the wire model of
Staude, respectively.
"
"  Let $M$ be a II$_1$ factor with a von Neumann subalgebra $Q\subset M$ that
has infinite index under any projection in $Q'\cap M$ (e.g., $Q$ abelian; or
$Q$ an irreducible subfactor with infinite Jones index). We prove that given
any separable subalgebra $B$ of the ultrapower II$_1$ factor $M^\omega$, for a
non-principal ultrafilter $\omega$ on $\Bbb N$, there exists a unitary element
$u\in M^\omega$ such that $uBu^*$ is orthogonal to $Q^\omega$.
"
"  In this paper we consider the Witten Laplacian on 0-forms and give sufficient
conditions under which the Witten Laplacian admits a compact resolvent. These
conditions are imposed on the potential itself, involving the control of high
order derivatives by lower ones, as well as the control of the positive
eigenvalues of the Hessian matrix. This compactness criterion for resolvent is
inspired by the one for the Fokker-Planck operator. Our method relies on the
nilpotent group techniques developed by Helffer-Nourrigat [Hypoellipticité
maximale pour des opérateurs polynômes de champs de vecteurs, 1985].
"
"  For $p > 1$ let a function $\varphi_p(x) = x^2/2$ if $|x|\le 1$ and
$\varphi_p(x) = 1/p|x|^p -1/p + 1/2$ if $|x| > 1$. For a random variable $\xi$
let $\tau_{\varphi_p}(\xi)$ denote $\inf\{c\ge 0 :\;
\forall_{\lambda\in\mathbb{R}}\;
\ln\mathbb{E}\exp(\lambda\xi)\le\varphi_p(c\lambda)\}$; $\tau_{\varphi_p}$ is a
norm in a space $Sub_{\varphi_p}(\Omega) =\{\xi:
\; \tau_{\varphi_p}(\xi) <\infty\}$ of $\varphi_p$-subgaussian random
variables which we call {\it subgaussian of rank $p$ random variables}. For $p
= 2$ we have the classic subgaussian random variables. The Azuma inequality
gives an estimate on the probability of the deviations of a zero-mean
martingale $(\xi_n)_{n\ge 0}$ with bounded increments from zero. In its classic
form is assumed that $\xi_0 = 0$. In this paper it is shown a version of the
Azuma inequality under assumption that $\xi_0$ is any subgaussian of rank $p$
random variable.
"
"  We develop the general theory for the construction of Extended Topological
Quantum Field Theories (ETQFTs) associated with the Costantino-Geer-Patureau
quantum invariants of closed 3-manifolds. In order to do so, we introduce
relative modular categories, a class of ribbon categories which are modeled on
representations of unrolled quantum groups, and which can be thought of as a
non-semisimple analogue to modular categories. Our approach exploits a
2-categorical version of the universal construction introduced by Blanchet,
Habegger, Masbaum, and Vogel. The 1+1+1-EQFTs thus obtained are realized by
symmetric monoidal 2-functors which are defined over non-rigid 2-categories of
admissible cobordisms decorated with colored ribbon graphs and cohomology
classes, and which take values in 2-categories of complete graded linear
categories. In particular, our construction extends the family of graded
2+1-TQFTs defined for the unrolled version of quantum $\mathfrak{sl}_2$ by
Blanchet, Costantino, Geer, and Patureau to a new family of graded ETQFTs. The
non-semisimplicity of the theory is witnessed by the presence of non-semisimple
graded linear categories associated with critical 1-manifolds.
"
"  Growth in both size and complexity of modern data challenges the
applicability of traditional likelihood-based inference. Composite likelihood
(CL) methods address the difficulties related to model selection and
computational intractability of the full likelihood by combining a number of
low-dimensional likelihood objects into a single objective function used for
inference. This paper introduces a procedure to combine partial likelihood
objects from a large set of feasible candidates and simultaneously carry out
parameter estimation. The new method constructs estimating equations balancing
statistical efficiency and computing cost by minimizing an approximate distance
from the full likelihood score subject to a L1-norm penalty representing the
available computing resources. This results in truncated CL equations
containing only the most informative partial likelihood score terms. An
asymptotic theory within a framework where both sample size and data dimension
grow is developed and finite-sample properties are illustrated through
numerical examples.
"
"  We construct a family of vertex algebras associated with a family of
symplectic singularity/resolution, called hypertoric varieties. While the
hypertoric varieties are constructed by a certain Hamiltonian reduction
associated with a torus action, our vertex algebras are constructed by
(semi-infinite) BRST reduction. The construction works algebro-geometrically
and we construct sheaves of $\hbar$-adic vertex algebras over hypertoric
varieties which localize the vertex algebras. We show when the vertex algebras
are vertex operator algebras by giving explicit conformal vectors. We also show
that the Zhu algebras of the vertex algebras, associative algebras associated
with non-negatively graded vertex algebras, gives a certain family of filtered
quantizations of the coordinate rings of the hypertoric varieties.
"
"  This paper provides a set of sensitivity analysis and activity identification
results for a class of convex functions with a strong geometric structure, that
we coined ""mirror-stratifiable"". These functions are such that there is a
bijection between a primal and a dual stratification of the space into
partitioning sets, called strata. This pairing is crucial to track the strata
that are identifiable by solutions of parametrized optimization problems or by
iterates of optimization algorithms. This class of functions encompasses all
regularizers routinely used in signal and image processing, machine learning,
and statistics. We show that this ""mirror-stratifiable"" structure enjoys a nice
sensitivity theory, allowing us to study stability of solutions of optimization
problems to small perturbations, as well as activity identification of
first-order proximal splitting-type algorithms. Existing results in the
literature typically assume that, under a non-degeneracy condition, the active
set associated to a minimizer is stable to small perturbations and is
identified in finite time by optimization schemes. In contrast, our results do
not require any non-degeneracy assumption: in consequence, the optimal active
set is not necessarily stable anymore, but we are able to track precisely the
set of identifiable strata.We show that these results have crucial implications
when solving challenging ill-posed inverse problems via regularization, a
typical scenario where the non-degeneracy condition is not fulfilled. Our
theoretical results, illustrated by numerical simulations, allow to
characterize the instability behaviour of the regularized solutions, by
locating the set of all low-dimensional strata that can be potentially
identified by these solutions.
"
"  This paper is about models for a vector of probabilities whose elements must
have a multiplicative structure and sum to 1 at the same time; in certain
applications, as basket analysis, these models may be seen as a constrained
version of quasi-independence. After reviewing the basic properties of these
models, their geometric features as a curved exponential family are
investigated. A new algorithm for computing maximum likelihood estimates is
presented and new insights are provided on the underlying geometry. The
asymptotic distribution of three statistics for hypothesis testing are derived
and a small simulation study is presented to investigate the accuracy of
asymptotic approximations.
"
"  A group theoretical formulation of Schramm--Loewner-evolution-type growth
processes corresponding to Wess--Zumino--Witten theories is developed that
makes it possible to construct stochastic differential equations associated
with more general null vectors than the ones considered in the most fundamental
example in [Alekseev et al., Lett. Math. Phys. 97, 243-261 (2011)]. Also given
are examples of Schramm--Loewner-evolution-type growth processes associated
with null vectors of conformal weight $4$ in the basic representations of
$\widehat{\mathfrak{sl}}_{2}$ and $\widehat{\mathfrak{sl}}_{3}$.
"
"  We recall first Gallai-simplicial complex $\Delta_{\Gamma}(G)$ associated to
Gallai graph $\Gamma(G)$ of a planar graph $G$. The Euler characteristic is a
very useful topological and homotopic invariant to classify surfaces. In
Theorems 3.2 and 3.4, we compute Euler characteristics of Gallai-simplicial
complexes associated to triangular ladder and prism graphs, respectively.
Let $G$ be a finite simple graph on $n$ vertices of the form $n=3l+2$ or
$3l+3$. In Theorem 4.4, we prove that $G$ will be $f$-Gallai graph for the
following types of constructions of $G$.
Type 1. When $n=3l+2$. $G=\mathbb{S}_{4l}$ is a graph consisting of two
copies of star graphs $S_{2l}$ and $S'_{2l}$ with $l\geq 2$ having $l$ common
vertices.
Type 2. When $n=3l+3$. $G=\mathbb{S}_{4l+1}$ is a graph consisting of two
star graphs $S_{2l}$ and $S_{2l+1}$ with $l\geq 2$ having $l$ common vertices.
"
"  We prove an equivalence between the infinitesimal Torelli theorem for top
forms on a hypersurface contained inside a Grassmannian $\mathbb G$ and the
theory of adjoint volume forms presented in L. Rizzi, F. Zucconi, ""Generalized
adjoint forms on algebraic varieties"", Ann. Mat. Pura e Applicata, in press.
More precisely, via this theory and a suitable generalization of Macaulay's
theorem we show that the differential of the period map vanishes on an
infinitesimal deformation if and only if certain explicitly given twisted
volume forms go in the generalized Jacobi ideal of $X$ via the cup product
homomorphism.
"
"  We consider the Lasso for a noiseless experiment where one has observations
$X \beta^0$ and uses the penalized version of basis pursuit. We compute for
some special designs the compatibility constant, a quantity closely related to
the restricted eigenvalue. We moreover show the dependence of the (penalized)
prediction error on this compatibility constant. This exercise illustrates that
compatibility is necessarily entering into the bounds for the (penalized)
prediction error and that the bounds in the literature therefore are - up to
constants - tight. We also give conditions that show that in the noisy case the
dominating term for the prediction error is given by the prediction error of
the noiseless case.
"
"  The Ensemble Kalman methodology in an inverse problems setting can be viewed
as an iterative scheme, which is a weakly tamed discretization scheme for a
certain stochastic differential equation (SDE). Assuming a suitable
approximation result, dynamical properties of the SDE can be rigorously pulled
back via the discrete scheme to the original Ensemble Kalman inversion.
The results of this paper make a step towards closing the gap of the missing
approximation result by proving a strong convergence result in a simplified
model of a scalar stochastic differential equation. We focus here on a toy
model with similar properties than the one arising in the context of Ensemble
Kalman filter. The proposed model can be interpreted as a single particle
filter for a linear map and thus forms the basis for further analysis. The
difficulty in the analysis arises from the formally derived limiting SDE with
non-globally Lipschitz continuous nonlinearities both in the drift and in the
diffusion. Here the standard Euler-Maruyama scheme might fail to provide a
strongly convergent numerical scheme and taming is necessary. In contrast to
the strong taming usually used, the method presented here provides a weaker
form of taming.
We present a strong convergence analysis by first proving convergence on a
domain of high probability by using a cut-off or localisation, which then
leads, combined with bounds on moments for both the SDE and the numerical
scheme, by a bootstrapping argument to strong convergence.
"
"  We build new algebraic structures, which we call genuine equivariant operads,
which can be thought of as a hybrid between equivariant operads and coefficient
systems. We then prove an Elmendorf-Piacenza type theorem stating that
equivariant operads, with their graph model structure, are equivalent to
genuine equivariant operads, with their projective model structure.
As an application, we build explicit models for the $N_{\infty}$-operads of
Blumberg and Hill.
"
"  In this work, we propose a novel method for quantifying distances between
Toeplitz structured covariance matrices. By exploiting the spectral
representation of Toeplitz matrices, the proposed distance measure is defined
based on an optimal mass transport problem in the spectral domain. This may
then be interpreted in the covariance domain, suggesting a natural way of
interpolating and extrapolating Toeplitz matrices, such that the positive
semi-definiteness and the Toeplitz structure of these matrices are preserved.
The proposed distance measure is also shown to be contractive with respect to
both additive and multiplicative noise, and thereby allows for a quantification
of the decreased distance between signals when these are corrupted by noise.
Finally, we illustrate how this approach can be used for several applications
in signal processing. In particular, we consider interpolation and
extrapolation of Toeplitz matrices, as well as clustering problems and tracking
of slowly varying stochastic processes.
"
"  Classes of locally compact groups having qualitative uncertainty principle
for Gabor transform have been investigated. These include Moore groups,
Heisenberg Group $\mathbb{H}_n, \mathbb{H}_{n} \times D,$ where $D$ is discrete
group and other low dimensional nilpotent Lie groups.
"
"  The inverse problem of antiplane elasticity on determination of the profiles
of $n$ uniformly stressed inclusions is studied. The inclusions are in ideal
contact with the surrounding matrix, the stress field inside the inclusions is
uniform, and at infinity the body is subjected to antiplane uniform shear. The
exterior of the inclusions, an $n$-connected domain, is treated as the image by
a conformal map of an $n$-connected slit domain with the slits lying in the
same line. The inverse problem is solved by quadratures by reducing it to two
Riemann-Hilbert problems on a Riemann surface of genus $n-1$. Samples of two
and three symmetric and non-symmetric uniformly stressed inclusions are
reported.
"
"  We present a construction of a 2-Hilbert space of sections of a bundle gerbe,
a suitable candidate for a prequantum 2-Hilbert space in higher geometric
quantisation. We introduce a direct sum on the morphism categories in the
2-category of bundle gerbes and show that these categories are cartesian
monoidal and abelian. Endomorphisms of the trivial bundle gerbe, or higher
functions, carry the structure of a rig-category, which acts on generic
morphism categories of bundle gerbes. We continue by presenting a
categorification of the hermitean metric on a hermitean line bundle. This is
achieved by introducing a functorial dual that extends the dual of vector
bundles to morphisms of bundle gerbes, and constructing a two-variable
adjunction for the aforementioned rig-module category structure on morphism
categories. Its right internal hom is the module action, composed by taking the
dual of higher functions, while the left internal hom is interpreted as a
bundle gerbe metric. Sections of bundle gerbes are defined as morphisms from
the trivial bundle gerbe to a given bundle gerbe. The resulting categories of
sections carry a rig-module structure over the category of finite-dimensional
Hilbert spaces. A suitable definition of 2-Hilbert spaces is given, modifying
previous definitions by the use of two-variable adjunctions. We prove that the
category of sections of a bundle gerbe fits into this framework, thus obtaining
a 2-Hilbert space of sections. In particular, this can be constructed for
prequantum bundle gerbes in problems of higher geometric quantisation. We
define a dimensional reduction functor and show that the categorical structures
introduced on bundle gerbes naturally reduce to their counterparts on hermitean
line bundles with connections. In several places in this thesis, we provide
examples, making 2-Hilbert spaces of sections and dimensional reduction very
explicit.
"
"  Ensemble Kalman filter (EnKF) is an important data assimilation method for
high dimensional geophysical systems. Efficient implementation of EnKF in
practice often involves the localization technique, which updates each
component using only information within a local radius. This paper rigorously
analyzes the local EnKF (LEnKF) for linear systems, and shows that the filter
error can be dominated by the ensemble covariance, as long as 1) the sample
size exceeds the logarithmic of state dimension and a constant that depends
only on the local radius; 2) the forecast covariance matrix admits a stable
localized structure. In particular, this indicates that with small system and
observation noises, the filter error will be accurate in long time even if the
initialization is not. The analysis also reveals an intrinsic inconsistency
caused by the localization technique, and a stable localized structure is
necessary to control this inconsistency. While this structure is usually taken
for granted for the operation of LEnKF, it can also be rigorously proved for
linear systems with sparse local observations and weak local interactions.
These theoretical results are also validated by numerical implementation of
LEnKF on a simple stochastic turbulence in two dynamical regimes.
"
"  This note is devoted to the study of the homology class of a compact Poisson
transversal in a Poisson manifold. For specific classes of Poisson structures,
such as unimodular Poisson structures and Poisson manifolds with closed leaves,
we prove that all their compact Poisson transversals represent non-trivial
homology classes, generalizing the symplectic case. We discuss several examples
in which this property does not hold, as well as a weaker version of this
property, which holds for log-symplectic structures. Finally, we extend our
results to Dirac geometry.
"
"  This paper provides an alternative approach to the theory of dynamic
programming, designed to accommodate the kinds of recursive preference
specifications that have become popular in economic and financial analysis,
while still supporting traditional additively separable rewards. The approach
exploits the theory of monotone convex operators, which turns out to be well
suited to dynamic maximization. The intuition is that convexity is preserved
under maximization, so convexity properties found in preferences extend
naturally to the Bellman operator.
"
"  A generic model for the shape optimization problems we consider in this paper
is the optimization of the Dirichlet eigenvalues of the Laplace operator with a
volume constraint. We deal with an obstacle placement problem which can be
formulated as the following eigenvalue optimization problem: Fix two positive
real numbers $r_1$ and $A$. We consider a disk $B\subset \mathbb{R}^2$ having
radius $r_1$. We want to place an obstacle $P$ of area $A$ within $B$ so as to
maximize or minimize the fundamental Dirichlet eigenvalue $\lambda_1$ for the
Laplacian on $B\setminus P$. That is, we want to study the behavior of the
function $\rho \mapsto \lambda_1(B\setminus\rho(P))$, where $\rho$ runs over
the set of all rigid motions of the plane fixing the center of mass for $P$
such that $\rho(P)\subset B$. In this paper, we consider a non-concentric
obstacle placement problem. The extremal configurations correspond to the cases
where an axis of symmetry of $P$ coincide with an axis of symmetry of $B$. We
also characterize the maximizing and the minimizing configurations in our main
result, viz., Theorem 4.1. Equation (6), Propositions 5.1 and 5.2 imply Theorem
4.1. We give many different generalizations of our result. At the end, we
provide some numerical evidence to validate our main theorem for the case where
the obstacle $P$ has $\mathbb{D}_4$ symmetry. For the $n$ odd case, we identify
some of the extremal configuration for $\lambda_1$. We prove that equation (6)
and Proposition 5.1 hold true for $n$ odd too. We highlight some of the
difficulties faced in proving Proposition 5.2 for this case. We provide
numerical evidence for $n=5$ and conjecture that Theorem 4.1 holds true for $n$
odd too.
"
"  The manifold which admits a genus-$2$ reducible Heegaard splitting is one of
the $3$-sphere, $\mathbb{S}^2 \times \mathbb{S}^1$, lens spaces and their
connected sums. For each of those manifolds except most lens spaces, the
mapping class group of the genus-$2$ splitting was shown to be finitely
presented. In this work, we study the remaining generic lens spaces, and show
that the mapping class group of the genus-$2$ Heegaard splitting is finitely
presented for any lens space by giving its explicit presentation. As an
application, we show that the fundamental groups of the spaces of the genus-$2$
Heegaard splittings of lens spaces are all finitely presented.
"
"  Two meromorphic functions $f(z)$ and $g(z)$ sharing a small function
$\alpha(z)$ usually is defined in terms of vanishing of the functions
$f-\alpha$ and $g-\alpha$. We argue that it would be better to modify this
definition at the points where $\alpha$ has poles. Related to this issue we
also point out some possible gaps in proofs in the published literature.
"
"  In the framework of the Laplacian transport, described by a Robin boundary
value problem in an exterior domain in $\mathbb{R}^n$, we generalize the
definition of the Poincaré-Steklov operator to $d$-set boundaries, $n-2<
d<n$, and give its spectral properties to compare to the spectra of the
interior domain and also of a truncated domain, considered as an approximation
of the exterior case. The well-posedness of the Robin boundary value problems
for the truncated and exterior domains is given in the general framework of
$n$-sets. The results are obtained thanks to a generalization of the continuity
and compactness properties of the trace and extension operators in Sobolev,
Lebesgue and Besov spaces, in particular, by a generalization of the classical
Rellich-Kondrachov Theorem of compact embeddings for $n$ and $d$-sets.
"
"  We obtain the optimal Bayesian minimax rate for the unconstrained large
covariance matrix of multivariate normal sample with mean zero, when both the
sample size, n, and the dimension, p, of the covariance matrix tend to
infinity. Traditionally the posterior convergence rate is used to compare the
frequentist asymptotic performance of priors, but defining the optimality with
it is elusive. We propose a new decision theoretic framework for prior
selection and define Bayesian minimax rate. Under the proposed framework, we
obtain the optimal Bayesian minimax rate for the spectral norm for all rates of
p. We also considered Frobenius norm, Bregman divergence and squared
log-determinant loss and obtain the optimal Bayesian minimax rate under certain
rate conditions on p. A simulation study is conducted to support the
theoretical results.
"
"  We solve a lifecycle model in which the consumer's chronological age does not
move in lockstep with calendar time. Instead, biological age increases at a
stochastic non-linear rate in time like a broken clock that might occasionally
move backwards. In other words, biological age could actually decline. Our
paper is inspired by the growing body of medical literature that has identified
biomarkers which indicate how people age at different rates. This offers better
estimates of expected remaining lifetime and future mortality rates. It isn't
farfetched to argue that in the not-too-distant future personal age will be
more closely associated with biological vs. calendar age. Thus, after
introducing our stochastic mortality model we derive optimal consumption rates
in a classic Yaari (1965) framework adjusted to our proper clock time. In
addition to the normative implications of having access to biological age, our
positive objective is to partially explain the cross-sectional heterogeneity in
retirement spending rates at any given chronological age. In sum, we argue that
neither biological nor chronological age alone is a sufficient statistic for
making economic decisions. Rather, both ages are required to behave rationally.
"
"  We derive in a direct way the exact controllability of the 1D free
Schrödinger equation with Dirichlet boundary control. We use the so-called
flatness approach, which consists in parametrizing the solution and the control
by the derivatives of a ""flat output"". This provides an explicit and very
regular control achieving the exact controllability in the energy space.
"
"  We prove the following conjecture of Leighton and Moitra. Let $T$ be a
tournament on $[n]$ and $S_n$ the set of permutations of $[n]$. For an arc $uv$
of $T$, let $A_{uv}=\{\sigma \in S_n \, : \, \sigma(u)<\sigma(v) \}$.
$\textbf{Theorem.}$ For a fixed $\varepsilon>0$, if $\mathbb{P}$ is a
probability distribution on $S_n$ such that
$\mathbb{P}(A_{uv})>1/2+\varepsilon$ for every arc $uv$ of $T$, then the binary
entropy of $\mathbb{P}$ is at most $(1-\vartheta_{\varepsilon})\log_2 n!$ for
some (fixed) positive $\vartheta_\varepsilon$.
When $T$ is transitive the theorem is due to Leighton and Moitra; for this
case we give a short proof with a better $\vartheta_\varepsilon$.
"
"  By assuming some widely-believed arithmetic conjectures, we show that the
task of accepting a number that is representable as a sum of $d\geq2$ squares
subjected to given congruence conditions is NP-complete. On the other hand, we
develop and implement a deterministic polynomial-time algorithm that represents
a number as a sum of 4 squares with some restricted congruence conditions, by
assuming a polynomial-time algorithm for factoring integers and
Conjecture~\ref{cc}. As an application, we develop and implement a
deterministic polynomial-time algorithm for navigating LPS Ramanujan graphs,
under the same assumptions.
"
"  This paper addresses the question of whether it can be beneficial for an
optimization algorithm to follow directions of negative curvature. Although
prior work has established convergence results for algorithms that integrate
both descent and negative curvature steps, there has not yet been extensive
numerical evidence showing that such methods offer consistent performance
improvements. In this paper, we present new frameworks for combining descent
and negative curvature directions: alternating two-step approaches and dynamic
step approaches. The aspect that distinguishes our approaches from ones
previously proposed is that they make algorithmic decisions based on
(estimated) upper-bounding models of the objective function. A consequence of
this aspect is that our frameworks can, in theory, employ fixed stepsizes,
which makes the methods readily translatable from deterministic to stochastic
settings. For deterministic problems, we show that instances of our dynamic
framework yield gains in performance compared to related methods that only
follow descent steps. We also show that gains can be made in a stochastic
setting in cases when a standard stochastic-gradient-type method might make
slow progress.
"
"  We generalise the notion of a separating intersection of links (SIL) to give
necessary and sufficient criteria on the defining graph $\Gamma$ of a
right-angled Coxeter group $W_\Gamma$ so that its outer automorphism group is
large: that is, it contains a finite index subgroup that admits the free group
$F_2$ as a quotient. When $Out(W_\Gamma)$ is not large, we show it is virtually
abelian. We also show that the same dichotomy holds for the outer automorphism
groups of graph products of finite abelian groups. As a consequence, these
groups have property (T) if and only if they are finite, or equivalently
$\Gamma$ contains no SIL.
"
"  We define causal estimands for experiments on single time series, extending
the potential outcome framework to dealing with temporal data. Our approach
allows the estimation of some of these estimands and exact randomization based
p-values for testing causal effects, without imposing stringent assumptions. We
test our methodology on simulated ""potential autoregressions,""which have a
causal interpretation. Our methodology is partially inspired by data from a
large number of experiments carried out by a financial company who compared the
impact of two different ways of trading equity futures contracts. We use our
methodology to make causal statements about their trading methods.
"
"  The protection number of a plane tree is the minimal distance of the root to
a leaf; this definition carries over to an arbitrary node in a plane tree by
considering the maximal subtree having this node as a root. We study the the
protection number of a uniformly chosen random tree of size $n$ and also the
protection number of a uniformly chosen node in a uniformly chosen random tree
of size $n$. The method is to apply singularity analysis to appropriate
generating functions. Additional results are provided as well.
"
"  For $e \in S^{2}$, the unit sphere in $\mathbb{R}^3$, let $\pi_{e}$ be the
orthogonal projection to $e^{\perp} \subset \mathbb{R}^{3}$, and let $W \subset
\mathbb{R}^{3}$ be any $2$-plane, which is not a subspace. We prove that if $K
\subset \mathbb{R}^{3}$ is a Borel set with $\dim_{\mathrm{H}} K \leq
\tfrac{3}{2}$, then $\dim_{\mathrm{H}} \pi_{e}(K) = \dim_{\mathrm{H}} K$ for
$\mathcal{H}^{1}$ almost every $e \in S^{2} \cap W$, where $\mathcal{H}^{1}$
denotes the $1$-dimensional Hausdorff measure and $\dim_{\mathrm{H}}$ the
Hausdorff dimension. This was known earlier, due to Järvenpää,
Järvenpää, Ledrappier and Leikas, for Borel sets $K \subset
\mathbb{R}^{3}$ with $\dim_{\mathrm{H}} K \leq 1$. We also prove a partial
result for sets with dimension exceeding $3/2$, improving earlier bounds by D.
Oberlin and R. Oberlin.
"
"  Let $\mathbb Q^{n+1}_c$ be the complete simply-connected $(n+1)$-dimensional
space form of curvature $c$. In this paper we obtain a new characterization of
geodesic spheres in $\mathbb Q^{n+1}_c$ in terms of the higher order mean
curvatures. In particular, we prove that the geodesic sphere is the only
complete bounded immersed hypersurface in $\mathbb Q^{n+1}_c,\;c\leq 0,$ with
constant mean curvature and constant scalar curvature. The proof relies on the
well known Omori-Yau maximum principle, a formula of Walter for the Laplacian
of the $r$-th mean curvature of a hypersurface in a space form, and a classical
inequality of G\aa rding for hyperbolic polynomials.
"
"  Let us say that an $n$-sided polygon is semi-regular if it is
circumscriptible and its angles are all equal but possibly one, which is then
larger than the rest. Regular polygons, in particular, are semi-regular. We
prove that semi-regular polygons are spectrally determined in the class of
convex piecewise smooth domains. Specifically, we show that if $\Omega$ is a
convex piecewise smooth planar domain, possibly with straight corners, whose
Dirichlet or Neumann spectrum coincides with that of an $n$-sided semi-regular
polygon $P_n$, then $\Omega$ is congruent to $P_n$.
"
"  The class of Cressie-Read empirical likelihoods are constructed with weights
derived at a minimum distance from the empirical distribution in the
Cressie-Read family of divergences indexed by $\gamma$ under the constraint of
an unbiased set of $M$-estimating equations. At first order, they provide valid
posterior probability statements for any given prior, but the bias in coverage
of the resulting empirical quantile is inversely proportional to the asymptotic
efficiency of the corresponding $M$-estimator. The Cressie-Read empirical
likelihoods based on the maximum likelihood estimating equations bring about
quantiles covering with $O(n^{-1})$ accuracy at the underlying posterior
distribution. The choice of $\gamma$ has an impact on the variance in small
samples of the posterior quantile function. Examples are given for the $M$-type
estimating equations of location and for the quasi-likelihood functions in the
generalized linear models.
"
"  The aim of this paper is to prove a generalization of the famous Theorem A of
Quillen for strict $\infty$-categories. This result is central to the homotopy
theory of strict $\infty$-categories developed by the authors. The proof
presented here is of a simplicial nature and uses Steiner's theory of augmented
directed complexes. In a subsequent paper, we will prove the same result by
purely $\infty$-categorical methods.
"
"  The efficient simulation of isotropic Gaussian random fields on the unit
sphere is a task encountered frequently in numerical applications. A fast
algorithm based on Markov properties and fast Fourier Transforms in 1d is
presented that generates samples on an n x n grid in O(n^2 log n). Furthermore,
an efficient method to set up the necessary conditional covariance matrices is
derived and simulations demonstrate the performance of the algorithm. An open
source implementation of the code has been made available at
this https URL .
"
"  This paper is concerned with the initial-boundary value problem to 2D
magnetohydrodynamics-Boussinesq system with the temperature-dependent
viscosity, thermal diffusivity and electrical conductivity. First, we establish
the global weak solutions under the minimal initial assumption. Then by
imposing higher regularity assumption on the initial data, we obtain the global
strong solution with uniqueness. Moreover, the exponential decay estimate of
the solution is obtained.
"
"  $\frac{3}{2}$-institutions have been introduced as an extension of
institution theory that accommodates implicitly partiality of the signature
morphisms together with its syntactic and semantic effects. In this paper we
show that ordinary institutions that are equipped with an inclusion system for
their categories of signatures generate naturally $\frac{3}{2}$ -institutions
with explicit partiality for their signature morphisms. This provides a general
uniform way to build 3 -institutions for the foundations of conceptual blending
and software evolution. Moreover our general construction allows for an uniform
derivation of some useful technical properties.
"
"  In this paper we study an anisotropic variant of the Rudin-Osher-Fatemi
functional with $L^1$ fidelity term of the form \[ E(u) = \int_{\mathbb{R}^n}
\phi(\nabla u) + \lambda \| u -f \|_{L^1(\mathbb{R}^n)}. \] We will
characterize the minimizers of $E$ in terms of the Wulff shape of $\phi$ and
the dual anisotropy. In particular we will calculate the subdifferential of
$E$. We will apply this characterization to the special case $\phi = |\cdot|_1$
and $n=2$, which has been used in the denoising of 2D bar codes. In this case,
we determine the shape of a minimizer $u$ when $f$ is the characteristic
function of a circle.
"
"  In this paper we first study partial regularity of weak solutions to the
initial boundary value problem for the system
$-\mbox{div}\left[(I+\mathbf{m}\otimes \mathbf{m})\nabla p\right]=S(x),\ \
\partial_t\mathbf{m}-D^2\Delta \mathbf{m}-E^2(\mathbf{m}\cdot\nabla p)\nabla
p+|\mathbf{m}|^{2(\gamma-1)}\mathbf{m}=0$, where $S(x)$ is a given function and
$D, E, \gamma$ are given numbers. This problem has been proposed as a PDE model
for biological transportation networks. Mathematically, it seems to have a
connection to a conjecture by De Giorgi \cite{DE}. Then we investigate the
life-span of classical solutions. Our results show that local existence of a
classical solution can always be obtained and the life-span of such a solution
can be extended as far away as one wishes as long as the term $\|{\bf
m}(x,0)\|_{\infty, \Omega}+\|S(x)\|_{\frac{2N}{3}, \Omega}$ is made suitably
small, where $N$ is the space dimension and $\|\cdot\|_{q,\Omega}$ denotes the
norm in $L^q(\Omega)$.
"
"  In this paper we construct some regular sequences which arise naturally from
determinantal conditions.
"
"  Let $X$ be a locally compact zero-dimensional space, let $S$ be an
equicontinuous set of homeomorphisms such that $1 \in S = S^{-1}$, and suppose
that $\overline{Gx}$ is compact for each $x \in X$, where $G = \langle S
\rangle$. We show in this setting that a number of conditions are equivalent:
(a) $G$ acts minimally on the closure of each orbit; (b) the orbit closure
relation is closed; (c) for every compact open subset $U$ of $X$, there is $F
\subseteq G$ finite such that $\bigcap_{g \in F}g(U)$ is $G$-invariant. All of
these are equivalent to a notion of recurrence, which is a variation on a
concept of Auslander-Glasner-Weiss. It follows in particular that the action is
distal if and only if it is equicontinuous.
"
"  We show that the Gurarij space $\mathbb{G}$ and its noncommutative analog
$\mathbb{NG}$ both have extremely amenable automorphism group. We also compute
the universal minimal flows of the automorphism groups of the Poulsen simplex
$\mathbb{P}$ and its noncommutative analogue $\mathbb{NP}$. The former is
$\mathbb{P}$ itself, and the latter is the state space of the operator system
associated with $\mathbb{NP}$. This answers a question of Conley and
Törnquist. We also show that the pointwise stabilizer of any closed proper
face of $\mathbb{P}$ is extremely amenable. Similarly, the pointwise stabilizer
of any closed proper biface of the unit ball of the dual of the Gurarij space
(the Lusky simplex) is extremely amenable.
These results are obtained via the Kechris--Pestov--Todorcevic
correspondence, by establishing the approximate Ramsey property for several
classes of finite-dimensional operator spaces and operator systems (with
distinguished linear functionals), including: Banach spaces, exact operator
spaces, function systems with a distinguished state, and exact operator systems
with a distinguished state. This is the first direct application of the
Kechris--Pestov--Todorcevic correspondence in the setting of metric structures.
The fundamental combinatorial principle that underpins the proofs is the Dual
Ramsey Theorem of Graham and Rothschild.
In the second part of the paper, we obtain factorization theorems for
colorings of matrices and Grassmannians over $\mathbb{R}$ and ${\mathbb{C}}$,
which can be considered as continuous versions of the Dual Ramsey Theorem for
Boolean matrices and of the Graham-Leeb-Rothschild Theorem for Grassmannians
over a finite field.
"
"  We define holomorphic quadratic differentials for spacelike surfaces with
constant mean curvature in the Lorentzian homogeneous spaces
$\mathbb{L}(\kappa,\tau)$ with isometry group of dimension 4, which are dual to
the Abresch-Rosenberg differentials in the Riemannian counterparts
$\mathbb{E}(\kappa,\tau)$, and obtain some consequences. On the one hand, we
give a very short proof of the Bernstein problem in Heisenberg space, and
provide a geometric description of the family of entire graphs sharing the same
differential in terms of a 2-parameter conformal deformation. On the other
hand, we prove that entire minimal graphs in Heisenberg space have negative
Gauss curvature.
"
"  We study time decay estimates of the fourth-order Schrödinger operator
$H=(-\Delta)^{2}+V(x)$ in $\mathbb{R}^{d}$ for $d=3$ and $d\geq5$. We analyze
the low energy and high energy behaviour of resolvent $R(H; z)$, and then
derive the Jensen-Kato dispersion decay estimate and local decay estimate for
$e^{-itH}P_{ac}$ under suitable spectrum assumptions of $H$. Based on
Jensen-Kato decay estimate and local decay estimate, we obtain the
$L^1\rightarrow L^{\infty}$ estimate of $e^{-itH}P_{ac}$ in $3$-dimension by
Ginibre argument, and also establish the endpoint global Strichartz estimates
of $e^{-itH}P_{ac}$ for $d\geq5$. Furthermore, using the local decay estimate
and the Georgescu-Larenas-Soffer conjugate operator method, we prove the
Jensen-Kato type decay estimates for some functions of $H$.
"
"  The purpose of this note is to verify that the results attained in [6] admit
an extension to the multidimensional setting. Namely, for subsets of the two
dimensional torus we find the sharp growth rate of the step(s) of a generalized
arithmetic progression in terms of its size which may be found in an
exponential systems satisfying the Riesz sequence property.
"
"  In this paper we develop methods to extend the minimal hypersurface approach
to positive scalar curvature problems to all dimensions. This includes a proof
of the positive mass theorem in all dimensions without a spin assumption. It
also includes statements about the structure of compact manifolds of positive
scalar curvature extending the work of \cite{sy1} to all dimensions. The
technical work in this paper is to construct minimal slicings and associated
weight functions in the presence of small singular sets and to show that the
singular sets do not become too large in the lower dimensional slices. It is
shown that the singular set in any slice is a closed set with Hausdorff
codimension at least three. In particular for arguments which involve slicing
down to dimension $1$ or $2$ the method is successful. The arguments can be
viewed as an extension of the minimal hypersurface regularity theory to this
setting of minimal slicings.
"
"  We consider the four structures $(\mathbb{Z}; \mathrm{Sqf}^\mathbb{Z})$,
$(\mathbb{Z}; <, \mathrm{Sqf}^\mathbb{Z})$, $(\mathbb{Q};
\mathrm{Sqf}^\mathbb{Q})$, and $(\mathbb{Q}; <, \mathrm{Sqf}^\mathbb{Q})$ where
$\mathbb{Z}$ is the additive group of integers, $\mathrm{Sqf}^\mathbb{Z}$ is
the set of $a \in \mathbb{Z}$ such that $v_{p}(a) < 2$ for every prime $p$ and
corresponding $p$-adic valuation $v_{p}$, $\mathbb{Q}$ and
$\mathrm{Sqf}^\mathbb{Q}$ are defined likewise for rational numbers, and $<$
denotes the natural ordering on each of these domains. We prove that the second
structure is model-theoretically wild while the other three structures are
model-theoretically tame. Moreover, all these results can be seen as examples
where number-theoretic randomness yields model-theoretic consequences.
"
"  We offer an umbrella type result which extends weak convergence of the
classical empirical process on the line to that of more general processes
indexed by functions of bounded variation. This extension is not contingent on
the type of dependence of the underlying sequence of random variables. As a
consequence we establish weak convergence for stationary empirical processes
indexed by general classes of functions under alpha mixing conditions.
"
"  In this paper, we obtain new results related to Minkowski fractional integral
inequality using generalized k-fractional integral operator which is in terms
of the Gauss hypergeometric function.
"
"  The Bayesian update can be viewed as a variational problem by characterizing
the posterior as the minimizer of a functional. The variational viewpoint is
far from new and is at the heart of popular methods for posterior
approximation. However, some of its consequences seem largely unexplored. We
focus on the following one: defining the posterior as the minimizer of a
functional gives a natural path towards the posterior by moving in the
direction of steepest descent of the functional. This idea is made precise
through the theory of gradient flows, allowing to bring new tools to the study
of Bayesian models and algorithms. Since the posterior may be characterized as
the minimizer of different functionals, several variational formulations may be
considered. We study three of them and their three associated gradient flows.
We show that, in all cases, the rate of convergence of the flows to the
posterior can be bounded by the geodesic convexity of the functional to be
minimized. Each gradient flow naturally suggests a nonlinear diffusion with the
posterior as invariant distribution. These diffusions may be discretized to
build proposals for Markov chain Monte Carlo (MCMC) algorithms. By
construction, the diffusions are guaranteed to satisfy a certain optimality
condition, and rates of convergence are given by the convexity of the
functionals. We use this observation to propose a criterion for the choice of
metric in Riemannian MCMC methods.
"
"  Let $\mathcal{A}(1)^*$ be the subHopf algebra of the mod~$2$ Steenrod algebra
$\mathcal{A}^*$ generated by $\mathrm{Sq}^1$ and $\mathrm{Sq}^2$. The
\emph{Joker} is the cyclic $\mathcal{A}(1)^*$-module
$\mathcal{A}(1)^*/\mathcal{A}(1)^*\{\mathrm{Sq}^3\}$ which plays a special
rôle in the study of $\mathcal{A}(1)^*$-modules. We discuss realisations of
the Joker both as an $\mathcal{A}^*$-module and as the cohomology of a
spectrum. We also consider analogous $\mathcal{A}(n)^*$-modules for $n\geq2$
and prove realisability results (both stable and unstable) for $n=2,3$ and
non-realisability results for $n\geq4$.
"
"  We obtain a formula for the Turaev-Viro invariants of a link complement in
terms of values of the colored Jones polynomial of the link. As an application
we give the first examples for which the volume conjecture of Chen and the
third named author\,\cite{Chen-Yang} is verified. Namely, we show that the
asymptotics of the Turaev-Viro invariants of the Figure-eight knot and the
Borromean rings complement determine the corresponding hyperbolic volumes. Our
calculations also exhibit new phenomena of asymptotic behavior of values of the
colored Jones polynomials that seem not to be predicted by neither the
Kashaev-Murakami-Murakami volume conjecture and various of its generalizations
nor by Zagier's quantum modularity conjecture. We conjecture that the
asymptotics of the Turaev-Viro invariants of any link complement determine the
simplicial volume of the link, and verify it for all knots with zero simplicial
volume. Finally we observe that our simplicial volume conjecture is stable
under connect sum and split unions of links.
"
"  We consider a system of differential equations of Monge-Kantorovich type
which describes the equilibrium configurations of granular material poured by a
constant source on a network. Relying on the definition of viscosity solution
for Hamilton-Jacobi equations on networks, recently introduced by P.-L. Lions
and P. E. Souganidis, we prove existence and uniqueness of the solution of the
system and we discuss its numerical approximation. Some numerical experiments
are carried out.
"
"  The classical Descartes' rule of signs limits the number of positive roots of
a real polynomial in one variable by the number of sign changes in the sequence
of its coefficients. One can ask the question which pairs of nonnegative
integers $(p,n)$, chosen in accordance with this rule and with some other
natural conditions, can be the pairs of numbers of positive and negative roots
of a real polynomial with prescribed signs of the coefficients. The paper
solves this problem for degree $8$ polynomials.
"
"  In this note, we give a so-called representative classification for the
strata by automorphism group of smooth $\bar{k}$-plane curves of genus $6$,
where $\bar{k}$ is a fixed separable closure of a field $k$ of characteristic
$p = 0$ or $p > 13$. We start with a classification already obtained by the
first author and we use standard techniques.
Interestingly, in the way to get these families for the different strata, we
find two remarkable phenomenons that did not appear before. One is the
existence of a non $0$-dimensional final stratum of plane curves. At a first
sight it may sound odd, but we will see that this is a normal situation for
higher degrees and we will give a explanation for it.
We explicitly describe representative families for all strata, except for the
stratum with automorphism group $\mathbb{Z}/5\mathbb{Z}$. Here we find the
second difference with the lower genus cases where the previous techniques do
not fully work. Fortunately, we are still able to prove the existence of such
family by applying a version of Luroth's theorem in dimension $2$.
"
"  We introduce a spreading out technique to deduce finiteness results for
étale fundamental groups of complex varieties by characteristic $p$ methods,
and apply this to recover a finiteness result proven recently for local
fundamental groups in characteristic $0$ using birational geometry.
"
"  The problem of analyzing the number of number field extensions $L/K$ with
bounded (relative) discriminant has been the subject of renewed interest in
recent years, with significant advances made by Schmidt, Ellenberg-Venkatesh,
Bhargava, Bhargava-Shankar-Wang, and others. In this paper, we use the geometry
of numbers and invariant theory of finite groups, in a manner similar to
Ellenberg and Venkatesh, to give an upper bound on the number of extensions
$L/K$ with fixed degree, bounded relative discriminant, and specified Galois
closure.
"
"  If several independent algorithms for a computer-calculated quantity exist,
then one can expect their results (which differ because of numerical errors) to
follow approximately Gaussian distribution. The mean of this distribution,
interpreted as the value of the quantity of interest, can be determined with
better precision than what is the precision provided by a single algorithm.
Often, with lack of enough independent algorithms, one can proceed differently:
many practical algorithms introduce a bias using a parameter, e.g. a small but
finite number to compute a limit or a large but finite number (cutoff) to
approximate infinity. One may vary such parameter of a single algorithm and
interpret the resulting numbers as generated by several algorithms. A numerical
evidence for the validity of this approach is shown for differentiation.
"
"  This paper focuses on best-arm identification in multi-armed bandits with
bounded rewards. We develop an algorithm that is a fusion of lil-UCB and
KL-LUCB, offering the best qualities of the two algorithms in one method. This
is achieved by proving a novel anytime confidence bound for the mean of bounded
distributions, which is the analogue of the LIL-type bounds recently developed
for sub-Gaussian distributions. We corroborate our theoretical results with
numerical experiments based on the New Yorker Cartoon Caption Contest.
"
"  Researchers often have datasets measuring features $x_{ij}$ of samples, such
as test scores of students. In factor analysis and PCA, these features are
thought to be influenced by unobserved factors, such as skills. Can we
determine how many components affect the data? This is an important problem,
because it has a large impact on all downstream data analysis. Consequently,
many approaches have been developed to address it. Parallel Analysis is a
popular permutation method. It works by randomly scrambling each feature of the
data. It selects components if their singular values are larger than those of
the permuted data. Despite widespread use in leading textbooks and scientific
publications, as well as empirical evidence for its accuracy, it currently has
no theoretical justification.
In this paper, we show that the parallel analysis permutation method
consistently selects the large components in certain high-dimensional factor
models. However, it does not select the smaller components. The intuition is
that permutations keep the noise invariant, while ""destroying"" the low-rank
signal. This provides justification for permutation methods in PCA and factor
models under some conditions. Our work uncovers drawbacks of permutation
methods, and paves the way to improvements.
"
"  We consider the framework proposed by Burgard and Kjaer (2011) that derives
the PDE which governs the price of an option including bilateral counterparty
risk and funding. We extend this work by relaxing the assumption of absence of
transaction costs in the hedging portfolio by proposing a cost proportional to
the amount of assets traded and the traded price. After deriving the nonlinear
PDE, we prove the existence of a solution for the corresponding
initial-boundary value problem. Moreover, we develop a numerical scheme that
allows to find the solution of the PDE by setting different values for each
parameter of the model. To understand the impact of each variable within the
model, we analyze the Greeks of the option and the sensitivity of the price to
changes in all the risk factors.
"
"  Sheng and Zuo's characteristic forms are invariants of a variation of Hodge
structure. We show that they characterize Gross's canonical variations of Hodge
structure of Calabi-Yau type over (Hermitian symmetric) tube domains.
"
"  We investigate quantum graphs with infinitely many vertices and edges without
the common restriction on the geometry of the underlying metric graph that
there is a positive lower bound on the lengths of its edges. Our central result
is a close connection between spectral properties of a quantum graph and the
corresponding properties of a certain weighted discrete Laplacian on the
underlying discrete graph. Using this connection together with spectral theory
of (unbounded) discrete Laplacians on infinite graphs, we prove a number of new
results on spectral properties of quantum graphs. Namely, we prove several
self-adjointness results including a Gaffney type theorem. We investigate the
problem of lower semiboundedness, prove several spectral estimates (bounds for
the bottom of spectra and essential spectra of quantum graphs, CLR-type
estimates) and study spectral types.
"
"  We consider the problem of estimating a large rank-one tensor ${\boldsymbol
u}^{\otimes k}\in({\mathbb R}^{n})^{\otimes k}$, $k\ge 3$ in Gaussian noise.
Earlier work characterized a critical signal-to-noise ratio $\lambda_{Bayes}=
O(1)$ above which an ideal estimator achieves strictly positive correlation
with the unknown vector of interest. Remarkably no polynomial-time algorithm is
known that achieved this goal unless $\lambda\ge C n^{(k-2)/4}$ and even
powerful semidefinite programming relaxations appear to fail for $1\ll
\lambda\ll n^{(k-2)/4}$.
In order to elucidate this behavior, we consider the maximum likelihood
estimator, which requires maximizing a degree-$k$ homogeneous polynomial over
the unit sphere in $n$ dimensions. We compute the expected number of critical
points and local maxima of this objective function and show that it is
exponential in the dimensions $n$, and give exact formulas for the exponential
growth rate. We show that (for $\lambda$ larger than a constant) critical
points are either very close to the unknown vector ${\boldsymbol u}$, or are
confined in a band of width $\Theta(\lambda^{-1/(k-1)})$ around the maximum
circle that is orthogonal to ${\boldsymbol u}$. For local maxima, this band
shrinks to be of size $\Theta(\lambda^{-1/(k-2)})$. These `uninformative' local
maxima are likely to cause the failure of optimization algorithms.
"
"  We introduce a new method to qualify the goodness of fit parameter estimation
of compound Wishart models. Our method based on the free deterministic
equivalent Z-score, which we introduce in this paper. Furthermore, an
application to two dimensional autoregressive moving-average model is provided.
Our proposal method is a generalization of statistical hypothesis testing to
one dimensional moving average model based on fluctuations of real compound
Wishart matrices, which is a recent result by Hasegawa, Sakuma and Yoshida.
"
"  For non-Gaussian stochastic dynamical systems, mean exit time and escape
probability are important deterministic quantities, which can be obtained from
integro-differential (nonlocal) equations. We develop an efficient and
convergent numerical method for the mean first exit time and escape probability
for stochastic systems with an asymmetric Lévy motion, and analyze the
properties of the solutions of the nonlocal equations. We also investigate the
effects of different system factors on the mean exit time and escape
probability, including the skewness parameter, the size of the domain, the
drift term and the intensity of Gaussian and non-Gaussian noises. We find that
the behavior of the mean exit time and the escape probability has dramatic
difference at the boundary of the domain when the index of stability crosses
the critical value of one.
"
"  We associate to each iterated function system consisting of
phi-max-contractions an operator (on the space of continuous functions from the
shift space on the metric space corresponding to the system) having a unique
fixed point whose image turns out to be the attractor of the system. Moreover,
we prove that the unique fixed point of the operator associated to an iterated
function system consisting of convex contractions is the canonical projection
from the shift space on the attractor of the system.
"
"  In this paper, we study the energy decay for the thermoelastic Bresse system
in the whole line with two different dissipative mechanism, given by heat
conduction (Types I and III). We prove that the decay rate of the solutions are
very slow. More precisely, we show that the solutions decay with the rate of
$(1+t)^{-\frac{1}{8}}$ in the $L^2$-norm, whenever the initial data belongs to
$L^1(R) \cap H^{s}(R)$ for a suitable $s$. The wave speeds of propagation have
influence on the decay rate with respect to the regularity of the initial data.
This phenomenon is known as \textit{regularity-loss}. The main tool used to
prove our results is the energy method in the Fourier space.
"
"  We compute the Frobenius number for sequences of triangular and tetrahedral
numbers. In addition, we study some properties of the numerical semigroups
associated to those sequences.
"
"  In this paper, we study the Cauchy problem for radially symmetric homogeneous
non-cutoff Boltzmann equation with Maxwellian molecules, the initial datum
belongs to Shubin space of the negative index which can be characterized by
spectral decomposition of the harmonic oscillators. The Shubin space of the
negative index contains the measure functions. Based on this spectral
decomposition, we construct the weak solution with Shubin class initial datum,
we also prove that the Cauchy problem enjoys Gelfand-Shilov smoothing effect,
meaning that the smoothing properties are the same as the Cauchy problem
defined by the evolution equation associated to a fractional harmonic
oscillator.
"
"  In this paper we study the systole growth of arithmetic locally symmetric
spaces up congruence covers and show that this growth is at least logarithmic
in volume. This generalizes previous work of Buser and Sarnak as well as Katz,
Schaps and Vishne where the case of compact hyperbolic 2- and 3-manifolds was
considered.
"
"  We introduce a Schrödinger model for the unitary irreducible
representations of a Heisenberg motion group and we show that the usual Weyl
quantization then provides a Stratonovich-Weyl correspondence.
"
"  Dimension reduction is often a preliminary step in the analysis of large data
sets. The so-called non-Gaussian component analysis searches for a projection
onto the non-Gaussian part of the data, and it is then important to know the
correct dimension of the non-Gaussian signal subspace. In this paper we develop
asymptotic as well as bootstrap tests for the dimension based on the popular
fourth order blind identification (FOBI) method.
"
"  A flag domain of a real from $G_0$ of a complex semismiple Lie group $G$ is
an open $G_0$-orbit $D$ in a (compact) $G$-flag manifold. In the usual way one
reduces to the case where $G_0$ is simple. It is known that if $D$ possesses
non-constant holomorphic functions, then it is the product of a compact flag
manifold and a Hermitian symmetric bounded domain. This pseudoconvex case is
rare in the geography of flag domains. Here it is shown that otherwise, i.e.,
when $\mathcal{O}(D)\cong\mathbb{C}$, the flag domain $D$ is pseudoconcave. In
a rather general setting the degree of the pseudoconcavity is estimated in
terms of root invariants. This estimate is explicitly computed for domains in
certain Grassmannians.
"
"  We survey problems and results from combinatorial geometry in normed spaces,
concentrating on problems that involve distances. These include various
properties of unit-distance graphs, minimum-distance graphs, diameter graphs,
as well as minimum spanning trees and Steiner minimum trees. In particular, we
discuss translative kissing (or Hadwiger) numbers, equilateral sets, and the
Borsuk problem in normed spaces. We show how to use the angular measure of
Peter Brass to prove various statements about Hadwiger and blocking numbers of
convex bodies in the plane, including some new results. We also include some
new results on thin cones and their application to distinct distances and other
combinatorial problems for normed spaces.
"
"  We consider the recovery of regression coefficients, denoted by
$\boldsymbol{\beta}_0$, for a single index model (SIM) relating a binary
outcome $Y$ to a set of possibly high dimensional covariates $\boldsymbol{X}$,
based on a large but 'unlabeled' dataset $\mathcal{U}$, with $Y$ never
observed. On $\mathcal{U}$, we fully observe $\boldsymbol{X}$ and additionally,
a surrogate $S$ which, while not being strongly predictive of $Y$ throughout
the entirety of its support, can forecast it with high accuracy when it assumes
extreme values. Such datasets arise naturally in modern studies involving large
databases such as electronic medical records (EMR) where $Y$, unlike
$(\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies,
an example of $Y$ and $S$ would be the true disease phenotype and the count of
the associated diagnostic codes respectively. Assuming another SIM for $S$
given $\boldsymbol{X}$, we show that under sparsity assumptions, we can recover
$\boldsymbol{\beta}_0$ proportionally by simply fitting a least squares LASSO
estimator to the subset of the observed data on $(\boldsymbol{X}, S)$
restricted to the extreme sets of $S$, with $Y$ imputed using the surrogacy of
$S$. We obtain sharp finite sample performance bounds for our estimator,
including deterministic deviation bounds and probabilistic guarantees. We
demonstrate the effectiveness of our approach through multiple simulation
studies, as well as by application to real data from an EMR study conducted at
the Partners HealthCare Systems.
"
"  The liar paradox is widely seen as not a serious problem. I try to explain
why this view is mistaken.
"
"  Fan et al. (2015) recently introduced a remarkable method for increasing
asymptotic power of tests in high-dimensional testing problems. If applicable
to a given test, their power enhancement principle leads to an improved test
that has the same asymptotic size, uniformly non-inferior asymptotic power, and
is consistent against a strictly broader range of alternatives than the
initially given test. We study under which conditions this method can be
applied and show the following: In asymptotic regimes where the dimensionality
of the parameter space is fixed as sample size increases, there often exist
tests that can not be further improved with the power enhancement principle.
However, when the dimensionality of the parameter space increases sufficiently
slowly with sample size and a marginal local asymptotic normality (LAN)
condition is satisfied, every test with asymptotic size smaller than one can be
improved with the power enhancement principle. While the marginal LAN condition
alone does not allow one to extend the latter statement to all rates at which
the dimensionality increases with sample size, we give sufficient conditions
under which this is the case.
"
"  Based on independently distributed $X_1 \sim N_p(\theta_1, \sigma^2_1 I_p)$
and $X_2 \sim N_p(\theta_2, \sigma^2_2 I_p)$, we consider the efficiency of
various predictive density estimators for $Y_1 \sim N_p(\theta_1, \sigma^2_Y
I_p)$, with the additional information $\theta_1 - \theta_2 \in A$ and known
$\sigma^2_1, \sigma^2_2, \sigma^2_Y$. We provide improvements on benchmark
predictive densities such as plug-in, the maximum likelihood, and the minimum
risk equivariant predictive densities. Dominance results are obtained for
$\alpha-$divergence losses and include Bayesian improvements for reverse
Kullback-Leibler loss, and Kullback-Leibler (KL) loss in the univariate case
($p=1$). An ensemble of techniques are exploited, including variance expansion
(for KL loss), point estimation duality, and concave inequalities.
Representations for Bayesian predictive densities, and in particular for
$\hat{q}_{\pi_{U,A}}$ associated with a uniform prior for $\theta=(\theta_1,
\theta_2)$ truncated to $\{\theta \in \mathbb{R}^{2p}: \theta_1 - \theta_2 \in
A \}$, are established and are used for the Bayesian dominance findings.
Finally and interestingly, these Bayesian predictive densities also relate to
skew-normal distributions, as well as new forms of such distributions.
"
"  Feigin-Stoyanovsky's type subspaces for affine Lie algebras of type
$C_\ell^{(1)}$ have monomial bases with a nice combinatorial description. We
describe bases of whole standard modules in terms of semi-infinite monomials
obtained as ""a limit of translations"" of bases for Feigin-Stoyanovsky's type
subspaces.
"
"  For a general class of contractions of a variety X to a base Y, I discuss
recent joint work with M. Wemyss defining a noncommutative enhancement of the
locus in Y over which the contraction is not an isomorphism, along with
applications to the derived symmetries of X. This note is based on a talk given
at the Kinosaki Symposium in 2016.
"
"  We present a simple model of a non-equilibrium self-organizing market where
asset prices are partially driven by investment decisions of a bounded-rational
agent. The agent acts in a stochastic market environment driven by various
exogenous ""alpha"" signals, agent's own actions (via market impact), and noise.
Unlike traditional agent-based models, our agent aggregates all traders in the
market, rather than being a representative agent. Therefore, it can be
identified with a bounded-rational component of the market itself, providing a
particular implementation of an Invisible Hand market mechanism. In such
setting, market dynamics are modeled as a fictitious self-play of such
bounded-rational market-agent in its adversarial stochastic environment. As
rewards obtained by such self-playing market agent are not observed from market
data, we formulate and solve a simple model of such market dynamics based on a
neuroscience-inspired Bounded Rational Information Theoretic Inverse
Reinforcement Learning (BRIT-IRL). This results in effective asset price
dynamics with a non-linear mean reversion - which in our model is generated
dynamically, rather than being postulated. We argue that our model can be used
in a similar way to the Black-Litterman model. In particular, it represents, in
a simple modeling framework, market views of common predictive signals, market
impacts and implied optimal dynamic portfolio allocations, and can be used to
assess values of private signals. Moreover, it allows one to quantify a
""market-implied"" optimal investment strategy, along with a measure of market
rationality. Our approach is numerically light, and can be implemented using
standard off-the-shelf software such as TensorFlow.
"
"  In this paper we prove a rigidity result for the equality case of the Penrose
inequality on $3$-dimensional asymptotically flat manifolds with nonnegative
scalar curvature and corners. Our result also has deep connections with the
equality cases of Theorem 1 in \cite{Miao2} and Theorem 1.1 in \cite{LM}.
"
"  We introduce an algebra model to study higher order sum rules for orthogonal
polynomials on the unit circle. We build the relation between the algebra model
and sum rules, and prove an equivalent expression on the algebra side for the
sum rules, involving a Hall-Littlewood type polynomial. By this expression, we
recover an earlier result by Golinskii and Zlatǒs, and prove a new case -
half of the Lukic conjecture in the case of a single critical point with
arbitrary order.
"
"  For the problems of nonparametric hypothesis testing we introduce the notion
of maxisets and maxispace. We point out the maxisets of $\chi^2-$tests,
Cramer-von Mises tests, tests generated $\mathbb{L}_2$- norms of kernel
estimators and tests generated quadratic forms of estimators of Fourier
coefficients. For these tests we show that, if sequence of alternatives having
given rates of convergence to hypothesis is consistent, then each altehrnative
can be broken down into the sum of two parts: a function belonging to maxiset
and orthogonal function. Sequence of functions belonging to maxiset is
consistent sequence of alternatives.
We point out asymptotically minimax tests if sets of alternatives are maxiset
with deleted ""small"" $\mathbb{L}_2$-balls.
"
"  In this paper we extend the known methodology for fitting stable
distributions to the multivariate case and apply the suggested method to the
modelling of daily cryptocurrency-return data. The investigated time period is
cut into 10 non-overlapping sections, thus the changes can also be observed. We
apply bootstrap tests for checking the models and compare our approach to the
more traditional extreme-value and copula models.
"
"  We consider orthogonal decompositions of invariant subspaces of Hardy spaces,
these relate to the Blaschke based phase unwinding decompositions. We prove
convergence in Lp. In particular we build an explicit multiscale wavelet basis.
We also obtain an explicit unwindinig decomposition for the singular inner
function, exp 2i\pi/x.
"
"  In 2015, Barber and Candes introduced a new variable selection procedure
called the knockoff filter to control the false discovery rate (FDR) and prove
that this method achieves exact FDR control. Inspired by the work of Barber and
Candes (2015), we propose and analyze a pseudo-knockoff filter that inherits
some advantages of the original knockoff filter and has more flexibility in
constructing its knockoff matrix. Although we have not been able to obtain
exact FDR control of the pseudo knockoff filter, we show that it satisfies an
expectation inequality that offers some insight into FDR control. Moreover, we
provide some partial analysis of the pseudo knockoff filter for the half Lasso
and the least squares statistics. Our analysis indicates that the inverse of
the covariance matrix of the feature matrix plays an important role in
designing and analyzing the pseudo knockoff filter. Our preliminary numerical
experiments show that the pseudo knockoff filter with the half Lasso statistic
has FDR control. Moreover, our numerical experiments show that the
pseudo-knockoff filter could offer more power than the original knockoff filter
with the OMP or Lasso Path statistic when the features are correlated and
non-sparse.
"
"  This paper is a sequel to [He11] and [GH17]. In [He11] a notion of marking of
isolated hypersurface singularities was defined, and a moduli space
$M_\mu^{mar}$ for marked singularities in one $\mu$-homotopy class of isolated
hypersurface singularities was established. It is an analogue of a
Teichmüller space. It comes together with a $\mu$-constant monodromy group
$G^{mar}\subset G_{\mathbb{Z}}$. Here $G_{\mathbb{Z}}$ is the group of
automorphisms of a Milnor lattice which respect the Seifert form. It was
conjectured that $M_\mu^{mar}$ is connected. This is equivalent to $G^{mar}=
G_{\mathbb{Z}}$. Also Torelli type conjectures were formulated. In [He11] and
[GH17] $M_\mu^{mar}, G_{\mathbb{Z}}$ and $G^{mar}$ were determined and all
conjectures were proved for the simple, the unimodal and the exceptional
bimodal singularities. In this paper the quadrangle singularities and the
bimodal series are treated. The Torelli type conjectures are true. But the
conjecture $G^{mar}= G_{\mathbb{Z}}$ and $M_\mu^{mar}$ connected does not hold
for certain subseries of the bimodal series.
"
"  The radiological characterization of contaminated elements (walls, grounds,
objects) from nuclear facilities often suffers from a too small number of
measurements. In order to determine risk prediction bounds on the level of
contamination, some classic statistical methods may then reveal unsuited as
they rely upon strong assumptions (e.g. that the underlying distribution is
Gaussian) which cannot be checked. Considering that a set of measurements or
their average value arise from a Gaussian distribution can sometimes lead to
erroneous conclusion, possibly underconservative. This paper presents several
alternative statistical approaches which are based on much weaker hypotheses
than Gaussianity. They result from general probabilistic inequalities and
order-statistics based formula. Given a data sample, these inequalities make it
possible to derive prediction intervals for a random variable, which can be
directly interpreted as probabilistic risk bounds. For the sake of validation,
they are first applied to synthetic data samples generated from several known
theoretical distributions. In a second time, the proposed methods are applied
to two data sets obtained from real radiological contamination measurements.
"
"  We prove that if a solution of the time-dependent Schr{ö}dinger equation on
an homogeneous tree with bounded potential decays fast at two distinct times
then the solution is trivial. For the free Schr{ö}dinger operator, we use the
spectral theory of the Laplacian and complex analysis and obtain a
characterization of the initial conditions that lead to a sharp decay at any
time. We then use the recent spectral decomposition of the Schr{ö}dinger
operator with compactly supported potential due to Colin de Verdi{è}rre and
Turc to extend our results in the presence of such potentials. Finally, we use
real variable methods first introduced by Escauriaza, Kenig, Ponce and Vega to
establish a general sharp result in the case of bounded potentials.
"
"  This paper can be viewed as a sequel to the author's long survey on the
Zimmer program \cite{F11} published in 2011. The sequel focuses on recent rapid
progress on certain aspects of the program particularly concerning rigidity of
Anosov actions and Zimmer's conjecture that there are no actions in low
dimensions. Some emphasis is put on the surprising connections between these
two different sets of developments and also on the key connections and ideas
for future research that arise from these works taken together.
"
"  The class of selfdecomposable distributions in free probability theory was
introduced by Barndorff-Nielsen and the third named author. It constitutes a
fairly large subclass of the freely infinitely divisible distributions, but so
far specific examples have been limited to Wigner's semicircle distributions,
the free stable distributions, two kinds of free gamma distributions and a few
other examples. In this paper, we prove that the (classical) normal
distributions are freely selfdecomposable. More generally it is established
that the Askey-Wimp-Kerov distribution $\mu_c$ is freely selfdecomposable for
any $c$ in $[-1,0]$. The main ingredient in the proof is a general
characterization of the freely selfdecomposable distributions in terms of the
derivative of their free cumulant transform.
"
"  The binomial system is an electoral system unique in the world. It was used
to elect the senators and deputies of Chile during 27 years, from the return of
democracy in 1990 until 2017. In this paper we study the real voting power of
the different political parties in the Senate of Chile during the whole
binomial period. We not only consider the different legislative periods, but
also any party changes between one period and the next. The real voting power
is measured by considering power indices from cooperative game theory, which
are based on the capability of the political parties to form winning
coalitions. With this approach, we can do an analysis that goes beyond the
simple count of parliamentary seats.
"
"  We prove a highly uniform stability or ""almost-near"" theorem for dual
lattices of lattices $L \subseteq \Bbb R^n$. More precisely, we show that, for
a vector $x$ from the linear span of a lattice $L \subseteq \Bbb R^n$, subject
to $\lambda_1(L) \ge \lambda > 0$, to be $\varepsilon$-close to some vector
from the dual lattice $L'$ of $L$, it is enough that the inner products $u\,x$
are $\delta$-close (with $\delta < 1/3$) to some integers for all vectors $u
\in L$ satisfying $\| u \| \le r$, where $r > 0$ depends on $n$, $\lambda$,
$\delta$ and $\varepsilon$, only. This generalizes an earlier analogous result
proved for integral vector lattices by M. Mačaj and the second author. The
proof is nonconstructive, using the ultraproduct construction and a slight
portion of nonstandard analysis.
"
"  We study rates of convergence in central limit theorems for the partial sum
of squares of general Gaussian sequences, using tools from analysis on Wiener
space. No assumption of stationarity, asymptotically or otherwise, is made. The
main theoretical tool is the so-called Optimal Fourth Moment Theorem
\cite{NP2015}, which provides a sharp quantitative estimate of the total
variation distance on Wiener chaos to the normal law. The only assumptions made
on the sequence are the existence of an asymptotic variance, that a
least-squares-type estimator for this variance parameter has a bias and a
variance which can be controlled, and that the sequence's auto-correlation
function, which may exhibit long memory, has a no-worse memory than that of
fractional Brownian motion with Hurst parameter }$H<3/4$.{\ \ Our main result
is explicit, exhibiting the trade-off between bias, variance, and memory. We
apply our result to study drift parameter estimation problems for subfractional
Ornstein-Uhlenbeck and bifractional Ornstein-Uhlenbeck processes with
fixed-time-step observations. These are processes which fail to be stationary
or self-similar, but for which detailed calculations result in explicit
formulas for the estimators' asymptotic normality.
"
"  Certain fibered hyperbolic 3-manifolds admit a $\mathit{\text{layered veering
triangulation}}$, which can be constructed algorithmically given the stable
lamination of the monodromy. These triangulations were introduced by Agol in
2011, and have been further studied by several others in the years since. We
obtain experimental results which shed light on the combinatorial structure of
veering triangulations, and its relation to certain topological invariants of
the underlying manifold.
"
"  Let $\mathbb{K}$ be an infinite field. We prove that if a variety of
anti-commutative $\mathbb{K}$-algebras - not necessarily associative, where
$xx=0$ is an identity - is locally algebraically cartesian closed, then it must
be a variety of Lie algebras over $\mathbb{K}$. In particular,
$\mathsf{Lie}_{\mathbb{K}}$ is the largest such. Thus, for a given variety of
anti-commutative $\mathbb{K}$-algebras, the Jacobi identity becomes equivalent
to a categorical condition: it is an identity in~$\mathcal{V}$ if and only if
$\mathcal{V}$ is a subvariety of a locally algebraically cartesian closed
variety of anti-commutative $\mathbb{K}$-algebras. This is based on a result
saying that an algebraically coherent variety of anti-commutative
$\mathbb{K}$-algebras is either a variety of Lie algebras or a variety of
anti-associative algebras over $\mathbb{K}$.
"
"  In this paper, we are concerned with the existence of the least energy
sign-changing solutions for the following fractional Schrödinger-Poisson
system: \begin{align*}
\left\{ \begin{aligned} &(-\Delta)^{s} u+V(x)u+\lambda\phi(x)u=f(x, u),\quad
&\text{in}\, \ \mathbb{R}^{3},\\ &(-\Delta)^{t}\phi=u^{2},& \text{in}\,\
\mathbb{R}^{3}, \end{aligned} \right. \end{align*} where $\lambda\in
\mathbb{R}^{+}$ is a parameter, $s, t\in (0, 1)$ and $4s+2t>3$, $(-\Delta)^{s}$
stands for the fractional Laplacian. By constraint variational method and
quantitative deformation lemma, we prove that the above problem has one least
energy sign-changing solution. Moreover, for any $\lambda>0$, we show that the
energy of the least energy sign-changing solutions is strictly larger than two
times the ground state energy.
Finally, we consider $\lambda$ as a parameter and study the convergence
property of the least energy sign-changing solutions as $\lambda\searrow 0$.
"
"  We study the action of monads on categories equipped with several monoidal
structures. We identify the structure and conditions that guarantee that the
higher monoidal structure is inherited by the category of algebras over the
monad. Monoidal monads and comonoidal monads appear as the base cases in this
hierarchy. Monads acting on duoidal categories constitute the next case. We
cover the general case of $n$-monoidal categories and discuss several naturally
occurring examples in which $n\leq 3$.
"
"  In this paper we obtain the variational characterization of Hardy space $H^p$
for $p\in(\frac n{n+1},1]$ and get estimates for the oscillation operator and
the $\lambda$-jump operator associated with approximate identities acting on
$H^p$ for $p\in(\frac n{n+1},1]$. Moreover, we give counterexamples to show
that the oscillation and $\lambda$-jump associated with some approximate
identity can not be used to characterize $H^p$ for $p\in(\frac n{n+1},1]$.
"
"  In this article, we provide a new algorithm for solving constraint
satisfaction problems over templates with few subpowers, by reducing the
problem to the combination of solvability of a polynomial number of systems of
linear equations over finite fields and reductions via absorbing subuniverses.
"
"  This article consists of two parts. In Part 1, we present a formulation of
two-dimensional topological quantum field theories in terms of a functor from a
category of Ribbon graphs to the endofuntor category of a monoidal category.
The key point is that the category of ribbon graphs produces all Frobenius
objects. Necessary backgrounds from Frobenius algebras, topological quantum
field theories, and cohomological field theories are reviewed. A result on
Frobenius algebra twisted topological recursion is included at the end of Part
1.
In Part 2, we explain a geometric theory of quantum curves. The focus is
placed on the process of quantization as a passage from families of Hitchin
spectral curves to families of opers. To make the presentation simpler, we
unfold the story using SL_2(\mathbb{C})-opers and rank 2 Higgs bundles defined
on a compact Riemann surface $C$ of genus greater than $1$. In this case,
quantum curves, opers, and projective structures in $C$ all become the same
notion. Background materials on projective coordinate systems, Higgs bundles,
opers, and non-Abelian Hodge correspondence are explained.
"
"  Positively (resp. negatively) associated point processes are a class of point
processes that induce attraction (resp. inhibition) between the points. As an
important example, determinantal point processes (DPPs) are negatively
associated. We prove $\alpha$-mixing properties for associated spatial point
processes by controlling their $\alpha$-coefficients in terms of the first two
intensity functions. A central limit theorem for functionals of associated
point processes is deduced, using both the association and the $\alpha$-mixing
properties. We discuss in detail the case of DPPs, for which we obtain the
limiting distribution of sums, over subsets of close enough points of the
process, of any bounded function of the DPP. As an application, we get the
asymptotic properties of the parametric two-step estimator of some
inhomogeneous DPPs.
"
"  In this paper, we find explicit formulas for higher order derivatives of the
inverse tangent function. More precisely, we study polynomials which are
induced from the higher-order derivatives of arctan(x). Successively, we give
generating functions, recurrence relations and some particular properties for
these polynomials. Connections to Chebyshev, Fibonacci, Lucas and Matching
polynomials are established.
"
"  A standard theorem in nonsmooth analysis states that a piecewise affine
function $F:\mathbb R^n\rightarrow\mathbb R^n$ is surjective if it is
coherently oriented in that the linear parts of its selection functions all
have the same nonzero determinant sign. In this note we prove that surjectivity
already follows from coherent orientation of the selection functions which are
active on the unbounded sets of a polyhedral subdivision of the domain
corresponding to $F$. A side bonus of the argumentation is a short proof of the
classical statement that an injective piecewise affine function is coherently
oriented.
"
"  We derive an explicit formula for the scalar curvature over a two-torus with
a Dirac operator conformally rescaled by a globally diagonalizable matrix. We
show that the Gauss-Bonnet theorem holds and extend the result to all Riemann
surfaces with Dirac operators modified in the same way.
"
"  The twisted equivariant K-theory given by Freed and Moore is a K-theory which
unifies twisted equivariant complex K-theory, Atiyah's `Real' K-theory, and
their variants. In a general setting, we formulate this K-theory by using
Fredholm operators, and establish basic properties such as the Bott periodicity
and the Thom isomorphism. We also provide formulations of the K-theory based on
Karoubi's gradations in both infinite and finite dimensions, clarifying their
relationship with the Fredholm formulation.
"
"  Fréchet mean and variance provide a way of obtaining mean and variance for
general metric space valued random variables and can be used for statistical
analysis of data objects that lie in abstract spaces devoid of algebraic
structure and operations. Examples of such spaces include covariance matrices,
graph Laplacians of networks and univariate probability distribution functions.
We derive a central limit theorem for Fréchet variance under mild regularity
conditions, utilizing empirical process theory, and also provide a consistent
estimator of the asymptotic variance. These results lead to a test to compare k
populations based on Fréchet variance for general metric space valued data
objects, with emphasis on comparing means and variances. We examine the finite
sample performance of this inference procedure through simulation studies for
several special cases that include probability distributions and graph
Laplacians, which leads to tests to compare populations of networks. The
proposed methodology has good finite sample performance in simulations for
different kinds of random objects. We illustrate the proposed methods with data
on mortality profiles of various countries and resting state Functional
Magnetic Resonance Imaging data.
"
"  We describe a Hopf ring structure on the direct sum of the cohomology groups
$\bigoplus_{n \geq 0} H^* \left( B_n; \mathbb{Z}_2 \right)$ of the Coxeter
groups of type $B_n$, and an almost-Hopf ring structure on the direct sum of
the cohomology groups $\bigoplus_{n \geq 0} H^* \left( D_n; \mathbb{Z}_2
\right)$ of the Coxeter groups of type $D_n$, with coefficient in the field
with two elements $\mathbb{Z}_2$. We give presentations with generators and
relations, determine additive bases and compute the Steenrod algebra action.
The generators are described both in terms of a geometric construction by De
Concini and Salvetti and in terms of their restriction to elementary abelian
2-subgroups.
"
"  In this note, we shall compute the categorical entropy of an autoequivalence
on a generic abelian surface.
"
"  In this paper, we discuss recent results about generalized metric spaces and
fixed point theory. We introduce the notion of $\eta$-cone metric spaces, give
some topological properties and prove some fixed point theorems for contractive
type maps on these spaces. In particular we show that theses $\eta$-cone metric
spaces are natural generalizations of both cone metric spaces and metric type
spaces.
"
"  For a given cluster-tilted algebra $A$ of tame type, it is proved that
different indecomposable $\tau$-rigid $A$-modules have different dimension
vectors. This is motivated by Fomin-Zelevinsky's denominator conjecture for
cluster algebras. As an application, we establish a weak version of the
denominator conjecture for cluster algebras of tame type. Namely, we show that
different cluster variables have different denominators with respect to a given
cluster for a cluster algebra of tame type. Our approach involves
Iyama-Yoshino's construction of subfactors of triangulated categories. In
particular,we obtain a description of the subfactors of cluster categories of
tame type with respect to an indecomposable rigid object, which is of
independent interest.
"
"  In this paper we study Rota-Baxter modules with emphasis on the role played
by the Rota-Baxter operators and resulting difference between Rota-Baxter
modules and the usual modules over an algebra. We introduce the concepts of
free, projective, injective and flat Rota-Baxter modules. We give the
construction of free modules and show that there are enough projective,
injective and flat Rota-Baxter modules to provide the corresponding resolutions
for derived functor.
"
"  In this work, we find an equation that relates the Ricci curvature of a
riemannian manifold $M$ and the second fundamental forms of two orthogonal
foliations of complementary dimensions, $\mathcal{F}$ and $\mathcal{F}^{\bot}$,
defined on $M$. Using this equation, we show a sufficient condition for the
manifold M to be locally a riemannian product of the leaves of $\mathcal{F}$
and $\mathcal{F}^{\bot}$, if one of the foliations is totally umbilical. We
also prove an integral formula for such foliations.
"
"  In the paper, we show that the transformations between modified Jacobi and
Bernstein bases of the constrained space of polynomials of degree at most $n$
can be performed with the complexity $O(n^2)$. As a result, the algorithm of
degree reduction of Bézier curves that was first presented in (Bhrawy et al.,
J. Comput. Appl. Math. 302 (2016), 369--384), and then corrected in (Lu and
Xiang, J. Comput. Appl. Math. 315 (2017), 65--69), can be significantly
improved, since the necessary transformations are done in those papers with the
complexity $O(n^3)$. The comparison of running times shows that our
transformations are also faster in practice.
"
"  We show non-linear stability and instability results in spherical symmetry
for the interior of a charged black hole -approaching a sub-extremal
Reissner-Nordström background fast enough at infinity- in presence of a
massive and charged scalar field, motivated by the strong cosmic censorship
conjecture in that setting :
1. Stability : We prove that spherically symmetric characteristic initial
data to the Einstein-Maxwell- Klein-Gordon equations approaching a
Reissner-Nordström background with a sufficiently decaying polynomial decay
rate on the event horizon gives rise to a space-time possessing a Cauchy
horizon in a neighbourhood of time-like infinity. Moreover if the decay is even
stronger, we prove that the spacetime metric admits a continuous extension to
the Cauchy horizon. This generalizes the celebrated stability result of
Dafermos for Einstein-Maxwell-real-scalar-field in spherical symmetry.
2. Instability : We prove that for the class of space-times considered in the
stability part, whose scalar field in addition obeys a polynomial averaged-L^2
(consistent) lower bound on the event horizon, the scalar field obeys an
integrated lower bound transversally to the Cauchy horizon. As a consequence we
prove that the non-degenerate energy is infinite on any null surface crossing
the Cauchy horizon and the curvature of a geodesic vector field blows up at the
Cauchy horizon near time-like infinity. This generalizes an instability result
due to Luk and Oh for Einstein-Maxwell-real-scalar-field in spherical symmetry.
This instability of the black hole interior can also be viewed as a step
towards the resolution of the C^2 strong cosmic censorship conjecture for
one-ended asymptotically initial data.
"
"  Confidence is a fundamental concept in statistics, but there is a tendency to
misinterpret it as probability. In this paper, I argue that an intuitively and
mathematically more appropriate interpretation of confidence is through
belief/plausibility functions, in particular, those that satisfy a certain
validity property. Given their close connection with confidence, it is natural
to ask how a valid belief/plausibility function can be constructed directly.
The inferential model (IM) framework provides such a construction, and here I
prove a complete-class theorem stating that, for every nominal confidence
region, there exists a valid IM whose plausibility regions are contained by the
given confidence region. This characterization has implications for statistics
understanding and communication, and highlights the importance of belief
functions and the IM framework.
"
"  We provide a new and simple characterization of the multivariate generalized
Laplace distribution. In particular, this result implies that the product of a
Gaussian matrix with independent and identically distributed columns by an
independent isotropic Gaussian vector follows a symmetric multivariate
generalized Laplace distribution.
"
"  In this paper we prove a refined version of a theorem by Tamagawa and
Mochizuki on isomorphisms between (tame) arithmetic fundamental groups of
hyperbolic curves over finite fields, where one ""ignores"" the information
provided by a ""small"" set of primes.
"
"  Let $(\sigma,\delta)$ be a quasi derivation of a ring $R$ and $M_R$ a right
$R$-module. In this paper, we introduce the notion of $(\sigma,\delta)$-skew
McCoy modules which extends the notion of McCoy modules and $\sigma$-skew McCoy
modules. This concept can be regarded also as a generalization of
$(\sigma,\delta)$-skew Armendariz modules. Some properties of this concept are
established and some connections between $(\sigma,\delta)$-skew McCoyness and
$(\sigma,\delta)$-compatible reduced modules are examined. Also, we study the
property $(\sigma,\delta)$-skew McCoy of some skew triangular matrix extensions
$V_n(M,\sigma)$, for any nonnegative integer $n\geq 2$. As a consequence, we
obtain: (1) $M_R$ is $(\sigma,\delta)$-skew McCoy if and only if
$M[x]/M[x](x^n)$ is $(\overline{\sigma},\overline{\delta})$-skew McCoy, and (2)
$M_R$ is $\sigma$-skew McCoy if and only if $M[x;\sigma]/M[x;\sigma](x^n)$ is
$\overline{\sigma}$-skew McCoy.
"
"  The aim of this paper, is to define a bivariate exponentiated generalized
linear exponential distribution based on Marshall-Olkin shock model.
Statistical and reliability properties of this distribution are discussed. This
includes quantiles, moments, stress-strength reliability, joint reliability
function, joint reversed (hazard) rates functions and joint mean waiting time
function. Moreover, the hazard rate, the availability and the mean residual
lifetime functions for a parallel system, are established. One data set is
analyzed, and it is observed that, the proposed distribution provides a better
fit than Marshall-Olkin bivariate exponential, bivariate generalized
exponential and bivariate generalized linear failure rate distributions.
Simulation studies are presented to estimate both the relative absolute bias,
and the relative mean square error for the distribution parameters based on
complete data.
"
"  We study the higher gradient integrability of distributional solutions $u$ to
the equation $div(\sigma \nabla u) = 0$ in dimension two, in the case when the
essential range of $\sigma$ consists of only two elliptic matrices, i.e.,
$\sigma\in\{\sigma_1, \sigma_2\}$ a.e. in $\Omega$.
In [4], for every pair of elliptic matrices $\sigma_1$ and $\sigma_2$,
exponents $p_{\sigma_1,\sigma_2}\in(2,+\infty)$ and $q_{\sigma_1,\sigma_2}\in
(1,2)$ have been characterised so that if $u\in
W^{1,q_{\sigma_1,\sigma_2}}(\Omega)$ is solution to the elliptic equation then
$\nabla u\in L^{p_{\sigma_1,\sigma_2}}_{\rm weak}(\Omega)$ and the optimality
of the upper exponent $p_{\sigma_1,\sigma_2}$ has been proved. In this paper we
complement the above result by proving the optimality of the lower exponent
$q_{\sigma_1,\sigma_2}$. Precisely, we show that for every arbitrarily small
$\delta$, one can find a particular microgeometry, i.e., an arrangement of the
sets $\sigma^{-1}(\sigma_1)$ and $\sigma^{-1}(\sigma_2)$, for which there
exists a solution $u$ to the corresponding elliptic equation such that $\nabla
u \in L^{q_{\sigma_1,\sigma_2}-\delta}$, but $\nabla u \notin
L^{q_{\sigma_1,\sigma_2}}.$ The existence of such optimal microgeometries is
achieved by convex integration methods, adapting to the present setting the
geometric constructions provided in [2] for the isotropic case.
"
"OBJECTIVE: The present study was undertaken to investigate the relationship between estimated folate and vitamin B12 intakes and their biochemical status in elderly persons.SUBJECTS: Twenty-eight males and 30 females ( > 65 years) were randomly selected from a larger sample of free-living elderly residents of Edmonton, Canada. Subjects were contacted through a seniors' service organization. Any subject using vitamin supplements or alcohol other than the social drink was excluded from the study.DESIGN: Dietary intake was estimated using a three-nonconsecutive-day food record. Biochemical status was assessed by measuring the plasma levels of folate and vitamin B12, as well as the red blood cell (r.b.c.) folate levels.RESULTS: Average daily intakes of both folate and vitamin B12 met the recommended requirements. Their mean plasma levels were within the accepted normal ranges in both males and females. However, probability analysis of dietary intake revealed an appreciable number of subjects at risk of deficiency, especially of folate (male 26%; female 21%). According to the interpretive guidelines of r.b.c. folate levels, an appreciable number of the study subjects were also found to be at risk of folate deficiency (male 57%; female 67%).CONCLUSIONS: The folate status appears to be a greater concern than the vitamin B12 status for the elderly population. It seems that although mean values of dietary intake and plasma concentrations of folate may indicate nutritional adequacy, a proportion of the older population may still be at nutritional risks."
"The process of clonal evolution was analyzed in a line of methylcholanthrene-induced mouse fibrosarcomas. The tumor cells were transfected with pSV2neo gene and 22 clones were randomly isolated. Genetically tagged clones were mixed and inoculated into syngeneic mice. Southern blot analysis revealed that one of the clones, no. 11, dominated both in tumors in situ and in lung metastatic nodules. No. 11 clone and other clones were similar in growth rates in vitro and in vivo, in spontaneous and experimental metastatic abilities, in immunogenicity, and in the capacity of intercellular communication in vitro. Although no. 11 clone overgrew other clones in vivo, this was not the case when clones were mixed and maintained in vitro. We conclude that clonal interactions in vivo may be responsible for the dominance of no. 11 clone in the tumor. It is likely that the preferential metastasis of no. 11 clone to the lung may be a simple reflection of the proliferative advantage of the dominant clone in the tumor in situ."
"A biotin-streptavidin-enhanced enzyme-linked immunosorbent assay (ELISA) which uses monoclonal antibodies (MAbs) for the detection of group C rotaviruses was developed. An assay in which plates were coated with three pooled MAbs and biotinylated polyclonal immunoglobulin G (IgG) (polyclonal antibody [PAb]) was used as the detector (MAb capture-PAb detector) was found to be the most sensitive and specific of the assays when it was compared with assays in which plates were coated with polyclonal antiserum and detection was done with either biotinylated polyclonal antiserum (PAb capture-PAb detector) or biotinylated pooled MAbs (PAb capture-MAb detector). The MAb capture-PAb detector ELISA detected 83% of samples confirmed to be positive for group C rotaviruses, whereas the PAb capture-PAb detector assay detected 63% of positive samples and the PAb capture-MAb detector assay detected 65% of positive samples. All three procedures detected both of the bovine and the two human group C rotaviruses, but none of the three procedures detected fecal samples containing group A and B rotaviruses or fecal samples negative for group C rotaviruses used in this study. The sensitivity of the MAb capture-PAb detector ELISA was determined by serially diluting fecal group C rotaviruses; antigens were detected in maximal positive dilution ranges of 1:1,000 to 1:3,000 for the samples tested. On the basis of the cell culture immunofluorescence assay infectivity titer of semipurified cell culture-passaged Cowden group C rotavirus, the sensitivity of the MAb capture-PAb detection ELISA for detection of homologous group C rotavirus was 53 fluorescent focus units per ml. Epitope mapping by use of the biotinylated MAbs in competition assay suggested that our MAbs may bind to three different but overlapping epitopes. These results suggest that the MAb capture-PAb detector ELISA can be used to study the epidemiology of group C rotaviruses in humans and animals."
"The effect of the absence of trace elements on the conversion of a mixture of volatile fatty acids by a distillery anaerobic granular sludge was investigated. Two UASB reactors were operated under identical operational conditions except for the influent trace metal concentrations, during 140 days. Experiments were carried out in three periods, where different organic loading rates (OLR) were applied to the reactors. The total trace metal concentration steadily decreased at a rate of 48 microg metal/g TS.d in the deprived reactor (down to 35% of their initial value). In contrast, trace metals accumulated in granules present in the control reactor. At the end of the experiment, the COD removal efficiencies were 99% and 77% for the control and deprived reactors, respectively, due to the lack of propionate conversion. Cobalt sorption experiments were carried out in order to study its speciation, and its effects on the speciation of other metals as well. A paper mill wastewater treating granular sludge was also included in the study as a comparison. Results obtained showed that the principal metal forms normally associated with any sludge are a function of each soluble metal concentration in the system, and the characteristics of the particular sludge."
"The impact of venous thromboembolism (VTE) and bleeding in patients undergoing major joint surgery has not been thoroughly studied. The Spanish National Discharge Database during the years 2005-2006 was used to assess the frequency and clinical impact of VTE and bleeding after elective total knee (TKA) or hip (THA) arthroplasty. Of 58,037 patients undergoing TKA, 0.18% (95% confidence interval [CI]: 0.15-0.22) were diagnosed with pulmonary embolism (PE), 0.57% (95% CI: 0.51-0.63) with deep-vein thrombosis (DVT), 1.20% (95% CI: 1.12-1.30) had bleeding complications, and 0.09% (95% CI: 0.07-0.12) died. Of 54 patients who died, 20.4% (95% CI: 10.7-35.4) had been diagnosed with PE, 3.70% (95% CI: 0.63-11.7) with DVT, and 13.0% (95% CI: 5.67-25.6) had bled. Of 31,769 patients undergoing elective THA, 0.23% (95% CI: 0.18-0.29) were diagnosed with PE, 0.44% (95% CI: 0.37-0.52) with DVT, 1.21% (95% CI: 1.10-1.34) bled, and 0.16% (95% CI: 0.12-0.21) died. Of 52 patients who died, 13.5% (95% CI: 6.08-24.8) had been diagnosed with PE, and 9.61% (95% CI: 3.52-21.3) had bled. On multivariable analysis, PE (odds ratio [OR]: 157; 95% CI: 75-328), DVT (OR: 6.3; 95% CI: 1.5-27) and bleeding (OR: 8.5; 95% CI: 3.6-20) were independent predictors for death after TKA. After THA, only PE (OR: 65; 95% CI: 26-160) and bleeding (OR: 6.4; 95% CI: 2.3-17) predicted the risk for death. Bleeding, DVT, and PE, arising after TKA were all independent predictors for death. Their increase in risk was, however, substantially higher for PE. After THA, only PE and bleeding independently predicted death."
"Determining response dynamics of hypoxic air hunger may provide information of use in clinical practice and will improve understanding of basic dyspnea mechanisms. It is hypothesized that air hunger arises from projection of reflex brain stem ventilatory drive (""corollary discharge"") to forebrain centers. If perceptual response dynamics are unmodified by events between brain stem and cortical awareness, this hypothesis predicts that air hunger will exactly track ventilatory response. Thus, during sustained hypoxia, initial increase in air hunger would be followed by a progressive decline reflecting biphasic reflex ventilatory drive. To test this prediction, we applied a sharp-onset 20-min step of normocapnic hypoxia and compared dynamic response characteristics of air hunger with that of ventilation in 10 healthy subjects. Air hunger was measured during mechanical ventilation (minute ventilation = 9 +/- 1.4 l/min; end-tidal Pco(2) = 37 +/- 2 Torr; end-tidal Po(2) = 45 +/- 7 Torr); ventilatory response was measured during separate free-breathing trials in the same subjects. Discomfort caused by ""urge to breathe"" was rated every 30 s on a visual analog scale. Both ventilatory and air hunger responses were modeled as delayed double exponentials corresponding to a simple linear first-order response but with a separate first-order adaptation. These models provided adequate fits to both ventilatory and air hunger data (r(2) = 0.88 and 0.66). Mean time constant and time-to-peak response for the average perceptual response (0.36 min(-1) and 3.3 min, respectively) closely matched corresponding values for the average ventilatory response (0.39 min(-1) and 3.1 min). Air hunger response to sustained hypoxia tracked ventilatory drive with a delay of approximately 30 s. Our data provide further support for the corollary discharge hypothesis for air hunger."
"A comprehensive anatomic, pathologic, and radiographic study was undertaken to define reliable soft-tissue landmarks about the hip. Methodology included an analysis of 300 ""normal"" adult hip radiographs, tissue maceration, cadaveric intra-articular injection studies and review of selected clinical cases. Results, when applied to clinical situations, indicate: (a) The iliopsoas and ""capsular"" fat planes are poor indicators of small to moderate amounts of intra-articular fluid in the adult. (b) The ""capsular"" fat plane is not associated with the joint capsule, but in fact is a fat plane between two muscle bundles anterior to the articulation. (c) Distinct soft-tissue planes are available for dissemination of fluid from the hip joint. These include the iliopsoas bursa, which may distend in association with articular disease, and the fat plane of the obturator externus muscle."
"The intraocular pressure of four New Zealand albino rabbit eyes was elevated when we replaced the aqueous humor of these eyes repeatedly with a chondroitin sulfate solution. Seen by electron microscopy, the trabecular meshwork of these eyes showed moderately increased collagen fibers, elastic fibers, and fine fibrils, and thickening of basement membrane. The amount of extracellular material present was markedly increased when compared with control eyes. Three types of basement membranes were noted. Compact multilaminated basement membrane and placoid accumulations of filamentous material with a granular background were found adjacent to the endothelial cells of the trabecular meshwork, and fine fibrils were observed around the angular aqueous veins. The ultrastructural alterations we observed were similar to those seen in some human glaucoma cases. The metabolism of trabecular meshwork cells may be affected by the long-term chondroitin sulfate treatment. As a result, extracellular matrix material appears to be accumulated in the trabecular meshwork, which may then contribute to an increased outflow resistance and a mild intraocular pressure elevation."
"Some aspects of reproductive function in the GnRH-deficient hypogonadal (hpg) mutant mouse can be restored by transplanting normal fetal brain tissue containing GnRH cells into the central nervous system of adult hpg mice. However, hpg males showing physiological response to the graft fail to display sexual behavior and are infertile. We hypothesized that the reproductive deficit of these males is due to insufficient perinatal exposure to testicular androgens as a consequence of the GnRH deficiency. To test this hypothesis we androgenized hpg males by giving them neonatal injections of testosterone propionate (TP). Controls consisted of hpg males not androgenized neonatally and of normal males. All three groups received a TP implant in adulthood, and their copulatory behavior and reproductive capability were recorded. In addition, other hpg males, not androgenized neonatally, received fetal brain transplants containing GnRH neurons and were also tested for copulatory behavior and reproductive capability before and after receiving a TP implant. Three of 8 neonatally androgenized hpg males expressed the full repertoire of male sexual behavior, including intromission and ejaculation, and sired several litters. Three of 7 control hpg males that were not androgenized neonatally but received TP implants in adulthood also displayed mounting and intromission, but there was no evidence of ejaculation, and these males failed to impregnate normal females. Of the 8 hpg males that responded to a fetal transplant with testicular growth, only 1 displayed mounting behavior. However, when given a TP implant, 4 of 8 hpg males with grafts displayed mounting and intromissions.(ABSTRACT TRUNCATED AT 250 WORDS)"
"PURPOSE: To report the results of complex wavefront-guided LASIK retreatments.METHODS: Twenty eyes (15 patients) with histories of conventional LASIK surgery and significant visual complaints of glare and halos due to higher order aberrations were treated. Wavefront-guided retreatments were performed with the LADARVision CustomCornea system (Alcon, Ft Worth, Tex). Pre- and postoperative topographies, wavefront measurements, and subjective reports were analyzed.RESULTS: Postoperatively, patients had an expanded optical zone, many with improved centration. Lower and higher order aberrations decreased following wavefront-guided ablation. Mean higher order root-mean-square decreased from 1.01 +/- 0.25 microm preoperatively to 0.84 +/- 0.23 microm postoperatively. Mean coma decreased from 0.59 +/- 0.26 microm to 0.43 +/- 0.21 microm. Mean spherical aberration decreased from 0.66 +/- 0.25 microm to 0.54 +/- 0.27 microm. Subjective reports of glare and halo symptoms improved in all patients.CONCLUSIONS: CustomCornea wavefront-guided treatments are effective in reducing lower and higher order aberrations, expanding optical zones, and improving subjective reports of adverse aberration sequelae such as glare and halos."
"A 41-year-old woman, suffering from continuous abdominal pain, only presented a non-specific inflammation of the whole colon and an unclaryfied hyponatriaemia; in spite of the only doubtful explanation by an enormous elongation of the colon, it was partially resected. Thereafter, the patient's decline, hypaesthesia, areflexia and tetraparesis required intensive care. Despite immunoglobulin therapy, assuming a Guillain-Barr? syndrome, the patient needed resuscitation, followed by signs of severe hypoxia (high level of neuron-specific enolase, hippocampal lesions). The abdominal pain, hyponatriaemia, persistent tachycardia, sensory deficits, tetraplegia, circulation arrest, later epileptic seizures and unusual urine color were finally explained by an acute intermittent porphyria (AIP). Although the symptoms were classic, the disease was recognized only very late. Indeed, it is so rare that most physicians will never be confronted with an AIP or only once or twice."
"The aim of this study was to elucidate the mechanisms involved in the inhibition of renal gluconeogenesis occurring under conditions of lowered activity of NADPH oxidase (Nox), the enzyme considered to be one of the main sources of reactive oxygen species in kidneys. The in vitro experiments were performed on primary cultures of rat renal proximal tubules, with the use of apocynin, a selective Nox inhibitor, and TEMPOL (4-hydroxy-2,2,6,6-tetramethylpiperidine-1-oxyl), a potent superoxide radical scavenger. In the in vivo experiments, Zucker diabetic fatty (ZDF) rats, a well established model of diabetes type 2, were treated with apocynin solution in drinking water. The main in vitro findings are the following: (1) both apocynin and TEMPOL attenuate the rate of gluconeogenesis, inhibiting the step catalyzed by phosphoenolpyruvate carboxykinase (PEPCK), a key enzyme of the process; (2) in the presence of the above-noted compounds the expression of PEPCK and the phosphorylation of transcription factor CREB and ERK1/2 kinases are lowered; (3) both U0126 (MEK inhibitor) and 3-(2-aminoethyl)-5-((4-ethoxyphenyl)methylene)-2,4-thiazolidinedione (ERK inhibitor) diminish the rate of glucose synthesis via mechanisms similar to those of apocynin and TEMPOL. The observed apocynin in vivo effects include: (1) slight attenuation of hyperglycemia; (2) inhibition of renal gluconeogenesis; (3) a decrease in renal PEPCK activity and content. In view of the results summarized above, it can be concluded that: (1) the lowered activity of the ERK1/2 pathway is of importance for the inhibition of renal gluconeogenesis found under conditions of lowered superoxide radical production by Nox; (2) the mechanism of this phenomenon includes decreased PEPCK expression, resulting from diminished activity of transcription factor CREB; (3) apocynin-evoked inhibition of renal gluconeogenesis contributes to the hypoglycemic action of this compound observed in diabetic animals. Thus, the study has delivered some new insights into the recently discussed issue of the usefulness of Nox inhibition as a potential antidiabetic strategy."
"OBJECTIVES: To determine the prevalence and the risk factors associated with HCV infection among women at childbirth, and to assess potential for infectivity of anti-HCV-positive women.METHODS: A total of 6995 women were interviewed and screened for HCV antibodies. Association and logistic regression analyses were conducted.RESULTS: The anti-HCV prevalence was 1.5% by EIA-3 and 0.8% by RIBA-3; HCV-RNA (RT-PCR) was detected in 74% of the RIBA-positive samples. Blood transfusion, race (blacks), alcohol abuse, a history of STD and anti-HBc positivity were independent risk factors for HCV positivity. Except for parenteral exposure, independent predictors of anti-HCV were a history of STD, anti-HBc positivity, a sex partner with multiple sex partners and a sex partner with a history of hepatitis.CONCLUSIONS: The prevalence of anti-HCV is higher in pregnant women than in blood donors. Sexual exposure may facilitate the spread of HCV and there is a high potential for mother-to-infant transmission."
"Crohn's disease and ulcerative colitis belong to a group of inflammatory bowel diseases (IBD). IBD are characterized by a chronic character of inflammatory process and overlapping immunological abnormalities, which, along with therapeutic strategies are currently available, underlie an increased risk of venous thromboembolic events (VTE). The most common sites of VTE in IBD patients are deep venous thrombosis (DVT) and pulmonary embolism (PE). These complications are particularly important in clinical practice due to a very high mortality rate. Therefore, an early diagnosis of new IBD cases and the control of inflammatory process are thought to play a crucial role in risk reduction for thromboembolic events. Despite considerable evidence supporting the association between IBD and VTE, there is still a lack of recognition of this risk, with dangerous consequences for patients. In this paper authors report three cases of VTE in IBD patients and discuss the most relevant clinical studies found in MEDLINE, Cochrane Library and EMBASE regarding its prevention and management."
"The function of pancreatic beta-cells is the synthesis and release of insulin, the main hormone involved in blood glucose homeostasis. Estrogen receptors, ER alpha and ER beta, are important molecules involved in glucose metabolism, yet their role in pancreatic beta-cell physiology is still greatly unknown. In this report we show that both ER alpha and ER beta are present in pancreatic beta-cells. Long term exposure to physiological concentrations of 17beta-estradiol (E2) increased beta-cell insulin content, insulin gene expression and insulin release, yet pancreatic beta-cell mass was unaltered. The up-regulation of pancreatic beta-cell insulin content was imitated by environmentally relevant doses of the widespread endocrine disruptor Bisphenol-A (BPA). The use of ER alpha and ER beta agonists as well as ER alphaKO and ER betaKO mice suggests that the estrogen receptor involved is ER alpha. The up-regulation of pancreatic insulin content by ER alpha activation involves ERK1/2. These data may be important to explain the actions of E2 and environmental estrogens in endocrine pancreatic function and blood glucose homeostasis."
"BACKGROUND AND PURPOSE: High blood pressure (BP) is associated independently with poor outcome after acute ischemic stroke, although in most analyses ""baseline"" BP was measured 24 hours or more postictus, and not during the hyperacute period.METHODS: Analyses included 1722 patients in hyperacute trials (recruitment <8 hours) from the Virtual Stroke International Stroke Trial Archive (VISTA) Collaboration. Data on BP at enrollment and after 1, 2, 16, 24, 48, and 72 hours, neurological impairment at 7 days (NIHSS), and functional outcome at 90 days (modified Rankin scale) were assessed using logistic regression models, adjusted for confounding variables; results are for 10-mm Hg change in BP.RESULTS: Mean time to enrollment was 3.7 hours (range 1.0 to 7.9). High systolic BP (SBP) was significantly associated with increased neurological impairment (odds ratio, OR 1.06, 95% confidence interval, 95% CI 1.01 to 1.12), and poor functional outcome; odds ratios for both increased with later BP measurements made at up to 24 hours poststroke. Smaller (versus larger) declines in SBP over the first 24 hours were significantly associated with poor NIHSS scores (OR 1.16, 95% CI 1.05 to 1.27) and functional outcome (OR 1.23, 95% CI 1.13 to 1.34). A large variability in SBP was also associated with poor functional outcome.CONCLUSIONS: High SBP and large variability in SBP in the hyperacute stages of ischemic stroke are associated with increased neurological impairment and poor functional outcome, as are small falls in SBP over the first 24 hours."
"Mesoporous silica has received considerable attention as a drug delivery vehicle because of its large surface area and large pore volume for loading drugs and large biomolecules. Recently, mesoporous silica microparticles have shown potential as a three-dimensional vaccine platform for modulating dendritic cells via spontaneous assembly of microparticles in a specific region after subcutaneous injection. For further in vivo applications, the biodegradation behavior of mesoporous silica microparticles must be studied and known. Until now, most biodegradation studies have focused on mesoporous silica nanoparticles (MSNs); here, we report the biodegradation of hexagonally ordered mesoporous silica, SBA-15, with micrometer-sized lengths (?32 ìm with a high aspect ratio). The degradation of SBA-15 microparticles was investigated in simulated body fluid (SBF) and in mice by analyzing the structural change over time. SBA-15 microparticles were found to degrade in SBF and in vivo. The erosion of SBA-15 under biological conditions led to a loss of the hysteresis loop in the nitrogen adsorption/desorption isotherm and fingerprint peaks in small-angle X-ray scattering, specifically indicating a degradation of ordered mesoporous structure. Via comparison to previous results of degradation of MSNs in SBF, SBA-15 microparticles degraded faster than MCM-41 nanoparticles presumably because SBA-15 microparticles have a pore size (?8 nm) and a pore volume larger than those of MCM-41 mesoporous silica. The surface functional groups, the residual amounts of organic templates, and the hydrothermal treatment during the synthesis could affect the rate of degradation of SBA-15. In in vivo testing, previous studies focused on the evaluation of toxicity of mesoporous silica particles in various organs. In contrast, we studied the change in the physical properties of SBA-15 microparticles depending on the duration after subcutaneous injection. The pristine SBA-15 microparticles injected into mice subcutaneously slowly degraded over time and lost ordered structure after 3 days. These findings represent the possible in vivo use of microsized mesoporous silica for drug delivery or vaccine platform after local injection."
"Individuals with autism spectrum disorder (ASD) and low intellectual/language abilities are often omitted from experimental studies because of the challenges of testing these individuals. It is vital to develop appropriate and accessible tasks so that this significant part of the spectrum is not neglected. The theory of mind (ToM) has been extensively assessed in ASD, predominantly in relatively high-functioning individuals with reasonable language skills. This study aims to assess the ToM abilities of a sample of 132 participants with intellectual disability (ID) with and without ASD, matched in verbal mental age (VMA) and chronological age, using a naturalistic and nonverbal deception task: the Penny Hiding Game (PHG). The relationship between performance on the PHG and everyday adaptation was also studied. The PHG proved accessible to most participants, suggesting its suitability for use with individuals with low cognitive skills, attentional problems, and limited language. The ASD + ID group showed significantly more PHG errors, and fewer tricks, than the ID group. PHG performance correlated with Vineland adaptation scores for both groups. VMA was a major predictor of passing the task in both groups, and participants with ASD + ID required, on average, 2 years higher VMA than those with ID only, to achieve the same level of PHG success. VMA moderated the association between PHG performance and real-life social skills for the ASD + ID more than the ID group, suggesting that severely impaired individuals with ASD may rely on verbal ability to overcome their social difficulties, whereas individuals with ID alone may use more intuitive social understanding both in the PHG and everyday situations."
"This article explores how behavioral theory can facilitate the development, implementation, and evaluation of health promotion software packages intended to influence personal health practices and/or assess health risks. Current behavioral theories and models are reviewed, and their relevance to developing health promotion software is discussed. A series of six steps is suggested for developing and evaluating health promotion and appraisal software within a behavioral theory framework. These steps should help to facilitate direct application of the theory-based process to health promotion software development."
"We report a series of 43 consecutive therapy-related myelodysplastic syndromes (t-MDS) or acute myeloid leukemias (t-AML) observed for 6 years. This series consisted of 26 women and 17 men, ages ranging from 9 to 85 years. These cases were classified into three groups according to the primary diagnosis. Conventional cytogenetic and fluorescent in situ hybridization (FISH)/ multiplex FISH (M-FISH) methods were used to analyze cytogenetic characteristics of secondary MDS/AML. The features of chromosomal abnormalities were linked to the nature of the therapy and protocols used. A considerable proportion of recurrent balanced translocations characterized t-AML secondary to therapy. FISH techniques showed that conventional cytogenetics often underestimated associated translocations; some deletions were in fact derivative chromosomes associated with deletions. After treatment for lymphomas and chronic myeloproliferative diseases, there were more complex unbalanced abnormalities than the control group. Compared to other series, recurrent translocations appeared to be more numerous (25%), probably reflecting an evolution of therapeutic modalities."
"OBJECTIVE: Children with ADHD frequently manifest behavioral difficulties in the morning prior to school. We sought to assess the reliability and validity of the Daily Parent Rating of Evening and Morning Behavior Scale, Revised (DPREMB-R) morning score as a measure of morning behaviors impaired by ADHD.METHOD: We used data from a clinical trial of HLD200 treatment in pediatric participants with ADHD to address our objectives.RESULTS: The DPREMB-R morning score showed significant internal homogeneity, test-retest reliability ( r = .52-.45), and good concurrent validity ( r = .50-.71).CONCLUSION: The DPREMB-R morning score could be a useful instrument for assessing treatment efficacy in the morning before school."
"In South Africa, as in much of the developing world, youth participation in the informal, unregulated street pesticide market results in exposures and risks of acute and chronic effects, yet has gone largely undocumented. A conceptual framework for understanding youth involvement in street pesticide sales and use includes contextual factors, health outcomes, and externalities (unintended negative consequences). An exploratory study based on this framework shows that highly-toxic pesticides, such as aldicarb, methamidophos, and chlorpyrifos, are easily available in informal markets in Cape Town's urban periphery. Youth are involved in the sale, distribution, and use of street pesticides, and are exposed during handling, transportation, spillage, storage, use and other activities, with little safety information available. Demand and supply for street pesticides is driven by joblessness, poverty, and inadequate pest management strategies. National and international efforts addressing underlying contextual determinants are required to protect children from exposures to street pesticides."
The objective of our study was to apply a quantitative analysis to the dynamic contrast enhanced MR imaging of the breast. Automated criteria increase the objectivity and reproducibility of the diagnostic interpretation of the imaging for differentiating benign and malignant lesions. The validation of this applied method was evaluated by analysing the time- signal intensity curves and the performance of the extracted enhancement parameters. The performance of some extracted parameters was evaluated by ROC (Receiver Operating Characteristic) analysis. These parameters were found to be particularly accurate in differentiating lesions.
"OBJECTIVE: To compare the clinical efficacy of two techniques for fabricating a Bimler device by assessing the patient's surface electromyography (sEMG) activity at rest before treatment and six months after treatment.METHODS: Twenty-four patients undergoing orthodontic treatment were enrolled in the study; 12 formed the test group and wore a Bimler device fabricated with a Myoprint impression using neuromuscular orthodontic technique and 12 formed the control group and were treated by traditional orthodontic technique with a wax bite in protrusion. The ""rest"" sEMG of each patient was recorded prior to treatment and six months after treatment.RESULTS: The neuromuscular-designed Bimler device was more comfortable and provided better treatment results than the traditional Bimler device.CONCLUSION: This study suggests that the patient group subjected to neuromuscular orthodontic treatment had a treatment outcome with more relaxed masticatory muscles and better function versus the traditional orthodontic treatment."
"Male sex workers are a highly marginalised group in Hong Kong and it is increasingly so with an influx of them travelling from mainland China to work as ""freelance"" sex workers. This study aimed to measure important work environment variables that might affect the likelihood of condom use among male sex workers working in Hong Kong. A cross-sectional survey of 161 participants recruited by snowball and convenience sampling methods through outreach workers of a local non-governmental organization was conducted in 2007-2008. Only 27.4%, 54.7% and 42.6% reported consistent condom use when engaging in oral, anal and vaginal sex, respectively. Logistic regression shows unsafe sex was nearly four times (OR=3.41; 95%CI 1.51-7.69) as common in institutionalised male sex workers as among their independent counterparts. Lack of condoms provided at workplaces was a major barrier in this socio-legal context and was strongly associated with condom non-use amongst institutionalised sex workers (OR= 10.86; 95%CI 2.94-40.17). The present study finds that when compared with independent Male sex workers (MSWs), institutionalised MSWs were older, less educated, earned a higher income but more likely to engage in unsafe sex with their clients and their partners. Public health physicians must work with law-enforcing authorities to provide clear guidelines to remove these HIV prevention barriers."
"Few studies have examined relationships among neurophysiological, psychological, and behavioral factors with regard to their effects on sleep quality. We used a structure equation model to investigate behavioral and psychological factors that influence neurophysiological regulation of sleep in shift workers. Using a cross-sectional study design, we tested the model with a sample of 338 female nurses working rotating shifts at an urban regional hospital. The Morningness-Eveningness Questionnaire (MEQ) and short-form Menstrual Distress Questionnaire (MDQ) were used to measure neurophysiological factors involved in morningness-eveningness and menstrual distress. The Sleep Hygiene Awareness and Practice Scale (SHAPS) and Profile of Mood States Short Form (POMS-SF) were completed to measure behavioral factors of sleep hygiene practices and psychological factors of mood states. In addition, the Pittsburgh Sleep Quality Index (PSQI) measured participant's self-reported sleep quality. The results revealed that sleep hygiene practices and mood states mediated the effects of morningness-eveningness and menstrual distress on sleep quality. Our findings provide support for developing interventions to enhance sleep hygiene and maintain positive mood states to reduce the influence of neurophysiological factors on sleep quality among shift workers."
"The amount of narrative clinical text documents stored in Electronic Patient Records (EPR) of Hospital Information Systems is increasing. Physicians spend a lot of time finding relevant patient-related information for medical decision making in these clinical text documents. Thus, efficient and topical retrieval of relevant patient-related information is an important task in an EPR system. This paper describes the prototype of a medical information retrieval system (MIRS) for clinical text documents. The open-source information retrieval framework Apache Lucene has been used to implement the prototype of the MIRS. Additionally, a multi-label classification system based on the open-source data mining framework WEKA generates metadata from the clinical text document set. The metadata is used for influencing the rank order of documents retrieved by physicians. Combining information retrieval and automated document classification offers an enhanced approach to let physicians and in the near future patients define their information needs for information stored in an EPR. The system has been designed as a J2EE Web-application. First findings are based on a sample of 18,000 unstructured, clinical text documents written in German."
"BACKGROUND AND PURPOSE: ISRs remain a major issue in the endovascular management of ICAD, requiring retreatment by reangioplasty. The aim of the present study was to evaluate the technical feasibility, safety, and efficiency of the novel DEBs for neurovascular ISRs.MATERIALS AND METHODS: Fifty-one patients (median age, 67 years; age range, 34-82 years; male/female ratio, 37:14) underwent 63 balloon dilation procedures for ISRs in intracranial stented arterial segments between November 2007 and August 2010 in a single center. Of the 63 procedures, 20 (32%) were performed by using a conventional balloon and 43 (68%), by using a paclitaxel-eluting balloon (SeQuent Please). Angiographic and clinical follow-up was performed at 6 and 12 weeks, 6 and 12 months, and yearly thereafter. Technical success rate, periprocedural complications, occurrence of recurrent ischemic symptoms, and the development of a recurrent ISR after reangioplasty were analyzed.RESULTS: Technical success, defined as <50% residual stenosis was achieved in all cases (100%), with failure of the DEB treatment in 6% of the attempts; those lesions were finally successfully treated with a conventional balloon. The combined permanent neurologic morbidity and mortality rate (stroke, ICH, and SAH) at 30 days was 1.6%. Substantial difference was found in the rate of recurrent stenosis when comparing conventional balloons and DEBs, with recurrent stenosis rates of 50% and 9%, respectively.CONCLUSIONS: The initial results of reangioplasty of intracranial ISRs with DEBs are encouraging; further technical developments are, nevertheless, mandatory."
"The peripheral astrocyte process (PAP) is the glial compartment largely handling inactivation of transmitter glutamate, and supplying glutamate to the axon terminal. It is not clear how these energy demanding processes are fueled, and whether the PAP exhibits oxidative capability. Whereas the GFAP-positive perinuclear cytoplasm and stem process are rich in mitochondria, the PAP is often considered too narrow to contain mitochondria and might thus not rely on oxidative metabolism. Applying high resolution light microscopy, we investigate here the presence of mitochondria in the PAPs of freshly dissociated, isolated astrocytes. We provide an overview of the subcellular distribution and the approximate size of astrocytic mitochondria. A substantial proportion of the astrocyte's mitochondria are contained in the PAPs and, on the average, they are smaller there than in the stem processes. The majority of mitochondria in the stem and peripheral processes are surprisingly small (0.2-0.4 µm), spherical and not elongate, or tubular, which is supported by electron microscopy. The density of mitochondria is two to several times lower in the PAPs than in the stem processes. Thus, PAPs do not constitute a mitochondria free glial compartment but contain mitochondria in large numbers. No juxtaposition of mitochondria-containing PAPs and glutamatergic synapses has been reported. However, the issue of sufficient ATP concentrations in perisynaptic PAPs can be seen in the light of (1) the rapid, activity dependent PAP motility, and (2) the recently reported activity-dependent mitochondrial transport and immobilization leading to spatial, subcellular organisation of glutamate uptake and oxidative metabolism."
"The mechanism of mitochondrial damage during reperfusion injury of ischemic myocardium was studied using mongrel dogs in vivo and isolated mitochondria in vitro. Seventy-seven adult dogs were divided into three groups: the control group (n = 38), the Coenzyme Q10 (CoQ10)-5 mg group (n = 24), and the CoQ10-15 mg group (n = 15). In the control group, the left anterior descending coronary artery (LAD) of the dog was occluded for 15 min followed by 5 min of reperfusion after 40 min of premedication with physiological saline. In both CoQ10 groups, 5 mg/kg or 15 mg/kg of CoQ10 was infused intravenously for 20 min and then physiological saline was administered for 20 min before 15 min occlusion of the LAD. Subsequently, reperfusion was allowed for 5 min. Each group was further divided into two subgroups depending on the presence (arrhythmia group) or the absence (non-arrhythmia group) of ventricular arrhythmias. Immediately after 15 min occlusion, myocardial samples were taken from the normal and reperfused areas to measure CoQ10 content of myocardium. Heart mitochondria were prepared after 5 min of reperfusion from both areas. Arrhythmias appeared in 12 of 38 dogs in the control group (32%), two of 24 dogs in the CoQ10-5 mg group (8%) and none of 15 dogs in the CoQ10-15 mg group (0%). Premedication with CoQ10 increased tissue CoQ10 content in a dose-dependent manner. In the CoQ10-5 mg group, the increase in CoQ10 content of dogs with reperfusion arrhythmias was relatively less than that of dogs without reperfusion arrhythmias. In each group, mitochondrial function was decreased in the arrhythmia group compared to that of the non-arrhythmia group. The increase in free fatty acid (FFA) content and the decrease in phospholipid content were also observed in mitochondria from the reperfused area of each arrhythmia group. The increase in FFA and mitochondrial dysfunction were induced by the incubation of mitochondria in vitro with phospholipase (PLase) A2 or PLase C, and protected by the addition of CoQ10. These results suggest that PLase plays an important role in the development of mitochondrial damage associated with reperfusion."
"A growing body of literature suggests that ADHD is associated with emotion recognition impairments that may be linked to deficient interpersonal functioning. However, our understanding of the mechanisms underlying these recognition impairments is extremely limited. Here, we used dynamic stimuli to investigate whether impaired emotion recognition in children with ADHD may be associated with impairments in perceptual sensitivity. Participants (ADHD: N = 26; Controls: N = 26) viewed video sequences of neutral faces slowly developing into one of the six basic emotional expressions (angry, happy, fearful, sad, disgusted and surprised) and were instructed to indicate via a button press the precise moment at which they were able to correctly recognize the emotional expression. The results showed that compared to controls, children with ADHD exhibited lower accuracy rates across all emotional expressions while there was no evidence for impaired perceptual sensitivity. Thus, the study provides evidence for a generalized categorization impairment across all emotional categories and is consistent with developmental delay accounts of ADHD. Future studies are needed in order to further investigate the developmental course of social cognition deficits in ADHD."
"PURPOSE: To prevent perioperative hypothermia, forced air warming blanket was compared with a passive insulation suit.DESIGN: Prospective, open, randomized controlled trial.METHODS: Thirty patients were scheduled for orthopedic spinal surgery. The intervention group (group TS) received the thermal suit T-Balance before premedication and throughout the perioperative period, whereas the control group (group C) received forced air warming (FAW) during surgery.FINDINGS: No statistically significant difference (ns) was found between the groups for core temperature 30 minutes after induction of general anesthesia. Perioperative hypothermia occurred in 10 (66.7%) patients in group TS and 6 (40%) in group C (ns). For hypothermic patients, re-establishment of normothermia took significantly longer in group TS, mean 108 ± 111 minutes, than in group C, 33 ± 59.5 minutes (P = .03).CONCLUSIONS: The thermal suit did not prevent hypothermia in this study. FAW was significantly more efficient in re-establishing normothermia."
"Ki67 is a nuclear protein that is tightly linked to the cell cycle. It is a marker of cell proliferation and has been used to stratify good and poor prognostic categories in invasive breast cancer. Its correlation with gene expression patterns has not been fully elucidated. In this study, Ki67 immunohistochemistry using the MIB-1 antibody was performed on sections cut from 21 formalin-fixed, paraffin-embedded invasive breast cancers. Scoring was determined as nil (no immunostaining), low (10% or less immunopositivity) or high (>10% immunoreactive cells) respectively. The relationship of Ki67 immunohistochemical detection with clinicopathologic parameters was evaluated. Using Affymetrix U133A GeneChips, expression profiles for these tumors were generated and correlated with Ki67 immunohistochemical findings. Analysis of variance was used to define genes that were differentially regulated between the groups. Real-time polymerase chain reaction (PCR) was used to confirm the presence of a downregulated gene. Our results showed high, low and nil Ki67 immunostaining in nine (43%), six (28.5%) and six (28.5%) invasive breast cancers respectively, with increased Ki67 protein expression correlating with high histologic grade (P=0.02), mitotic score (P=0.001) and estrogen receptor immunonegativity (P=0.002). Expression profiling trends of the Ki67 gene mirrored the observed proportions of immunostained cells when the Ki67 immunoscore was >10%. Genes related to apoptosis and cell death (bcl2, MAP2K4, TNF10) were noted to be downregulated in tumors that disclosed >40% Ki67 immunostaining (P<0.001). Downregulation of the bcl2 gene was confirmed at the RNA level by real-time RT-PCR. Differential regulation of these genes, especially bcl2, may contribute to the biological nature of clinically more aggressive and highly proliferative breast cancers."
"BACKGROUND: Due to demographic projections, and lack of an algorithm in the case of a prostate specific antigen (PSA)-positive donor, the loss of organ recovery may occur more frequently in the near future without approved procedures. In Poland in recent years it has been recommended to determine tumor markers in potential donors. In the first year of the recommendation 10% of potential deceased donors were disqualified in our transplantation center on the basis of the elevated PSA levels (high PSA >10 ng/mL). Histopathologic evaluation of prostate was implemented in a donor qualification procedure to prevent reduction of the actual organ donor pool.MATERIAL AND METHODS: In the period of January 2010-January 2014 each donor reported to a coordination center (n = 52; median age, 54 years) and underwent the routine histological evaluation of the whole prostate, regardless of the PSA level.RESULTS: Pathologist revealed in the study group of 52 male donors, 6 cases of carcinoma of the prostate (CaP; 12%). There was no correlation between PSA level and CaP (-)/CaP(+) (median 7.0 vs 3.9 ng/mL, respectively; P = .51) nor high-grade prostate intraepithelial neoplasia (HGPIN) (+)/HGPIN (-) (median 5.9 vs 4.3 ng/mL; P = .14). All of the recovered organs (12 kidneys and 3 livers) from donors with CaP were transplanted, resulting in a 15% increase in the organ donor pool.CONCLUSIONS: There is no association between PSA values and CaP occurrence in deceased organ donors. Histological verification allowed for an increase in the organ pool with maintenance of safety standards."
"The basic premise of this effort is that health care can be made more effective and affordable by applying modern computer technology to improve collaboration among diverse and distributed health care providers. Information sharing, communication, and coordination are basic elements of any collaborative endeavor. In the health care domain, collaboration is characterized by cooperative activities by health care providers to deliver total and real-time care for their patients. Communication between providers and managed access to distributed patient records should enable health care providers to make informed decisions about their patients in a timely manner. With an effective medical information infrastructure in place, a patient will be able to visit any health care provider with access to the network, and the provider will be able to use relevant information from even the last episode of care in the patient record. Such a patient-centered perspective is in keeping with the real mission of health care providers. Today, an easy-to-use, integrated health care network is not in place in any community, even though current technology makes such a network possible. Large health care systems have deployed partial and disparate systems that address different elements of collaboration. But these islands of automation have not been integrated to facilitate cooperation among health care providers in large communities or nationally. CERC and its team members at Valley Health Systems, Inc., St. Marys Hospital and Cabell Huntington Hospital form a consortium committed to improving collaboration among the diverse and distributed providers in the health care arena. As the first contract recipient of the multi-agency High Performance Computing and Communications (HPCC) Initiative, this team of computer system developers, practicing rural physicians, community care groups, health care researchers, and tertiary care providers are using research prototypes and commercial off-the-shelf technologies to develop an open collaboration environment for the health care domain. This environment is called ARTEMIS--Advanced Research TEstbed for Medical InformaticS."
"OBJECTIVE: The main pathological change of Parkinson's disease (PD) is progressive degeneration and necrosis of dopaminergic neurons in the midbrain, forming a Lewy body in many of the remaining neurons. Studies have found that in transgenic Drosophila, mutations in the PTEN-inducible kinase 1 (PINK1) gene may cause indirect flight muscle defects in Drosophila, and mitochondrial structural dysfunction as well.METHODS: In this study, Wnt4 gene overexpression and knockdown were performed in PINK1 mutant PD transgenic Drosophila, and the protective effect of Wnt4 gene on PD transgenic Drosophila and its possible mechanism were explored. The Wnt4 gene was screened in the previous experiment; And by using the PD transgenic Drosophila model of the MHC-Gal4/UAS system, the PINK1 gene could be specifically activated in the Drosophila muscle tissue.RESULTS: In PINK1 mutation transgenic fruit flies, the Wnt4 gene to study its implication on PD transgenic fruit flies' wing normality and flight ability. We found that overexpression of Wnt4 gene significantly reduced abnormality rate of PD transgenic Drosophila and improved its flight ability, and then, increased ATP concentration, enhanced mitochondrial membrane potential and normalized mitochondrial morphology were found. All of these findings suggested Wnt4 gene may have a protective effect on PD transgenic fruit flies. Furthermore, in Wnt4 gene overexpression PD transgenic Drosophila, down-regulation autophagy and apoptosis-related proteins Ref(2)P, Pro-Caspase3, and up-regulation of Beclin1, Atg8a, Bcl2 protein were confirmed by Western Blotting.CONCLUSION: The results imply that the restoring of mitochondrial function though Wnt4 gene overexpression in the PINK1 mutant transgenic Drosophila may be related to autophagy and/or apoptosis."
"The effect of misonidazole at different concentrations on the anoxic radiation sensitivity of Chinese hamster cells was investigated using the frequency of radiation induced micronuclei as criterion. The result indicates that, in a high radiation dose region, sensitization with a dose modifying factor of about 1.9 and 1.3 occurs after treatment with the substance at a concentration of 8 and 0.2 mmol/l, respectively. In a low dose region the corresponding values were 1.7 and 0.8. It was concluded that high concentration of the substance in combination with high radiation doses are most beneficial."
"Patients with the clinical diagnosis of ischemic heart disease who were found to be free of significant coronary artery atherosclerotic disease (n = 150) underwent coronary vasodilator reserve testing, 2-dimensional echocardiography, and dipyridamole limited-stress thallium testing. After exclusions (predominantly for technically poor coronary artery Doppler signals or suboptimal echocardiography), 100 patients formed the study population. The purpose was to characterize typical cardiac and coronary artery findings in hypertensive patients with severe left ventricular (LV) hypertrophy (n = 15) and to investigate the evidence for myocardial ischemia unrelated to coronary atherosclerosis in early and advanced hypertensive heart disease. Normotensive and hypertensive control groups without LV hypertrophy (n = 12 and 34, respectively) were used for comparison. Severe LV hypertrophy was defined as LV mass index greater than or equal to 50% above established gender specific norms using 2-dimensional-directed M-mode echocardiography and the cube equation corrected to agree with necropsy estimates of mass. Clinical characteristics more often associated with severe LV hypertrophy were black race (67%), diabetes mellitus (33%), proteinuria (47%) and elevated creatinine (1.5 +/- 0.9 mg/dl). Baseline electrocardiograms and dipyridamole limited-stress thallium scans were highly likely to be abnormal (94 and 73%, respectively). Both eccentric and concentric cardiac hypertrophies were found in the severe group. Ejection fraction was significantly lower (0.51 vs 0.68, p = 0.002) and basal coronary flow velocity higher (12.0 vs 5.0 cm/s, p = 0.0004) among these patients when compared with normotensive control patients. Coronary flow reserve did not differ between control groups but was significantly depressed in patients with severe LV hypertrophy (2.5 vs 3.9, p = 0.001).(ABSTRACT TRUNCATED AT 250 WORDS)"
"The world's river dolphins (Inia, Pontoporia, Lipotes and Platanista) are among the least known and most endangered of all cetaceans. The four extant genera inhabit geographically disjunct river systems and exhibit highly modified morphologies, leading many cetologists to regard river dolphins as an unnatural group. Numerous arrangements have been proposed for their phylogenetic relationships to one another and to other odontocete cetaceans. These alternative views strongly affect the biogeographical and evolutionary implications raised by the important, although limited, fossil record of river dolphins. We present a hypothesis of river dolphin relationships based on phylogenetic analysis of three mitochondrial genes for 29 cetacean species, concluding that the four genera represent three separate, ancient branches in odontocete evolution. Our molecular phylogeny corresponds well with the first fossil appearances of the primary lineages of modern odontocetes. Integrating relevant events in Tertiary palaeoceanography, we develop a scenario for river dolphin evolution during the globally high sea levels of the Middle Miocene. We suggest that ancestors of the four extant river dolphin lineages colonized the shallow epicontintental seas that inundated the Amazon, Paran?, Yangtze and Indo-Gangetic river basins, subsequently remaining in these extensive waterways during their transition to freshwater with the Late Neogene trend of sea-level lowering."
"We studied nine patients with arteriosclerotic coronary heart disease (Group I) and 10 patients with normal coronary arteries (Group II) to elucidate mechanisms by which nifedipine affects catecholamine arterial levels and myocardial extraction at rest and during pacing. Nifedipine induced signs of general and coronary vasodilatation and improved myocardial lactate metabolism during pacing. Noradrenaline arterial concentration increased significantly after nifedipine in both groups (p less than 0.01). In Group I, the values were 40-50% higher compared with Group II. The net myocardial extraction of noradrenaline was not significantly affected by nifedipine, except during pacing in Group II, in which an increase in myocardial release was seen (p less than 0.05). In Group I, there was no correlation between arterial concentration of noradrenaline and myocardial release, while there was a significant negative correlation in Group II (p less than 0.0001). Adrenaline arterial concentration and myocardial extraction were not altered by nifedipine, but there was a positive significant correlation in Group I between arterial concentration and myocardial extraction of adrenaline (p less than 0.001). In conclusion, nifedipine increased noradrenaline levels in both groups of patients, probably reflecting a rise in sympathetic activity. Because myocardial lactate production turned into extraction in patients with coronary artery disease, the increased sympathetic tone evidently did not outweigh the beneficial antianginal effects of nifedipine."
"The relationships of spirituality with human social cognition, as exemplified in autism spectrum and schizophrenia spectrum cognitive variation, remain largely unstudied. We quantified non-clinical levels of autism spectrum and schizotypal spectrum traits (using the Autism Quotient and the Schizotypal Personality Questionnaire-Brief Revised) and dimensions of spirituality (using the Hardt Spirituality Questionnaire) in a large sample of undergraduate students. We tested in particular the hypothesis, based on the diametrical model of autism and psychosis, that autism should be negatively associated, and positive schizotypal traits should be positively associated, with spirituality. Our primary findings were threefold. First, in support of the diametric model, total Spirituality score was significantly negatively correlated with total Autism Quotient score, and significantly positively correlated with Positive Schizotypal traits (the Schizotypal Personality Cognitive-Perceptual subscale), as predicted. Second, these associations were driven mainly by opposite patterns regarding the Search for Meaning Spirituality subscale, which was the only subscale that was significantly negatively associated with autism, and significantly positively associated with Positive Schizotypal traits. Third, Belief in God was positively correlated with Positive Schizotypal traits, but was uncorrelated with autism traits. The opposite findings for Search for Meaning can be interpreted in the contexts of well-supported cognitive models for understanding autism in terms of weak central coherence, and understanding Positive Schizotypal traits in terms of enhanced salience."
"Aqueous environments in living cells are crowded, with up to >50 wt% small and macromolecule-size solutes. We investigated quantitatively one important consequence of molecular crowding--reduced diffusion of biologically important solutes. Fluorescence correlation spectroscopy (FCS) was used to measure the diffusion of a series of fluorescent small solutes and macromolecules. In water, diffusion coefficients (D(o)w) were (in cm2/s x 10(-8)): rhodamine green (270), albumin (52), dextrans (75, 10 kDa; 10, 500 kDa), double-stranded DNAs (96, 20 bp; 10, 1 kb; 3.4, 4.5 kb) and polystyrene nanospheres (5.4, 20 nm diameter; 2.3, 100 nm). Aqueous-phase diffusion (Dw) in solutions crowded with Ficoll-70 (0-60 wt%) was reduced by up to 650-fold in an exponential manner: Dw = D(o)w exp (-[C]/[C]exp), where [C]exp is the concentration (in wt%) of crowding agent reducing D(o)w by 63%. FCS data for all solutes and Ficoll-70 concentrations fitted well to a model of single-component, simple (non-anomalous) diffusion. Interestingly [C]exp were nearly identical (11+/-2 wt%, SD) for diffusion of the very different types of macromolecules in Ficoll-70 solutions. However, [C]exp was dependent on the nature of the crowding agent: for example, [C]exp for diffusion of rhodamine green was 30 wt% for glycerol and 16 wt% for 500 kDa dextran. Our results indicate that molecular crowding can greatly reduce aqueous-phase diffusion of biologically important macromolecules, and demonstrate a previously unrecognized insensitivity of crowding effects on the size and characteristics of the diffusing species."
"BACKGROUND: Valve-sparing aortic root reconstruction (VSRR) is an accepted method to treat patients with aortic root dilation. The role of the VSRR is less well defined for patients with bicuspid aortic valve, severe aortic valve insufficiency, congenital heart defects, and type A aortic dissection. We studied the clinical outcome of patients who underwent VSRR for expanded indications.METHODS: Seventy-eight patients underwent VSRR between the 2005 and 2012. Seventy-two patients (92%) underwent reimplantation and 6 patients (8%) were operated on with the remodeling technique. The mean age was 51 ± 12 years (range 24 to 73). For 71 patients (91%), the operation was elective, and for 7 (9%; all with type A aortic dissection), on an emergency basis. Preoperatively, the degree of aortic insufficiency was graded as 2+ or greater for 27 patients (35%). Connective tissue disorder (Marfan or Loeys-Dietz), bicuspid aortic valve, or congenital heart disease was present in 15 (19%), 15 (19%), and 7 patients (9%), respectively. Concomitant aortic valve leaflet repair was performed for 39 patients (50%). The mean follow-up time was 2.4 ± 1.7 years (range, 0.1 to 7.0).RESULTS: Thirty-day mortality was zero. The rate of postoperative complications was low: stroke 3%, renal failure 3%, prosthesis infection 1%, and low cardiac output syndrome 1%. Survival was 100% at 1 year and 97% at 5 years. Freedom from recurrent aortic valve insufficiency (?2+) during the follow-up was 94%.CONCLUSIONS: The midterm results of VSRR in terms of survival, freedom from recurrent aortic valve insufficiency, and the need for reoperation are excellent, even for high-risk patients."
"BACKGROUND: The motivational and other factors used by medical students in making their career choices for specific medical specialities have been looked at in a number of studies in the literature. There are however few studies that assess the generic factors which make medicine itself of interest to medical students and to potential medical students. This study describes a novel questionnaire that assesses the interests and attractions of different aspects of medical practice in a varied range of medical scenarios, and relates them to demographic, academic, personality and learning style measures in a large group of individuals considering applying to medical school.METHODS: A questionnaire study was conducted among those attending Medlink, a two-day conference for individuals considering applying to medical school for a career in medicine. The main outcome measure was the Medical Situations Questionnaire, in which individuals ranked the attraction of three different aspects of medical practise in each of nine detailed, realistic medical scenarios in a wide range of medical specialities. As well as requiring clear choices, the questionnaire was also designed so that all of the possible answers were attractive and positive, thereby helping to eliminate social demand characteristics. Factor analysis of the responses found four generic motivational dimensions, which we labelled Indispensability, Helping People, Respect and Science. Background factors assessed included sex, ethnicity, class, medical parents, GCSE academic achievement, the 'Big Five' personality factors, empathy, learning styles, and a social desirability scale.RESULTS: 2867 individuals, broadly representative of applicants to medical schools, completed the questionnaire. The four generic motivational factors correlated with a range of background factors. These correlations were explored by multiple regression, and by path analysis, using LISREL to assess direct and indirect effects upon the factors. Helping People was particularly related to agreeableness; Indispensability to a strategic approach to learning; Respect to a surface approach to learning; and Science to openness to experience. Sex had many indirect influences upon generic motivations. Ethnic origin also had indirect influences via neuroticism and surface learning, and social class only had indirect influences via lower academic achievement. Coming from a medical family had no influence upon generic motivations.CONCLUSION: Generic motivations for medicine as a career can be assessed using the Medical Situations Questionnaire, without undue response bias due to demand characteristics. The validity of the motivational factors is suggested by the meaningful and interpretable correlations with background factors such as demographics, personality, and learning styles. Further development of the questionnaire is needed if it is to be used at an individual level, either for counselling or for student selection."
"Highly purified and well-characterised preparations of equine prolactin and growth hormone from equine pituitary glands were employed to set up highly sensitive and specific homologous radioimmunoassays (RIA) for the measurement of hormone in horse plasma. The limit of sensitivity of the GH RIA was 1.2 ng/ml with mean intra- and inter-assay coefficients of variation (CV) of 6.6 and 10%, respectively. The sensitivity of the equine prolactin (ePRL) RIA was 0.5 ng/ml with mean intra and inter-assay CV of 9.1 and 15.6%, respectively. Dose-response curves of a crude pituitary gland extract and plasma samples collected from a mare and foal were parallel to the standards and the PRL RIA was clinically validated by administration of thyrotropin-releasing hormone (TRH). Plasma samples taken at 15 min intervals over 24 h from lactating mares gave 24 h mean GH values in the range 5.5 to 7.95 ng/ml. Large intermittent elevations of GH activity were detected. The mean 24 h PRL concentrations were between 3.2-10.4 ng/ml in the lactating animals, with higher concentrations earlier in lactation. Long episodic bursts of PRL were detected."
"The move from a retrospective payment system (value added) to a prospective payment system (diagnostic related) has not only influenced the health care business but also changed their information systems' requirements. The change in requirements can be attributed both to an increase in data processing tasks and also to an increase in the need for information to more effectively manage the organization. A survey was administered to capture the response of health care institutions, in the area of information systems, to the prospective payment system. The survey results indicate that the majority of health care institutions have responded by increasing their information resources, both in terms of hardware and software, and have moved to integrate the medical and financial data. In addition, the role of the information system has changed from a cost accounting system to one intended to provide a competitive edge in a highly competitive marketing environment."
"A technique of arthroscopic excision of a torn meniscus has evolved during two years' experience of therapeutic arthroscopy of the knee joint. The same method is used for both medial and lateral meniscus lesions under full visual control throughout the procedure, with a standard arthroscope inserted centrally through the patellar tendon. The first 18 consecutive patients thus treated have been followed up. Endoscopic operation was complemented by arthrotomy in three; in 15 no other treatment was given. The time in hospital, convalescence, sick leave, and knee function at follow-up were compared in these 15 patients with matched controls treated in the ordinary way by arthrotomy. Knee function did not differ between the groups, whereas all other variables showed better results after arthroscopic excision."
The conformations adopted by beta-casein and the total apoprotein from serum high density lipoprotein when spread at the air-water interface are compared; the monolayer data are consistent with the apoprotein being alpha-helical and the beta-casein being disordered with segments distributed in loops and trains. The penetration of these hydrophobic proteins into phosphatidylcholine monolayers in different physical states was investigated. More protein can penetrate into monolayers when they are in the liquid-expanded state; for penetration at constant total surface area the lateral compressibility of the lipid is an important factor. The charge and conformation of the polar group of the phospholipid does not have a major influence on the interaction. The mixed films of lipid and protein have a mosaic structure; probably the beta-casein is in a compressed state whereas the apoprotein is extended as alpha-helices in the plane of the interface. The chain-length depedences of the interaction of the apoprotein with phosphatidylcholine monolayers and bilayers are different; when the apoprotein binds to bilayers of shorter-chain phosphatidylcholines it alters the shape of the lipid-water interface whereas with monolayers the interface remains planar throughout.
"A hypothesis was examined that carboxypeptidase H (CpAse H), which is known to catalyse the release of lysine and arginine from the C-terminus of peptides, can also release histidine, tyrosine, and phenylalanine. Synthetic peptides terminating in -His-Lys or -Tyr-Lys were used as model substrates for the enzyme and amino acid analysis was employed to detect release of the terminal amino acids. With N-acetyl-beta-Ala-Asn-Ala-His-Lys and N-acetyl-beta-Ala-Asn-Ala-Tyr-Lys, which correspond to intermediates in the processing of porcine and human beta-endorphin, lysine was removed rapidly and quantitatively but no release of histidine or tyrosine could be detected. To allow more sensitive analysis, radiolabelled substrates were employed and the amounts of the products formed on incubation with CpAse H were determined after separation by ion-exchange chromatography. With 125I-D-Tyr-Ala-His-Lys-Lys as substrate at pH 5.7, very small amounts of D-Tyr-Ala were released; the main product was D-Tyr-Ala-His. At pH 5.0 the release of histidine from 125I-D-Tyr-Ala-His took place 6,000 times more slowly than the release of lysine from 125I-D-Tyr-Ala-Lys. When the tripeptides were incubated at pH 5 with porcine pituitary secretory granules, the lysine was released rapidly but no release of histidine could be detected. The results demonstrate that CpAse H catalyses the release of C-terminal histidine with great difficulty.(ABSTRACT TRUNCATED AT 250 WORDS)"
"BACKGROUND: Hearing impairment is the most common sensory impairment in humans, affecting 1:1,000 births. We have identified an ENU generated mouse mutant, Mozart, with recessively inherited, non-syndromic progressive hearing loss caused by a mutation in the synaptojanin 2 (Synj2), a central regulatory enzyme in the phosphoinositide-signaling cascade.METHODOLOGY/PRINCIPAL FINDINGS: The hearing loss in Mozart is caused by a p.Asn538Lys mutation in the catalytic domain of the inositol polyphosphate 5-phosphatase synaptojanin 2. Within the cochlea, Synj2 mRNA expression was detected in the inner and outer hair cells but not in the spiral ganglion. Synj2(N538K) mutant protein showed loss of lipid phosphatase activity, and was unable to degrade phosphoinositide signaling molecules. Mutant Mozart mice (Synj2(N538K/N538K)) exhibited progressive hearing loss and showed signs of hair cell degeneration as early as two weeks of age, with fusion of stereocilia followed by complete loss of hair bundles and ultimately loss of hair cells. No changes in vestibular or neurological function, or other clinical or behavioral manifestations were apparent.CONCLUSIONS/SIGNIFICANCE: Phosphoinositides are membrane associated signaling molecules that regulate many cellular processes including cell death, proliferation, actin polymerization and ion channel activity. These results reveal Synj2 as a critical regulator of hair cell survival that is essential for hair cell maintenance and hearing function."
"Ambroise Par? and Percivall Pott were preeminent clinicians, writers, and educators who advanced the cause of surgery through their independent thinking and insightful observations. In addition to their legacies and numerous enduring contributions, both men sustained open tibial fractures that were successfully managed during times when most often amputation was prescribed for such injuries. This article explores their lives and their fractures."
"The present study, concerning 145 insulin-dependent diabetics showed positive relationships between the severity of retinal disease on the one hand, and body weight, blood pressure, and serum cholesterol level on the other. These relationships remain significant when the duration of the clinical diabetes and the age of the patient are taken into account. Two interpretations are suggested. They are not incompatible. In diabetic subjects, either the increase in blood pressure and serum cholesterol level causes an aggravation of diabetic retinopathy or there exists a common factor at the origin of retinal lesions and of an increase in risk of cardiovascular disease through atherosclerosis."
"Since the discovery of dipeptide self-assembly, diphenylalanine (Phe-Phe)-based dipeptides have been widely investigated in a variety of fields. Although various supramolecular Phe-Phe-based structures including tubes, vesicles, fibrils, sheets, necklaces, flakes, ribbons, and wires have been demonstrated by manipulating the external physical or chemical conditions applied, studies of the morphological diversity of dipeptides other than Phe-Phe are still required to understand both how these small molecules respond to external conditions such as the type of solvent and how the peptide sequence affects self-assembly and the corresponding molecular structures. In this work, we investigated the self-assembly of valine-alanine (Val-Ala) and alanine-valine (Ala-Val) dipeptides by varying the solvent medium. It was observed that Val-Ala dipeptide molecules may generate unique self-assembly-based morphologies in response to the solvent medium used. Interestingly, when Ala-Val dipeptides were utilized as a peptide source instead of Val-Ala, we observed distinct differences in the final dipeptide structures. We believe that such manipulation may not only provide us with a better understanding of the fundamentals of the dipeptide self-assembly process but also may enable us to generate novel peptide-based materials for various applications."
"The regioselective deprotonation of pyridine in the gas phase has been investigated by using chemical reactivity studies. The mixture of regioisomers, trapped as carboxylates, formed in an equilibrium mixture is determined to result from 70-80% deprotonation in the 4-position, and 20-30% deprotonation at the 3-position. The ion formed by deprotonation in the 2-position is not measurably deprotonated at equilibrium because the ion is destabilized by lone-pair repulsion. From the composition of the mixture, the gas-phase acidities (DeltaH degrees acid) at the 4-, 3-, and 2-positions are determined to be 389.9 +/- 2.0, 391.2-391.5, and >391.5 kcal/mol, respectively. The relative acidities of the 4- and 3-positions are explained by using Hammett-Taft parameters, derived by using the measured gas-phase acidities of pyridine carboxylic acids. The values of sigmaF and sigmaR are -0.18 and 0.74, respectively, showing the infused nitrogen in pyridine to have a strong pi electron-withdrawing effect, but with little sigma-inductive effect."
"The adaptability of modern orthodontic appliances to meet the esthetic needs of the adult patient while satisfying the treatment goals agreed upon has been shown. Some of the treatments cited could be accomplished by the general practitioner, while other modalities should be handled by the orthodontist. The outstanding advances in orthognathic surgical technique provide the patient with additional options for orthodontic corrections of malocclusions while reducing the time needed to wear braces. Proper planning and interaction among the dental disciplines can greatly enhance a patient's final outcome. It behooves all clinicians to look at all the options available in treating the adult so these patients can benefit most from our services."
"The distribution of type-VII collagen, the main molecular component of the anchoring fibrils (AF) attaching the basal lamina (BL, lamina densa of the basement membrane) to the surrounding connective tissue, was investigated in four xenografted human carcinomas of the hypopharynx (H-Stg 1), the lung (L 261), the sigmoid colon (CA 1), and the rectum (R 85). The studies were performed with a recently prepared, affinity-purified and highly specific antibody to type-VII collagen by using the indirect immunofluorescence and the APAAP (alkaline phosphatase anti-alkaline phosphatase) techniques. For comparison, the localization of the intrinsic BL components laminin and type-IV collagen were additionally analyzed in all four carcinomas. It was shown that type-VII collagen usually colocalized to laminin and type-IV collagen and was deposited at the borderline between carcinoma cell clusters and the surrounding strands of connective tissue in a similar, but more diffuse and less continuous distribution than both intrinsic BL components. In the squamous cell carcinoma H-Stg 1 and the adenocarcinoma L261, type-VII collagen was additionally accumulated in enlarged extracellular spaces between carcinoma cells, away from the contact zone to the connective tissue and again colocalized to laminin and type-IV collagen. Numerous carcinoma cells of both xenografts showed remarkable intracytoplasmic immunoreactivity for the antibody to type-VII collagen. Even in the case of the gastrointestinal carcinomas CA 1 and R 85, faint immunoreactivity for type-VII collagen was found at the contact zone between the mucosal epithelium and the surrounding connective tissue. These results confirm that epithelial carcinoma cells are obviously involved with the synthesis of the main molecular component of AF usually attaching the BL to the adjacent connective tissue and hint at a possible correlation between the localization of type-VII collagen and the observed pattern of the BL. However, it cannot be decided whether there is a direct causal relation between both phenomena or whether they are both the consequence of an independent but common cause, such as abnormal cellular differentiation of carcinoma cells. In no case, can the discontinuities in the distribution of type-VII collagen be explained by active tumor cell invasion since xenografted human carcinomas neither invade nor metastasize."
"The sucrose gap technique was employed to investigate both synaptic and amino acid evoked responses from motoneurones or primary afferents of frog spinal cord. alpha-D,L-Aminoadipic acid (alpha-D,L-AAD) selectively antagonized responses to acidic amino acids, especially aspartate. The drug was most effective in antagonizing the polysynaptic components of synaptic potentials evoked by dorsal root or lateral column stimulation but had little effect on their monosynaptic components. The ventral root dorsal root potential which is thought to be mediated by a pathway that does not involve acidic amino acids was insensitive to alpha-D,L-AAD. These data, which were confirmed by intracellular recording from motoneurones, provided further evidence for the role of acidic amino acids in polysynaptic pathways in frog spinal cord."
"The electron-transport system of cell-free extracts obtained from Hydrogenomonas H-20 has been studied with particular reference to phosphorylation associated with the oxyhydrogen reaction. Cell-free preparations of this organism exhibit oxidative phosphorylation with hydrogen and succinate as electron donors. This activity could be uncoupled with a number of agents. Ratios of phosphorylative activity to oxidative activity observed varied from 0.2 to 0.7. Factors affecting the efficiency of phosphorylation were examined. Inhibitor and spectrophotometric studies indicated that phosphorylation with hydrogen as electron donor occurs exclusively at a site in an abbreviated electron transport chain between H(2) and cytochrome b. The possible occurrence of a cytochrome b oxidase and the requirement for a quinone are discussed, as well as the correlation between the abbreviated pathway and the energy generation by the cell. Evidence is presented which indicates that nicotinamide adenine dinucleotide does not participate in the hydrogen oxidation path which is coupled to adenosine triphosphate formation."
"The aim of this study was to compare the metabolic effects of a single equimolar subcutaneous injection of hepatic directed vesicle-insulin (HDV-insulin) and regular insulin on glucose levels and intermediary metabolism during a 75-g oral glucose tolerance test (OGTT). Nine Type 1 diabetic patients underwent two experiments separated by 4 weeks. Each experimental protocol consisted of an identical evening meal followed by overnight euglycemic control achieved by a continuous low-dose insulin infusion. The next morning a subcutaneous injection (0.1 U/kg) of HDV-insulin or regular insulin was administered 30 min before a 75-g OGTT. The overnight basal insulin infusion was maintained unaltered throughout the 150-min OGTT. Plasma glucose, glucoregulatory hormones (insulin, glucagon, cortisol), and intermediary metabolites (lactate, alanine, glycerol, NEFA, beta-hydroxybutyrate) were measured to assess the metabolic effects of the two insulin preparations. Compared to regular insulin, an equivalent subcutaneous dose of HDV-insulin significantly lowered glucose levels during OGTT (mean reduction 2.2+/-0.4 mmol/l; P<.005). Plasma levels of insulin and glucagon were equivalent during both series of experiments. Blood lactate, glycerol and plasma NEFA levels were not different during OGTT indicating similar peripheral action of the insulins. beta-Hydroxybutyrate levels were significantly reduced (P<.05) following HDV-insulin supporting a preferential hepatic action of the preparation. We conclude that HDV-insulin can significantly lower plasma glucose excursions compared to an equivalent dose of regular insulin during an OGTT in Type 1 diabetic patients. The metabolic profile of equivalent peripheral insulin, glucagon and glycerol levels but reduced beta-hydroxybutyrate values support a hepatospecific effect of HDV-insulin."
"BACKGROUND: Cerebrospinal fluid (CSF) biomarkers reflecting neuronal and astroglial injury, such as total tau (T-tau), glial fibrillary acidic protein (GFAP), and neurofilament light (NFL), have been extensively investigated in neurologic diseases in adults, but no large study has investigated these biomarkers in children.METHODS: This study presents a detailed evaluation of CFS T-tau, GFAP, NFL, and CSF:albumin ratio in a large cohort of pediatric patients. This is a retrospective multicenter study on pediatric patients aged <16 years (n = 607), where neuronal injury biomarkers T-tau, GFAP, NFL, and CSF albumin ratio were analyzed during 2000-2010 at the Clinical Neurochemistry Laboratory, Sahlgrenska University Hospital, Sweden. The patients were grouped into eight categories: epilepsy, infectious and inflammatory central nervous system disorders, progressive encephalopathy, static encephalopathy, tumors, movement disorders, miscellaneous disorders, and a control group.RESULTS: T-tau, GFAP, and NFL were increased in progressive encephalopathy (P < 0.001), epilepsy (P < 0.001), and infectious and inflammatory central nervous system disorders (P < 0.001) compared with controls. T-tau was the biomarker with the highest diagnostic accuracy with the area under the curve of 0.83 (95% confidence interval (CI), 0.77-0.90; P < 0.0001) for progressive encephalopathy followed by epilepsy 0.80 (95% CI, 0.75-0.87; P < 0.0001). The combination of all four biomarkers further improved the area under the curve for the progressive encephalopathy 0.87 (95% CI, 0.77-0.89; P < 0.0001), followed by epilepsy 0.81 (95% CI, 0.74-0.80; P = 0.030). The combination of the biomarkers also separated progressive from static encephalopathy 0.88 (95% CI, 0.83-0.93; P < 0.0001).CONCLUSIONS: CSF T-tau, GFAP, and NFL are differently altered across different neurologic diseases in children. Importantly, the biomarker pattern distinguishes between progressive and static neurologic disorders."
"Traditionally, the multibasic cleavage site (MBCS) of surface protein H5-hemagglutinin (HA) is converted to a monobasic one so as to weaken the virulence of recombinant H5N1 influenza viruses and to produce inactivated and live attenuated vaccines. Whether such modification benefits new candidate vaccines has not been adequately investigated. We previously used retroviral vectors to generate wtH5N1 pseudotypes containing the wild-type HA (wtH5) from A/swine/Anhui/ca/2004 (H5N1) virus. Here, we generated mtH5N1 pseudotypes, which contained a mutant-type HA (mtH5) with a modified monobasic cleavage site. Groups of mice were subcutaneously injected with the two types of influenza pseudotypes. Compared to the group immunized with wtH5N1 pseudotypes, the inoculation of mtH5N1 pseudotypes induced significantly higher levels of HA specific IgG and IFN-ã in immunized mice, and enhanced protection against the challenge of mouse-adapted avian influenza virus A/Chicken/Henan/12/2004 (H5N1). This study suggests modification of the H5-hemagglutinin MBCS in retroviral pseudotypes enhances protection efficacy in mice and this information may be helpful for development of vaccines from mammalian cells to fight against H5N1 influenza viruses."
"OBJECTIVE: To understand the mechanism of action of oxygen treatment in cluster headache.BACKGROUND: Trigeminal autonomic cephalalgias, including cluster headache, are characterized by unilateral head pain in association with ipsilateral cranial autonomic features. They are believed to involve activation of the trigeminovascular system and the parasympathetic outflow to the cranial vasculature from the superior salivatory nucleus (SuS) projections through the sphenopalatine ganglion, via the greater petrosal nerve of the VIIth (facial) cranial nerve. Cluster headache is remarkably responsive to treatment with oxygen, and yet our understanding of its mode of action is unknown.METHODS: Combining models of trigeminovascular nociception and a novel approach that activates the trigeminal-autonomic reflex, using SuS/facial nerve stimulation, we explored the effect of oxygen on trigeminal nerve activation as well as on autonomic responses through blood flow observations of the lacrimal duct/sac.RESULTS: Meningeal vasodilation and neuronal firing in the trigeminocervical complex (TCC), in response to dural electrical stimulation, was unaffected by treatment with 100% oxygen. Stimulation of the SuS via the facial nerve caused only marginal changes in dural blood vessel diameter, but did result in evoked firing in the TCC. Two populations of neurons were characterized, those responsive to 100% oxygen treatment, with a maximal inhibition of 33%, 20 minutes after the start of oxygen treatment (t(15) = 4.4, P < .0001). A second population of neurons were not inhibited by oxygen and tended to have shorter latency. Oxygen also inhibited evoked blood flow changes in the lacrimal sac/duct caused by SuS stimulation.CONCLUSIONS: The data provide the first systematic, experimental evidence for a mechanism of action of oxygen in cluster headache. The data show oxygen has no direct effect on trigeminal afferents, acting specifically on the parasympathetic/facial nerve projections to the cranial vasculature to inhibit both evoked trigeminovascular activation and activation of the autonomic pathway during cluster headache attacks. Moreover, the studies begin to characterize a novel laboratory model for the most painful primary headache syndrome known--cluster headache."
Traditional open repair of thoraco-abdominal aortic aneurysms Crawford type II-IV carries a high perioperative risk and mortality. The hybrid technique for combined surgical and endovascular treatment offers an interesting alternative with reduced risk of paraparesis and possibly a reduced mortality rate. Propositions for refinement of this approach are outlined based on a single centre experience.
"Mouse blastocysts collected on day 4 were cultured in [3H]thymidine (0.01 muCi/ml) for 24 h and transferred to the uteri of pseudopregnant recipients. Autoradiography revealed that when such blastocysts were allowed to develop for 48 h in utero, label was apparent in the nuclei of decidual cells. The experimental conditions were physiological since blastocysts developed into normal offspring when gestation was allowed to proceed in pseudopregnant recipient animals. The transfer of foetal DNA into maternal decidual cells may be of important immunological significance."
"An apparently new strain of bluetongue virus was first isolated in Kenya in 1965 and since, has been obtained on 7 further occasions from diseased sheep during clinical outbreaks of disease. It proved to be serologically different from the 16 bluetongue virus strains then held at this laboratory. The virus was modified by passage in embryonated hens eggs to produce a live virus strain suitable for inclusion in a polyvalent vaccine. Recent neutralisation tests, carried out with 24 guinea pig immune sera prepared at Pirbright against the currently known World serotypes, have confirmed the earlier results and show that it is different from any of the existing serotypes."
"A case is presented of a 23-year-old male who sustained a traumatic transmetacarpal amputation of his nondominant hand. The injury consisted of complete severance of structures distal to the midpalm. Microsurgical reconstruction involved the primary repair of arteries, veins, nerves, extensor and flexor tendons, and metacarpal fractures. Skeletal reconstruction also employed a primary Swanson prosthesis for the fifth metacarpophalangeal (MP) joint. Early postoperative range-of-motion exercises were encouraged, with the achievement of a functionally capable replanted extremity. The general management of an amputation injury is also discussed, as it applies to a community hospital environment."
"BACKGROUND: Celiac disease (CD) has a negative impact on the health-related quality of life (HRQL) of affected patients. Although HRQL and its determinants have been examined in Spanish CD patients specifically recruited in hospital settings, these aspects of CD have not been assessed among the general Spanish population.METHODS: An observational, transversal study of a non-randomized, representative sample of adult celiac patients throughout all of Spain's Autonomous Regions. Subjects were recruited through celiac patient associations. A Spanish version of the self-administered Celiac Disease-Quality of Life (CD-QOL) questionnaire was used. Determinant factors of HRQL were assessed with the aid of multivariate analysis to control for confounding factors.RESULTS: We analyzed the responses provided by 1,230 patients, 1,092 (89.2%) of whom were women. The overall mean value for the CD-QOL index was 56.3 ± 18.27 points. The dimension that obtained the most points was dysphoria, with 81.3 ± 19.56 points, followed by limitations with 52.3 ± 23.43 points; health problems, with 51.6 ± 26.08 points, and inadequate treatment, with 36.1 ± 21.18 points. Patient age and sex, along with time to diagnosis, and length of time on a gluten-free diet were all independent determinant factors of certain dimensions of HRQL: women aged 31 to 40 expressed poorer HRQL while time to diagnosis and length of time on a gluten-free diet were determinant factors for better HRQL scores.CONCLUSIONS: The HRQL of adult Spanish celiac subjects is moderate, improving with the length of time patients remain on a gluten-free diet."
"Definitions of healthy ageing include survival to a specific age, being free of chronic diseases, autonomy in activities of daily living, wellbeing, good quality of life, high social participation, only mild cognitive or functional impairment, and little or no disability. The working group Epidemiology of Ageing of the German Association of Epidemiology organized a workshop in 2012 with the aim to present different indicators used in German studies and to discuss their impact on health for an ageing middle-European population. Workshop presentations focused on prevalence of chronic diseases and multimorbidity, development of healthy life expectancy at the transition to oldest-age, physical activity, assessment of cognitive capability, and functioning and disability in old age. The communication describes the results regarding specific indicators for Germany, and hereby contributes to the further development of a set of indicators for the assessment of healthy ageing."
"Three-dimensional CT scan imaging obtained by using helicoidal CT scan provides the basis for an endoscopic exam said to be virtual since no invasive procedure is actually performed. Compared to optical endoscopy this easily accessible exam offers additional information especially for the analysis of the infraglottic and tracheal areas, which are two anatomically rigid segments. This property facilitates their three-dimensional reconstruction. Our study encompassed 6 patients presenting with a stenosis of the laryngotracheal tract. In 5 of them it was possible to correlate optical and virtual endoscopic imaging. Coupling both exams significantly improved the diagnostic investigation and facilitated the management of the disease. However, the real contribution of virtual endoscopy to the exploration of tumoral conditions still remains to be determined given the low degree of tissue resolution. As a consequence parietal and extraparietal lesional spreading is more accurately assessed by axial scan imaging."
"In this paper, we propose a parallel cellular automaton tumor growth model that includes load balancing of cells distribution among computational threads with the introduction of adjusting parameters. The obtained results show a fair reduction in execution time and improved speedup compared with the sequential tumor growth simulation program currently referenced in tumoral biology. The dynamic data structures of the model can be extended to address additional tumor growth characteristics such as angiogenesis and nutrient intake dependencies."
"BACKGROUND: The goal of this work was to point to the possibilities of health status examination in young men in the Czech Republic. A sample of young men, volunteers, conscripts (servicemen) at the time, was studied within the presented work.METHODS: A questionnaire--screening protocol--was filled in by 104 young men, 83 of them subsequently underwent a clinical examination. The participants expressed their interest in the research and the chosen form of primary prevention during directed talks; 64 participants took part in these talks.RESULTS: Analyzing the results we found out the following: the values of pulmonary function testing prove that despite their relatively low age, young men have unsatisfactory pulmonary function; the physical condition of the generation of young men, however, is satisfactory. Although young men do consume fruits and vegetables, they nevertheless have serious shortcomings in their dietary habits. Our clinical examination points to the symptoms of vitamin deficiency.CONCLUSIONS: The chosen method of work including the assessment and the possibility of making use of primary prevention should be reproducible and usable for a wide range of our young population."
"After the U.S. mandate of folic acid fortification of enriched grain products, a report indicated higher than expected fortification. Limited information is available on folic acid in enriched products. We measured the folate content in 92 sandwich breads (46 white breads and 46 whole wheat breads) in Birmingham, Alabama, during 2001-2003. The mean folate content in white bread declined significantly from 2001 to 2002 or 2003, whereas the decline in folate content in whole wheat bread containing enriched flour was not significant. White bread contained significantly more folate than whole wheat bread containing enriched flour in 2001 and 2003. In 2002 and 2003, >40% of breads made of enriched flour contained <115 microg of folate/100 g and >70% contained <160 microg/100 g. These percentages were markedly higher than those in 2001. Our data suggest that folic acid in breads containing enriched flour declined after 2001 and monitoring of fortification may be necessary."
"OBJECTIVES: To establish reference values and determine test-retest reliability for usual and maximal 4-meter gait speed.DESIGN: Cross-sectional observational study.SETTING: Offices in 10 geographically dispersed cities in the United States.PARTICIPANTS: Men and women (N=1320), aged 18 to 85 years, enrolled in the National Institutes of Health Toolbox norming study.INTERVENTION: Not applicable.MAIN OUTCOME MEASURES: Specifically used were data from men and women who were timed over 4 meters (after a static start) while walking at their usual and maximum speeds. Norms for usual and maximum gait speed were derived using data from 1320 participants. Test-retest reliability for 164 participants was described using paired t tests, intraclass correlation coefficients (ICCs), and minimal detectable changes (MDCs).RESULTS: Mean usual speed was 1.12 meters per second, whereas mean maximum speed was 1.61 meters per second. As a general linear model showed 4-meter gait speed to differ significantly according to gait condition (speed), sex, and age group; estimates of normal were calculated accordingly. The usual speed of 80- to 85-year-old women was lowest at 0.95 meters per second; the maximum speed of 18- to 29-year-old men was highest at 1.85 meters per second. Test-retest measures did not differ significantly, but the ICCs were only fair and the MDCs were high.CONCLUSIONS: Normative reference values provided herein may be helpful in interpreting measurements of 4-meter gait speed obtained from adult men and women. The limited reliability of the gait speed measurements, however, limits their usefulness in making judgments regarding change."
"Rejection is a direct threat to an individual's need to belong that has serious consequences for mental health. Rejection sensitivity may explain why some individuals are more likely to perceive rejection in social situations and experience subsequent psychological distress. The current study examined suicide ideation among psychiatric inpatients (N = 103) through the lenses of the rejection sensitivity model and the interpersonal theory of suicide. We hypothesized that rejection sensitivity would be indirectly associated with suicide ideation (i.e., a cognitive-affective reaction to social rejection) through greater perceptions of rejection (i.e., thwarted belongingness and perceived burdensomeness, constructs from the interpersonal theory of suicide), in parallel. Results from bootstrapped parallel mediation regression procedures indicated that the relation between rejection sensitivity and suicide ideation was significantly indirectly associated through the additive effect of thwarted belongingness and perceived burdensomeness, such that greater rejection sensitivity was associated with greater thwarted belongingness and perceived burdensomeness and subsequently greater suicide ideation. Further, rejection sensitivity was significantly indirectly associated with suicide ideation independently through thwarted belongingness, but not perceived burdensomeness. These findings provide support for the rejection sensitivity model and the interpersonal theory of suicide in an effort to advance our conceptualization of suicide risk among psychiatric inpatients."
"Side effects of ethyl apovincaminate (RGH-4405, Cavinton), a cerebral vasodilatory agent, on the central nervous system were studied on mice and rats. Anticonvulsive effect in electroshock was observed as major CNS effect, locomotor activity was reduced due to muscle relaxant effect. The compound had no sedative effect on rodents. Its analgesic effect was negligible. Cavinton had neither antidepressive, nor anticholinergic activity."
"Although N-nitrosodimethylamine (NDMA) has been the most prevalent N-nitrosamine detected in disinfected waters, it remains unclear whether NDMA is indeed the most significant N-nitrosamine or just one representative of a larger pool of N-nitrosamines. A widely used assay applied to quantify nitrite, S-nitrosothiols, and N-nitrosamines in biological samples involves their reduction to nitric oxide by acidic tri-iodide, followed by chemiluminescent detection of the evolved nitric oxide in the gas phase. We here describe an adaptation of this method for analyzing total N-nitrosamine (TONO) concentrations in disinfected pools. Optimal sensitivity for N-nitrosamines was obtained using a reduction solution containing 13.5 mL glacial acetic acid and 1 mL of an aqueous 540 g/L iodide and 114 g/L iodine solution held at 80 degrees C. The method detection limit for N-nitrosamines was 110 nM using 100 microL sample injections and NDMA as a standard. N-nitrosamines featuring a range of polarities were converted to nitric oxide with 75-103% efficiency compared to NDMA. Evaluation of potential interfering species indicated that only nitrite and S-nitrosothiols were a concern, but both interferences were effectively eliminated using group-specific sample pretreatments previously employed for biological samples. To evaluate the low TONO concentrations anticipated for pools, 1 L samples were extracted by continuous liquid-liquid extraction with ethyl acetate for 24 h, and concentrated to 1 mL. N-nitrosamine recovery during extraction ranged from 37-75%, and there was a potential for artifactual nitrosation of amines during solvent reflux in the presence of significant nitrite concentrations, but not at the low nitrite concentrations prevalent in most pools. Using the 1000-fold concentration factor and 56% average extraction efficiency, the method detection limit would be 62 pM (5 ng/L as NDMA). The TONO assay was applied to six pools and their common tap water source in conjunction with analysis for specific nitrosamines. Even accounting for the range of N-nitrosamine extraction recoveries, NDMA accounted for an average of only 13% (range 3-46%) of the total nitrosamine pool."
"A simple determination method of diphenyl (DP) and o-phenylphenol (OPP) in agricultural products by GC/MS was examined. DP and OPP were extracted with ethyl acetate in the presence of anh. sodium sulfate. After addition of n-butanol, the extract solution was concentrated. Clean-up was achieved by shaking with graphitized bulk carbon (Supelclean ENVI-Carb). Addition of polyethylene glycol sharpened the OPP peak on GC/MS analysis. The recoveries from 9 kinds of agricultural products spiked at 0.01 and 0.5 microg/g each were mostly in the range of 70 to 120%, except for 50% recovery of OPP from barley spiked at 0.01 microg/g. The quantification limits (S/N > or =10) of DP and OPP were 0.0013 and 0.005 microg/g (0.0025 and 0.01 microg/g in barley and soybean), respectively."
"The inflammatory mediator leukotriene B(4) (LTB(4)) binds to and activates a G-protein-coupled receptor named BLT(1). We have previously produced two monoclonal antibodies, named 7B1 and 14F11, that bind specifically to this receptor. Using a HeLa cell line expressing human BLT(1), we find that both antibodies inhibit LTB(4)-induced calcium release, and activation of a MAP-kinase-sensitive luciferase reporter system. The normal chemotactic movement of polymorphonuclear cells towards higher LTB(4) concentrations was also strongly inhibited by both antibodies. Neither antibody was found to activate BLT(1), and experiments using cyclic peptide fragments of the BLT(1) n-terminal and extracellular loops showed that these antibodies bind only to complex epitopes in the tertiary, membrane bound, conformation of the receptor protein. In ligand binding experiments, 7B1 was found to be a competitive antagonist, while 14F11 was a noncompetitive antagonist that inhibited receptor activation, but not agonist (LTB(4)) binding. 14F11 will be a useful tool for studying the mechanisms of receptor activation."
"The current study explored whether oxytocin can improve social cognition and social skills in individuals with schizophrenia using a six-week, double-blind design. Fourteen participants with schizophrenia were randomized to receive either intranasal oxytocin or a placebo solution and completed a battery of social cognitive, social skills and clinical psychiatric symptom measures. Results showed within group improvements in fear recognition, perspective taking, and a reduction in negative symptoms in the oxytocin group. These preliminary findings indicate oxytocin treatment may help improve certain components of functioning in schizophrenia. Implications for the treatment of social functioning in schizophrenia are discussed."
"BACKGROUND: Preeclampsia, defined as the presence of hypertension and proteinuria, is usually related with maternal and neonatal adverse effects. However, the exact predictor of preeclampsia is still lacking. Even though there are some conflicting data, mean platelet value or MPV, that is, platelet ratio with or without Doppler velocimetry was determined as highly sensitive markers for preeclampsia. We aimed to investigate the utility of MPV in prediction of preeclampsia.METHODS: Seventy-four preeclamptic pregnant women (21 in mild, 53 in severe preeclampsia groups) were included in the study. To assess the difference of MPV between preeclamptic, normal pregnant, and healthy control rather than mild and severe preeclamptic pregnant women, we included in the analysis 31 healthy pregnant women and 35 healthy nonpregnant women.RESULTS: Mean age of the preeclamptic patients was 25.3 (17-38) years. Platelet levels were higher in mild preeclampsia (group 1) than severe preeclampsia (group 2), whereas alanine aminotransferase (AST), hemoglobin, and hematocrit level was higher in group 2. MPV levels were found to be similar in groups 1 and 2, MPV level increased from healthy control to preeclamptic women (P = 0.003). MPV:platelet ratio was similar according to the severity of preeclampsia (P = 0.123). Doppler velocimetry did not add an additional benefit to predict preeclampsia or its severity.CONCLUSION: Our results showed that MPV level was higher in the pregnant than the control group. However, MPV did not differ both between mild and severe preeclampsia, and preeclampsia and non-preeclamptic pregnant women."
"Interferon gamma (IFN-ã) is one of the key cytokines that plays a major role against viral and intracellular bacterial infection. In addition to the IFN-ã gene, teleost fish possess a second copy known as IFN-ã-related (IFN-ãrel) gene. This report describes structural and functional properties of IFN-ãrel gene in the Indian major carp, rohu (Labeo rohita), a commercially important freshwater fish species in the Indian subcontinent. The rohu IFN-ãrel gene consisted of four exons with three intervening introns and phylogenetically closely related to grass carp. The full-length IFN-ãrel cDNA comprised 927 bp nucleotides with a single open reading frame of 504 bp, encoding 167 amino acids (aa) polypeptide with a signal peptide of 24 aa. The mature rohu IFN-ãrel protein was 143 aa with a predicted molecular weight of 16.85 kDa. Basal expression analysis of IFN-ãrel showed its wide range of expression in all examined tissues: The highest was in the skin and the lowest was in the liver. In response to LPS, poly I:C, iE-DAP, muramyl dipeptide stimulations, and bacterial infections, IFN-ãrel gene expression was significantly (p<0.05) induced in treated fish tissues as compared with their control. The IFN-ãrel was expressed as recombinant protein (rIFN-ãrel) and confirmed through western blot. Stimulation of peripheral blood leukocytes with rIFN-ãrel protein resulted in the activation of IFN-ã receptor and marked induction of inducible nitric oxide synthase gene expression. These results together may suggest the important role of IFN-ãrel as an antimicrobial cytokine in fish."
"Sarcoidosis is a chronic granulomatous disorder, which is characterized by the accumulation of activated CD4+ T lymphocytes (T cells) at disease sites. There is up-regulation of cell surface expression of MHC molecules in sarcoidosis, and it has been suggested that specific MHC class II alleles are associated with the disease. A study of chronic beryllium disease (CBD), a granulomatous disorder which is pathologically similar to sarcoidosis, has identified an association between this disease and the presence of a glutamine residue at position 69 (Glu 69+) of the B1 chain of the HLA-DPB molecule. A further study also suggested the importance of Glu at position 55 of the same chain. The aims of the present study were to attempt to define MHC class II alleles associated with sarcoidosis by comparison of their frequency in two groups of subjects and to compare the frequency of HLA-DPB1 Glu 69+/- and Glu 55+/-alleles in the same subjects. Forty-one subjects with sarcoidosis and 76 normal subjects were studied. The polymorphic regions of the class II MHC were identified by PCR in association with sequence-specific oligonucleotide probes. There were no significant differences in the phenotype frequencies of MHC class II or Glu 55+ alleles between the two groups of subjects. However, there was a significant increase (P = 0.02) in the frequency of HLA-DPB1* Glu 69+ alleles compared with the control population. We therefore suggest that the presence of a Glu residue at position 69 on the DPB1 chain may play an important role in antigen presentation and recognition in chronic granulomatous diseases."
"Several human cell lines derived from primary cancer of the liver are able to grow under serum-free conditions and produce spreading and growth factors which are released into the culture medium. Since this autocrine growth under hormone-free conditions might play a basic role in malignant transformation, we studied the effect on cell replication and the presence of specific membrane receptors of epidermal growth factor (EGF) and insulin on a dedifferentiated human hepatoma cell line, named HA22T/VGH. Our results point to a similar inhibitory effect on cell replication in the presence of both EGF and insulin, in spite of detecting different affinities of binding."
"Two new species of Oswaldocruzia, O. manuensis sp. nov., and O. urubambaensis sp. nov. are described and illustrated from Peru, these are parasites of the cane toad Rhinella marina. O. manuensis is characterized by having cervical alae which are not well developed, ridges without chitinous supports, caudal bursa type II and branches of fork of dissimilar length. O. urubambaensis is characterized by a caudal bursa of type I, ridges with chitinous supports, a thin cephalic vesicle and origin of rays 9 in tip of the dorsal trunk."
"The management of a 'hyperactive' child entails a thorough individual assessment, close communication with the school, intensive work with the family and the development of an individualised management plan which takes account of the unique circumstances of the child and his or her environment."
"Cell fusion-mediated formation of multinuclear osteoclasts (OCs) plays a key role in bone resorption. It is reported that 2 unique OC-specific fusogens [ i.e., OC-stimulatory transmembrane protein (OC-STAMP) and dendritic cell-specific transmembrane protein (DC-STAMP)], and permissive fusogen CD9, are involved in OC fusion. In contrast to DC-STAMP-knockout (KO) mice, which show the osteopetrotic phenotype, OC-STAMP-KO mice show no difference in systemic bone mineral density. Nonetheless, according to the ligature-induced periodontitis model, significantly lower level of bone resorption was found in OC-STAMP-KO mice compared to WT mice. Anti-OC-STAMP-neutralizing mAb down-modulated in vitro: 1) the emergence of large multinuclear tartrate-resistant acid phosphatase-positive cells, 2) pit formation, and 3) mRNA and protein expression of CD9, but not DC-STAMP, in receptor activator of NF-êB ligand (RANKL)-stimulated OC precursor cells (OCps). While anti-DC-STAMP-mAb also down-regulated RANKL-induced osteoclastogenesis in vitro, it had no effect on CD9 expression. In our mouse model, systemic administration of anti-OC-STAMP-mAb suppressed the expression of CD9 mRNA, but not DC-STAMP mRNA, in periodontal tissue, along with diminished alveolar bone loss and reduced emergence of CD9+ OCps and tartrate-resistant acid phosphatase-positive multinuclear OCs. The present study demonstrated that OC-STAMP partners CD9 to promote periodontal bone destruction by up-regulation of fusion during osteoclastogenesis, suggesting that anti-OC-STAMP-mAb may lead to the development of a novel therapeutic regimen for periodontitis.-Ishii, T., Ruiz-Torruella, M., Ikeda, A., Shindo, S., Movila, A., Mawardi, H., Albassam, A., Kayal, R. A., Al-Dharrab, A. A., Egashira, K., Wisitrasameewong, W., Yamamoto, K., Mira, A. I., Sueishi, K., Han, X., Taubman, M. A., Miyamoto, T., Kawai, T. OC-STAMP promotes osteoclast fusion for pathogenic bone resorption in periodontitis via up-regulation of permissive fusogen CD9."
"On a dairy farm 22 animals die in 14 days. After 10 days the clinical diagnosis is confirmed: clostridium botulinum type D intoxication. The clinical, diagnosis, therapy and prevention are discussed."
"We have synthesized three 123I-labeled histamine H3 receptor ligands, i.e., [123I]GR 190028, [123I]FUB 271, and [123I]iodoproxyfan, in moderate to good radiochemical yields via a Cu+-assisted I-for-123I exchange method. Biodistribution in the rat of these compounds revealed high hepatic and pulmonary uptake. Brain uptake was moderate, but for [123I]iodoproxyfan, brain uptake was high enough for a pilot single photon emission computed tomography (SPECT) study in the rabbit. However, for this compound, the cerebral uptake could not be blocked by a pretreatment with [R]-alpha-methylhistamine, a selective, high-affinity histamine H3 receptor agonist, both in the SPECT study in the rabbit and in the biodistribution study in the rat. Apparently, [123I]iodoproxyfan is binding to a non-H3 receptor binding site. None of the three investigated compounds is suitable for use as a SPECT ligand for the H3 receptor in the brain."
"The compound 2-furyl-1-nitroethene (G-0) was evaluated for genotoxicity in cultured human peripheral blood lymphocytes, at concentrations ranging from 1 to 15microg/ml. Micronuclei (MN) and sister-chromatid exchanges (SCEs) were scored as genetic endpoints. In order to detect the role of metabolic enzymes on the genotoxicity of this furylethylenic derivative, the cultures for MN and SCE demonstrations were also treated with S9 microsomal fraction. The results indicate that, under the conditions of the study, the test agent does not seem to induce significant increases in the frequency of micronucleated cells, irrespective of the presence of metabolic activation. Nevertheless, a slight increase in the SCE frequency was observed in those cultures treated without the S9 mix; although this increase disappeared in presence of the microsomal fraction. In addition, cytostatic effects of 2-furyl-1-nitroethene were observed mainly in cultures without S9 fraction, as indicated by the reduction of cell proliferation."
"Truffles are hypogeous Ascomycete fungi belonging to the genus Tuber and forming fruiting bodies highly prized for their taste and aroma. The identification of the genus Tuber and its species is important to investigate their ecology and avoid fraud in the food market. As genus-specific primers are not available, the aims of this work were (1) to assess the usefulness of the beta-tubulin gene as a DNA barcoding region for designing Tuber genus-specific primers, (2) to test the primers on a range of fruiting bodies, representing a large part of truffle biodiversity and (3) to check their ecological usefulness, applying them to truffle-ground soil. The new primers designed on the beta-tubulin gene were specific to the Tuber genus in nested PCR. When applied to DNA from soils, they gave a positive signal for 23 of 32 soils. Phylogenetic analysis confirmed that the bands corresponded to Tuber and that at least five Tuber species were present in the truffle-ground. beta-tubulin was found to be a good barcoding region for designing Tuber genus-specific primers, detecting a high Tuber diversity in a natural environment. These primers will be useful for understanding truffle ecology and for practical needs in plantation management."
"A high-performance liquid chromatographic procedure for recovering subnanomole amounts of protein from SDS/polyacrylamide gel electroeluates in a form suitable for gas-phase sequence analysis has been developed. By a judicious choice of reversed-phase column packing, proteins can be retained at high concentrations of n-propanol (90-100%) where sodium dodecylsulfate and acrylamide gel-related contaminants are washed through the column. Retained proteins can be recovered from the column in high yield (greater than 90%) by the simultaneous adding of an ion-pairing reagent into the mobile phase and elution with a gradient of decreasing n-propanol concentration (i.e. an 'inverse or negative gradient'). Furthermore, by using a steep gradient (e.g. 50%/min) at a low flow rate (20-200 microliters/min) the proteins can be recovered in less than 100 microliters and can be used for gas-phase sequence analysis without further manipulation. This procedure is independent of sodium dodecylsulfate concentration (up to 1.2% w/v) in sample loading volumes of up to 1.5 ml. Microbore columns (2.1 mm internal diameter) have been employed for recovering small amounts of protein (1-100 micrograms from electroeluates of protein-containing gel spots while conventional columns (4.6 mm internal diameter) were used for isolating larger amounts of protein (greater than 500 micrograms) from electroeluates of preparative gel bands. The general utility of this inverse-gradient high-performance liquid chromatography procedure has been demonstrated by its successful application in recovering a wide variety of proteins from sodium dodecylsulfate gel electroeluates in a form suitable for N-terminal sequence analysis in the 10-500 pmol range."
"OBJECTIVES: To investigate the potential influence of standard dental materials on dental MRI (dMRI) by estimating the magnetic susceptibility with the help of the MRI-based geometric distortion method and to classify the materials from the standpoint of dMRI.METHODS: A series of standard dental materials was studied on a 1.5 T MRI system using spin echo and gradient echo pulse sequences and their magnetic susceptibility was estimated using the geometric method. Measurements on samples of dental materials were supported by in vivo examples obtained in dedicated dMRI procedures.RESULTS: The tested materials showed a range of distortion degrees. The following materials were classified as fully compatible materials that can be present even in the tooth of interest: the resin-based sealer AH Plus(®) (Dentsply, Maillefer, Germany), glass ionomer cement, gutta-percha, zirconium dioxide and composites from one of the tested manufacturers. Interestingly, composites provided by the other manufacturer caused relatively strong distortions and were therefore classified as compatible I, along with amalgam, gold alloy, gold-ceramic crowns, titanium alloy and NiTi orthodontic wires. Materials, the magnetic susceptibility of which differed from that of water by more than 200 ppm, were classified as non-compatible materials that should not be present in the patient's mouth for any dMRI applications. They included stainless steel orthodontic appliances and CoCr.CONCLUSIONS: A classification of the materials that complies with the standard grouping of materials according to their magnetic susceptibility was proposed and adopted for the purposes of dMRI. The proposed classification can serve as a guideline in future dMRI research."
"In 2007 and 2008 Danish Cancer Patient Pathways for 32 cancer types were developed and afterwards implemented on a national scale. Often bureaucrats, health professionals and politicians look upon the health sector in different ways and work independent of each other. In Denmark, as indeed internationally, patient pathways are frequently developed solely by health professionals and the consequence may be major difficulties in implementing the pathways on a national scale. In this article we describe how national Danish Cancer Patient Pathways were developed with a consensus seeking model and the impact it has had on the health system. The model used in Denmark ensured involvement and cooperation between bureaucrats, health professionals and politicians and afterwards a successful national implementation. The Cancer Patient Pathways has significantly reduced waiting times which is thought to increase survival. This experience gives important input to the continuous challenges on how to implement evidence based medicine on a national scale and stipulates a model for this process."
"BACKGROUND: Increased cytoplasmic HuR expression has been noted in several cancer types, where it may contribute to the increased cyclooxygenase-2 (COX-2) expression observed during tumorigenesis and metastasis.METHODS: To assess the correlation between COX-2 and HuR in oral squamous cell carcinoma (OSCC), the expression patterns of HuR and COX-2 were assessed via immunohistochemistry analyses of 103 OSCC samples.RESULTS: Cytoplasmic HuR expression was significantly associated with COX-2 expression (p < .025) and lymph node metastasis (p < .050) and distant metastasis (p < .025). In multivariate analysis, cytoplasmic HuR expression was identified as an independent prognostic parameter for reduced overall survival. The inhibition of HuR expression by siRNA or leptomycin B (LMB) caused a reduction in the inducibility of COX-2 in oral cancer cells.CONCLUSION: Our results indicate that the cytoplasmic expression of HuR is associated with COX-2 expression in OSCCs and HuR can regulate COX-2 expression in oral cancer cell lines."
"Photopolymerized thermosensitive A-B-A triblock copolymer hydrogels composed of poly(N-(2-hydroxypropyl)methacrylamide lactate) A-blocks, partly derivatized with methacrylate groups to different extents (10, 20, and 30%) and hydrophilic poly(ethylene glycol) B-blocks of different molecular weights (4, 10, and 20 kDa) were synthesized. The aim of the present study was to correlate the polymer architecture with the hydrogel properties, particularly rheological, swelling, degradation properties and release behavior. It was found that an increasing methacrylation extent and a decreasing PEG molecular weight resulted in increasing gel strength and cross-link density, which tailored the degradation profiles from 25 to more than 300 days. Polymers having small PEG blocks showed a remarkable phase separation into polymer- and water-rich domains, as demonstrated by confocal microscopy. Depending on the hydrophobic domain density, the loaded protein resides in the hydrophilic pores or is partitioned into hydrophilic and hydrophobic domains, and its release from these compartments is tailored by the extent of methacrylation and by PEG length, respectively. As the mechanical properties, degradation, and release profiles can be fully controlled by polymer design and concentration, these hydrogels are suitable for controlled protein release."
"The endoderm forms the gut and associated organs, and develops from a layer of cells which emerges during gastrula stages in the vertebrate embryo. In comparison to mesoderm and ectoderm, little is known about the signals which induce the endoderm. The origin of the endoderm is intimately linked with that of mesoderm, both by their position in the embryo, and by the molecules that can induce them. We characterised a gene, zebrafish gata5, which is expressed in the endoderm from blastula stages and show that its transcription is induced by signals originating from the yolk cell. These signals also induce the mesoderm-expressed transcription factor no tail (ntl), whose initial expression coincides with gata5 in the cells closest to the blastoderm margin, then spreads to encompass the germ ring. We have characterised the induction of these genes and show that ectopic expression of activin induces gata5 and ntl in a pattern which mimics the endogenous expression, while expression of a dominant negative activin receptor abolishes ntl and gata5 expression. Injection of RNA encoding a constitutively active activin receptor leads to ectopic expression of gata5 and ntl. gata5 is activated cell-autonomously, whereas ntl is induced in cells distant from those which have received the RNA, showing that although expression of both genes is induced by a TGF-beta signal, expression of ntl then spreads by a relay mechanism. Expression of a fibroblast growth factor (eFGF) or a dominant negatively acting FGF receptor shows that ntl but not gata5 is regulated by FGF signalling, implying that this may be the relay signal leading to the spread of ntl expression. In embryos lacking both squint and cyclops, members of the nodal group of TGF-beta related molecules, gata5 expression in the blastoderm is abolished, making these factors primary candidates for the endogenous TGF-beta signal inducing gata5."
A porous molecular crystalline solid based on amide-containing Pd(II) triangles was created for size-selective heterogeneous catalysis of the Knoevenagel condensation reaction.
"Plasma cortisol was measured by a protein binding technique in 81 patients with malignant tumors of different extent and various sites and in 82 patients with benign surgical diseases. The mean value of the tumor patients (x +/- s = 165 +/- 69 micrograms cortisol/l plasma) was increased significantly compared with the benign surgical disorders (100 +/- 45 micrograms/l). Within the group of patients with benign surgical disorders there was little variation by the type of disease (cortisol mean values given in brackets): benign breast tumors (95), gall stones (107), ulcer of the stomach and duodenum (96), hernia (78), appendicitis acuta (112), and struma (90). The results are in accordance with the hypothesis that glucocorticoids are involved in the increased protein catabolism of skeletal muscles and other signs of cachectic tumor patients."
"Even though rotenone has been used extensively in recent years to produce a model of Parkinson disease in rats, its systemic effects either on neurons apart from dopaminergic structures or non-neuronal tissues have not been elucidated well. In our present study, 30 adult Sprague-Dawley rats were divided into three even groups. A short-term rotenone-treated group received 10 mg/kg b.w. rotenone daily for 7 days. The long-term rotenone-treated group received 3 mg/kg b.w. rotenone daily for 30 days. The control group received vehicle only and were kept 5 rats each in parallel to both short- and long-term rotenone treated groups. It was found that short-term rotenone treatment produced marked vascular damages associated with ischemic neuronal degeneration particularly in the thalamus, cerebellum and nucleus dentatus. In long-term rotenone-treated group, vascular changes were less severe and neuronal degeneration was associated with mild microglial proliferation and astrocytosis. Non-neuronal pathology as the result of short-term rotenone exposure consisted of degeneration and necrosis of seminiferous tubular epithelia with formation of spermatide multinucleate giant cells. On the other hand, long-term rotenone treatment did not affect testicles and only caused sinusoidal dilatation in the liver, myocardial degeneration in the heart and interstitial hemorrhages in the kidneys and lungs. In conclusions, damage to blood vasculature by rotenone appeared mediating neuronal and non-neuronal pathology in Sprague-Dawley rats. This effect might provide new insights for ethiopathogenesis of neurodegenerative diseases and contributes to the understanding of hemorrhagic stroke."
"We report the case of a woman who developed central pontine myelinolysis in spite of gradual correction of hyponatremia. The good clinical evolution as well as the influence of rapid correction of serum sodium concentration in the unfolding of this rare condition are discussed. Several diagnostic procedures, mainly auditory-evoked responses and magnetic resonance imaging are also analysed. We remark the particular interest of the auditory-evoked responses in the attestation of an eventual remyelination."
"Nine fasting, healthy, adult male volunteers were given oral carbohydrate before exposures to normoxia (PIO2 = 149 torr) and mild hypoxia (PIO2 = 98 torr). Following recovery, they were given oral ethanol before similar exposure to normoxia and mild hypoxia. Repeated measures of arterial blood and expired gases were made. Ethanol diminished respiratory gas exchange (R), causing lower alveolar and arterial oxygen pressures during normoxia and mild hypoxia and a reduction in arterial oxygen saturation from 89.9 to 87.4% during mild hypoxia. It is suggested that carbohydrates are preferable to ethanol and fats as nutrients during limited oxygen transport situations, such as high-altitude, carbon monoxide exposure, or during heavy exertion, and for patients with cardiovascular or pulmonary disease."
"Cytokines often display substantial toxicities at low concentrations, preventing their escalation for therapeutic treatment of cancer. Fusion proteins comprising cytokines and recombinant antibodies may improve the anticancer activity of proinflammatory cytokines. Murine IFNã was appended in the diabody format at the C-terminus of the F8 antibody, generating the F8-IFNã fusion protein. The F8 antibody is specific for the extra-domain A (EDA) of fibronectin, a tumor-associated antigen that is expressed in the vasculature and stroma of almost all tumor types. Tumor-targeting properties were measured in vivo using a radioiodinated preparation of the fusion protein. Therapy experiments were performed in three syngeneic murine models of cancer [F9 teratocarcinoma, WEHI-164 fibrosarcoma, and Lewis lung carcinoma (LLC)]. F8-IFNã retained the biologic activity of both the antibody and the cytokine moiety in vitro, but, unlike the parental F8 antibody, it did not preferentially localize to the tumors in vivo. However, when unlabeled F8-IFNã was administered before radioiodinated F8-IFNã, a selective accumulation at the tumor site was observed. F8-IFNã showed dose-dependent anticancer activity with a clear superiority over untargeted recombinant IFNã. The anticancer activity was potentiated by combining with F8-IL4 without additional toxicities, whereas combination of F8-IFNã with F8-TNF was lethal in all mice. Unlike other antibody-cytokine fusions, the use of IFNã as payload for anticancer therapy is associated with a receptor-trapping mechanism, which can be overcome by the administration of a sufficiently large amount of the fusion protein without any detectable toxicity at the doses used."
"Bone marrow-derived mesenchymal stem cells (BMSCs) are a major source for cell transplantation. The proliferative ability of BMSCs is an important determinant of the efficiency of transplant therapy. Sertoli cells are ""nurse"" cells for development of sperm cells. Our recent study showed that Sertoli cells promoted proliferation of human umbilical cord mesenchymal stem cells (hUCMSCs) in co-culture. Studies by other groups also showed that Sertoli cells promoted growth of endothelial cells and neural stem cells. In this study, we investigated the effect of Sertoli cells on proliferation of BMSCs. Our results showed that Sertoli cells in co-culture significantly enhanced proliferation of BMSCs (P < 0.01). Moreover, co-culture with Sertoli cells also markedly increased mRNA and/or protein expressions of Mdm2, p-Akt and Cyclin D1, and decreased p53 expression in BMSCs (P < 0.01 or < 0.05). These findings indicate that Sertoli cells have the potential to enhance proliferation of BMSCs."
The diagnosis at postmortem examination and the results of the chemical analysis in a case of a diltiazem intoxication are presented. Two metabolites were identified as desacetyldiltiazem and N-desmethyldiltiazem.
"A two-stage probability sample of community subjects was developed with a full psychiatric examination employing DSM-III criteria in conjunction with the Epidemiological Catchment Area (ECA) survey conducted in Baltimore, MD. This report details the observation on those subjects diagnosed with compulsive personality disorder and compulsive personality traits. The results indicate that this condition has a prevalence of 1.7% in a general population. Male, white, married and employed individuals receive this diagnosis most often. Our data suggest a dimensional rather than categorical character for this disorder. The disorder imparts a vulnerability for the development of anxiety disorders."
"It has long been recognized that anatomic location is an important feature for defining distinct subtypes of plaque psoriasis. However, little is known about the molecular differences between scalp, palmoplantar, and conventional plaque psoriasis. To investigate the molecular heterogeneity of these psoriasis subtypes, we performed RNA-seq and flow cytometry on skin samples from individuals with scalp, palmoplantar, and conventional plaque psoriasis, along with samples from healthy control patients. We performed differential expression analysis and network analysis using weighted gene coexpression network analysis (WGCNA). Our analysis revealed a core set of 763 differentially expressed genes common to all sub-types of psoriasis. In contrast, we identified 605, 632, and 262 genes uniquely differentially expressed in conventional, scalp, and palmoplantar psoriasis, respectively. WGCNA and pathway analysis revealed biological processes for the core genes as well as subtype-specific genes. Flow cytometry analysis revealed a shared increase in the percentage of CD4+ T regulatory cells in all psoriasis subtypes relative to controls, whereas distinct psoriasis subtypes displayed differences in IL-17A, IFN-gamma, and IL-22 production. This work reveals the molecular heterogeneity of plaque psoriasis and identifies subtype-specific signaling pathways that will aid in the development of therapy that is appropriate for each subtype of plaque psoriasis."
"OBJECTIVES: Using a novel measure, examine maternal perceptions of the process by which issues pertaining to family communication of BRCA test results are addressed during cancer genetic counseling.METHODS: After receiving BRCA results, mothers (N = 211) of minor-age children reported on their counseling experiences with providers using a communication process measure as well as other psychosocial variables.RESULTS: The novel Genetic Counseling Communication Process measure demonstrated good internal consistency of its 2 factors: patient-led communication (Cronbach's á = 0.73) and provider-led communication (Cronbach's á = 0.82). Participants most often reported that discussions about family communication of BRCA test results to children and adult relatives were led only by their providers (38.2%-39.2%), as opposed to being led by the patient, both parties, or neither party. Providers were most likely to lead these discussions when mothers had stronger family histories of cancer and expressed more confidence about making a decision to talk to their children about BRCA. However, mothers typically led such discussions if they were raising older children and held more positive attitudes about pediatric BRCA testing.CONCLUSIONS: When the assessment of BRCA genetic counseling outcomes includes family communication to potentially at-risk relatives, we learned that most but not all sessions addressed this topic. Cancer family history, child age, and maternal attitudes are important co-factors in these patient-provider communication exchanges. Providers delivering BRCA genetic counseling should be attentive to mothers' information and support needs regarding communicating cancer genetic test results to at-risk relatives, including children."
"BACKGROUND: Many clinicians are reluctant to prescribe inhaled corticosteroids because of concerns over potential effects on the hypothalamic-pituitary-adrenal axis.OBJECTIVE: The purpose of this study was to compare the adrenal responses to 6-hour cosyntropin infusion after treatment with fluticasone propionate aerosol, triamcinolone acetonide aerosol, prednisone, and placebo for 4 weeks, a sufficient time interval to assess any effects on the adrenal response to stress.METHODS: This double-blind, triple-dummy, randomized, placebo-controlled study was conducted in 128 patients to evaluate adrenal response to 6-hour cosyntropin infusion (a clinically relevant method for evaluating adrenal function) after 28 days of treatment with fluticasone propionate aerosol 88 microg or 220 microg twice daily, triamcinolone acetonide aerosol 200 microg 4 times daily or 400 microg twice daily, prednisone 10 mg once daily, and placebo.RESULTS: After 28 days of treatment, mean plasma cortisol response to cosyntropin over 12 hours after initiation of the 6-hour infusion was similar among fluticasone, triamcinolone, and placebo groups; cortisol response was significantly (P <.05) reduced after treatment with prednisone compared with the other treatment groups. Mean 8-hour area under the plasma cortisol concentration-time curves and peak plasma cortisol concentrations were significantly (P </=.003) lower with prednisone than any other treatment; no significant differences were noted between placebo and either of the fluticasone groups in any assessment. Mean reductions from baseline in area under the plasma cortisol concentration time curves and peak cortisol concentrations were significantly (P <.05) greater with triamcinolone 400 microg twice daily compared with placebo.CONCLUSION: These results suggest that fluticasone propionate at therapeutic doses has effects on the hypothalamic-pituitary-adrenal axis comparable to that of placebo and has significantly less effect than prednisone as measured by 6-hour cosyntropin infusion after 28 days of treatment."
"BACKGROUND: Little information exists concerning the periimplant soft tissue response to plaque compared to the gingiva of the dentition. The purpose of this study was to compare this relative tissue response to plaque in humans over time.METHODS: Two hundred seventy-five (275) hydroxyapatite-coated implants were placed in the maxillae of 50 subjects followed by prosthetic rehabilitation. Baseline gingival (GI) and plaque (PI) index scores were obtained for all implants. Two to 4 teeth per subject were similarly measured, serving as controls. Measurements were repeated at 6-month intervals over 30-months. GI scores were evaluated relative to PI scores at 4 separate sites for each implant, to implant location within the oral cavity, and to length of time that each implant was in function.RESULTS: The peri-implant mucosa demonstrated a significantly greater likelihood of having elevated GI scores relative to PI scores when compared to the gingiva (chi-square for combined PI scores of 0 and 1 = 85.0, df = 1, P <0.001; for combined P1 scores of 2 and 3 = 114.6, df = 1, P <0.001). A logistic generalized linear model confirmed the significance of these results (Student t for implant effect = 21.602). It further demonstrated significantly elevated GI scores for implant sites over time and for implants located in the posterior oral cavity.CONCLUSIONS: The results indicate that maxillary peri-implant soft tissues are at increased risk for plaque-induced inflammation relative to the gingiva of the dentition. Hygiene recall standards and treatment regimens may require revisions to minimize peri-implantitis and prevent bone loss."
"OBJECTIVES: As a member of miR-29 family, miR-29a can act as either oncogene or tumor suppressor. However, its expression patterns in acute myeloid leukemia (AML) are controversial according to previous studies. Thus, the aim of this study was to determine the expression and clinical significance of miR-29a in pediatric AML.METHODS: Expression levels of miR-29a in bone marrow mononuclear cells were detected by real-time quantitative PCR in a cohort of 106 patients with newly diagnosed pediatric AML. The prognostic values of miR-29a in pediatric AML were also analyzed.RESULTS: Compared with normal controls, we demonstrated a significantly decreased expression of miR-29a in the bone marrow of pediatric AML patients (P<0.001). The expression levels of miR-29a were significantly lower in French-American-British classification subtype M7 than in other subtypes (P<0.001) and differed significantly across cytogenetic risk groups (P=0.002) with high miR-29a expression among those with favorable karyotypes. Moreover, low miR-29a expression was significantly associated with shorter relapse-free (P<0.001) and overall (P=0.008) survival in pediatric AML patients. Cox proportional hazards multivariate analysis of the univariate predictors identified cytogenetic risk and miR-29a expression as independent prognostic factors for relapse-free survival and overall survival. More interestingly, the prognostic value of miR-29a expression was more obvious in the subgroup of patients with intermediate-risk cytogenetics.CONCLUSION: Our data indicate for the first time that the down-regulation of miR-29a was associated with advanced clinical features and poor prognosis of pediatric AML patients, suggesting that miR-29a down-regulation may be used as an unfavorable prognostic marker in pediatric AML."
"OBJECTIVE: To assess any outcome differences between young men and women who are admitted for asthma.METHODS: We conducted a retrospective cohort study based on hospitalizations. An inclusion criterion was admission for asthma between January 1, 1998 and July 1, 2001. Exclusion criteria included age >45, chronic obstructive pulmonary disease (COPD), and emphysema. Data were collected on 10 potential confounding variables. Four outcome variables were assessed, including length of stay, intensive care unit (ICU) length of stay, mortality, and respiratory failure.RESULTS: Patients admitted for asthma were significantly more likely to be female (374 females vs. 106 males, p <0.05). There was no difference between the genders comparing month of admission. The women were significantly older, with more Medicaid insured, and more anxiety/depression (p <0.05). There was no difference between the genders for obesity, race, tobacco history, gastroesophageal reflux disease (GERD), hypertension, diabetes, and pneumonia. There was no reported mortality. Using regression analysis, there was no difference between the genders for length of stay (odds ratio [OR] = 1.06, 95% confidence interval [CI] 0.97-1.17) and respiratory failure (OR = 1.58, 95% CI 0.53-4.76). Men stayed significantly longer in the ICU (OR = 1.18, 95% CI 1.01-1.38).CONCLUSIONS: Patients admitted with asthma are significantly more likely to be female. Males stay significantly longer in the ICU. There is no difference between the genders for length of stay and respiratory failure. There was no reported mortality for either gender."
"During episodes of blood loss, several apparently redundant mechanisms are activated to maintain arterial blood pressure. This study was designed to examine one such compensatory mechanism involving enhanced vasopressin release during hemorrhage when the autonomic nervous system (ANS) is pharmacologically blocked. First, to confirm that this compensatory mechanism exists in canines, conscious dogs were hemorrhaged under normal conditions and during ANS blockade. In dogs with intact cardiac nerves (intact, n = 7), hemorrhage at 0.8 ml/kg/min increased plasma vasopressin (PAVP) from 3.0 +/- 0.7 to 6.6 +/- 2.4 and 78 +/- 50 pg/ml at blood losses of 10 and 20 ml/kg, respectively. At the same amount of blood loss during hemorrhage with ANS blockage, PAVP was enhanced significantly from 33 +/- 17 to 230 +/- 90 and 610 +/- 270 pg/ml. ANS blockade did not, however, alter the hemorrhage-induced increases in plasma renin activity. Next, to examine the afferent mechanisms responsible for the enhanced PAVP response, cardiac-denervated dogs (CD, n = 9) were hemorrhaged with and without ANS blockade. Without blockade, PAVP increased from 3.7 +/- 0.9 to 5.2 +/- 0.8 and 26 +/- 11 pg/ml at blood losses of 10 and 20 ml/kg. PAVP was significantly higher in response to hemorrhage with ANS blockade, increasing from 17 +/- 6 to 76 +/- 18 and 330 +/- 80 pg/ml. The rise in PAVP in the CD dogs suggested that peripheral baroreceptors were involved in eliciting vasopressin release under these conditions. Therefore, the influence of arterial baroreceptors was examined by infusing norepinephrine during hemorrhage in order to maintain blood pressure constant. Under these conditions, PAVP increased significantly in the intact dogs at 10 ml/kg blood loss, but did not change in the CD dogs. These results demonstrate that the enhanced release of AVP during hemorrhage with ANS blockade can be mediated either by cardiac or arterial baroreceptors; however, the maximum response is elicited only when both sets of receptors are functioning normally."
"Three new complexes with 3,6-dichlorobenzene-1,2-dithiol (bdtCl2), namely methyltriphenylphosphonium bis(3,6-dichlorobenzene-1,2-dithiolato-ê(2)S,S')cobaltate(1-), (C19H18P)[Co(C6H2Cl2S2)2], (I), bis(methyltriphenylphosphonium) bis(3,6-dichlorobenzene-1,2-dithiolato-ê(2)S,S')cuprate(2-) dimethyl sulfoxide disolvate, (C19H18P)2[Cu(C6H2Cl2S2)2]·2C2H6OS, (II), and methyltriphenylphosphonium bis(3,6-dichlorobenzene-1,2-dithiolato-ê(2)S,S')cuprate(1-), (C19H18P)[Cu(C6H2Cl2S2)2], (III), have been synthesized and characterized by single-crystal X-ray diffraction. The X-ray structure analyses of all three complexes confirm that the four donor S atoms form a slightly distorted square-planar coordination arrangement around the central metal atom. An interesting finding for both the Cu(II) and Cu(III) complexes, i.e. (II) and (III), respectively, is that the coordination polyhedra are principally the same and differ only slightly with respect to the interatomic distances."
"Tracer experiments in rats mimicking type II primary hyperoxaluria, with an expanded intracellular pool of hydroxypyruvate, showed that the excess formation of oxalate did not originate from its immediate precursor glyoxylate. In these animals, the hepatic and kidney activities of oxalate synthesising enzymes such as lactate dehydrogenase and glycolate oxidase were normal, but tissue lipid peroxidation was significantly higher. In vitro experiments established that in a mild alkaline solution, hydroxypyruvate underwent auto-oxidation to form oxalate and H2O2 and also inhibited lactate dehydrogenase and glycolate oxidase from oxidising glyoxylate to oxalate. On the basis of the experimental evidence, we suggest that in type II primary hyperoxaluria, the accumulating hydroxypyruvate could reduce the intracellular pool of glyoxylate and on ageing, give rise to excess oxalate and H2O2, to cause oxalosis in the former and free radical mediated-cell injuries in the latter."
"The present study describes the development of an analytical method for the determination of cesium in biological fluid samples (human urine and blood samples) by laser-induced breakdown spectroscopy (LIBS). The developed method is based on sample presentation by liquid-to-solid conversion, enhancing the emission signal by drying the liquid into small ""pockets"" created in a metal support (zinc plate), and allows the analysis to be carried out on as little as 1 ìL of sample volume, in a closed sample cell. Absolute detection limits on the Cs I 852.1 nm spectral line were calculated by the IUPAC 3ó method to be 6 ng in the urine sample and 27 ng in the blood serum sample. It is estimated that LIBS may be used to detect highly elevated concentration levels of Cs in fluid samples taken from people potentially exposed to surges of Cs from non-natural sources."
"PURPOSE: The role of serum AMH levels in prediction of ovarian response in idiopathic hypogonadotropic hypogonadism (IHH) was evaluated. MATERIAL METHOD(S): Twelve patients with IHH underwent controlled ovarian hyperstimulation (COH) for IVF were enrolled in this prospective study. Serum AMH levels were studied on the 2nd or 3rd day of an induced menstrual cycle by a preceding low-dose oral contraceptive pill treatment. A fixed dose (150-300 IU/day) of hMG was given in all COH cycles. Correlations between serum AMH levels, COH outcomes and embryological data were investigated.RESULTS: Mean serum AMH levels was 3.47 ± 2.15 ng/mL and mean serum peak estradiol was 2196 ± 1705 pg/mL. Mean number of follicles >14 mm, >17 mm on hCG day and MII oocytes were 4.14 ± 3.2, 4 ± 2.5 and 7.28 ± 3.5, respectively. Mean number of grade A embryos and transferred embryos were 3.28 ± 2.4 and 2.5 ± 0.7, respectively. The clinical pregnancy rate per patient was 41.6 % (5/12). Positive correlations were observed between serum AMH levels and MII oocytes (r = 0.84), grade A embryos (r = 0.85), serum peak estradiol levels (r = 0.87), and number of follicles >14 mm (r = 0.83) and >17 mm (r = 0.81) on hCG day, respectively.CONCLUSION: AMH appears as a promising marker of ovarian response in patients with IHH undergoing IVF."
"Genetic polymorphism of human bleomycin hydrolase (hBH) has been reported to be associated with the risk of sporadic Alzheimer's disease (AD). The regional and cellular distribution of mRNA encoding hBH in the brain from controls and patients with AD was examined using in situ hybridization. A hybridization signal, in the form of clusters of single cells, was observed in the white matter. Our results indicate a predominantly astrocytic expression of hBH in the investigated human brain regions. Although the signal intensity was generally reduced in AD brains, the large variability among controls rendered this trend statistically insignificant."
"This study was undertaken to look at the feasibility of a health visitor risk assessment for falls at the time of the routine health check for people aged 75 years and above. A total of 162 people were eligible for inclusion in the study. The standard over-75 assessment check was carried out either in the GP surgery or the person's home. A questionnaire was developed to obtain additional information not collected in the routine health check. The results identified two key risk areas: a history of polypharmacy and living in sheltered housing. There were no differences for a range of physical, emotional and environmental factors between people who had fallen and those who had not. A larger study is required to look at the identification of risk factors for falling at the routine over-75 health check, and appropriate referrals that can be put into place to deal with any problems uncovered. Education of health professionals on the risk factors of falls is also required."
"We have studied 37 cases of ovarian epithelial tumors in post-menopausal women, histopathologically and endocrinologically. The normal values for androstenedione, estrone, estradiol and progesterone in healthy post-menopausal women were less than 83 ng/dl, 75 pg/ml, 30 pg/ml and 0.6 ng/ml, respectively. The numbers of cases, the serum values for which were higher than normal, were 9 of 16 (androstenedione), 9 of 16 (estrone), 22 of 29 (estradiol) and 14 of 20 (progesterone). The serum levels of these 4 sex hormones in cases with ovarian stromal condensation (stromal cell hyperplasia) were higher than normal in 8 of 11 (androstenedione), 8 of 11 (estrone), 17 of 19 (estradiol) and 9 of 12 (progesterone), whereas those in cases with no stromal condensation were elevated in 1 of 5, 1 of 5, 5 of 8 and 5 of 7, respectively. After complete removal of the tumors, these elevated sex hormone levels dropped to normal. After the dexamethasone suppression test, the suppression rates for cortisol, 17-OHCS and 17-KS were 4 times as great as those of DHEA-S (dehydroepiandrosterone-sulfate), estrone, estradiol and progesterone. 17 beta-estradiol was localized in hyperplastic ovarian stromal cells in all cases with stromal condensation. We concluded that many of the ovarian epithelial tumors produce these sex hormones and that hyperplastic stromal cells are the source an increased amount of serum estradiol."
"PURPOSE: To document the contamination rate of sutures used in strabismus surgery and evaluate the reduction of contamination using antibiotic-coated and antiseptic/antibiotic-coated sutures.METHODS: This was a prospective randomized analysis of suture contamination and potential prophylaxis measures after strabismus surgery. Muscle sutures (6-0 polyglactin) used in 302 consecutive cases of strabismus from October 2008 to May 2009 were collected and randomly assigned to three groups: (1) a control without pretreatment sutures (61); (2) antibiotic/steroid-coated sutures (200); and (3) antiseptic-soaked and antibiotic/steroid-coated sutures (141). The sutures were used under sterile conditions and then cut into pieces and transferred to blood agar plates, which were incubated for 48 hours and then checked for growth.RESULTS: Group 1 had bacterial growth in 17 of 61 (28%) sutures; group 2 had growth in 44 of 200 (22%) sutures; and group 3 had growth in 12 of 141 (9%) sutures. The reduction in bacterial growth using the antibiotic/antiseptic coating was significant (P = .006). One patient developed coagulase-negative Staphylococcus epidermidis endophthalmitis 1 week after surgery, which was promptly diagnosed and successfully treated. No complications from the antibiotic-coated or antiseptic-soaked sutures were noted.CONCLUSIONS: Although endophthalmitis after strabismus surgery is rare, estimated at 1 in 35,000 to 1 in 185,000, visual outcome is uniformly poor. The authors hypothesize that strabismus sutures can be contaminated via contact with the eyelashes and skin, providing a possible conduit for endophthalmitis. Bacterial contamination of strabismus sutures is high (28%) and can be reduced significantly if sutures are soaked in antiseptic before use."
"Lymphocytary subpopulations have been determined, by means of monoclonal antibodies, in 24 patients afflicted with iron deficient anaemia, repeating the determination in 15 of the patients after an adequate iron therapy. In iron deficient patients a significant reduction of the percentage of lymphocytes T (OKT3+) has been observed, in comparison with normal controls, as well as a significant reduction of the percentage of cells OKT8+, with a significant increase of the OKT4+/OKT8+ ratio. In the 10 patients who at the moment of the revaluation did not present any longer either anaemia or iron deficiency, the percentage of lymphocytes OKT3+ and OKT8+ had returned to normal values, as well as the OKT4+/OKT8+ ratio. The 5 patients still resulting anaemic in course of control presented, on the contrary, a significant reduction of cells OKT3+ and OKT8+, while the OKT4+/OKT8+ ratio resulted lightly increased, however not significantly."
"We have investigated the endocytosis by rat liver of asialofetuin coupled to [125I] tyramine cellobiose: [125I] TCASF. Subcellular distribution of radioactive compounds was established after differential and isopycnic centrifugation and by analysing the fractions by SDS electrophoresis. Labelling secondary lysosomes was performed by injecting rats with Triton WR 1339 four days before injecting the protein. Results show that after being associated with endosomes [125I] TCASF is recovered in organelles where they are subjected to a first degradation, the density of these organelles is practically not affected by Triton WR 1339 injection. Later the degradation products are associated with lysosomes whose density is markedly lowered by Triton WR 1339 treatment. These observations suggest that the first intracellular organelles where [125I] TCASF is subjected to digestion are distinct from the secondary lysosome population. This could be in agreement with the hypothesis that supposes that endosomes acquire enzymes from primary lysosomes before fusion with secondary lysosomes."
"OBJECTIVES/HYPOTHESIS: The aim of our study was the objective assessment of endolymphatic hydrops in asymptomatic ears in unilateral M?ni?re's disease with a noninvasive electrophysiological test and investigation of significant clinical signs. The null hypothesis was that there would be no signs of endolymphatic hydrops in the asymptomatic ear.STUDY DESIGN: Prospective study using the traveling wave velocity test for endolymphatic hydrops.METHODS: The traveling wave velocity test was used in conjunction with standard audiological tests to investigate both ears of 181 M?ni?re's patients attending the Medical Research Council Institute of Hearing Research in Southampton, United Kingdom. The test uses derived auditory brainstem responses to estimate the velocity of the cochlear traveling wave that is altered in endolymphatic hydrops. M?ni?re's disease was assessed using Arenberg's five staging criteria. Significant correlations were evaluated using standard statistical methods.RESULTS: Of 100 patients with clinically unilateral M?ni?re's disease, 27% showed evidence of endolymphatic hydrops in their asymptomatic ear. There was a significant correlation between signs of hydrops and the mean air-conduction threshold at 500 Hz.CONCLUSIONS: We recommend that a full assessment of incipient disease in the asymptomatic ear in unilateral M?ni?re's disease should be undertaken before offering any treatment options to patients. Any suspicion of early disease in the asymptomatic ear in unilateral M?ni?re's disease should lead to full electrophysiological assessment to assess the evidence of endolymphatic hydrops in that ear."
"OBJECTIVE: Worries about the risk to personal health from new technology and features of modern life have been shown to be associated with the use of health care services, health behaviours, mood and reporting of physical symptoms. We examined the frequency and nature of these concerns in a large national sample and the relationship of modern health worries to demographic factors, depression, symptom reporting and health-related quality of life.METHODS: A representative sample of the German population (n=2485) completed a face-to-face survey which included demographic information, the Modern Health Worries Scale, as well as measures of depression, symptom reporting, and health-related quality of life.RESULTS: The majority of the population reports high or extremely high concerns about aspects of modernity affecting their personal health, while only six percent reported no concerns at all. Higher levels of modern health worries were found in females but were not associated with income or age. Higher levels of modern health worries were significantly associated with depression, symptom reporting and lower health-related quality of life. We found the relationship between modern health worries and both symptom reporting and health-related quality of life was only partially explained by depression for most outcome variables, while the association between MHW and physical component (SF-12) was fully mediated by depression.CONCLUSIONS: Concerns about aspects of modernity affecting health are common in a general population sample and associated with depression, symptom reporting and quality of life."
"The pulmonary vascular effects of dopamine and of dobutamine have been reported variably in the literature. We investigated the effects of dopamine and of dobutamine, at doses of 10 and 20 micrograms/kg/min, on the relationships of overall mean pulmonary arterial pressure (Ppa) to cardiac index (Cl) in 14 dogs ventilated alternatively in hyperoxic (FIO2, 0.4) and in hypoxic (FIO2, 0.1) conditions. Five-point Ppa/Cl plots were constructed by opening an arteriovenous femoral fistula or by stepwise inflations of a balloon in the inferior vena cava. These Ppa/Cl plots were rectilinear in all experimental conditions. Hypoxia was associated with an increase in Ppa over the entire range of Cl studied (2 to 5 L/min/m2). A deterioration in arterial oxygenation and an increase in O2 consumption constantly occurred after dopamine as well as after dobutamine administration. At 10 micrograms/kg/min (n = 6 dogs) neither drug affected Ppa over the entire range of Cl at both 0.4 and 0.1 FIO2. At 20 micrograms/kg/min (n = 8 dogs), dopamine and dobutamine increased Ppa at the lowest Cl (2 to 4 and 2 to 3 L/min/m2, respectively) at 0.4 FIO2, and attenuated hypoxia-induced increases in Ppa over the entire range of Cl. Two repetitions of alternated 0.4 and 0.1 FIO2 exposures had no effect on Ppa/Cl plots in 6 additional dogs given no drug. We concluded that at dosages as great as 20 micrograms/kg/min, as generally given in clinical practice, dopamine and dobutamine exerted similar effects upon the pulmonary circulation of intact dogs; either no change or an increase in hyperoxic pulmonary vascular tone and either no change or an attenuation of hypoxic pulmonary vasoconstriction."
"Drugs capable of specifically recognizing and killing cancer cells while sparing healthy cells are of great interest in anti-cancer therapy. An example of such a drug is edelfosine, the prototype molecule of a family of synthetic lipids collectively known as antitumor lipids (ATLs). A better understanding of the selectivity and the mechanism of action of these compounds would lead to better anticancer treatments. Using Caenorhabditis elegans, we modeled key features of the ATL selectivity against cancer cells. Edelfosine induced a selective and direct killing action on C. elegans embryos, which was dependent on cholesterol, without affecting adult worms and larvae. Distinct ATLs ranked differently in their embryonic lethal effect with edelfosine > perifosine > erucylphosphocholine >> miltefosine. Following a biased screening of 57 C. elegans mutants we found that inactivation of components of the insulin/IGF-1 signaling pathway led to resistance against the ATL edelfosine in both C. elegans and human tumor cells. This paper shows that C. elegans can be used as a rapid platform to facilitate ATL research and to further understand the mechanism of action of edelfosine and other synthetic ATLs."
"We present a region-based segmentation method in which seeds representing both object and background pixels are created by combining morphological filtering of both the original image and the gradient magnitude of the image. The seeds are then used as starting points for watershed segmentation of the gradient magnitude image. The fully automatic seeding is done in a generous fashion, so that at least one seed will be set in each foreground object. If more than one seed is placed in a single object, the watershed segmentation will lead to an initial over-segmentation, i.e. a boundary is created where there is no strong edge. Thus, the result of the initial segmentation is further refined by merging based on the gradient magnitude along the boundary separating neighbouring objects. This step also makes it easy to remove objects with poor contrast. As a final step, clusters of nuclei are separated, based on the shape of the cluster. The number of input parameters to the full segmentation procedure is only five. These parameters can be set manually using a test image and thereafter be used on a large number of images created under similar imaging conditions. This automated system was verified by comparison with manual counts from the same image fields. About 90% correct segmentation was achieved for two- as well as three-dimensional images."
"The muscarinic cholinoceptors in porcine coronary artery were identified and characterized by a binding assay using (-)-[3H]quinuclidinyl benzilate (QNB) and also by pharmacological method. Specific (-)-[3H]QNB binding in the coronary artery was saturable and of high affinity (Kd = 0.08 nM), and it showed a pharmacological specificity as well as stereoselectivity which characterized muscarinic receptors. Muscarinic antagonists competed with the (-)-[3H]QNB binding in order: nonlabeled QNB greater than dexetimide greater than atropine greater than pirenzepine greater than AF-DX 116 greater than levetimide greater than gallamine. Dexetimide was approximately 2000 times as potent as levetimide. The potencies (pKi) of these muscarinic antagonists in competing for (-)-[3H]QNB binding sites in porcine coronary artery correlated well with their pharmacological potencies (pA2 for antagonistic effect of acetylcholine-induced contraction of coronary artery). The decrease in the (-)-[3H]QNB binding by atropine and pirenzepine was due to a reduction in the apparent affinity with little change in the number of maximal binding sites, suggesting a competitive antagonism. Specific (-)-[3H]QNB binding (Kd and maximal number of binding sites) in porcine coronary artery was not changed by the removal of endothelium. We conclude: 1) (-)-[3H]QNB selectively labels the physiologically relevant muscarinic receptors in porcine coronary artery and 2) the majority of these receptors is localized on vascular smooth muscles and the receptors mediate the acetylcholine-induced contractile response of coronary artery."
"Equine cytomegalovirus (ECMV) contains a linear, double-stranded DNA genome composed of a 146-kbp unique region flanked by a pair of 18-kbp direct repeat (DR) sequences at the termini. Cycloheximide, actinomycin D, and phosphonoacetic acid were applied to infected cell cultures to divide viral transcription into immediate-early (IE), early, and late phases. Eight IE transcripts were identified and mapped to two regions (I and II) of the viral genome. Two of these IE RNAs (13.0 and 5.5 kb in size) were transcribed from region I, which is located within the DR regions; these IE genes are diploid. The other IE transcripts (17.0, 9.0, 7.2, 6.8, 4.5, and 4.2 kb) originated from region II. IE region II is adjacent to region I and spans both unique and DR sequences at the left terminus of the genome. Region II IE transcripts are spliced and transcribed in the opposite direction from region I IE transcripts. IE transcripts from region I were present throughout the replication cycle, whereas those from region II were more abundant during the IE stage than at the early and late stages of infection. These studies demonstrate that ECMV differs from other herpesviruses in the organization and unusually large transcription units of its IE genes."
"OBJECTIVE: To investigate the clinical features and epidemiological data of 72 cases of male perianal warts.METHODS: Seventy-two cases of perianal warts in our clinic dated from June, 2004 to April, 2006 were enrolled in the study, whose clinical information and epidemiological data were collected and analyzed.RESULTS: Perianal warts were most commonly seen in young and middle-aged men aged from 18 to 45, only 12.5% of whom had homosexual behaviors. Sauna was another predisposing factor of perianal warts in males in China (chi2 = 5.03, P < 0.05). Primary eruptions of the anus and rectum, like perianal pruritus, eczema, anus fissure, and haemorrhoids, often impaired the local integrity of skin/mucosa. Classical condyloma acuminate was found in 61 (84.72%) of patients, who were susceptive to the infections of HPV 6/11, and were flat condylomas related to HPV16/18. Cryotherapy was believed to be one of the most efficient therapeutic choices for flat perianal warts. Suppression of cellular immune response was identified in the patients by comparison between the subgroups of peripheral T cells and the normal control.CONCLUSION: Sauna is an essential predisposing factor of perianal warts in males, while anus sexual intercourse is not the main route of HPV infection. Classical condylomata acuminate constitute the majority of the eruptions, and flat condylomata come next. The study also provides some useful data for understanding the clinical and epidemiological features of perianal warts in Chinese males for the sake of prevention and treatment of the disease."
"The spatial organization of the microfilament system as the main component of the cytoskeleton in Amoeba proteus was preserved by a glutaraldehyde-lysine-fixation and visualized with fluorescent phallotoxins (NBD- phallacidin , R-phalloidin). Results obtained by means of this method coincide exactly with observations gained from immunocytochemical, ultrastructural and molecular cytochemical studies, i.e., the microfilament system is mainly displayed beneath the cell membrane, at the hyalo - granuloplasmic border and around the cell nucleus. The preparation procedure employed is suitable for the rapid demonstration of cytoplasmic microfilaments in cells difficult to preserve by chemical fixation."
"BACKGROUND: New efforts are being made to improve understanding of the epidemiology of the helminths and intensifying the control efforts against these parasites. In contrast, relatively few studies are being carried out in this direction for the intestinal protozoa. To contribute to a better comprehension of the epidemiology of the intestinal protozoa, prevalence, and spatial distribution of Entamoeba histolytica/dispar and Giardia lamblia, and their association with drinking water supplies, were determined in the Agboville department in southeast C?te d'Ivoire.METHODS/FINDINGS: Stool samples were taken from more than 1,300 schoolchildren in the third year of primary education (CE1) from 30 primary schools and preserved in SAF (sodium acetate-acetic acid-formalin). The samples were analyzed by formalin-ether concentration. Then, a survey questionnaire addressed to schoolchildren and school directors was used to collect data on water supplies. Prevalence of E. histolytica/dispar and G. lamblia were, respectively, 18.8% and 13.9%. No particular focus zone was observed in the spatial distribution of the two species. Significant negative association was observed between use of tap water and high prevalence of E. histolytica/dispar infection (OR = 0.83, p = 0.01). High prevalence of G. lamblia infection was positively associated with use of ponds as the source of drinking water (OR = 1.28, p = 0.009).CONCLUSION: These two species of pathogenic protozoa are present with substantial prevalence in this area of C?te d'Ivoire. Although their spatial distribution is not focused in any one place, determination of the population segments with the highest levels of infection will help to target the chemotherapeutic fight. To reinforce treatment with chemotherapeutic agents, tap water should be made available in all the localities of this area."
"BACKGROUND: Aortic complicated lesions (ACLs) are key parameters for evaluating aortic embolic sources in embolic stroke, and are usually diagnosed using transesophageal echocardiography (TEE). However, alternative methods for diagnosing ACLs have not been well established. We investigated associations between high-intensity areas on T1-weighted imaging (T1WI) with magnetization-prepared rapid acquisition with gradient echo (MPRAGE) and ACLs on TEE among ischemic stroke patients.METHODS: Participants comprised 135 patients (mean age, 71 years; 35 women) with ischemic stroke or transient ischemic attack who underwent TEE for evaluation of embolic sources and plaque imaging using MPRAGE for evaluation of aortic or carotid plaques. Aortic plaque with signal intensity ?200% of sternocleidomastoid intensity on MPRAGE was categorized as ""high intensity"". ACLs on TEE were defined by focal increases in intima-media thickness (IMT) ?4.0 mm or the presence of ulcerated or mobile plaques.RESULTS: Fifty-six patients (42%) showed high-intensity areas on MPRAGE at the aortic arch. Aortic maximum IMT was significantly higher in patients with high intensities than in those without (p < 0.001). Incidences of ACLs (66 vs. 20%, p < 0.001) or ulcerated or mobile plaques (30 vs. 6%, p < 0.001) were significantly higher in patients with high intensities than in patients without. Multivariable logistic regression analysis showed that high intensities on MPRAGE were independently associated with the presence of ACLs (OR 5.72; 95% CI 2.38-13.70) and ulcerated or mobile plaques (OR 4.18; 95% CI 1.29-13.50).CONCLUSIONS: High intensities on T1WI with MPRAGE in the aortic arch were significantly associated with the presence of ACLs. An evaluation of the aortic arch using MPRAGE may be useful for predicting ACLs."
"Using screening, electrophoretic and spectrophotometric examinations, the authors studied the polymorphism of serum cholinesterase (CE) (CP 3.1.1.8) in children with undifferentiated oligophrenia. The rates of the identified phenotypes were as follows: CHE1U--0.769; CHE1UD--0.039; CHE1UF--0.192; CHE2(5-)-1.000. The incidence of the CHE1UF variant of the enzyme considerably exceeded that in the control groups (u = 2.44; u = 3.23) while the frequency of the CHE1U variant detection was significantly below that in the control groups (u = 2.70; u = 3.01; u = 3.46). The author discusses the advisability of the identification of CE variants in patients with psychoneurological diseases in order to prevent the development of side effects of myorelaxants and local anesthetics."
"Infection of legume hosts by rhizobial bacteria results in the formation of a specialized organ, the nodule, in which atmospheric nitrogen is reduced to ammonia. Nodulation requires the reprogramming of the plant cell, allowing the microsymbiont to enter the plant tissue in a highly controlled manner. We have found that, in Crotalaria (Fabaceae), this reprogramming is associated with the biosynthesis of pyrrolizidine alkaloids (PAs). These compounds are part of the plant's chemical defense against herbivores and cannot be regarded as being functionally involved in the symbiosis. PAs in Crotalaria are detectable only when the plants form nodules after infection with their rhizobial partner. The identification of a plant-derived sequence encoding homospermidine synthase (HSS), the first pathway-specific enzyme of PA biosynthesis, suggests that the plant and not the microbiont is the producer of PAs. Transcripts of HSS are detectable exclusively in the nodules, the tissue with the highest concentration of PAs, indicating that PA biosynthesis is restricted to the nodules and that the nodules are the source from which the alkaloids are transported to the above ground parts of the plant. The link between nodulation and the biosynthesis of nitrogen-containing alkaloids in Crotalaria highlights a further facet of the effect of symbiosis with rhizobia on the ecologically important trait of the plant's chemical defense."
"Mutations of p53 are remarkably rare in acute promyelocytic leukemias (APLs). Here, we demonstrate that the APL-associated fusion proteins PML-RAR and PLZF-RAR directly inhibit p53, allowing leukemic blasts to evade p53-dependent cancer surveillance pathways. PML-RAR causes deacetylation and degradation of p53, resulting in repression of p53 transcriptional activity, and protection from p53-dependent responses to genotoxic stress. These phenomena are dependent on the expression of wild-type PML, acting as a bridge between p53 and PML-RAR. Recruitment of histone deacetylase (HDAC) to p53 and inhibition of p53 activity were abrogated by conditions that either inactivate HDACs or trigger HDAC release from the fusion protein, implicating recruitment of HDAC by PML-RAR as the mechanism underlying p53 inhibition."
"Nitrification, the microbial oxidation of ammonium to nitrate, is a central part of the nitrogen cycle. In the ocean's surface layer, the process alters the distribution of inorganic nitrogen species available to phytoplankton and produces nitrous oxide. A widely held idea among oceanographers is that nitrification is inhibited by light in the ocean. However, recent evidence that the primary organisms involved in nitrification, the ammonia-oxidizing archaea (AOA), are present and active throughout the surface ocean has challenged this idea. Here we show, through field experiments coupling molecular genetic and biogeochemical approaches, that competition for ammonium with phytoplankton is the strongest regulator of nitrification in the photic zone. During multiday experiments at high irradiance a single ecotype of AOA remained active in the presence of rapidly growing phytoplankton. Over the course of this three day experiment, variability in the intensity of competition with phytoplankton caused nitrification rates to decline from those typical of the lower photic zone (60 nmol L-1 d-1) to those in well-lit layers (<1 nmol L-1 d-1). During another set of experiments, nitrification rates exhibited a diel periodicity throughout much of the photic zone, with the highest rates occurring at night when competition with phytoplankton is lowest. Together, the results of our experiments indicate that nitrification rates in the photic zone are more strongly regulated by competition with phytoplankton for ammonium than they are by light itself. This finding advances our ability to model the impact of nitrification on estimates of new primary production, and emphasizes the need to more strongly consider the effects of organismal interactions on nutrient standing stocks and biogeochemical cycling in the surface of the ocean."
"The endoscopic findings in the colon and terminal ileum in eight cases of Yersinia enterocolitica enterocolitis infection were studied. The diagnosis was based on the isolation of Y. enterocolitica in the feces and/or elevated serum antibody titers to the organism. Total colonoscopy was performed between 7 and 38 days (mean, 24 days) after the onset of symptoms. In all patients, the terminal ileum was affected, followed by frequent involvement of the ileocecal valve and the cecum, and less frequently, the ascending colon. In the terminal ileum, round or oval elevations with or without ulcers were detected. Small ulcers were detected on the ileocecal valve and in the cecum. These findings were observed even 4 to 5 weeks after the onset of symptoms, suggesting a relatively long course for this disease."
"Outpatient-based sharp debridement is considered an important element for the care of a chronic ulcer.OBJECTIVE: The aim of this study is to evaluate the change in bacterial amounts with sharp debridement in a clinical setting.MATERIALS AND METHODS: Bacterial autofluorescence, quantitative cultures, semiquantitative cultures, and qualitative speciation were performed predebridement and postdebridement during a single clinic visit.RESULTS: Thirty-six wounds were included in the analysis. The mean patient age was 62 years (range, 27-83 years), and there were 13 (36.11%) women and 23 (63.89%) men with an average body mass index of 33.8 kg/m² (range, 16.7-55.9 kg/m²). Of the 36 patients, 24 (66.67%) had type 2 diabetes and 19 (52.78%) had a prior history of lower extremity amputation. Majority of the ulcers were diabetic neuropathic (27, 75%); the most common location was on the plantar aspect of the foot (14, 41.67%) with a mean ulcer duration of 10 months (range, 1-36), mean ulcer area of 6.3 ± 12.8 cm² (range, 0.18-62.06 cm²), and mean volume of 2.2 ± 4.4 cm³ (range, 0.05-9.66 cm³). There was no statistically significant difference in bacterial autofluorescence between the predebridement (4.15 ± 8.82) and the postdebridement (4.65 ± 9.48) images (P = .32). There was a statistically significant difference in quantitative culture results between the predebridement (6.7 x 104 ± 1.4 x 106 CFU/cm²) and the postdebridement (1.7 x 104 ± 3.1 x 106 CFU/cm²) cultures (P = .04), although this is not a log reduction.CONCLUSIONS: There is no statistically significant difference between the predebridement versus postdebridement semiquantitative culture results or a detectable pattern of change for the most common bacterial species encountered. These results suggest little impact of clinic-based sharp debridement on bacteria."
"After a group of autoptic kidneys, taken from new-born babies, had been injected via the ureter with micronised barium at various pressures, they were examined micro-radiographically and histologically in order to observe the appearance of intra-renal reflux and the anatomical structures involved. The radiological patterns, clinically observed during cysto-uretrography, were also considered. The intra-renal reflux is tubular, when injection is carried out at a low pressure, whereas at higher pressures one can observe the erosion of the fornices of the calyces and the following sino-lymphatic and/or venous drainage."
"The aim of this study was to investigate the effect of rumen fluid and leachate-based media on the ability of rumen and anaerobic digester derived microorganisms to degrade cellulose. The results demonstrated that rumen microorganisms are not capable of solubilising cellulose, or generating biomass, at an optimal rate when grown in leachate-based media when compared to the rates achieved when grown in rumen-based media. In contrast, the rate of biomass generation and cellulose solubilisation by digester microorganisms was not strongly affected by a change in media type. Several authors have theorised that cellulose degradation rates in anaerobic digesters can be increased by inoculation with rumen-derived microorganisms. The results from this study show that this approach is unlikely to be successful, because the rumen microorganisms would likely be unable to solubilise and out-compete native solid waste microorganisms for the cellulose in a foreign (leachate based) medium."
"The major histocompatibility complex (MHC) class I homolog MIC-A functions as a stress-inducible antigen that is recognized by a subset of gammadelta T cells independent of beta2-microglobulin and bound peptides. Its crystal structure reveals a dramatically altered MHC class I fold, both in detail and overall domain organization. The only remnant of a peptide-binding groove is a small cavity formed as the result of disordering a large section of one of the groove-defining helices. Loss of beta2-microglobulin binding is due to a restructuring of the interaction interfaces. Structural mapping of sequence variation suggests potential receptor binding sites on the underside of the platform on the side opposite of the surface recognized by alphabeta T cell receptors on MHC class I-peptide complexes."
"Medical patients' (75 with chronic fatigue complaints, 61 with dizziness, and 88 with disabling tinnitus; N = 224) current and past psychiatric diagnoses and personality characteristics were assessed to determine if they could independently explain the number of medically unexplained physical symptoms that the patients had experienced. Cloninger's Tridimensional Personality Questionnaire (TPQ) and the Diagnostic Interview Schedule based on DSM-III-R were used to assess the personality and psychiatric diagnoses, respectively. The results revealed that the number of lifetime medically unexplained symptoms were significantly, independently, and positively related to increasing numbers of current and past anxiety and depressive disorders and to the harm avoidance dimension of the TPQ. In a second analysis, the ""worry/pessimism"" and ""impulsiveness"" subscales were positively related to the number of medically unexplained symptoms. The results suggest that somatization is associated with current and past history of psychiatric illnesses and harm avoidance in this sample of medical patients."
Another report from the Institute of Medicine in March 2001 has joined a large body of literature documenting serious quality and safety problems. Eight health care leaders discuss ways in which organizations can reduce medical errors and improve patient outcomes.
"BACKGROUND: To examine changing demography and survival of patients with malignant phase hypertension (MHT) over 40 years.METHODS: Patients from our MHT registry whose survival status on 31 December 2006 was known were included, with analyses conducted based on decade of MHT diagnosis.RESULTS: Four-hundred and forty-six patients with MHT (overall mean (s.d.) age 48.2 (12.9), years; 65.5% male; 64.7% white-European; 20.4% African Caribbean, and 14.8% South-Asian) were included. No significant demographic differences at diagnosis were evident over the 40 years, with the exception of a significant increase (P = 0.001) in the proportion of MHT among ethnic minorities (South-Asian and Afro-Caribbeans). There were no significant differences in mean systolic blood pressure (SBP) at presentation but baseline diastolic BP (DBP) was significantly lower after 1976 (P < 0.0001). The total number of person-years of observation was 5,725.5 years, with a median (interquartile range (IQR)) length of follow-up of 103.8 (31.3-251.2) months. Overall 203 patients (55.6%) died, 125 (32.0%) within 5 years of diagnosis. There was a significant improvement in 5-year survival from 32.0% prior to 1977 to 91.0% for patients diagnosed between 1997 and 2006. SBP and DBP improved significantly during follow-up (P < 0.0001). Multivariate analyses revealed that age, decade of MHT diagnosis, baseline creatinine, and follow-up SBP were independent predictors of survival (all P < 0.0001).CONCLUSIONS: Demography and number of new cases of MHT have not changed dramatically over the past 40 years. Five-year post-MHT survival has improved significantly, possibly related to lower BP targets, tighter BP control, and availability of new classes of antihypertensive drugs."
"The authors present the anesthetic and ventilation techniques, used in 106 children, who were suspected of foreign body aspiration in the respiratory tract. In 62 children a foreign body was found. The youngest child was 8 months old and the oldest 13 years, with an age distribution peak in the 1 to 2 years age group. A predominance for the male sex (60%) was present. Foreign bodies of organic nature were found most frequently (80%), 39 of them consisting of peanuts. The bronchi were involved more often than the trachea and the foreign body was located more frequently at the right bronchus (38 pt). The children were ventilated initially with an intermittent oxygen jet injection technique, using a home made apparatus, but since 1978 with HFPPV, using the AGA Bronchovent. Induction of anesthesia was done with halothane and maintenance with etomidate infusion (10-20 micrograms/kg/min.) or thiopental increments (2 to 3 mg/kg). The technique so far used, proved to be satisfactory, specially since HFPPV is used. Few complications occurred. One child died during the bronchoscopic procedure and in an other child a tracheostomy had to be performed for extraction of the foreign body."
"The influence of seasonal greenhouse climate on the efficacy of predatory mites for thrips control was determined for potted chrysanthemum. Trials in controlled environment chambers, small-scale greenhouses and commercial greenhouses were conducted to determine which biological control agent-that is, Amblyseius swirskii Athias-Henriot or Neoseiulus cucumeris (Oudemans)-is more efficacious for control of western flower thrips, Frankliniella occidentalis (Pergande), in different seasons. Under simulated summer conditions, no differences were observed in the predation and oviposition rates of both predatory mites in the laboratory trials. However, small-scale greenhouse trials showed that A. swirskii performed better than N. cucumeris in summer (i.e., more efficacious thrips control, higher predator abundance and less overall damage to the crop). Under simulated winter conditions, laboratory trials demonstrated variable differences in predation rates of the two predatory mites. The small-scale greenhouse trials in winter showed no differences in thrips control and predatory mite abundance between the two predatory mites, but plants with A. swirskii had less damage overall. The results from the small-scale trials were validated and confirmed in commercial greenhouse trials. Overall, A. swirskii performed better in the summer and equally good or better (less damage overall) under winter conditions, whereas N. cucumeris is a more cost effective biological control agent for winter months."
"Restriction fragment length polymorphisms (RFLPs) in types I and III procollagen genes were studied in 62 scleroderma patients and 138 healthy controls. Allelic frequencies were determined for each RFLP, and comparisons were made between the 2 populations, stratifying them by race when appropriate. No statistically significant differences were observed for the frequencies of any of the RFLPs studied."
"The application of reversed phase liquid chromatography (RP-LC) hyphenated to inductively coupled plasma mass spectrometry (ICP-MS) for the accurate quantification of bio-molecules via covalently bound hetero atoms such as phosphorus is restricted, due to the known effects of increasing amounts of organic solvents on the ionization behavior of certain elements. An approach for the compensation of variations in the elemental response, due to changes in the solvent composition during the RP gradient separation of phosphorylated peptides is described, which includes the application of a second, matched reversed gradient, that is mixed post-column with the RP column outflow before entering the LC-ICP-MS interface. The experimental design allows the application of gradient separations, while the element-specific detection is carried out under isocratic conditions with a constant organic solvent intake into the plasma. A constant elemental response is a general pre-requisite for the application of ICP-MS for the absolute quantification of peptides via their hetero atom content, especially when no corresponding high purity standards are available or natural mono-isotopic hetero element tags are utilized. As complementary technique LC-electrospray ionization linear ion trap mass spectrometry (ESI-QTRAP-MS) has been used for peptide identification and to elucidate their phosphorus stoichiometry. Highly reproducible separations have been obtained with retention time and peak area RSDs of 0.05% and 7.6% (n=6), respectively. Detection limits for phosphorus of 6 microg L(-1) (6 pg absolute), have been realized, which corresponds to approximately 200 fmol of an average molecular weight, singly phosphorylated peptide. In addition an automatic routine for flow injection analysis (FIA) at the end of each chromatographic separation has been developed, to calibrate each chromatographic separation, which allows absolute quantification of the separated species, whenever their tag stoichiometry is known. Phosphorylated peptides as well as tryptic protein digests have been used as model compounds for method development and to demonstrate the applicability of the proposed setup for phosphopeptide quantification on the basis of simple inorganic phosphorus standards."
"Venous thromboembolism (VTE) is a major cause of in-hospital mortality. Several international guidelines provide thromboprophylaxis recommendations; however, guidelines adherence is missing worldwide. We evaluated the chest physicians' knowledge regarding VTE prophylaxis, using a systematically developed questionnaire. The Prophylaxis-foR-venOus-throMbOembolism-assessmenT-questionnairE (PROMOTE) questionnaire was developed using an algorithm encompassing the most important VTE prophylaxis topics and included 13 clinical scenarios. Responses were evaluated with reference to the eighth edition of American College of Chest Physicians guidelines for VTE prevention to assess thromboprophylaxis appropriateness. The questionnaires were distributed during the fourth International Congress on Pulmonary Disease, Intensive Care and Tuberculosis. From the 88 received questionnaires (response rate: 39.8%), 82 were acceptable (62 men, 20 women). The most commonly cited VTE risk factors were immobility (79.2%), surgery (68.2%), and cancer (60.9%). The mean correct response ratio to the questions was 67% [95% confidence interval (CI) 64-70%] with highest appropriateness ratios amongst cardiologists (77.1 ± 5.8%) and lowest ratios among thoracic surgeons (59.2 ± 5%). Physicians' specialty had a significant effect on the overall appropriateness (P = 0.04) and most of appropriateness subcategories. Thoracic surgeons had the lowest rate of over-prophylaxis (P = 0.02). Years passed from graduation were inversely associated with overall appropriateness (P = 0.006). Physicians with academic engagements had a higher overall appropriateness (P = 0.04). We found a wide gap between the guideline recommendations and the responses. PROMOTE is the first systematically developed questionnaire that addresses chest physicians' thromboprophylaxis knowledge and could be useful to strategies to improve VTE prophylaxis. Because of the dissimilar prophylaxis pitfalls of different specialists, distinct educational programs seem necessary to improve their knowledge of proper VTE prophylaxis."
"OBJECTIVES: Physical exercise up-regulates brain-derived neurotrophic factor (BDNF) in the brain and blood. However, there is yet no consensus about the adequate blood processing conditions to standardize its assessment. We aimed to find a reliable blood sample processing method to determine changes in BDNF due to exercise.DESIGN AND METHODS: Twelve healthy university students performed an incremental cycling test to exhaustion. At baseline, immediately after exercise, and 30 and 60 min of recovery, venous blood was drawn and processed under different conditions, i.e. whole blood, serum coagulated for 10 min and 24 h, total plasma, and platelet-free plasma. BDNF concentration was measured by ELISA.RESULTS: Exercise increased BDNF in whole blood and in serum coagulated for 24 h when corrected by hemoconcentration. We did not find effects of exercise on BDNF in serum coagulated for 10 min or in plasma samples. Plasma shows heterogeneous BDNF values in response to exercise that are not prevented when platelets are eliminated while homogeneous BDNF levels were found in whole blood or serum coagulated for 24 hour samples.CONCLUSIONS: In exercise studies, BDNF levels should be adjusted by hemoconcentration. Our data highlight the importance of blood sample selection since the differences between each one affect significantly the BDNF factor changes due to exercise."
"Transgenic mice were produced from DNA-injected embryos stored for 2 to 30 days in liquid nitrogen. Of the 500 zygotes collected from (C57BL/6 x CBA)F1 mice, 363 (73%) survived DNA injection into pronuclei and 246 (82%) morphologically normal 4- and 8-cell embryos were flushed from temporary recipients 48 h later. Of the 200 DNA-injected 8-cell embryos cryopreserved by vitrification in microdrops, 194 (97%) were recovered and 188 (94%) embryos were intact one hour after thawing. Of the 50 DNA-injected and frozen/thawed embryos, 48 (96%) developed to morulae or blastocysts within 30 h of in vitro culture. Transfer of 100 DNA-injected and cryopreserved 8-cell embryos into 20 day-1 recipients resulted in 47 young born. Two mice were transgenic."
"We report three cases of patients with Kock-pouches, who developed late complications related to old age. Based on these findings, the fate of aging patients with continent urinary diversions is discussed."
"There are little long-term clinical data regarding the safety and efficacy of using 2 drug-eluting stents (DESs) to treat coronary bifurcation lesions. We obtained clinical follow-up for 124 consecutive patients who underwent bifurcation stenting with 2 DESs. Major adverse cardiac events (MACEs) were defined as cardiac death, acute myocardial infarction (AMI), and target vessel revascularization (TVR). Sixty-four (52%) patients underwent ''crush,'' 42 (34%) patients underwent T stent, and 18 (14%) patients underwent kissing stent. Major adverse cardiac events were observed in 19 patients (17%) at 1 year: 6 (5%) AMI, 13 (12%) TVR, and no deaths, and 29 patients (26 %) at a mean follow-up of 22 months: 7 (6%) AMI, 21 (19%) TVR, and 1 (1%) death. No statistically significant risk factors for long-term MACEs were identified. It appears that treating bifurcation lesions with 2 DESs when necessary can be performed with an acceptable MACE rate."
"360 strains of Staphylococcus aureus isolated from various clinical specimens were subjected to bacteriophage typing. 247(68.6%) strains were typable. Among the typable strains 75(20.83%) belonged to phage group I, 45(12.5%) belonged to phage group III, 6(1.67%) belonged to phage group II and 14(3.89%) strains belonged to miscellaneous group. By far, the largest was the mixed group having 107(29.72%) strains. 113 strains (31.4%) were untypable. All the strains were tested for antibiotic sensitivity test. 287 (79.7%) were multiple drug resistant strains."
"This article presents an overview of ways to think about the brain and emotion and consider the role of evolution and expression in shaping the neural circuitry of affective processing. Issues pertaining to whether there are separate unique neural modules hard-wired for emotion processing or whether affective processing uses more generalized circuitry are considered. Relations between affect and cognition--specifically, memory--are examined from the perspective of overlapping neural systems. The role of individual differences in neural function in affective style are discussed, and the concepts of affective chronometry, or the time course of emotional responding and emotion regulation, are introduced. Finally, the extent to which certain emotional traits can be viewed as trainable skills is considered, and the relevance of work on neural plasticity to the skill framework is addressed. Data from a variety of sources using different types of measures is brought to bear on these questions, including neuroimaging and psychophysiological measures, studies of individuals of different ages ranging from early childhood to old age, studies of nonhuman primates, and observations of patients with localized brain damage. Emotions are viewed as varying in both type and dimension. Honoring brain circuitry in parsing the domain of affects will result in distinctions and differentiations that are not currently incorporated in traditional classification schemes."
"BACKGROUND: The (13)C-urea breath test ((13)C-UBT) is a noninvasive method for diagnosing Helicobacter pylori (H. pylori) infection. The aims of this study were to evaluate the diagnostic validity of the (13)C-UBT cutoff value and to identify influencing clinical factors responsible for aberrant results.METHODS: (13)C-UBT (UBiTkit; Otsuka Pharmaceutical, cutoff value: 2.5‰) results in the range 2.0‰ to 10.0‰ after H. pylori eradication therapy were compared with the results of endoscopic biopsy results of the antrum and body. Factors considered to affect test results adversely were analyzed.RESULTS: Among patients with a positive (13)C-UBT result (2.5‰ to 10.0‰, n = 223) or a negative (13)C-UBT result (2.0‰ to < 2.5‰, n = 66) after H. pylori eradication, 73 patients (34.0%) were false positive, and one (1.5%) was false negative as determined by endoscopic biopsy. The sensitivity, specificity, false-positive rate, and false-negative rate for a cutoff value of 2.5‰ were 99.3%, 47.1%, 52.9%, and 0.7%, respectively, and positive and negative predictive values of the (13)C-UBT were 67.3% and 98.5%, respectively. Multivariate analysis showed that a history of two or more previous H. pylori eradication therapies (OR = 2.455, 95%CI = 1.299-4.641) and moderate to severe gastric intestinal metaplasia (OR = 3.359, 95%CI = 1.572-7.178) were associated with a false-positive (13)C-UBT result.CONCLUSION: The (13)C-UBT cutoff value currently used has poor specificity for confirming H. pylori status after eradication, and this lack of specificity is exacerbated in patients that have undergone multiple prior eradication therapies and in patients with moderate to severe gastric intestinal metaplasia. In addition, the citric-free (13)C-UBT would increase a false-positive (13)C-UBT result."
"The immunomodulatory properties of NIM-76 have been described in this paper. Pre-treatment of rats with a single i.p. injection of NIM-76 resulted in an increase in polymorphonuclear (PMN) leukocytes with a concomitant decrease in lymphocyte counts. The immunomodulatory activity of NIM-76 was found to be concentration-dependent. At 120 mg/kg body weight, there was an enhanced macrophage activity and lymphocyte proliferation response, while the humoral component of immunity was unaffected. At higher concentrations of NIM-76 (300 mg/kg body weight), there was a stimulation of mitogen-induced lymphocyte proliferation, while macrophage activity remained unaffected. However, a fall in primary and secondary antibody titres was observed. The study indicates that NIM-76 acts through cell-mediated mechanisms by activating macrophages and lymphocytes."
"The Revised Illness Perception Questionnaire (IPQ-R) has been shown to assess illness perception reproducibly in primary cutaneous T-cell lymphomas (CTCL). Illness perception reflects patients' individual concepts of understanding and interpretation of the disease, influencing illness behaviour and health-related quality of life (HRQOL). This study investigated the clinical relevance of the relationships between illness perception, illness behaviour, and HRQOL in CTCL and cutaneous B-cell lymphomas (CBCL). A total of 92 patients completed the IPQ-R, the Scale for the Assessment of Illness Behavior (SAIB), and a skin-specific HRQOL tool (Skindex-29). Data on illness behaviour were not evidently related to illness perception, whereas illness perception was significantly associated with HRQOL. Both, IPQ-R and HRQOL results correlated with disease entity, stage, and socio-demographics. Only IPQ-R results provided practical information on patients' needs to train personal coping strategies. IPQ-R assessment in CTCL and CBCL might be a useful instrument to improve individual disease management."
"Aim To compare the characteristics of mental health and physical health participants attending an exercise referral scheme (ERS) and investigate associations with their adherence to exercise.BACKGROUND: While people referred to an ERS with a mental health diagnosis have similar initial rates of uptake as physical health participants, they are more likely to drop out. Comparisons of the groups to understand their differences and how these might impact on their adherence have been limited by the typically low numbers of mental health referrals in many schemes.METHODS: Retrospective analysis of a participant cohort. Data were extracted on all participants enrolled over a 12- month period (n = 701) and included measurements at baseline, mid-point (13 weeks) and end of programme (20-26 weeks). Differences were explored between the mental health (n=141) and physical health (n=560) subcohorts, and between adherers and non-adherers in each group. Binomial logistic regression estimated the effect of group-level factors associated with adherence. Findings Mental health referrals were more likely to be younger, White and unemployed, and had a lower mean body mass index and lower proportion of participants with high blood pressure. They were also more likely to drop out. While occupation was associated with exercise adherence among the physical health group, no predictive factors were identified in the mental health group.CONCLUSION: Participants referred for mental health disorders are more likely to drop out of exercise referral schemes than those with physical health problems. While no factors were found to be predictive of their exercise adherence, an understanding of their distinguishing characteristics and attendance behaviour can guide in making better referral decisions concerning them and planning more appropriately tailored support."
"Testicular germ cell tumors (TGCTs) of adolescents and adults have been shown to contain proteins of the human endogenous retrovirus type K family. In a recent study, expression of these retroviral sequences was confirmed using in situ hybridization, which also showed expression in carcinoma in situ, the precursor of all TGCTs. Because of the clinical significance of a test for early diagnosis of TGCTs, we studied whether expression of human endogenous retrovirus type K genes could be an informative parameter. Therefore, we investigated TGCTs of various histologies and testicular parenchyma with and without carcinoma in situ using reverse transcription-polymerase chain reaction for expression of the gag, env, and prt genes. The gag and prt genes were expressed in all samples tested. The env transcripts were not found in TGCTs showing somatic differentiation only but could be detected in most normal testicular parenchyma samples. Therefore, detection of human endogenous retrovirus type K transcripts cannot be used for early diagnosis of TGCTs. Simultaneous expression of multiple gag sequences was found both in normal parenchyma and TGCTs, and we demonstrated that expression of gag sequences with an extra G, necessary to generate a functional protein, was not limited to TGCTs."
"Phosphatidylcholine (PC) metabolism stimulated by caerulein (Cae), a cholecystokinin analogue, was investigated in rat pancreatic acini prelabeled with [3H]choline or [3H]-myristic acid. Both labels were incorporated mostly into PC. An inhibition of choline incorporation into PC was first observed in response to Cae (100 and 500 pM) stimulation, as indicated by reduced [3H]choline incorporation into trichloroacetic acid-precipitable material. Whereas choline incorporation was reduced in PC, Cae (500 pM) significantly increased [3H]choline metabolites release in the incubation medium. Separation of these metabolites by thin-layer chromatography showed that approximately 90% of the labeled products released into the medium was phosphocholine; however, Cae caused significant increases of [3H]choline release after 5, 15, and 30 min. In response to Cae, manoalide, a phospholipase C (PLC) inhibitor, totally prevented phosphocholine release into the medium but did not affect choline release. Staurosporine, a protein kinase C inhibitor, did not influence basal and Cae-induced choline release. In cells prelabeled with [3H]myristic acid, Cae stimulated within 5 min a rapid increase in intracellular [3H]phosphatidic acid (PA) levels in the presence of the PA phosphohydrolase inhibitor, propranolol; this PA production was further increased after 15 and 30 min of stimulation. The time course of [3H]PA formation in the presence of propranolol was similar to that of choline release in the medium. Staurosporine partially blocked PA accumulation stimulated by Cae after 30 min. In contrast, manoalide significantly reduced basal PA accumulation but did not prevent its production in response to Cae. In the presence of ethanol, Cae also significantly stimulated above control values the formation of [3H]phosphatidylethanol. These data indicate that Cae-induced PC hydrolysis in rat pancreatic acini is mediated mostly by phospholipase D (PLD) to produce PA and choline; they suggest a direct action of Cae on PLD activation, an effect independent of PLC activation."
"INTRODUCTION: Endodontic therapy is often complicated and technically demanding. The aim of this study was to develop a reproducible biomimetic root canal model for pre-clinical and postgraduate endodontic training.MATERIAL AND METHODS: A specific ceramic shaping technique (3D printing and slip casting of a root canal mould) was developed to reproduce canal systems with the desired shape and complexity using a microporous hydroxyapatite (HAp)-based matrix. The microstructural morphology, pore size and porosity, as well as the Vickers microhardness of the ceramic simulators (CS) were assessed and were compared with natural dentin and commercial resin blocks. The reproducibility of the root canal shapes was assessed using the Dice-S?rensen similarity index. Endodontic treatments, from refitting the access cavity to obturation, were performed on the CS. Each step was controlled by radiography.RESULTS: Many properties of the CS were similar to those of natural dental roots, including the mineral component (HAp), porosity (20%, porous CS), pore size (3.4 ± 2.6 ìm) and hardness (120.3 ± 18.4 HV).DISCUSSION: We showed that it is possible to reproduce the radio-opacity of a tooth and variations in root canal morphology. The endodontic treatments confirmed that the CS provided good tactile sensation during instrumentation and displayed suitable radiological behaviour.CONCLUSIONS: This novel anatomic root canal simulator is well suited for training undergraduate and postgraduate students in endodontic procedures."
"A rapid and reliable method to determine deoxynivalenol (DON) in wheat, barley, and malt is described. Samples are extracted with acetonitrile-water (84 + 16). Extracts are eluted through a C18-alumina (1 + 3) column, evaporated to dryness, and derivatized with trimethylsilylimidazole-trimethylchlorosilane (100 + 1). DON is identified and quantitated by capillary gas chromatography with electron capture detection. This method can quantitate DON levels from 0.2 to 40 ppm. Multiple analyses of wheat spiked at 2 ppm and of naturally contaminated wheat, malt, and barley resulted in coefficients of variation of 5.1, 5.1, 6.0, and 9.8%, respectively. Recoveries of DON spikes at 3 levels were 94-100% for wheat, 100-105% for barley, and 100-105% for malt. Results for wheat sample analyzed with this procedure (1.9 +/- 0.1 ppm DON) compared well with results for the same sample analyzed by enzyme-linked immunosorbent assay (1.9 ppm DON) and by liquid chromatography (1.7 ppm DON)."
"A new 3D parallel magnetic resonance imaging (MRI) method named Generalized Unaliasing Incorporating Support constraint and sensitivity Encoding (GUISE) is presented. GUISE allows direct image recovery from arbitrary Cartesian k-space trajectories. However, periodic k-space sampling patterns are considered for reconstruction efficiency. Image recovery methods such as 2D SENSE (SENSitivity Encoding) and 2D CAIPIRINHA (Controlled Aliasing In Parallel Imaging Results IN Higher Acceleration) are special instances of GUISE where specific restrictions are placed on the k-space sampling patterns used. It is shown that the sampling pattern has large impacts on the image reconstruction error due to noise. An efficient sampling pattern design method that incorporates prior knowledge of object support and coil sensitivity profile is proposed. It requires no experimental trials and could be used in clinical imaging. Comparison of the proposed sampling pattern design method with 2D SENSE and 2D CAIPIRINHA are made based on both simulation and experiment results. It is seen that this new adaptive sampling pattern design method results in a lower noise level in reconstructions due to better exploitation of the coil sensitivity variation and object support constraint. In addition, elimination of the non-object region from reconstruction potentially allows an acceleration factor higher than the number of receiver coils used."
"Myocardial perfusion studies suffer from artifacts caused by misalignment of the transmission and emission data due to the influences of voluntary and involuntary patient motion. Regardless of 68Ge or respiratory-averaged CT based attenuation correction and good patient cooperation, approximately 21% of perfusion studies exhibit artifacts arising from misalignment that cannot be corrected by manipulating the attenuation acquisition protocol. This misalignment, termed cardiac drift, is caused by slow-moving abdominal cavity contents that reposition the heart in the thorax and appear as myocardial uptake overlying the left CT lung in fused PET/CT images. This study evaluates three postimaging registration techniques to correct PET/CT misalignment by altering the transmission map to match myo-cardial uptake. Simulated misalignment studies were performed with a cardiac torso phantom filled with [18F]FDG at 10:1 myocardium/background. An air-filled saline bag affixed to the medial left lung surface served as a distensible lung. An initial CT acquisition was followed by successive PET acquisitions consisting of small displacements of the cardiac insert into the left lung. Phantom transmission scans were aligned to the myocardial uptake in the emission scans by applying 1) full rigid-body translations and rotations, 2) rigid-body restricted to medial / lateral and superior / inferior translation, or 3) an emission-driven method that adds myocardial tissue to the transmission scan. These methods were also applied to 10 low-likelihood coronary artery disease (CAD) patients showing signs of cardiac drift. Full rigid-body registration showed significant over-correction (p < 0.004) of activity concentrations in the artifact areas of the phantom data due the relocation of highly attenuating structures (i.e., spine). Inaccurate regional activity distributions were also observed as streaks extending from the spine and these results were replicated in the patient population. There was no significant difference between the true phantom activity concentration after correction with the emission-driven method. Misalignment corrected with the rigid-body registration results in an increase in activity concentration but fails to accurately recover the true concentration. These data suggest that a nonlinear image registration approach such as an emission-driven method results in a more uniform activity distribution throughout the myocardium, and is more appropriate for addressing the cardiac drift misalignment problem."
"OBJECTIVE: In two preliminary studies, pindolol produced robust results in hastening clinical response to antidepressant drugs in depressed patients. Validity of those pilot studies was limited by use of an open-label, unblinded study design, and so the authors conducted a double-blind, placebo-controlled trial to assess the effectiveness of pindolol in hastening response to fluoxetine.METHOD: Drug-free outpatients with major depression were concurrently treated with fluoxetine (20 mg/day) and either placebo or pindolol (5.0 mg b.i.d. or 2.5 mg t.i.d.), for 6 weeks, in a randomized, double-blind manner. After 6 weeks, all patients received fluoxetine and placebo and were followed for 3 further weeks in a single-blind manner.RESULTS: Forty-three patients completed at least 1 week of the protocol. Rates of partial remission after 2 weeks of treatment with fluoxetine and either pindolol or placebo were 17% (four of 23 patients) and 20% (four of 20 patients), respectively. At study completion, 65% of the patients (N = 28) demonstrated at least a partial remission, and there was no difference between treatment groups. The pindolol group, but not the placebo group, demonstrated significant reductions in blood pressure and pulse rate. The average time to remission and the rates of attrition, overall response, and side effects were similar in the two groups.CONCLUSIONS: These findings do not support the efficacy of pindolol in hastening clinical response in patients treated with fluoxetine."
"106 burned patients were divided into two groups: group L consisted of 57 patients with burns larger than 30% TBSA, and group S consisted of 49 patients with burns smaller than 30% TBSA. The contents of Zn, Cu, Fe, Ca, Mg in serum, urine and blister fluid were determined on 1, 2, 3, 7, 14, 21, 28 days after burn injury. The results indicated that all of the elements in serum were decreased except serum Fe on first day, and the levels were much lower in group L than in group S. The contents of Zn, Cu, Fe in urine were increased significantly and Ca, Mg in urine decreased. The contents of Zn, Fe, Ca in blister fluid were similar to normal values of serum, and Cu, Mg in blister fluid were less than that of serum. The studies indicated that the decrease in contents of Zn, Cu, Fe, Ca, Mg after burn injury was propably related to loss of these elements from urine and wound."
"In order to investigate palatability influences on the ad lib eating behavior of free-living humans, 564 participants were paid to maintain food intake diaries for 7 days. They recorded their intake along with a global rating of the palatability of the entire meal on a seven-point scale. It was found that most meals that are self-selected are palatable and that only 9.3% are rated as unpalatable. Meals that were highest in palatability were 44% larger than meals that were low in palatability, but palatability only accounted for around 4% of the variance in meal sizes. Multiple regression demonstrated that palatability appears to act on intake independent of the levels of other influential factors. These results were very similar to those observed for the French and suggest that palatability operates similarly on intake regardless of culture. Palatability appears to be an influence on the amounts ingested by free-living humans in their natural environments but appears to be only one of many influential factors and accounts for only a small proportion of the variance in intake."
"Reactive hyperemia (RH) in the forearm skin after an arterial occlusion of 5 min was investigated in 29 ICU patients and 17 age-matched healthy control subjects using a transcutaneous PO2/PCO2 electrode heated to 37 degrees C. There was no difference in preocclusive baseline PtCO2 between patients (8 +/- 5 torr) and control subjects (8 +/- 4 torr). Patients exhibited a significantly decreased RH (16 +/- 9 torr) in comparison with control subjects (26 +/- 8 torr) and a diminished CO2 elimination. There was no correlation between the RH response and the oxygen extraction ratio, Hgb concentration, and hemodynamic and blood gas variables in patients. In contrast with control subjects, there was a significant correlation between CO2 elimination from the skin and the amount of RH in patients. The finding of a diminished RH in the patients was not related to a specific disease but correlated with the degree of physiologic derangement as assessed by the APACHE II score."
"The general responses of microorganisms to environmental onslaughts are modulated by altering the gene expression pattern to reduce damage in the cell and produce compensating stress responses. The present study attempts to unravel the response of the Gram-positive Exiguobacterium sp. PS NCIM 5463 in the presence of [As(III)] and arsenate [As(V)] using comparative proteomics via two-dimension gel electrophoresis (2-DE) coupled with identification of proteins using matrix-assisted laser desorption/ionisation (MALDI-TOF/MALDI-TOF/TOF). Out of 926 Coomassie-stained proteins, 45 were differentially expressed (p < 0.05). Considering the resolution and abundance level, 24 spots (peptides) were subjected to MALDI analysis, identified and categorised into several functional categories, viz., nitrogen metabolism, energy and stress regulators, carbohydrate metabolism, protein synthesis components and others. A functional role of each protein is discussed in Exiguobacterium sp. PS 5463 under arsenic stress and validated at their transcript level using a quantitative real-time polymerase chain reaction. Unlike previous reports that unravel the responses toward arsenic stress in Gram-negative organisms, the present study identified new proteins under arsenic stress in a Gram-positive organism, Exiguobacterium sp. PS NCIM 5463, which could elucidate the physiology of organisms under arsenic stress."
"Paroxysms of atrial tachycardia are oftentimes associated with polyuria. The plasma levels of the potent diuretic hormone, atrial natriuretic peptide (ANP), are elevated during episodes of atrial tachycardia, suggesting that ANP may play a role in mediating the diuresis. The mechanism of enhanced ANP secretion associated with atrial tachycardia is not known. We examined the effect of altering the pacing frequency of isolated left rat atria on ANP secretion. Atria were suspended between an electrode and hook connected to a force transducer and superfused with medium 199. The ANP content of the superfusate was measured by radioimmunoassay. Increasing the frequency of pacing from 2 to 4 Hz resulted in a 46 +/- 5% (means +/- SE, n = 9) rise in immunoreactive ANP secretion above base line (P less than 0.01). Lowering the frequency from 4 to 2 Hz lowered immunoreactive ANP secretion by 36 +/- 3% (n = 6) relative to base line (P less than 0.02). Incremental increases in the pacing frequency from 2 to 8 Hz resulted in a continual rise in immunoreactive ANP with a peak increase of 191 +/- 6% of base line (n = 8) at 8 Hz. To examine the possibility that release of norepinephrine or acetylcholine from endogenous nerves mediated this effect, the atria were superfused with the combination of 0.1 microM propranolol, 1.0 microM phentolamine, and 10 microM atropine. The concentrations of the antagonists were 125-fold or higher than the dissociation constant for binding to receptors.(ABSTRACT TRUNCATED AT 250 WORDS)"
"BACKGROUND: We aimed to evaluate whether atrial electromechanical delay (AEMD) measured by tissue Doppler imaging (TDI), which is an indicator for structural and electrical remodelling of the atria, is prolonged in patients with active or inactive acromegaly, or both, compared with a control group.METHODS: A total of 34 patients with acromegaly (18 active/16 inactive) and 35 patients as a control group were enrolled. Both intra- and inter-AEMD were calculated by TDI. The correlation between clinical variables and AEMD were analyzed.RESULTS: Both inter-AEMD and right and left intra-AEMD were prolonged in patients with acromegaly compared with the control group (P < 0.001, P < 0.001, and P = 0.004, respectively). Also, patients with active acromegaly showed higher inter-AEMD and right intra-AEMD compared with patients with inactive acromegaly (P < 0.05). There was no significant difference in left intra-AEMD between patients with active acromegaly and those with inactive acromegaly (P = 0.977). The growth hormone and insulin-like growth factor (IGF-1) levels positively correlated with inter-AEMD (r = 0.577; P < 0.001; r = 0.614; P < 0.001, respectively). Additionally, we found that inter-AEMD was significantly and positively correlated with relationship between maximal values of passive mitral inflow (E, PW-Doppler) and lateral early diastolic mitral annular velocities (e', TDI) (r = 0.316; P = 0.008). Only the serum IGF-1 level was independently associated with inter-AEMD in multivariate linear regression analysis (â = 0.500; P = 0.011).CONCLUSIONS: Our study findings showed that both inter- and intra-AEMD are prolonged in patients with acromegaly. Also, AEMD was observed to be more prolonged in patients with active acromegaly than in those with inactive acromegaly. IGF-1 was an independent predictor of inter- AEMD in patients with acromegaly. Being a noninvasive, inexpensive, and simple technique, AEMD may be used as an indicator for atrial electrical and structural remodelling in patients with acromegaly."
"Wood frogs, Rana sylvatica, can undergo prolonged periods of whole body freezing during winter, locking as much as 65-70% of total body water into extracellular ice and imposing both anoxia and dehydration on their cells. Metabolic rate depression (MRD) is an adaptation used by R. sylvatica to survive these environmental stresses, where a finite amount of ATP generated through anaerobic metabolism is directed towards maintaining pro-survival functions, while most ATP-expensive cellular processes are temporarily reduced in function. Pyruvate dehydrogenase (PDH) is a vital metabolic enzyme that links anaerobic glycolysis to the aerobic TCA cycle and is an important regulatory site in MRD. PDH enzymatic activity is regulated via reversible protein phosphorylation in response to energetic demands of cells. This study explored the posttranslational regulation of PDH at three serine sites (S232, S293, S300) on the catalytic E1á subunit along with protein expression of four pyruvate dehydrogenase kinases (PDHK1-4) in response to 24 h Freezing, 8 h Thaw, 24 h Anoxia, and 4 h Recovery in the liver and skeletal muscle of R. sylvatica using Luminex multiplex technology and western immunoblotting. Overall, inhibitory regulation of PDH was evident during 24 h Freezing and 24 h Anoxia, which could indicate a notable reduction in glycoytic flux and carbon entry into the tricarboxylic acid cycle as part of MRD. Furthermore, the expression of PDHK1-4 and phosphorylation of PDH at S232, S293, and S300 were highly tissue and stress-specific, indicative of how different tissues respond differently to stress within the same organism."
"An enantioselective redox-relay oxidative Heck arylation of 1,1-disubstituted alkenes to construct â-stereocenters was developed using a new pyridyl-oxazoline ligand. Various 1,2-diaryl carbonyl compounds were readily obtained in moderate yield and good to excellent enantioselectivity. Additionally, analysis of the reaction outcomes using multidimensional correlations revealed that enantioselectivity is tied to specific electronic features of the 1,1-disubstituted alkenol and the extent of polarizability of the ligand."
"Adverse (tumor-enhancing) effects of cyclophosphamide at moderate dose (50 mg/kg) given therapeutically were confirmed on the intraperitoneally implanted Lewis lung carcinoma (LLC) in allogeneic DBA/2 and BALB/c mice. It was recently demonstrated that antitumor effects of five standard drugs (actinomycin D, adriamycin, BCNU, 5-fluorouracil and methotrexate) were abolished or diminished when cyclophosphamide (50 mg/kg) was therapeutically combined with such drugs against intraperitoneally implanted LLC both in syngeneic C56BL/6 and allogeneic DBA/2 and BALB/c mice, while the effects of three drugs (cis-diamminedichloroplatinum, 5-thioguanine and vincristine) were not affected by the cyclophosphamide therapy. It is suggested that cyclophosphamide therapy at adjusted lower doses in man might not only be effective but also have a risk to enhance tumor growth and diminish the beneficial effects of other drugs in combination chemotherapy."
"Co-infection of malaria and tuberculosis, although not thoroughly investigated, has been noted. With the increasing prevalence of tuberculosis in the African region, wherein malaria is endemic, it is intuitive to suggest that the probability of co-infection with these diseases is likely to increase. To avoid the issue of drug-drug interactions when managing co-infections, it is imperative to investigate new molecules with dual activities against the causal agents of these diseases. To this effect, a small library of quinolone-thiosemicarbazones was synthesised and evaluated in vitro against Plasmodium falciparum and Mycobacterium tuberculosis, the causal agents of malaria and tuberculosis, respectively. The compounds were also evaluated against HeLa cells for overt cytotoxicity. Most compounds in this series exhibited activities against both organisms, with compound 10, emerging as the hit; with an MIC90 of 2 µM against H37Rv strain of M. tuberculosis and an IC50 of 1 µM against the 3D7 strain of P. falciparum. This study highlights quinolone-thiosemicarabazones as a class of compounds that can be exploited further in search of novel, safe agents with potent activities against both the causal agents of malaria and tuberculosis."
"Retinal dystrophy is an inherited, heterogeneous, chronic and progressive disorder of visual functions. The mutations of patients with autosomal recessive retinal retinopathy cone-and-rod dysfunction and macular dystrophy have not been well described in the Chinese population. In this study, a three-generation Chinese retinal dystrophy family was recruited. Ophthalmic examinations were performed. Targeted next generation sequencing (TGS) was used to identify causative genes, and Sanger sequencing was conducted to verify candidate mutations and co-segregation. Reverse transcription (RT)-PCR was applied to investigate the spatial and temporal expression patterns of cdhr1 gene in mouse. A novel, homozygous, deleterious and nonsense variant (c.T1641A; p.Y547*) in the CDHR1 gene was identified in the family with autosomal recessive retinal dystrophy, which was co-segregated with the clinical phenotypes in this family. RT-PCR analysis revealed that cdhr1 is ubiquitously expressed in eye, particularly very high expression in retina; high expression in lens, sclera, and cornea; and high expression in brain. In conclusion, our study is the first to indicate that the novel homozygous variant c.T1641A (p.Y547*) in the CHDR1 gene might be the disease-causing mutation for retinal dystrophy in our patient, extending its mutation spectrums. These findings further the understanding of the molecular pathogenesis of this disease and provide new insights for diagnosis as well as new implications for genetic counselling."
"Bile acids have been used for the first time as the building block for the construction of dendritic units. Orthogonally functionalized 7-deoxycholic and cholic acid derivatives were synthesized. The construction of a bile acid heptamer, a nonamer, and a decamer using the convergent strategy are described in detail. Chromatographic, spectral, and optical properties of these molecules have been investigated. Molecular modeling suggests that these molecules have globular shapes with nanometric dimensions."
"Phosphatidylinositol-3-kinase pathway is constitutively activated in chronic lymphocytic leukemia mainly due to microenvironment signals, including stromal cell interaction and CXCR4 and B-cell receptor activation. Because of the importance of phosphatidylinositol-3-kinase signaling in chronic lymphocytic leukemia, we investigated the activity of the NVP-BKM120, an orally available pan class I phosphatidylinositol-3-kinase inhibitor. Sensitivity to NVP-BKM120 was analyzed in chronic lymphocytic leukemia primary samples in the context of B-cell receptor and microenvironment stimulation. NVP-BKM120 promoted mitochondrial apoptosis in most primary cells independently of common prognostic markers. NVP-BKM120 activity induced the blockage of phosphatidylinositol-3-kinase signaling, decreased Akt and FoxO3a phosphorylation leading to concomitant Mcl-1 downregulation and Bim induction. Accordingly, selective knockdown of BIM rescued cells from NVP-BKM120-induced apoptosis, while the kinase inhibitor synergistically enhanced the apoptosis induced by the BH3-mimetic ABT-263. We also found NVP-BKM120 to inhibit B-cell receptor- and stroma-dependent Akt pathway activation, thus sensitizing chronic lymphocytic leukemia cells to bendamustine and fludarabine. Furthermore, NVP-BKM120 down-regulated secretion of chemokines after B-cell receptor stimulation and inhibited cell chemotaxis and actin polymerization upon CXCR4 triggering by CXCL12. Our findings establish that NVP-BKM120 effectively inhibits the phosphatidylinositol-3-kinase signaling pathway and disturbs the protective effect of the tumor microenvironment with the subsequent apoptosis induction through the Akt/FoxO3a/Bim axis. We provide here a strong rationale for undertaking clinical trials of NVP-BKM120 in chronic lymphocytic leukemia patients alone or in combination therapies."
"Cerebral glucose uptake and perfusion are accepted as tightly coupled measures of energy utilization in both normal and diseased brain. The coupling of brain electrical activity to perfusion has been demonstrated, however, only in the presence of chronic brain disease. Very few studies have examined the relationship between cerebral electrical activity and energy utilization in normal brain tissue. To clarify this relationship, we performed 33 H2(15)O-positron emission tomography (PET) scans in six normal subjects both at rest and during a simple motor task, and acquired surface-recorded quantitative electroencephalogram (QEEG) data simultaneously with isotope injection. We examined the associations between cerebral perfusion directly underlying each recording electrode and three QEEG measures (absolute power, relative power, and cordance). All EEG measures had moderately strong coupling with perfusion at most frequency bands, although the directions of the associations differed from those previously reported in subjects with stroke or dementia. Of the three QEEG measures examined, cordance had the strongest relationship with perfusion (multiple R2 = 0.58). Cordance and PET were equally effective in detecting lateralized activation associated with the motor task, while EEG power did not detect this activation. Electrodes in the concordant state had a significantly higher mean perfusion than those in the discordant state. These results indicate that normal brain electrical activity has a moderately strong association with cerebral perfusion. Cordance may be the most useful QEEG measure for monitoring cerebral perfusion in subjects without chronic brain disease."
"PURPOSE: There is evidence that some cases of patients with dementia with Lewy bodies (DLB) can demonstrate Alzheimer disease (AD) like reduced glucose metabolism without amyloid deposition. The aim of this study was to clarify whether regional hypometabolism is related to amyloid deposits in the DLB brain and measure the degree of regional hypometabolism.METHODS: Ten consecutive subjects with DLB and 10 AD patients who underwent both Pittsburgh compound B (PiB)-PET and (18)F-fluoro-2-deoxyglucose (FDG)-PET were included in this study. Regional standardized uptake value ratio (SUVR)s normalised to cerebellar cortices were calculated in the FDG- and PiB-PET images.RESULTS: All AD patients and five DLB patients showed amyloid deposits (PiB positive). In the DLB group the parietotemporal and occipital metabolism were significantly lower than those in the AD group but there was no difference between the posterior cingulate hypometabolism between DLB and AD groups. There were no differences in regional glucose metabolism between PiB positive and negative DLB patients.CONCLUSIONS: In the DLB brain, it is suggested that decreased regional glucose metabolism is unrelated to amyloid deposits, although the hypometabolic area overlaps with the AD hypometabolic area and the degree of parietotemporal and occipital hypometabolism in DLB brain is much larger than that in AD brain."
"Rapid advances in positional cloning studies have identified most of the genes on the human Y chromosome, thereby providing resources for studying the expression of its genes in prostate cancer. Using a semiquantitative reverse transcription-polymerase chain reaction (RT-PCR) procedure, we had examined the expression of the Y chromosome genes in a panel of prostate samples diagnosed with benign prostatic hyperplasia (BPH), low and/or high grade carcinoma, and the prostatic cell line, LNCaP, stimulated by androgen treatment. Results from this expression analysis of 31 of the 33 genes, isolated so far from the Y chromosome, revealed three types of expression patterns: i) specific expression in other tissues (e.g., AMELY, BPY1, BPY2, CDY, and RBM); ii) ubiquitous expression among prostate and control testis samples, similar to those of house-keeping genes (e.g., ANT3, XE7,ASMTL, IL3RA, SYBL1, TRAMP, MIC2, DBY, RPS4Y, and SMCY); iii) differential expression in prostate and testis samples. The last group includes X-Y homologous (e.g., ZFY, PRKY, DFFRY, TB4Y, EIF1AY, and UTY) and Y-specific genes (e.g., SRY, TSPY, PRY, and XKRY). Androgen stimulation of the LNCaP cells resulted in up-regulation of PGPL, CSFR2A, IL3RA, TSPY, and IL9R and down regulation of SRY, ZFY, and DFFRY. The heterogeneous and differential expression patterns of the Y chromosome genes raise the possibility that some of these genes are either involved in or are affected by the oncogenic processes of the prostate. The up- and down-regulation of several Y chromosome genes by androgen suggest that they may play a role(s) in the hormonally stimulated proliferation of the responsive LNCaP cells."
"Variants of the pyruvate dehydrogenase subunit (E1; EC ) of the Escherichia coli pyruvate dehydrogenase multienzyme complex with Y177A and Y177F substitutions were created. Both variants displayed pyruvate dehydrogenase multienzyme complex activity at levels of 11% (Y177A E1) and 7% (Y177F E1) of the parental enzyme. The K(m) values for thiamin diphosphate (ThDP) were 1.58 microm (parental E1) and 6.65 microm (Y177A E1), whereas the Y177F E1 variant was not saturated at 200 microm. According to fluorescence studies, binding of ThDP was unaffected by the Tyr(177) substitutions. The ThDP analogs thiamin 2-thiazolone diphosphate (ThTDP) and thiamin 2-thiothiazolone diphosphate (ThTTDP) behaved as tight-binding inhibitors of parental E1 (K(i) = 0.003 microm for ThTDP and K(i) = 0.064 microm for ThTTDP) and the Y177A and Y177F variants. This analysis revealed that ThTDP and ThTTDP bound to parental E1 via a two-step mechanism, but that ThTDP bound to the Y177A variant via a one-step mechanism. Binding of ThTDP was affected and that of ThTTDP was unaffected by substitutions at Tyr(177). Addition of ThDP or ThTDP to parental E1 resulted in similar CD spectral changes in the near-UV region. In contrast, binding of ThTTDP to either parental E1 or the Y177A and Y177F variants was accompanied by the appearance of a positive band at 330 nm, indicating that ThTTDP was bound in a chiral environment. In combination with x-ray structural evidence on the location of Tyr(177), the kinetic and spectroscopic data suggest that Tyr(177) has a role in stabilization of some transition state(s) in the reaction pathway, starting with the free enzyme and culminating with the first irreversible step (decarboxylation), as well as in reductive acetylation of the dihydrolipoamide acetyltransferase component."
"This prospective investigation derived a prediction model for identifying risk of incident lung cancer among patients with visible lung nodules identified on computed tomography (CT). Among 2,924 eligible patients referred for evaluation of a pulmonary nodule to the Stony Brook Lung Cancer Evaluation Center between January 1, 2002 and December 31, 2015, 171 developed incident lung cancer during the observation period. Cox proportional hazard models were used to model time until disease onset. The sample was randomly divided into discovery (n = 1,469) and replication (n = 1,455) samples. In the replication sample, concordance was computed to indicate predictive accuracy and risk scores were calculated using the linear predictions. Youden index was used to identify high-risk versus low-risk patients and cumulative lung cancer incidence was examined for high-risk and low-risk groups. Multivariable analyses identified a combination of clinical and radiologic predictors for incident lung cancer including ln-age, ln-pack-years smoking, a history of cancer, chronic obstructive pulmonary disease, and several radiologic markers including spiculation, ground glass opacity, and nodule size. The final model reliably detected patients who developed lung cancer in the replication sample (C = 0.86, sensitivity/specificity = 0.73/0.81). Cumulative incidence of lung cancer was elevated in high-risk versus low-risk groups [HR = 14.34; 95% confidence interval (CI), 8.17-25.18]. Quantification of reliable risk scores has high clinical utility, enabling physicians to better stratify treatment protocols to manage patient care. The final model is among the first tools developed to predict incident lung cancer in patients presenting with a concerning pulmonary nodule."
"Inflammation is a hallmark of several disease states ranging from neurodegeneration to sepsis but is also implicated in physiological processes like ageing. Non-resolving inflammation and prolonged neuroinflammation are unclear processes implicated in several conditions, including ageing. In this study we studied the long-term effects of endotoxemia, as systemic lipopolysaccharide (LPS) injection, focusing on the role of astrocyte activation and cytokine release in the brain of aged rats. A single dose of LPS (2 mg/kg) or 0.9% saline was injected intraperitoneally in aged rats. Levels of pro-inflammatory cytokines (TNFá and IL-1â) and NF-êB p65 activation were measured systemically and in hippocampal tissue. Astrocytes and cytokines release in the CNS were detected via double immunofluorescence staining at different time-points up to day 30. Serum levels of TNFá and IL-1â were significantly increased acutely after 30 minutes (p<0.001) and up to 6 hours (p<0.001) following LPS-injection. Centrally, LPS-treated rats showed up-regulated mRNA expression and protein levels of pro-inflammatory cytokines in the hippocampus. These changes associated with astrogliosis in the hippocampus dentate gyrus (DG), IL-1â immunoreactivity and elevated NF-êB p65 expression up to day 30 post LPS exposure. Overall, these data demonstrate that LPS induces prolonged neuroinflammation and astrocyte activation in the hippocampus of aged rats. Hippocampal NF-êB p65 and excessive astrocytes-derived IL-1â release may play a pivotal role in regulating long-lasting neuroinflammation."
"Some chemicals in the environment disrupt thyroid hormone (TH) systems leading to alterations in organism development, but their effect mechanisms are poorly understood. In fish, this has been limited by a lack of fundamental knowledge on thyroid gene ontogeny and tissue expression in early life stages. Here we established detailed expression profiles for a suite of genes in the hypothalamic-pituitary-thyroid (HPT) axis of zebrafish (Danio rerio) between 24-120 h post fertilisation (hpf) and quantified their responses following exposure to 3,3',5-triiodo-L-thyronine (T3) using whole mount in situ hybridisation (WISH) and qRT-PCR (using whole-body extracts). All of the selected genes in the HPT axis demonstrated dynamic transcript expression profiles across the developmental stages examined. The expression of thyroid receptor alpha (thraa) was observed in the brain, gastrointestinal tract, craniofacial tissues and pectoral fins, while thyroid receptor beta (thrb) expression occurred in the brain, otic vesicles, liver and lower jaw. The TH deiodinases (dio1, dio2 and dio3b) were expressed in the liver, pronephric ducts and brain and the patterns differed depending on life stage. Both dio1 and dio2 were also expressed in the intestinal bulb (96-120 hpf), and dio2 expression occurred also in the pituitary (48-120 hpf). Exposure of zebrafish embryo-larvae to T3 (30 and 100 ìg L-1) for periods of 48, 96 or 120 hpf resulted in the up-regulation of thraa, thrb, dio3b, thyroid follicle synthesis proteins (pax8) and corticotropin-releasing hormone (crhb) and down-regulation of dio1, dio2, glucuronidation enzymes (ugt1ab) and thyroid stimulating hormone (tshb) (assessed via qRT-PCR) and responses differed across life stage and tissues. T3 induced thraa expression in the pineal gland, pectoral fins, brain, somites, gastrointestinal tract, craniofacial tissues, liver and pronephric ducts. T3 enhanced thrb expression in the brain, jaw cartilage and intestine, while thrb expression was suppressed in the liver. T3 exposure suppressed the transcript levels of dio1 and dio2 in the liver, brain, gastrointestinal tract and craniofacial tissues, while dio2 signalling was also suppressed in the pituitary gland. Dio3b expression was induced by T3 exposure in the jaw cartilage, pectoral fins and brain. The involvement of THs in the development of numerous body tissues and the responsiveness of these tissues to T3 in zebrafish highlights their potential vulnerability to exposure to environmental thyroid-disrupting chemicals."
"The effects of the infusion of melatonin (MT) and/or of pinealectomy upon glucose utilization in anatomically discrete regions of the brain of freely moving rats have been studied by the quantitative autoradiographic 2-deoxy-D-[1-14C]glucose technique. The experiments were made from 14.00 to 16.00 h, when MT is normally not secreted by the pineal gland, in order to compare the local cerebral glucose utilization (LCGU) response to MT from animals with long-term (pinealectomized) or short-term (pineal intact) absence of MT secretion. The majority of the 98 brain areas examined showed no change in LCGU after MT administration and/or pinealectomy. Pinealectomy increased the LCGU in the median mammillary nucleus only, whereas following MT administration, an increase in LCGU was observed in 3 cerebral regions of intact rats (paraventricular nucleus of the hypothalamus, nucleus of the solitary tract, choroid plexus of the third ventricle) and in 5 cerebral regions of pinealectomized rats (paraventricular nucleus of the hypothalamus, nucleus of the solitary tract, choroid plexuses of the lateral ventricles, third and fourth ventricles). Except for the choroid plexuses of the fourth ventricle, there was no difference in LCGU response to MT between pinealectomized and intact animals. This does not favor the hypothesis of receptor supersensitivity under the conditions of this experiment. Our results suggest that the hypothalamus, nucleus of the solitary tract and choroid plexuses represent a neural substrate through which MT could influence the activity of the central nervous system when administered at a low dose under near-physiological conditions."
"Endo180 (CD280; MRC2; uPARAP) regulates collagen remodelling and chemotactic cell migration through cooperation with membrane type-1 matrix metalloproteinase (MT1-MMP), urokinase-type plasminogen activator receptor (uPAR) and urokinase-type plasminogen activator (uPA). One hundred and sixty nine prostate tissue sections clinically graded as benign prostatic hyperplasia (BPH) (n=29) or prostate cancer (PCA) with Gleason scores indicating low (< or =7(3+4); n=26), intermediate (7(4+3)-8; n=96) or high (9-10; n=19) clinical risk were immunofluorescently stained for Endo180, pan-cytokeratin (pCk), vimentin, MT1-MMP and uPAR-uPA. Quantification of % Endo180(+)/pCk(-) and Endo180(+)/pCk(+) cells in entire tissue cores revealed stromal (p=0.0001) and epithelial (p=0.0001) upregulation of Endo180 in PCA compared to BPH. Epithelial Endo180 expression was significantly different between the three clinical risk groups of PCA (p<0.05). Correlations with MT1-MMP and uPAR-uPA confirmed the functionality of Endo180 during PCA progression. This molecular evaluation is the first step in the exploration of Endo180 in PCA diagnosis and therapy."
"The purpose of the present work was to systematically study the chromatographic behaviour of different aromatic stationary phases in a subcritical fluid mobile phase. We attempted to assess the chemical origin of the differences in retention characteristics between the different columns. Various types of aromatic stationary phases, all commercially available, were investigated. The effect of the nature of the aromatic bonding on interactions between solute and stationary phases and between solute and carbon dioxide-methanol mobile phase was studied by the use of a linear solvation energy relationship (LSER): the solvation parameter model. This study was performed to provide a greater knowledge of the properties of these phases in subcritical fluid chromatography, and to allow a more rapid and efficient choice of aromatic stationary phase in regard of the chemical nature of the solutes to be separated. Charge transfer interactions naturally contribute to the retention on all these stationary phases but are completed by various other types of interactions, depending on the nature of the aromatic group. The solvation vectors were used to compare the different phase properties. In particular, the similarities in the chromatographic behaviour of porous graphitic carbon (PGC), polystyrene-divinylbenzene (PS-DVB) and aromatic-bonded silica stationary phases are evidenced."
"Catechol-2, 3-dioxygenase (C23O) from Pseudomonas sp. CGMCC2953 identified in our laboratory, which is one of the key enzymes responsible for phenanthrene biodegradation, was expected to get better characteristics tolerant to environment for its further application. With the aim of improving the enzyme properties by introducing intermolecular disulfide bonds, X-ray structure of a C23O from Pseudomonas putida MT-2, a highly conserved homologous with the C23O from Pseudomonas sp. CGMCC2953, was directly used to find the potential sites for forming disulfide bonds between two monomers of the target C23O. Two sites, Ala229 and His294, were identified and mutated to cysteine, respectively, by using site mutagenesis. The expected disulfide bond between these two CYS residues was confirmed with both molecular modeling and experimental results. The optimum temperature of the mutated enzyme was widened from 40 to 40 approximately 50 degrees C. The mutated C23O became more alkalescency stable compared with the wild-type enzyme, e.g., 75% of the maximal enzyme activity retained even under pH 9.5 while 50% residue for the wild-type one. Improvement of thermostability of the mutated C230 with the redesigned disulfide was also confirmed."
"Cajal-Retzius cells are critical in cortical lamination, but very little is known about their origin and development. The homeodomain transcription factor Dbx1 is expressed in restricted progenitor domains of the developing pallium: the ventral pallium (VP) and the septum. Using genetic tracing and ablation experiments in mice, we show that two subpopulations of Reelin(+) Cajal-Retzius cells are generated from Dbx1-expressing progenitors. VP- and septum-derived Reelin(+) neurons differ in their onset of appearance, migration routes, destination and expression of molecular markers. Together with reported data supporting the generation of Reelin(+) cells in the cortical hem, our results show that Cajal-Retzius cells are generated at least at three focal sites at the borders of the developing pallium and are redistributed by tangential migration. Our data also strongly suggest that distinct Cajal-Retzius subtypes exist and that their presence in different territories of the developing cortex might contribute to region-specific properties."
"Twenty-eight patients (17 women, 11 men, average age 67 years, range 40-85 years) with embolic occlusions of the popliteal arteries were treated by aspiration embolectomy. 6 patients were in clinical stage IIb and 22 in stage III. In 25 of the 28 patients the occlusion was treated successfully. Complications could be treated non-surgically at the same time. 2 of the patients died within the first week of cerebral emboli, 2 further patients suffered recurrent emboli in the treated extremity during the first month. 17 patients who were followed up for six months were free of recurrences."
"The YtvA protein, which is one of the proteins that comprises the network carrying out the signal transfer inducing the general stress response in Bacillus subtilis, is composed of an N-terminal LOV domain (that binds a flavin [FMN]) and a C-terminal STAS domain. This latter domain shows sequence features typical for a nucleotide (NTP) binding protein. It has been proposed (FEBS Lett., 580 [2006], 3818) that BODIPY-GTP can be used as a reporter for nucleotide binding to this site and that activation of the LOV domain by blue light is reflected in an alteration of the BODIPY-GTP fluorescence. Here we confirm that BODIPY-GTP indeed binds to YtvA, but rather nonspecifically, and not limited to the STAS domain. Blue-light modulation of fluorescence emission of YtvA-bound BODIPY-GTP is observed both in the full-length YtvA protein and in a truncated protein composed of the LOV-domain plus the LOV-STAS linker region (YtvA(1-147)) as a light-induced decrease in fluorescence emission. The isolated LOV domain (i.e. without the linker region) does not show such BODIPY-GTP fluorescence changes. Dialysis experiments have confirmed the blue-light-induced release of BODIPY-GTP from YtvA."
"This article examines how best to identify the leading causes of mortality in England an Wales, by using different way of grouping causes of death, based on a list developed by the World Health Organization (WHO). Four different versions of this list are compared. The leading cause of death across all age groups depends on the ways in which common diseases and external causes are aggregated or disaggregated into groups. Areas of particular debate, examined in this article, are the grouping or splitting of accidents by mechanism and cancers by site within leading cause lists. These affect which causes appear in the top ten, and their order in different age groups."
"In order to characterize further the cholesterolemic effect of casein compared with soybean protein isolate, each of these proteins were fed to male rats at different levels of dietary cholesterol administration. An increase in the dietary cholesterol level from zero to 0.25% led to a several-fold increase in plasma triacylglycerols with a comparatively small rise in plasma cholesterol. Further increase in the cholesterol content of diet to 0.5, 1 and 2% resulted in a return of plasma triacylglycerols to normal or even subnormal values, whereas the plasma cholesterol values rose progressively. The triacylglycerolemic and cholesterolemic effects were 2-3-fold higher with casein diet than with soybean protein diet, respectively. A major part of the increased plasma cholesterol value was accounted for by cholesterol in the very-low-density lipoprotein (VLDL) fraction. Even at moderate levels (0.05-0.5%), the dietary cholesterol administration produced a series of changes in the electrophoretic pattern of plasma lipoproteins. These changes were more prominent in the rats fed casein diet, which often showed an enhanced VLDL band even on a cholesterol-free diet. An additional lipoprotein band, localized between VLDL and the LDL/HDL1 band, was observed in several rats fed casein diet with 0.05 or 0.1% cholesterol, and in rats fed soybean protein diet containing 0.25 or 0.5% cholesterol. In contrast, at the 0.25 and 0.5% levels of dietary cholesterol, plasma of casein-fed rats contained only one broad band within the VLDL-LDL/HDL1 region. All these findings indicate that casein diet promotes the appearance of one or more specific type(s) of cholesterol-induced plasma lipoprotein particles even at a comparatively low level of dietary cholesterol."
"This study aimed to assess the validity and normative statistics of the Farsi version of the Social Responsiveness Scale-2 (SRS-2). Among the mainstream elementary schools, 191 boys and 342 girls with a mean age of 9.46 (+ 1.72) years were recruited. Teachers and parents completed the SRS-2. The parents also answered the Social Communication Questionnaire (SCQ) and the Vineland Adaptive Behavior Scale (VABS). There were not any significant differences regarding the parents' and teachers' ratings of the SRS mean scores in terms of gender, academic level, and age. The SRS was significantly correlated with the SCQ (0.438) and VABS (- 0.142) mean scores. The study supported the validity of the SRS as a screening instrument for social communication problems in Farsi-speaking school-aged children."
"Two major pathways are implicated in the stimulation of insulin secretion by glucose. The K+-ATP channel-dependent pathway involves closure of these channels, depolarization of the beta-cell membrane, acceleration of Ca2+ influx, and a rise in cytosolic free Ca2+ ([Ca2+]i). The K+-ATP channel-independent pathway potentiates the stimulation of exocytosis by high [Ca2+]i. To determine whether this second pathway is influenced by the configuration of the channel, we compared the effects of glucose on [Ca2+]i and insulin secretion in mouse islets under three conditions. First, in the presence of 20, 25, and 30 mM K+, i.e. without pharmacological action on K+-ATP channels, [Ca2+]i and insulin secretion were already elevated at 3 mM glucose. High glucose (20 mM) caused a transient decrease in [Ca2+]i followed by an ascent to slightly above control levels, and rapidly stimulated insulin secretion. Second, opening of K+-ATP channels with diazoxide did not influence [Ca2+]i and insulin secretion at 3 mM glucose and high K+. However, high glucose now caused a sustained lowering of [Ca2+]i accompanied by a slow increase in secretion that augmented with the K+ concentration. Third, when K+-ATP channels were blocked and beta-cells depolarized by high concentrations of tolbutamide or glibenclamide, [Ca2+]i and insulin secretion were elevated even in low glucose. High glucose transiently lowered [Ca2+]i, which then increased to or slightly above control levels, while insulin secretion was rapidly stimulated. Under all conditions the correlation between [Ca2+]i and insulin secretion was excellent at low and high glucose levels, and high glucose increased release at all [Ca2+]i. The potentiation of Ca2+-induced exocytosis by glucose is thus independent of the closed or open state of K+-ATP channels. It is only when the channels are opened by diazoxide that the increase in release is a strict amplification of the action of Ca2+. When the channels are closed (sulfonylureas) or still closable (high K+ alone), the effect of glucose on secretion also comprises a slight increase in [Ca2+]i and, in the latter case, is not strictly K+-ATP channel independent."
"The greatest source of human exposure to methylmercury (MeHg) is the diet, in particular the consumption of seafood. To investigate the importance of dietary MeHg speciation on neurotoxicity, balb/c mice dams were exposed to MeHgCys (the naturally-occurring salt) and MeHgCl (the laboratory salt), at concentrations up to 4.5 mg/kg, for 11 weeks (inclusive of 3 weeks gestational and 2 weeks post-partum exposure). Impacts of developmental exposure were assessed in their offspring by monitoring transcriptomic (brain gene expression via microarray and quantitative PCR), tissue mercury (Hg) accumulation, and neurobehavioral endpoints. There were no differences in tissue Hg accumulation between the two forms of MeHg presented, but differences in pup behavior and gene expression endpoints were noted. For example, MeHgCl, but not MeHgCys, impaired pup activity in an open field assessment. Similar impacts of MeHgCl were noted in adults. A total of 131 genes were differentially-regulated in pup brains following maternal exposure to MeHg, 50 of which were specific to MeHgCys and 35 specific to MeHgCl. Regulated genes were significantly enriched for several annotation categories including metal/zinc-binding and transcription regulation. In contrast few antioxidant genes were differentially regulated. This analysis provided insight into mechanisms by which MeHg may impair cellular processes in addition to behavioral impairments such as those associated with learning and memory. The results show differences between the toxic impacts of MeHg species, and also highlight the potential utility of an integrated approach incorporating gene expression with behavioral endpoints."
"Degenerative lumbosacral stenosis is a common disease in dogs characterised by intervertebral disc herniation, loss of disc height and stenosis. Decompressive dorsal laminectomy and partial discectomy can cause spinal instability and worsen foraminal stenosis. Pedicle screw and rod fixation (PSRF) with an intervertebral body cage allows for distraction and restoration of disc height and restores foraminal apertures. The aim of this study was to evaluate the ex vivo biomechanical properties of a titanium intervertebral cage alone and in combination with PSRF in the lumbosacral spine of dogs. The range of motion, neutral zone, neutral zone stiffness and elastic zone stiffness of the lumbosacral joint (L7-S1) of nine canine cadavers were determined in flexion/extension, lateral bending and axial rotation for four conditions: (1) native (unmodified) spine; (2) dorsal laminectomy and discectomy; (3) stand-alone cage; and (4) cage in combination with PSRF. The intervertebral disc height decreased after dorsal laminectomy, but increased after insertion of the cage. Insertion of the stand-alone cage decreased the range of motion and neutral zone compared to the laminectomy-discectomy and increased neutral zone stiffness in all directions. The range of motion further decreased after PSRF. From a biomechanical point of view, the use of a stand-alone intervertebral cage is a potential alternative to dorsal fixation of the lumbosacral junction, since it increases spinal stability and restores disc height."
"Nickel deficiency was induced in 2- to 4-year-old goats by feeding 0.1 mg Ni/kg dry matter with a semisynthetic diet. The control group consumed 5.0 mg Ni/kg d.m. Activity of several enzymes (SDH, LDH, HBDH, AST, ALT, ALD, CK, CHE) was determined in the serum, liver, heart and kidneys. Serum urea-N level was also measured and transmission electron microscopic (TEM) examinations were performed. Signs characteristic of nickel deficiency (retarded growth, increased mortality of dam and offspring, parakeratosis of the skin) appeared in the low-nickel group. The activity of SDH and ALD, as well as the level of urea-N was significantly lower in the serum of Ni-deficient animals than in the control. Ni-deficient animals also had significantly lower enzyme activities in the heart (SDH, HBDH, AST, ALT, ALD and CK), liver (SDH and CHE) and kidneys (HBDH and CK). Electron micrographs showed degeneration of cardiac and skeletal muscle in the Ni-deficient animals. Ni deficiency elicited changes primarily in the heart and these resulted in depressed activity of several enzymes."
"BACKGROUND: Costs and potential benefits of an intensive chest X-ray (CXR) screening program to detect asymptomatic pulmonary metastases in patients with intermediate-thickness, local, cutaneous melanoma were assessed.METHODS: Cost-effectiveness analysis from a societal perspective was performed using data on recurrence detection from an historic cohort at Roswell Park Cancer Institute and other published studies, estimates of new cases of melanoma in 1996 from the National Cancer Institute's Surveillance, Epidemiology, and End Results program, and estimates of cost and treatment benefits from published articles retrieved through MEDLINE. Net costs were calculated as the added cost of CXR screening to regular follow-up and the costs incurred in the surgical treatment of lung recurrences. Net benefits were calculated as potential savings in nonquality-adjusted life years (NQALY) and quality-adjusted life years (QALY) resulting from surgical treatment. Cost-effectiveness ratios were calculated as the present value of net costs divided by net benefits, with benefits presented in discounted and undiscounted forms.RESULTS: For the base case, cost of screening per NQALY was $150,000 and was $165,000 for QALY in 1996 dollars using undiscounted health benefits. Screening accounted for approximately 80% of program costs and treatment accounted for 20%. Annual cost-effectiveness ratios were lowest in Years 3-10 of screening. The total cost of a 20-year screening program for patients diagnosed in 1996 was estimated to be between $27-$32 million.CONCLUSIONS: Even in the absence of certain benefits, the model demonstrates that significant cost savings may be possible by decreasing screening frequency in the first 2 years and limiting screening to the first 5-10 years after diagnosis."
"The acs operon of Gluconacetobacter is thought to encode AcsA, AcsB, AcsC, and AcsD proteins that constitute the cellulose synthase complex, required for the synthesis and secretion of crystalline cellulose microfibrils. A few other genes have been shown to be involved in this process, but their precise role is unclear. We report here the use of Tn5 transposon insertion mutagenesis to identify and characterize six non-cellulose-producing (Cel(-)) mutants of Gluconacetobacter hansenii ATCC 23769. The genes disrupted were acsA, acsC, ccpAx (encoding cellulose-complementing protein [the subscript ""Ax"" indicates genes from organisms formerly classified as Acetobacter xylinum]), dgc1 (encoding guanylate dicyclase), and crp-fnr (encoding a cyclic AMP receptor protein/fumarate nitrate reductase transcriptional regulator). Protein blot analysis revealed that (i) AcsB and AcsC were absent in the acsA mutant, (ii) the levels of AcsB and AcsC were significantly reduced in the ccpAx mutant, and (iii) the level of AcsD was not affected in any of the Cel(-) mutants. Promoter analysis showed that the acs operon does not include acsD, unlike the organization of the acs operon of several strains of closely related Gluconacetobacter xylinus. Complementation experiments confirmed that the gene disrupted in each Cel(-) mutant was responsible for the phenotype. Quantitative real-time PCR and protein blotting results suggest that the transcription of bglAx (encoding â-glucosidase and located immediately downstream from acsD) was strongly dependent on Crp/Fnr. A bglAx knockout mutant, generated via homologous recombination, produced only ?16% of the wild-type cellulose level. Since the crp-fnr mutant did not produce any cellulose, Crp/Fnr may regulate the expression of other gene(s) involved in cellulose biosynthesis."
"A partial trisomy 12q243 leads to qter resulting from a maternal balanced translocation, 46,XX,t(9;12)(p243;q243) was detected in a male newborn with multiple congenital abnormalities. The maternal grandmother was also a carrier of the 9;12 translocation. Our patient exhibited a number of clinica features similar to two others reported, who were also trisomic for the distal part of 12q. Aberrations of chromosome 12 are very rare. There have been only two reports of partial trisomy 12q, both the result of a familial translocation. We describe a third unbalanced case."
"An ABC transporter, TliDEF, from Pseudomonas fluorescens SIK W1, mediates the secretion of its cognate lipase, TliA, in a temperature-dependent secretion manner; the TliDEF-mediated secretion of TliA was impossible at the temperatures over 33°C. To isolate a mutant TliDEF capable of secreting TliA at 35°C, the mutagenesis of ABC protein (TliD) was performed. The mutated tliD library where a random point mutation was introduced by error-prone PCR was coexpressed with the wild-type tliE, tliF and tliA in Escherichia coli. Among approximately 10,000 colonies of the tliD library, we selected one colony that formed transparent halo on LB-tributyrin plates at 35°C. At the growth temperature of 35°C, the selected mutant TliD showed 1.75 U/ml of the extracellular lipase activity, while the wild-type TliDEF did not show any detectable lipase activity in the culture supernatant of E. coli. Moreover, the mutant TliD also showed higher level of TliA secretion than the wild-type TliDEF at other culture temperatures, 20°C, 25°C and 30°C. The mutant TliD had a single amino acid change (Ser287Pro) in the predicted transmembrane region in the membrane domain of TliD, implying that the corresponding region of TliD was important for causing the temperature-dependent secretion of TliDEF. These results suggested that the property of ABC transporter could be changed by the change of amino acid in the ABC protein."
"Ultrasound (US) serves as a stimulus to change shear viscosity of aqueous polysaccharides of é-carrageenan, ê-carrageenan and, agar. The US effect was compared in their aqueous solutions at 60 °C for the US frequency of 23, 45, and 83 kHz. Under the US condition with 50 W at 45 kHz, the shear viscosity of each aqueous solution was decreased significantly. Subsequently, when the US was stopped, the shear viscosity returned back to the original value. In addition, the US showed different effects of the US frequency over the viscosity change in the three kinds of polysaccharides. When the US frequency was changed, the US effects were less at 83 kHz and 28 kHz for the shear viscosity change. In addition, as NaCl was present in the aqueous solution, the viscosity change decreased by the US exposure. These results suggest that the US effect on the viscosity reduction was influenced by the condition of polymer coil conformation, which was expanded or shrank by electrostatic repulsion of the SO3(-) groups. FT-IR analysis supported that the hydrogen bonds of carrageenans were broken during the US exposure. Using Fourier self-deconvolution for the FT-IR spectra without and with US exposure suggests that the US influenced the hydrogen bonds of water and the OH group of polysaccharides."
"The genome of virulent coliphage T5 contains about 30 sites which form stable complexes with E. coli RNA polymerase. Some of these sites bind RNA polymerase with high rates, others form extremely stable complexes as compared with promotors of other E. coli systems. The transcriptional activity of these promotors in vivo and in vitro reflects the rate of complex formation with RNA polymerase rather than the stability of the enzyme/promotor complex. The fastest, i.e. the most active promotors are found in the ""early"" region of gene expression followed by promotors of the ""preearly"" class. The few binding sites for the E. coli holoenzyme within the ""late"" region react more slowly with the enzyme."
"The kinetics of transfer of [3H]cholesterol between human erythrocytes and plasma at 37 degrees C in physiological buffer had these features. 1) Cholesterol transfer was strikingly similar in both directions. 2) Transfer progressed to isotopic equilibrium in a monotonic, apparently first order fashion, except for a minor rapid component (approximately 15%) observed in the transfer of cholesterol from cells to plasma. 3) The mechanism of transfer was not via transient collisions in that the rate of the reaction was quite insensitive to the concentration of reactants over a wide range. 4) The mechanism of transfer did not involve specific, stable complex formation in that there was little difference in the behavior of erythrocytes and inside-out plasma membrane vesicles derived therefrom or between plasma and sonicated liposomes as acceptors. Furthermore, transfer was not affected by vigorous proteolysis of either the cells or the plasma. 5) The kinetics of transfer were fully compatible with diffusion of cholesterol through the aqueous compartment. This was shown by fitting our data to a rigorous model for diffusion equilibrium between three compartments. 6) The partition coefficient of [3H]cholesterol between red cells and buffer was shown to be 10(7). 7) The rate constants for cholesterol release from both red cells and plasma were approximately 1 X 10(-4) s-1 (t 1/2 approximately 2 h). The rate constant for cholesterol uptake into red cells was approximately 1 X 10(3) s-1 (t 1/2 approximately 1 ms). 8) The similarity of the corresponding kinetic constants among red cells, plasma, and liposomes suggests that phospholipids in a variety of physical forms are equivalent solvents for cholesterol. We conclude that despite its extremely low solubility in water, cholesterol moves between lipid compartments by aqueous diffusion."
"We employed sialidase procedures followed by lectin stainings combined with oxidizing and deacetylating agents to visualize the distribution and sequentiate sialoglycoconjugates in the bovine submandibular gland. In particular we evidenced in acinar and ductal cells the dishomogeneous presence of sialic acids acetylated in the polyhydroxy side chain (C7, C8, C9), whereas O-acetyl substituents at position C1 and/or C4 were not found. Sialoglycoderivatives were also differentiated by the occurrence of penultimate sugars; indeed the dimers sialic acid-(alpha 2----3,6)-beta-galactose and sialic acid-(alpha 2----6)-alpha-N-acetylgalactosamine were identified. Using such technique we supported further the possibility to develop methods for the identification of the positions of O-acetyl groups and the reconstruction of terminal disaccharides within surface and cytoplasm glycoconjugates. in situ the distribution of different O-acylated sialoglyco-derivatives in the bovine submandibular gland. To this purpose we employed oxydizing and deacetylating agents combined with sialidase digestion and lectin binding."
"A middle-aged women with hypothyroidism, idiopathic portal hypertension and nephrotic syndrome is presented. This unusual clinical appearance could not be explained as SLE by serological examinations. Pathohistological examinations showed ""Banti's liver"", Hashimoto's thyroiditis and diffuse proliferative glomerulonephritis with severe tubulo-interstitial nephritis. Immunohistochemical studies revealed IgA deposits in glomeruli. Electron microscopic study disclosed peculiar lucent areas of rarefaction with osmiophilic particles in tubular basement membranes. This tubulointerstitial nephritis was considered to be related to the immunological mechanism involving thyroid gland, liver and kidney disorders. This case thus had a clinically rare combination of these three."
"This study aimed to identify the effect of plyometric training (PT), when added to habitual training (HT) regimes, on swim start performance. After the completion of a baseline competitive swim start, 22 adolescent swimmers were randomly assigned to either a PT (n = 11, age: 13.1 +/- 1.4 yr, mass: 50.6 +/- 12.3 kg, stature: 162.9 +/- 11.9 cm) or an HT group (n = 11, age: 12.6 +/- 1.9 yr, mass: 43.3 +/- 11.6 kg, stature: 157.6 +/- 11.9 cm). Over an 8-week preseason period, the HT group continued with their normal training program, whereas the PT group added 2 additional 1-hour plyometric-specific sessions, incorporating prescribed exercises relating to the swimming block start (SBS). After completion of the training intervention, post-training swim start performance was reassessed. For both baseline and post-trials, swim performance was recorded using videography (50 Hz Canon MVX460) in the sagital plane of motion. Through the use of Silicon Coach Pro analysis package, data revealed significantly greater change between baseline and post-trials for PT when compared with the HT group for swim performance time to 5.5 m (-0.59 s vs. -0.21 s; p < 0.01) and velocity of take-off to contact (0.19 ms vs. -0.07 ms; p < 0.01). Considering the practical importance of a successful swim start to overall performance outcome, the current study has found that inclusion of suitable and safely implemented PT to adolescent performers, in addition to HT routines, can have a positive impact on swim start performance."
Based on animal experiments authors established that the different polarity high-concentration aeroions have an effect on the vegetative centres of the Central Nervous System (CNS). Through vegetative centres they influence both electro- and neuro-physiological relations of the nervous system and by this means they make an effect on the capacity. Aeroions have some explicitly favourable effect both on the orientation reflex action and the alert system of the brain stem.  These effects can also be registered either by pharmacology tests or sinstrumentally (electronystagmography).
"The ultrastructure of the juxta-oral organ has been investigated in the goldhamster. Its morphology was comparable to that already described in other species. Located in the cheek or in its anatomical homologue, the juxta-oral organ consists of a string of epithelial cells surrounded by two distinct layers. The first one, made of connective tissue, contains numerous terminal nerve fibres. The second one, external and separated from the first one by a fluid-filled space, is very similar to a perineurium. The juxta-oral organ of the goldhamster presented a new feature: its central string of parenchymatous cells always contained some nervous fibres. The parenchymatous cells were compared to the keratinocytes in the basal layers of the oral epithelium and the general structure of the juxta-oral organ to that of several mechanoreceptors. It is likely that the juxta-organ exerts a mechano-receptor function."
"Plant shoot systems derive from the shoot apical meristems (SAMs), pools of stems cells that are regulated by a feedback between the WUSCHEL (WUS) homeobox protein and CLAVATA (CLV) peptides and receptors. The maize heterotrimeric G protein á subunit COMPACT PLANT2 (CT2) functions with CLV receptors to regulate meristem development. In addition to the sole canonical Gá CT2, maize also contains three eXtra Large GTP-binding proteins (XLGs), which have a domain with homology to Gá as well as additional domains. By either forcing CT2 to be constitutively active, or by depleting XLGs using CRISPR-Cas9, here we show that both CT2 and XLGs play important roles in maize meristem regulation, and their manipulation improved agronomic traits. For example, we show that expression of a constitutively active CT2 resulted in higher spikelet density and kernel row number, larger ear inflorescence meristems (IMs) and more upright leaves, all beneficial traits selected during maize improvement. Our findings suggest that both the canonical Gá, CT2 and the non-canonical XLGs play important roles in maize meristem regulation and further demonstrate that weak alleles of plant stem cell regulatory genes have the capacity to improve agronomic traits."
"AIM: The role of glycaemic control in the mortality of elderly diabetic patients remains uncertain. GERODIAB is the first multi-centre, prospective, observational study that aims to describe the link between HbA1c and 5-year mortality in French, type 2 diabetic patients aged ?70 years.METHODS: Consecutive patients (n=987; mean age 77 years) were included from 56 diabetes centres and followed for five years. Individual histories, risk factors, standard diabetes parameters and geriatric evaluations were regularly recorded. Survival was studied using the Kaplan-Meier method. Multivariable analyses used Cox regression.RESULTS: Twenty-one percent of the patients died, 13% were lost during follow-up. Patients with a 5-year mean HbA1c in the range [40-50) mmol/mol ([5.8-6.7) %) had the highest survival (84%); those in the range [50-70) mmol/mol ([6.7-8.6) %) or <40mmol/mol (<5.8%) an intermediary survival rate (79%); patients with HbA1c ?70mmol/mol (?8.6%) the worst survival (71%). Patients with mean HbA1c ?70mmol/mol (?8.6%) had a significantly higher mortality than those with lower HbA1c (P=0.011), and HbA1c remained a significant predictor of mortality after adjusting for individual, diabetic and geriatric factors (hazards ratio [95%CI]: 1.76 [1.21 to 2.57], P=0.0033). Survival was also significantly associated with both HbA1c variability and with the frequency of HbA1c determinations.CONCLUSION: In this large sample of elderly French type 2 diabetic patients, an HbA1c level <70mmol/mol (<8.6%) was associated with lower mortality. The range [40-50) mmol/mol ([5.8-6.7) %) could be an acceptable target provided patients are not exposed to hypoglycaemia."
"Chlamydia pneumoniae has proved to be difficult to isolate and propagate in cell culture. We compared the growth of three strains of C. pneumoniae, TW-183 and two clinical isolates from Brooklyn, N.Y., in five cell lines, including HeLa 229, McCoy, HL, HEp-2, and HTED, an immortalized human tracheal cell line. HEp-2 was the most sensitive cell line tested. When 10-fold dilutions of three C. pneumoniae strains at known titers were inoculated into the different cell lines, the mean number of inclusion-forming units per milliliter was 1 to 2 log units higher in the HEp-2 than in the other cell lines. This difference was statistically significant. Omission of pretreatment with DEAE-dextran resulted in larger inclusions than those seen in pretreated cells, with the exception of McCoy and HTED cells. Retrieval of clinical specimens previously cultured on HeLa 229 cells and comparison of mean inclusion counts in fresh clinical specimens simultaneously inoculated on HeLa 229 and HEp-2 cells suggested that culture in HEp-2 cells may require only the initial inoculation and one passage, compared with three to four passages, as required by culture in HeLa 229 cells."
"OBJECTIVE: To develop a strategy for evaluating drug efficacy over time that accounts for heterogeneous clinical courses evolving after initiation of therapy and to demonstrate its use in assessing the long-term therapeutic benefit of propranolol after myocardial infarction.DESIGN: Analysis of data from the Beta-Blocker in Heart Attack Trial (BHAT), a randomized, double-blind, placebo-controlled trial that enrolled patients from 1978 to 1980 and followed participants for vital status to April 1982.SETTING: Thirty-one clinical centers in the United States and Canada.PATIENTS: Eligible patients included 3297 men and women 30 to 69 years of age who survived 1 year after trial entry.INTERVENTION: Patients were classified as being on treatment at 12 months after randomization if they were receiving beta-blocker therapy at the 12-month visit and off treatment if they were not receiving beta-blocker therapy at that time.OUTCOME MEASURE: Vital status evaluated at 720 days of follow-up.RESULTS: A total of 2914 patients (88%) was classified as being at lower risk (strata I and II). For these patients, survival curves by treatment at 12 months were virtually indistinguishable. Among the 383 patients categorized as being at high risk on the basis of recurrent ischemic events, arrhythmias, congestive heart failure, or severe comorbidity during the first 12 months, the use of beta-blockers was associated with a 43% proportional decline in the subsequent risk for death (P = 0.01 by log-rank test).CONCLUSIONS: In patients who survived to 1 year with low- to moderate-risk clinical courses, beta-blocker therapy did not have long-term beneficial effect. In contrast, among patients who had a high-risk clinical course during the first year, beta-blockers significantly reduced mortality in the follow-up period."
"OBJECTIVES: Costing studies are needed to identify the resources used for treatment and inform payers of the costs incurred. The objectives were to determine the costs of diagnosing and treating atopic dermatitis, food allergy and asthma, and to compare the share of costs to society and to the family during the first two years of life.STUDY DESIGN: The data were obtained from an ongoing mother-infant nutrition study. The sample comprised 60 infants who developed allergic disease by the age of 24 months and 56 healthy infants with no allergic disease.METHODS: The costs included diagnosis and treatment of the allergy, disability allowances, travel expenses and time spent by parents.RESULTS: The median costs per infant were €275 (range 94-1306) for atopic dermatitis, €1408 (163-5408) for asthma, €3182 (628-11195) for food allergy, and €10 (0-619) for the healthy infants due to the suspicion of allergic disease. The highest costs in atopic dermatitis were caused by primary care visits, topical treatments, travel costs and parents' time, and those for food allergy by hospital out-patient care, infant formulae for cow's milk allergy, disability allowances and travel costs. The families paid 43% of the costs arising from atopic dermatitis, 13.6% of those from food allergy and 16.5% of those from asthma.CONCLUSIONS: Cow's milk allergy emerged as the most expensive allergic disease, especially for the society, and concurrent asthma in particular further increased the costs."
"PURPOSE: Patients with febrile neutropaenia (FN) can be stratified according to their risk of significant complications, allowing reduced intensity therapy for low risk (LR) episodes. Serious events are very rare in low risk episodes making randomised trials difficult. Introduction of new evidence-based guidelines followed by re-auditing of the outcome is an alternative strategy.METHODS: New guidelines for the management of LR FN were implemented in 4 specialist paediatric oncology centres (POCs) and in their associated shared care units (POSCUs). All patients commenced empirical intravenous antibiotic therapy and after 48h those with blood culture negative episodes designated LR were eligible for discharge on oral co-amoxiclav. Prospective data collection on FN episodes in all treatment centres was undertaken over a 1-year period.RESULTS: Seven hundred and sixty two eligible episodes of FN were recorded in 368 patients; 213 episodes were initiated in POCs and 549 episodes were initiated in POSCUs. In 40% of episodes no clinical or microbiological focus of infection was found. At 48h, 212 (27%) episodes were classified as LR and 143 of these (19%) were managed on the LR protocol. There was a low hospital readmission rate (8/143 episodes; 5.6%), no intensive care admissions and no deaths in LR episodes. Almost all LR episodes (209/212) occurred in the shared care setting.CONCLUSIONS: Rapid step-down to oral antibiotics was a feasible and safe management strategy for LR FN in the shared care setting in England."
"The human C1q receptor (C1q-R) is a 65-70-kd, highly acidic, hydrophobic glycoprotein that is expressed on a wide variety of cell surfaces. Although the C1q-R itself appears to bind preferentially to C1q, the region of the ligand to which C1q-R binds is the primary binding site for several other molecules, including fibronectin, laminin, and C1q inhibitor (chondroitin 4-sulfate proteoglycan) as well as the complement C1r2C1s2 tetramer. In order to further characterize the C1q-R molecule with regard to its structure and function, highly purified C1q-R was obtained from Raji cells using DEAE-Sephacel and C1q-Sepharose CL-4B chromatography. Studies performed with 125I-labeled C1q-R demonstrated that whereas the C1q-R molecule binds poorly to a variety of human collagens including types II, III, and V, markedly enhanced binding is observed with type IV collagen and moderately enhanced binding with type I collagen. Amino acid composition studies show that the C1q-R molecule contains approximately 44% hydrophobic and 12.6% hydrophilic residues with a ratio of negatively charged to positively charged residues of about 2:1. Treatment of 125I-labeled C1q-R with endoglycosidase F lowers the apparent molecular size from 70 to 58 kd, whereas endoglycosidase H lowered the size to 64 kd. Treatment with neuraminidase, on the other hand, shifted the size of C1q-R to 60 kd. These results suggest the presence of several highly sialylated complex-type or high mannose-type N-linked oligosaccharide side chains. Because purified C1q-R has a blocked amino terminus, amino acid sequences representing internal fragments of the molecule were generated by electroblotting and in situ enzymatic digestion. When these short sequences were searched against the National Biomedical Research Foundation computer data base, a seven-amino-acid sequence, VSWQGQI, showed significant homology (100% and 80% in a five-amino-acid overlap, respectively) with the alpha chains of the human fibronectin (alpha 5 beta 1) and vitronectin (alpha v beta 3) receptors, and to a lesser degree with epidermal growth factor receptor and T cell receptor. A second sequence, ISEDNIR, showed homology with mouse collagen type IV (86% in a six-amino-acid overlap), calmodulin (60% in a seven-amino-acid overlap), and a Leishmania major surface antigen, gp63. These observations seem to predict that C1q-R has pockets of conserved sequences that are similar to those not only present in its ligand(s) but also in other cell surface receptors that may, in part, fulfill similar functions."
"Retinoblastoma is a childhood ocular tumor caused by the inactivation of both alleles of the retinoblastoma gene (Rb1). Without Rb1 gene function, chromosomal aberrations are observed in retinoblastoma cells. The instability of the genome is closely associated with the repair of DNA double-strand breaks (DSBs). However, the precise molecular mechanism of action of Rb1 in DNA DSB repair remains unclear. Thus, in this study, we aimed to investigate whether the Rb1 gene affects DNA stability by assaying DNA DSB repair and also whether it regulates the proliferation of retinoblastoma cells. Rb1 immunofluorescence and RT-PCR were performed, demonstrating that the Rb1 gene is silenced in SO-Rb50 retinoblastoma cells, and the karyotype analysis of SO-Rb50 cells indicated that the loss of Rb1 function led to genomic instability; both numerical and structural chromosomal aberrations were observed in our study. In addition, the DNA DSB repair efficiency of the SO-Rb50 cells was measured by ã-H2AX immunofluorescence, a commonly used in situ marker of DNA DSBs, following exposure to ionizing radiation (IR) (2.5 and 5.0 Gy). We found that the DNA repair efficiency was significantly increased following IR-induced damage (P<0.01). However, there was no significant difference in DNA repair efficiency between the cells expressing exogenous Rb1 and the control (P>0.05). The assay for the screening of the effect of Rb1 on the sub-pathway of DNA DSB repair, non-homologous end joining (NHEJ) and homologous recombination (HR), indicated that Rb1 did not affect NHEJ activity, although it significantly promoted the HR pathway (HR levels increased by 2.46-fold) compared with the control (P<0.01). Furthermore, we found that the cell viability of the SO-Rb50 cells transfected with exogenous Rb1 was significantly inhibited (P<0.01) and cell cycle assay indicated that exogenous Rb1 induced S phase arrest (P<0.001) which also inhibited the proliferation of retinoblastoma cells (SO-Rb50) in vitro. Therefore, this study provides new insight into the mechanisms of action of the Rb1 gene in regulating the proliferation of retinoblastoma cells."
"Homogeneous preparations of pesticin, a bacteriocin produced by Yersinia pestis, neither significantly inhibited net synthesis of deoxyribonucleic acid, ribonucleic acid, or protein in Escherichia coli phi nor caused detectable degradation of deoxyribonucleic acid in vivo. Accordingly, its mode of action does not resemble that of colicin E2 as suggested by others. However, incorporation of cell wall-specific label ([14C]diaminopimelic acid) into trichloroacetic acid-insoluble material of growing cells was inhibited by pesticin which also promoted release of such radioactivity from both resting cells and purified mureinlipoprotein. Sodium dodecyl sulfate-polyacrylamide gel electrophoresis of reaction mixtures containing appropriately labeled mureinlipoprotein showed that [3H]N-acetylglucosamine comigrated either with [14C]diaminopimelic acid in the murein peptide or with [14C]isoleucine of the Braun lipoprotein. As judged by these findings and pesticin-dependent release of reducing equivalents but not 4-hydroxy-2-acetamido sugars, the bacteriocin possesses N-acetylglucosaminidase activity. Hydrolysis of murein-lipoprotein occurred over a broad pH, with an optimum of 4.7. Mureinlipoproteins from a variety of pesticin-sensitive and -resistant organisms were hydrolyzed by the bacteriocin, indicating that its antibacterial specificity resides at the level of absorption."
"A 5-point breast imaging classification modified from the seven-category Breast Imaging-Reporting and Data System has been applied for mammographic and ultrasonographic examinations in patients with palpable breast masses. The aim of this study was to confirm the value of combined imaging assessment. We included 5,296 cases (3,002 benign and 2,294 cancer) from January 2004 to December 2011. Ultrasonography showed a significantly (P < 0.01) higher sensitivity and specificity and lower false-negative rate and false-negative predictive value (false-NPV) than mammography. The sensitivity of combined imaging was significantly (P < 0.01) increased and the false-negative rate and false-NPV were significantly (P < 0.01) reduced compared to mammography or ultrasonography alone. However, the specificity was significantly (P < 0.01) declined for combined imaging versus mammography or ultrasonography alone. Compared with combined imaging assessment, a significant (P < 0.01) improvement was noted with substratified scoring, with increased specificity and false-negative rate and decreased sensitivity. In conclusion, the substratified combined imaging score has the potential to provide additional value in the workup of palpable breast lesions."
"Median arcuate ligament (MAL) syndrome, also known as the celiac axis compression syndrome (CACS) is rare, and a topic of ongoing academic controversy. CACS is a diagnosis of exclusion, characterized by the clinical triad of postprandial abdominal pain, weight loss, and vomiting. The classic management of CACS involves the surgical division of the MAL fibers. We report successful treatment of a 23-year-old woman with CACS utilizing the da Vinci Surgical System (Intuitive Surgical, Sunnyvale, California) via robotic-assisted minimally invasive surgical division of the MAL. To our knowledge this is the first report of this modality used in the treatment of the celiac axis compression syndrome."
"Lipomyelomeningocele is a type of neural tube defect characterized by lipomatous tissue causing a defect in the vertebrae, infiltrating the dura, and tethering the spinal cord. Despite significant neurologic consequences, the underlying etiology remains poorly understood. We present a father and son with remarkably similar presentations of lipomyelomeningocele. Genetic testing did not reveal an underlying cause but whole exome sequencing identified variants in the ARHGAP29 and RADIL genes in the proband and his affected father. Genetic analyses of asymptomatic family members revealed several carriers of the ARHGAP29 or RADIL variants, but only the proband and his father carried both variants, suggesting a possible shared genetic mechanism. Rare cases of siblings affected with lipomyelomeningocele have suggested the possibility of autosomal recessive or germline mosaicism. We present the first documented cases of transgenerational lipomyelomeningocele with important implications for family counseling about the recurrence of lipomyelomeningocele."
"Immunotherapy is an empirical treatment with proven clinical efficacy. Different results have been found when comparing the levels of the subclasses for immunoglobulin G and the clinical development during immunotherapy. A follow up study was carried out for one year on the IgG subclasses in 21 asthmatic children of both sexes and ages ranging from 8 to 11 years. The children were sensitized to Dermatophagoides Pteronyssinus and had undergone immunotherapy treatment. The clinical and analytical evaluation were carried out before treatment was started and later every four months. In terms of clinical evolution, the children were separated into two groups. The levels of IgG1 show different evolution in terms of a good or not good clinical evolution with a p of 0.056. Levels of IgG2 showed no differences. IgG3 shows a disorderly distribution. A continuous increase in IgG4 levels was observed from the start of immunotherapy though no differences in terms of clinical evolution were recorded. With the results obtained, it can be concluded that the gradual increase of IgG4 together with an early rise in IgG1 levels are related to the clinical efficacy of immunotherapy."
"The obligate intracellular bacterium Mycobacterium leprae is the causative agent of leprosy and primarily infects macrophages, leading to irreversible nerve damage and deformities. So far, the underlying reasons allowing M. leprae to persist and propagate in macrophages, despite the presence of cellular immunity, are still a mystery. Here, we investigated the role of autophagy, a cellular process that degrades cytosolic materials and intracellular pathogens, in M. leprae infection. We found that live M. leprae infection of macrophages resulted in significantly elevated autophagy level. However, macrophages with high autophagy levels preferentially expressed lower levels of proinflammatory cytokines, including interleukin (IL)-1â, IL-6, IL-12, and tumor necrosis factor-á, and preferentially primed anti-inflammatory T cells responses, characterized by high IL-10 and low interferon-ã, granzyme B, and perforin responses. These anti-inflammatory T cells could suppress further induction of autophagy, leading to improved survival of intracellular M. leprae in infected macrophages. Therefore, these data demonstrated that although autophagy had a role in eliminating intracellular pathogens, the induction of autophagy resulted in anti-inflammatory immune responses, which suppressed autophagy in a negative feedback loop and allowed the persistence of M. leprae."
"BACKGROUND: The impact of fair bowel preparation on endoscopists' recommendations and adenoma miss rates in average-risk patients undergoing colonoscopy is unknown.OBJECTIVE: To assess the impact of fair bowel preparation on endoscopists' interval colonoscopy recommendations and miss rates in colonoscopies performed within 3 years of the index colonoscopy in average-risk patients undergoing colorectal cancer screening.DESIGN: Retrospective chart review.SETTING: Tertiary-care center.PATIENTS: Average-risk patients undergoing index colonoscopy for colorectal cancer screening between 2004 and 2006.INTERVENTION: Colonoscopy.MAIN OUTCOME MEASUREMENTS: Endoscopists' interval recommendations, adenoma miss rates.RESULTS: A total of 16,251 colonoscopy records were reviewed over a 2-year period. Of these cases, 1943 colonoscopies were performed for the sole indication of average risk or screening. Of these, fair bowel preparation was reported in 619 patients (31.9%). A repeat colonoscopy within 5 years was recommended in 70.4% of patients. The follow-up colonoscopy compliance rate within 3 years was 55.9%. Adenoma detection rates at index and follow-up colonoscopy were 20.5% and 28.2%, respectively. Of the 39 patients with follow-up colonoscopy within 3 years, the overall adenoma miss rate was 28%. Of the patients with an adenoma identified on follow-up colonoscopy, 13.6% had normal colonoscopy results on index examination.LIMITATIONS: Retrospective design.CONCLUSION: Fair bowel preparation led to a deviation from national guidelines with early repeat colonoscopy follow-up recommendations in nearly 60% of average-risk patients with normal colonoscopy results. In patients who returned for repeat colonoscopy within 3 years, the overall adenoma miss rate was 28%. Further guidelines on timing for repeat colonoscopy for fair bowel preparation are needed."
"The proteasome is an essential proteolytic machine in eukaryotic cells, where it removes damaged proteins and regulates many cellular activities by degrading ubiquitinated proteins. Its heterohexameric AAA+ ATPase Rpt subunits play a central role in proteasome activity by the engagement of substrate unfolding and translocation for degradation; however, its detailed mechanism remains poorly understood. In contrast to AAA+ ATPase domains, their N-terminal regions of Rpt subunits substantially differ from each other. Here, to investigate the requirements and roles of the N-terminal regions of six Rpt subunits derived from Saccharomyces cerevisiae, we performed systematic mutational analysis using conditional knockdown yeast strains for each Rpt subunit and bacterial heterologous expression system of the base subcomplex. We showed that the formation of the coiled-coil structure was the most important for the N-terminal region of Rpt subunits. The primary role of coiled-coil structure would be the maintenance of the ring structure with the defined order. However, the coiled-coil region would be also be involved in substrate recognition and an interaction between lid and base subcomplexes."
"A fatal case of meningo-encephalitis due to Amoeba naegleria is discussed. It was a problem for diagnosis because of its unusual presentation. The patient, a young pregnant female presented with pyrexia, severe headache, and photophobia. Anti-biotic therapy was started after a provisional diagnosis of acute pyogenic meningitis had been made. There was no response to antibiotic therapy. Facial nerve palsy and abnormal activity in the left centro-temporal region in the EEG was observed and suspicion of an intra-cranial space occupying lesion was entertained. Carotid angiography and ventriculography, however, showed no abnormality. Repeat cerebrospinal fluid examination revealed motile amoebae. The patient, however, died shortly afterwards. This case is documented for its atypical clinical presentation and therapeutic problems."
"A serum hemagglutination inhibition (HAI) titer of 40 or greater is thought to be associated with reduced influenza virus pathogenesis in humans and is often used as a correlate of protection in influenza vaccine studies. We have previously demonstrated that intramuscular vaccination of guinea pigs with inactivated influenza virus generates HAI titers greater than 300 but does not protect vaccinated animals from becoming infected with influenza virus by transmission from an infected cage mate. Only guinea pigs intranasally inoculated with a live influenza virus or a live attenuated virus vaccine, prior to challenge, were protected from transmission (A. C. Lowen et al., J. Virol. 83:2803-2818, 2009.). Because the serum HAI titer is mostly determined by IgG content, these results led us to speculate that prevention of viral transmission may require IgA antibodies or cellular immune responses. To evaluate this hypothesis, guinea pigs and ferrets were administered a potent, neutralizing mouse IgG monoclonal antibody, 30D1 (Ms 30D1 IgG), against the A/California/04/2009 (H1N1) virus hemagglutinin and exposed to respiratory droplets from animals infected with this virus. Even though HAI titers were greater than 160 1 day postadministration, Ms 30D1 IgG did not prevent airborne transmission to passively immunized recipient animals. In contrast, intramuscular administration of recombinant 30D1 IgA (Ms 30D1 IgA) prevented transmission to 88% of recipient guinea pigs, and Ms 30D1 IgA was detected in animal nasal washes. Ms 30D1 IgG administered intranasally also prevented transmission, suggesting the importance of mucosal immunity in preventing influenza virus transmission. Collectively, our data indicate that IgG antibodies may prevent pathogenesis associated with influenza virus infection but do not protect from virus infection by airborne transmission, while IgA antibodies are more important for preventing transmission of influenza viruses."
"The relaxin receptor (LGR7) and the insulin-like peptide 3 (INSL3) receptor (LGR8) are unique LGR family members in possessing a single, functionally important amino terminal LDL-A module.1 Mouse and rat cDNA was screened for LGR7 and LGR7 splice variant expression. A uterus-specific exon 4 deleted variant was identified and named LGR7-Truncate. Deletion of exon 4 results in a premature stop codon and a transcript that putatively encodes a secreted protein containing LGR7's LDL-A module. Expression of LGR7-Truncate with LGR7 in HEK-293T cells resulted in decreased relaxin-induced signaling of LGR7. LGR7-Truncate is potentially an endogenous regulator of LGR7 signaling."
"BACKGROUND: Volume-to-outcomes relationships have been established for high-risk surgical procedures. To determine whether hospital volume and academic center status affect surgical outcomes in a lower-risk procedure, morbidity and mortality in patients undergoing abdominal hysterectomy for leiomyoma were evaluated.STUDY DESIGN: Administrative data from the National Inpatient Sample were used to conduct a retrospective analysis of 172,344 individuals who had primary diagnoses of leiomyomata (ICD-9 diagnosis codes of 218.x in the first 2 positions) and who underwent abdominal hysterectomy (ICD-9 procedure codes 68.4 in the first 2 positions) from 1999 to 2003. Comparison was made between teaching hospitals versus nonteaching hospitals and annual case volume in quintiles. Morbidity was considered to be any postoperative condition that is not an expected outcome of hysterectomy and defined as instances in which a patient suffered hemorrhage, ureteral injury, bladder injury, intestinal injury, wound dehiscence, wound infection, deep vein thrombosis, pulmonary embolism, or required blood transfusion.RESULTS: A total of 37 deaths were observed. Mortality was not significantly related to hospital volume or academic medical center status. In contrast, morbidity was found to have a positive association with academic medical center status (odds ratio = 1.34; 95% CI, 1.23 to 1.45), although an inverse relationship between volume and morbidity was observed for extended length of stay (> 3 days) and blood transfusion outcomes in the first 3 (lowest) volume quintiles and for pulmonary embolism in the highest-volume quintile. No important association with volume was found for hemorrhage, ureteral injury, bladder injury, or intestinal injury.CONCLUSIONS: Unlike high-risk procedures, such as esophagectomy, pediatric cardiac surgery, and pancreaticoduodenectomy, mortality for abdominal hysterectomy done for benign indication does not improve with hospital volume or academic center status. The statistically significant positive association between academic medical center status and morbidity merits additional characterization to target areas for improvement."
"A microchip device was demonstrated that integrated enzymatic reactions, electrophoretic separation of the reactants from the products and post-separation labeling of proteins and peptides prior to detection. A tryptic digestion of oxidized insulin B-chain was performed in 15 min under stopped flow conditions in a heated channel, and the separation was completed in 1 min. Localized thermal control of the reaction channel was achieved using a resistive heating element. The separated reaction products were then labeled with naphthalene-2,3-dicarboxaldehyde (NDA) and detected by laser-induced fluorescence. A second reaction at elevated temperatures was also demonstrated for the on-chip reduction of disulfide bridges using insulin as a model protein. This device represents one of the highest levels, to date, of monolithic integration of chemical processes on a microchip."
"In order to better understand the gelation process associated with collagen assembly, and the mechanism of the in vitro morphogenetic phenomenon of ""matrix-driven translocation"" [S.A. Newman et al. (1985) Science, 228, 885-889], the viscosity and elastic modulus of assembling collagen matrices in the presence and absence of polystyrene latex beads was investigated. Viscosity measurements at very low shear rates (0.016-0.0549 s(-1)) were performed over a range of temperatures (6.9-11.5 degrees C) in a Couette viscometer. A magnetic levitation sphere rheometer was used to measure the shear elastic modulus of the assembling matrices during the late phase of the gelation process. Gelation was detected by the rapid increase in viscosity that occurred after a lag time tL that varied between O and approximately 500 s. After a rise in viscosity that occurred over an additional approximately 500 s, the collagen matrix was characterized by an elastic modulus of the order of several Pa. The lag time of the assembly process was relatively insensitive to differences in shear rate within the variability of the sample preparation, but was inversely proportional to the time the sample spent on ice before being raised to the test temperature, for test temperatures > 9 degrees C. This suggests that structures important for fibrillogenesis are capable of forming at 0 degrees C. The time dependence of the gelation process is well-described by an exponential law with a rate constant K approximately 0.1 s(-1). Significantly, K was consistently larger in collagen preparations that contained cell-sized polystyrene beads. From these results, along with prior information on effective surface tension differences of bead-containing and bead-lacking collagen matrices, we conclude that changes in matrix organization contributing to matrix-driven translocation are initiated during the lag phase of fibrillogenesis when the viscosity is < or = 0.1 Poise. The phenomenon may make use of small differentials in viscosity and/or elasticity, resulting from the interaction of the beads with the assembling matrix. These properties are well described by standard models of concentrated solutions."
"Nuclear-encoded pre-proteins being imported into complex plastids of red algal origin have to cross up to five membranes. Thereby, transport across the second outermost or periplastidal membrane (PPM) is facilitated by SELMA (symbiont-specific ERAD-like machinery), an endoplasmic reticulum-associated degradation (ERAD)-derived machinery. Core components of SELMA are enzymes involved in ubiquitination (E1-E3), a Cdc48 ATPase complex and Derlin proteins. These components are present in all investigated organisms with four membrane-bound complex plastids of red algal origin, suggesting a ubiquitin-dependent translocation process of substrates mechanistically similar to the process of retro-translocation in ERAD. Even if, according to the current model, translocation via SELMA does not end up in the classical poly-ubiquitination, transient mono-/oligo-ubiquitination of pre-proteins might be required for the mechanism of translocation. We investigated the import mechanism of SELMA and were able to show that protein transport across the PPM depends on lysines in the N-terminal but not in the C-terminal part of pre-proteins. These lysines are predicted to be targets of ubiquitination during the translocation process. As proteins lacking the N-terminal lysines get stuck in the PPM, a 'frozen intermediate' of the translocation process could be envisioned and initially characterized."
"PURPOSE: Comparisons of bladder, rectal and tympanic temperatures versus pulmonary artery (PA) temperature during different therapeutic hypothermia (TH) phases.METHODS: Twenty-one patients admitted to our emergency department (ED) after out-of-hospital cardiac arrests were included in this study. For comparison, the temperature of four different sites, urinary bladder (BL), rectal (RE), tympanic membrane (TM) digital thermometers, and a Swan-Ganz catheter were used during TH, which were controlled by a surface cooling method. TH is divided into three phases: induction, maintenance, and rewarming phase.RESULTS: In the induction phase, the mean differences between PA temperatures and those of the other methods studied were: BL (-0.24 ± 1.30°C), RE (-0.52 ± 1.40°C), and TM (1.11 ± 1.53°C). The mean differences between PA temperatures and those of the other methods in the maintenance phase were BL (0.06 ± 0.79°C), RE (-0.30 ± 1.16°C), and TM (1.12 ± 1.29°C); in the rewarming phase: BL (0.08 ± 0.86°C), RE (-0.03 ± 1.71°C), and TM (0.89 ± 1.62°C); and in the total phase: BL (0.04 ± 0.90°C), RE (-0.22 ± 1.44°C), and TM (1.03 ± 1.47°C).CONCLUSIONS: The mean difference between BL and PA temperatures is lower than those in other sites during TH. On the contrary, there are larger differences between TM and PA temperatures when compared to other sites. The differences between RE and PA temperatures are comparatively less than those between TM and PA. However, RE temperature tends to be higher than the temperature recorded by a BL thermometer or Swan-Ganz catheter during the rapid induction phase."
"Infection and inflammation suppress the expression and activity of several drug transporters in the liver. In the intestine, P-glycoprotein (PGP/mdr1) and the multidrug resistance-associated protein 2 (MRP2) are important barriers to the absorption of many clinically important drugs. The protein expression and activity of these transporters were examined during inflammation induced by lipopolysaccharide (LPS). The transport of rhodamine123 (Rho123) and 5-carboxyfluorescein (5-CF) was determined in isolated ileal segments from endotoxin-treated or control rats in the presence or absence of inhibitors. The reverse transcription-polymerase chain reaction was used to measure mRNA levels. Compared with the controls, the mRNA levels of mdr1a and mrp2 were significantly decreased by approximately 50% in the ilea of the LPS-treated rats. Corresponding reductions in the basolateral-apical efflux of Rho123 and 5-CF were observed, resulting in significant increases in the apical-basolateral absorption of these compounds. Neither the permeability of fluorescein isothiocyanate labeled dextran 4000 (FD-4), a paracellular marker, nor membrane resistance was altered. These results indicate that endotoxin-induced inflammation reduces the intestinal expression and activity of PGP and MRP2 in rats, which eliciting corresponding changes in the intestinal transport of their substrates. Hence, infection and inflammatory diseases may induce variability in drug bioavailability through alterations in the intestinal expression and activity of drug transporters."
"BACKGROUND: Although self-reported and official measures of criminal behaviour are highly correlated, the concordance between self-reports and official records appears to vary across the population. Few studies, however, have considered the range of individual traits and characteristics that might influence the relative accuracy of self-reports and official records.METHOD: Using data collected from the Australian Temperament Project, we investigated the concordance between official records and self-reports together with some of the factors that might influence it.RESULTS: Those with criminal records were 3.5 times more likely to report police contact than those with no criminal record. However, there were significant sources of individual-level variation in their convergence, and notably honest respondents were less likely to report an interaction with police. Those at risk of crime and delinquency were less likely to consent to official records searches.CONCLUSIONS: Many individual characteristics that predisposed individuals towards a criminal career also affected their willingness to consent to official records searches and the concordance between criminal records and self-reports."
"Two hundred seventy-nine patients with cervical condylomas or cervical intraepithelial neoplasia (CIN) were treated as outpatients with cryotherapy. Every patient followed received a Papanicolaou smear, colposcopic evaluation, cervical biopsy and endocervical curettage four months following treatment. The treatment failure rates for CIN 1/condyloma, CIN 2 and CIN 3 were 2.9%, 5.7% and 4.3%, respectively. The percentage of patients eventually requiring conization was 0.7, 1.6 and 2.7, respectively. No patient who had a negative clinical and histologic examination at four months subsequently had a recurrence. The mean follow-up was 23.1, 26.0 and 35.0 months for CIN 1/condyloma, CIN 2 and CIN 3, respectively. Proper triage is important with CIN 3, and a complete colposcopic examination at the initial follow-up visit is valuable for predicting outcome."
"Abnormal vascularization of the peripheral retina and retinal detachment are common clinical characteristics of Norrie disease (ND), familial exudative vitreoretinopathy, Coats' disease, and retinopathy of prematurity. Although little is known about the molecular basis of these diseases, studies have shown that all of these diseases are associated with mutations in the ND gene. In spite of this, little is known about norrin, its molecular mechanism of action, and its functional relationship with the development of abnormal retinal vasculature. To obtain a large quantity of norrin for structural and functional studies, we have overproduced it in insect cells. For this purpose, a cDNA fragment (869 bp) was isolated from a human retinal cDNA library by amplification and was cloned into an expression vector. The purified plasmid was co-transfected with wild-type linearized Bac-N-Blue DNA into S. frugiperda Sf21 insect cells. The recombinant virus plaques were purified and clones were selected based on the level of recombinant protein expressed in Sf21 cells infected with a purified recombinant virus. From these, a high-titer stock was generated and subsequently used to prepare a fused protein on a large scale. The protein was partially purified by the process of immobilized metal affinity chromatography and the use of ion exchange chromatography"
"Overexpression of interferon regulatory factor 1 (IRF-1) can induce expression of the interferon (IFN) beta gene, at least in certain cells. A role of IRF-1 in the activation of IFN-alpha genes has also been claimed. We have generated embryonal stem cells in which both IRF-1 alleles were disrupted. In undifferentiated embryonal stem cells, virus-induced levels of IFN-alpha RNA were similar for wild-type and IRF-1%, and there was little induction of IFN-beta RNA in either cell type. In 8-day differentiated cells, the levels of virus-induced IFN-beta RNA, but not of IFN-alpha RNA, were about 10-fold higher than in undifferentiated cells and only slightly higher in wild-type than in IRF-1% cells. Thus, although IRF-1 at high levels may elicit or augment induction of IFN genes under certain circumstances, it is not essential for IFN gene induction by virus. Lack of IRF-1 had no effect on the IFN-induced expression levels of the IFN-inducible genes tested; however, there was little or no constitutive expression of (2'-5')oligoadenylate synthetase in IRF-1% embryonal stem cells, in contrast to wild-type cells."
"Microalgae have attracted growing attention due to their potential in biofuel feedstock production. However, current understanding of the regulatory mechanisms for lipid biosynthesis and storage in microalgae is still limited. This study revealed that the microalga Chlorella sorokiniana showed sequential accumulation of starch and lipids. When nitrogen was replete and/or depleted over a short period, starch was the predominant carbon storage form with basal levels of lipid accumulation. After prolonged nitrogen depletion, lipid accumulation increased considerably, which was partially due to starch degradation, as well as the turnover of primary metabolites. Lipid accumulation is also strongly dependent on the linear electron flow of photosynthesis, peaking at lower light intensities. Collectively, this study reveals a relatively clear regulation pattern of starch and lipid accumulation that is basically controlled by nitrogen levels. The mixotrophic growth of C. sorokiniana shows promise for biofuel production in terms of lipid accumulation in the final biomass."
"Thirteen patients with atherosclerotic renal artery stenosis and total abdominal aortic occlusion underwent extra-anatomic surgical renal revascularization without aortic replacement. Renal artery stenosis was present unilaterally (n = 2), bilaterally (n = 7), or in a solitary kidney (n = 4). Surgical renal revascularization was indicated for treatment of severe hypertension in all patients and for preservation of renal function in 10 patients. The level of abdominal aortic occlusion was suprarenal (n = 3), perirenal (n = 2), or infrarenal (n = 8). All patients had extensive collateral vascular supply to the lower extremities with absent (n = 7) or mild (n = 6) claudication. Surgical renal revascularization was achieved with hepatorenal bypass (n = 6), mesenterorenal bypass (n = 4), or splenorenal bypass (n = 3). None of the patients underwent concomitant aortic replacement. There were no operative deaths. Postoperatively, hypertension was improved in 10 patients, unchanged in 2 patients, and worse in 1 patient. Renal function was improved in 8 patients, stable in 2 patients, and worse in 3 patients. After surgical renal revascularization, no patient required aortic replacement, while 1 patient underwent extra-anatomic revascularization of the lower extremities. We conclude that some patients with renal artery stenosis and abdominal aortic occlusion can be managed by surgical renal revascularization alone without a more extensive and potentially hazardous aortic replacement. In these patients, extra-anatomic techniques can allow safe and successful surgical renal revascularization while avoiding surgery on the diseased aorta."
"Common variable immunodeficiency (CVID) is a primary antibody deficiency syndrome characterized by defective B-cell maturation and antibody formation resulting in low serum antibody levels of all immunoglobulin (Ig) isotypes. To investigate the pathogenesis of CVID, we developed a set of competitive polymerase chain reaction for membrane-bound Ig heavy chain (mHC) mRNAs for IgM, IgG and IgA. Data on three children with CVID in group A of Bryant's classification were analysed. All the three mHC mRNA levels in Patient 1 were almost same as those in healthy controls. In Patient 2, mHC mRNA for IgM was detected at a level similar to that in controls, but mHC mRNAs for IgG and IgA heavy chains were not detected. In Patient 3, all the three mHC mRNAs were undetectable. Our data suggest that a different molecular basis exists in these patients with CVID even though all belong to group A of Bryant's classification. Use of our method facilitates a better understanding of molecular events in CVID patients and may be useful for precise classifications of CVID."
"Major depression is associated with both hypothalamic-pituitary-adrenal (HPA) axis overactivity and immune system activation. Depression is a common occurrence following interferon (IFN)-a treatment. While IFN-alpha is known to stimulate the HPA axis, little is known about the effects of exogenous IFN-a in humans on the proinflammatory cytokine interleukin (IL)-6, a marker of immune system activation. This study examined the acute effects of IFN-alpha on cortisol and IL-6 release, and the time course of any changes in these variables. Serum cortisol and plasma IL-6 were assessed in healthy volunteers over an 8-h period following 3 million units subcutaneous IFN-alpha or placebo using a double-blind, placebo-controlled crossover design. IFN-alpha resulted in a significant increase in both cortisol and IL-6. Regular sampling over 8 h did not delineate any sequential effect of the rise in these variables over time. We conclude that IFN-alpha acutely stimulates both the HPA axis and proinflammatory cytokine release. The hypothesis that the effect of IFN-alpha on the HPA axis is indirect and mediated by IL-6 was not supported by this study. Our findings are nonetheless of relevance to the aetiology of depression following IFN-alpha."
"A chiral separation using carboxymethyl-beta-cyclodextrin and methyl-beta-cyclodextrin for the direct assay of tramadol in human urine by capillary electrophoresis (CE) with laser-induced native fluorescence detection was developed. Furthermore, the phase II metabolite O-demethyl tramadol glucuronide was determined from the urine samples and the ratio of the diasteromers was determined. The chiral method was validated. Correlation coefficients were higher than 0.999. Within day variation showed accuracy in the range 96.1-105.8% with a RSD less than 6.00%. Day to day variation present an accuracy ranging from 100.2 to 103.5% with a RSD less than 5.4%. After oral administration of 150 mg tramadol hydrochloride to a healthy volunteer, the urinary excretion was monitored during 24 h. About 11.4% of the dose was excreted as 1S,2S-tramadol, 16.4% as 1R,2R-tramadol and 23.7% as O-demethyl tramadol glucuronide. The amount of 1S,2S O-demethyl tramadol glucuronide was more than three fold higher as IR,2R-O-demethyl tramadol glucuronide. The enantiomeric ratio of tramadol and the diastereomeric ratio of O-demethyl tramadol glucuronide was deviated from 1.0 showing that a stereoselective metabolism of tramadol occurs."
The aim of this work was to study peripheral vegetative regulation of pacemaker activity of the sinus (sinoatrial) node (SN) by high resolution analysis of cardiac rhythm variability (CRV) in patients with type 1 and 2 diabetes mellitus (DM). All CRV waves in the temporal and spectral regions were shown to be reduced. Regulatory effects of SN were pathologically distributed as appears from the increase of inefficient sympatho-metabolic influence due to the decrease of sympatho-parasympathetic one. Such change and redistribution of effects of regulatory SN factors are the predictors of cardiovascular disorders associated with DM1 and DM2. The low-amplitude CRV fluctuations of certain period and frequency suggest their pathogenetic relationship with DM decompensation. They differed from normal parasympathetic lengthening of a single RR-interval due to the high speed of pulse passage along parasympathetic fibers. It is supposed that these waves with periods 2.33. ± 2.35 and 2.3 ± 2.1 s and spectral density peaks 0.23 ± 0.045 and 0.24 ± 0.16 Hz showing average and moderate correlation with clinical and laboratory data (r = 0.543 ± 0.028 in DM1 and 0.388 ± 0.034 in DM2) are markers of diabetic endotoxicosis.
"It has been suggested that an extracellular matrix - and cell surface - associated glycoprotein, fibronectin, plays a role in the positioning of cells in morphogenesis and in the maintenance of orderly tissue organization. In the present study the appearance and distribution of fibronectin during in ovo chick limb development has been investigated by indirect immunofluorescence techniques in H.H. stages 20-30. Fibronectin is not detectable until just prior to the transition from the morphogenetic to the cytodifferentiation phase of development. Beginning at H.H. stage 25, successive nonrandom patterns of fibronectin detection and distribution, which resemble the subsequent cartilaginous elements, precede overt chondrogenesis as detected by Alcian blue staining. This corresponds to the onset of the cytodifferentiation phase of limb development. As the accumulation of acidic proteoglycan increases in the cartilage matrix and the mesenchymal cells become more round in appearance, the presence of detectable fibronectin decreases and is ultimately seen only in the perichondria and basement membrane. However, predigestion of developed cartilage tissue with testicular hyaluronidase, prior to fibronectin staining, indicated that fibronectin remains a major constituent of cartilage matrix and is apparently masked by cartilage-specific proteoglycans. This study of chick limb development is consistent with the hypothesis that fibronectin may be a molecule that facilitates the spatial organization of cartilaginous primordia cytodifferentiation."
"In this study, we prepared mixed micelles composed of a pH-sensitive poly(ethylene glycol)-doxorubicin conjugate prodrug and d-alpha-tocopheryl polyethylene glycol succinate (TPGS). The average hydrodynamic size of the mixed micelles was approximately 144nm, measured by dynamic light scattering. In an MTT assay the pH-sensitive prodrug was non-cytotoxic at low concentration but inhibited drug-resistant cancer cell (MCF-7/ADR) growth at high dose. The mixed micelles showed concentration-dependent cytotoxicity and significantly increased the cytotoxicity of the prodrug in MCF-7/ADR cells. Confocal laser scanning images revealed that higher concentrations of doxorubicin were successfully delivered into cell nuclei, enabling effective drug-induced cell death. Fluorescence microscopy indicated that there was less escape of the internalized doxorubicin from cells. Therefore, the enhanced drug efficacy in MCF-7/ADR cells is most likely attributed to a synergistic effect of drug-release from the pH-sensitive prodrug inside cells and suppression of P-glycoprotein efflux activity by TPGS."
"Application of soluble antigen via the oral route results in systemic antigen-specific tolerance, a therapeutic approach that has already been used for uveitis patients. In the Lewis rat experimental autoimmune uveitis (EAU) can be induced by active immunisation with retinal antigens such as retinal soluble antigen (S-Ag) or interphotoreceptor retinoid-binding protein (IRBP) and peptides thereof. These normally pathogenic antigens can also be used to induce oral tolerance. In order to optimize oral tolerance induction we analysed the effect of Labrafil M 2125 CS, an orally administrable composition for pharmaceutical use, consisting of fatty acid esters and glycerides and capable of forming micro emulsions. Feeding peptide emulsified in Labrafil M 2125 CS/PBS prior to immunisation significantly improved oral tolerance compared to feeding peptide in PBS only. We observed a delayed onset of disease, reduced intraocular inflammation and less retinal destruction. Application of Labrafil M 2125 CS without tolerogen had no effect. Combined feeding of peptide with Labrafil M 2125 CS even allowed 10-fold reduction of the tolerogenic peptide dose. Furthermore, the effect of Labrafil M 2125 CS upon oral tolerance was dose-dependent, a peptide emulsion containing 0.5-2% Labrafil M 2125 CS achieved a maximal enhancement of oral tolerance induction, suggesting that Labrafil M 2125 CS might be a useful adjuvant to enhance therapeutic use of oral tolerance."
"Fructus Corni, the dried ripe sarcocarp of Cornus officinalis Sieb.et Zucc (Cornaceae), is widely used in traditional medicine. Pharmacological studies to date have attributed many biological effects to the high-polarity components. However, current quality control methods focus on only several iridoid glycoside components, and, of note, there is no comprehensive method available to simultaneously quantify the high-polarity components in Fructus Corni. Here, a simple, sensitive and robust liquid chromatography-electrospray ionization-tandem mass spectrometry method was developed to simultaneously determine 11 high-polarity constituents (5-hydroxymethyl-2-furfural, gallic acid, sweroside, cornin, loganin, morroniside, 7á-O-methylmorroniside, 7â-O-methylmorroniside, 7á-O-ethylmorroniside and 7â-O-ethylmorroniside, cornuside) of Fructus Corni. This method showed good specificity, linearity (r2 ? 0.9907), repeatability (RSD < 5.98%) and recovery (93.24 ~ 112.92%, RSD < 9.06%). This validated method was successfully employed to assess the component variation of crude Fructus Corni of three regional origins as well as after processing. In particular, the iridoid isomers were, for the first time, included as the quality markers for Fructus Corni. We propose that this method may provide a new and powerful tool for achieving comprehensive quality control of Fructus Corni."
"Indication for pediatric tracheostomy has changed. Upper airway obstruction secondary to infectious disorders is no longer the commonest indication. The aim of this study was to establish data on indications, outcome and complications of pediatric tracheostomy. A retrospective analysis of pediatric tracheostomies carried out between March 2002 to March 2004 was done. Eighteen patients were identified. The commonest indication was prolonged ventilation (94.5%) followed by pulmonary toilet (5.5%). None was performed for upper airway obstruction. Postoperative complications were encountered in six patients (33.3%), the commonest being accidental decannulation notably in children less than six years of age. Twelve patients (66.6%) were successfully decannulated. The mortality rate was 16.6%. All death were non tracheostomy related. The commonest indication for tracheostomy was prolonged ventilation and tracheostomy in children is relatively safe despite complications."
"The present study assessed the influence of salinity and temperature on body growth and on muscle cellularity of Lophiosilurus alexaxdri vitelinic larvae. Slightly salted environments negatively influenced body growth of freshwater fish larvae and we observed that those conditions notably act as an environmental influencer on muscle growth and on local expression of hypertrophia and hypeplasia markers (IGFs and PCNA). Furthermore, we could see that salinity tolerance for NaCl 4gl(-)(1) diminishes with increasing temperature, evidenced by variation in body and muscle growth, and by irregular morphology of the lateral skeletal muscle of larvae. We saw that an increase of both PCNA and autocrine IGF-II are correlated to an increase in fibre numbers and fibre diameter as the temperature increases and salinity diminishes. On the other hand, autocrine IGF-I follows the opposite way to the other biological parameters assessed, increasing as salinity increases and temperature diminishes, showing that this protein did not participate in muscle cellularity, but participating in molecular/cellular repair. Therefore, slightly salted environments may provide adverse conditions that cause some obstacles to somatic growth of this species, suggesting some osmotic expenditure with a salinity increment."
"Seventeen selected patients (mean age, 74 years)--14 with rectal prolapse and 3 with persisting anal incontinence after previous operations--underwent high anal encirclement with polypropylene mesh. There was no operative mortality. Prolapse recurred in 2 (15 percent) of the 13 patients followed up for 6 months or more (mean, 3.5 years). Three (27 percent) of the 11 patients with associated anal incontinence improved functionally, as did the three operated on for persisting incontinence, but only one patient regained normal continence. No breakage, cutting out, or infection related to the mesh was observed. Because of the risk of fecal impaction encountered in three of our patients, the procedure is not advocated for severely constipated patients. Despite the somewhat disappointing results regarding restoration of continence, we find this method useful in patients with rectal prolapse who are unfit for more extensive surgery, in controlling the prolapse to an acceptable degree."
"The hydrocortisone (HC) induction of glycerol phosphate dehydrogenase (GPDH; EC 1.1.1.8) in rat glial C6 cells was inhibited reversibly and in a dose-dependent manner by cytochalasin B (CB). CB had no effect on basal level GPDH, total cellular RNA, DNA or protein content nor did it act as a general inhibitor of the rate of protein synthesis. CB did not appear to be acting via dissociation of microtubules since colcemid had no effect on the induction process. The addition of an alternate energy source (sodium pyruvate) did not relieve the CB inhibition of GPDH induction suggesting that CB is not exerting its effect by blocking glucose utilization. The inhibition by CB is not dependent on the temporal sequence of the induction process since it specifically inhibited GPDH induction at any time it was added. CB did not alter the rate of degradation of GPDH in these cells and direct measurements of the specific rate of synthesis of GPDH demonstrated that CB decreased the induced rate of GPDH synthesis by about 60%. The site of inhibition was more precisely defined  by experiments which demonstrated a 60% decrease in specific nuclear binding of 3H-HC even though total cellular uptake of 3H-HC was unaffected. This effect on nuclear binding of HC is sufficient to account for the decreased accumulation of GPDH activity in CB-treated cells."
"A high-performance liquid chromatographic method is described for the selective determination of taurine in biological fluids by post-column fluorescence reaction. Taurine was separated on an adsorption-distribution type Shodex Ionpac KC-811 column. Then it was converted with hypochlorite into the corresponding N-chloramine, which was allowed to react with thiamine to give fluorescent thiochrome. As little as 6 ng per injection of taurine could be determined. The average recoveries of spiked taurine in serum and urine were 99.5 +/- 2.7 and 101.8 +/- 2.9%, respectively. The method could be applied to the assay of taurine in human serum and urine with simple pretreatment."
"Chronic orthopedic infections are commonly caused by bacterial biofilms, which are recalcitrant to antibiotic treatment. In many cases, the revision procedure for periprosthetic joint infection or trauma cases includes the implantation of antibiotic-loaded bone cement to kill infecting bacteria via the elution of a strong local dose of antibiotic(s) at the site. While many studies have addressed the elution kinetics of both non-absorbable and absorbable bone cements both in vitro and in vivo, the potency of ALBC against pathogenic bacteria after extended implantation time is not clear. In this communication, we use two case studies, a Viridans streptococci infected total knee arthroplasty (TKA) and a MRSA-polymicrobial osteomyelitis of a distal tibial traumatic amputation (TA) to demonstrate that an antibiotic-loaded poly(methyl methacrylate) (ALPMMA) coated intermedullary rod implanted for 117 days (TKA) and three ALPMMA suture-strung beads implanted for 210 days (TA) retained killing ability against Pseudomonas aeruginosa and Staphylococcus aureus in vitro, despite different clinical efficacies. The TKA infection resolved and the patient progressed to an uneventful second stage. However, the TA infection only resolved after multiple rounds of debridement, IV vancomycin and removal of the PMMA beads and placement of vancomycin and tobramycin loaded calcium sulfate beads."
"A biomimetic cocaine sensor was fabricated by using poly(p-phenylene) (PPP) with cyclodextrin (CD) units in the backbone and poly(ethylene glycol) (PEG) side chains (PPP-CD-g-PEG). The sensory platform was constructed by one step surface modification of glassy carbon electrode with PPP-CD-g-PEG by drop coating. The electrochemical measurements are based on the formation of CD-cocaine inclusion complex on the surface resulting in a significant decrease in electron transfer capacity of the selected redox probe. The changes in the surface features due to cocaine binding were explored via electrochemical techniques such as differential pulse voltammetry, cyclic voltammetry and electrochemical impedance spectrometry. The sensor exhibited linearity in the range of 25-200 nM cocaine, LOD was calculated as 28.62 nM (n = 5) according to 3Sb/m formula. Finally, the sensory platform was successfully applied for the cocaine analysis in synthetic urine samples and correlated with the chromatographic method."
"INTRODUCTION: Historically, medical students have been deployed to care for disaster victims but may not have been properly educated to do so. A previous evaluation of senior civilian medical students in Belgium revealed that they are woefully unprepared. Based on the nature of their military training, we hypothesised that military medical students were better educated and prepared than their civilian counterparts for disasters. We evaluated the impact of military training on disaster education in medical science students.METHODS: Students completed an online survey on disaster medicine, training, and knowledge, tested using a mixed set of 10 theoretical and practical questions. The results were compared with those of a similar evaluation of senior civilian medical students.RESULTS: The response rate was 77.5%, mean age 23 years and 59% were males. Overall, 95% of military medical students received some chemical, biological, radiological and nuclear training and 22% took part in other disaster management training; 44% perceived it is absolutely necessary that disaster management should be incorporated into the regular curriculum. Self-estimated knowledge ranged from 3.75 on biological incidents to 4.55 on influenza pandemics, based on a 10-point scale. Intention to respond in case of an incident ranged from 7 in biological incidents to 7.25 in chemical incidents. The mean test score was 5.52; scores improved with educational level attained. A comparison of survey data from civilian senior medical master students revealed that, except for influenza pandemic, military students scored higher on knowledge and capability, even though only 27% of them were senior master students. Data on willingness to work are comparable between the two groups. Results of the question/case set were significantly better for the military students.CONCLUSIONS: The military background and training of these students makes them better prepared for disaster situations than their civilian counterparts."
"We have investigated the effect of oxidants on ligand recognition and internalization by the macrophage mannose receptor. Rat bone marrow macrophages were treated with increasing concentrations of H2O2 for 30 min at 37 degrees C. Fifty percent inhibition of ligand uptake was observed at 250 microM, with only 10% of control uptake remaining following exposure to 1 mM H2O2 for 30 min. Electron micrographic analysis of macrophages following H2O2 treatment showed no morphological alterations compared to untreated cells. Ligand uptake was also inhibited by the following H2O2 generating systems: menadione, xanthine/xanthine oxidase, glucose/glucose oxidase, and phorbol 12-myristate 13-acetate-stimulated polymorphonuclear leukocytes. Inhibition could be blocked by catalase plus or minus superoxide dismutase. Treatment of macrophages at 4 degrees C with H2O2 had no effect on ligand binding, whereas treatment with H2O2 at 37 degrees C reduced binding to 15% of control levels and decreased the number of surface receptors to one-third of control cells. H2O2 treatment inhibited ligand degradation by macrophages, but did not prevent ligand movement from the surface to the interior of the cell. In addition, ligand delivery to lysosomes was blocked by oxidant treatment. These results suggest that treatment of macrophages with reagent H2O2 or H2O2-generating systems inhibits the normal ligand delivery and receptor recycling process involving the mannose receptor. Potential mechanisms might include receptor oxidation, alterations in ATP levels, or membrane lipid peroxidation."
"AIM: To establish the methylation profile of the promoter CpG islands of 31 genes that might play etiological roles in colon carcinogenesis.METHODS: The methylation specific PCR in conjunction of sequencing verification was used to establish the methylation-profile of the promoter CpG islands of 31 genes in colorectal cancer (n = 65), the neighboring non-cancerous tissues (n = 5), colorectal adenoma (n = 8), and normal mucosa (n = 1). Immunohistochemically, expression of 10 genes was assessed on the home-made tissue microarrays of tissues from 58 patients. The correlation of tumor specific changes with each of clinical-pathologic features was scrutinized with relevant statistic tools.RESULTS: In comparison with the normal mucosa of the non-cancer patients, the following 14 genes displayed no tumor associated changes: breast cancer 1, early onset (BRCA1), cadherin 1, type 1, E-cadherin (epithelial) (CDH1), death-associated protein kinase 1 (DAPK1), DNA (cytosine-5-)-methyltransferase 1 (DNMT1), melanoma antigen, family A, 1 (directs expression of antigen MZ2-E) (MAGEA1), tumor suppressor candidate 3 (N33), cyclin-dependent kinase inhibitor 1A (p21, Cip1) (p21(WAF1)), cyclin-dependent kinase inhibitor 1B (p27, Kip1) (p27(KIP1)), phosphatase and tensin homolog (mutated in multiple advanced cancers 1) (PTEN), retinoic acid receptor, beta (RAR- , Ras association (RalGDS/AF-6) domain family 1 C (RASSF1C), secreted frizzled-related protein 1 (SFRP1), tissue inhibitor of metalloproteinase 3 (Sorsby fundus dystrophy, pseudoinflammatory) (TIMP3), and von Hippel-Lindau syndrome (VHL). The rest 17 targets exhibited to various extents the tumor associated changes. As changes in methylation of the following genes occurred marginally, their impact on the formation of colorectal cancer were trivial: adenomatous polyposis coli (APC) (8%, 5/65), Ras association (RalGDS/AF-6) domain family 1A (RASSF1A) (3%, 2/65) and cyclin-dependent kinase inhibitor 2A, alternated reading frame (p14(ARF)) (6%, 4/65). The following genes exhibited moderate changes in methylation: O-6-methylguanine-DNA methyltransferase (MGMT) (20%, 13/65), mutL homolog 1, colon cancer, nonpolyposis type 2 (E. coli) (hMLH1) (18%, 12/65), cyclin-dependent kinase inhibitor 2A (melanoma, p16, inhibits CDK4) (p16(INK4a)) (10%, 10/65), methylated in tumor 1 (MINT1) (15%, 10/65), methylated in tumor 31 (MINT31) (11%, 7/65). The rest changed greatly in the methylation pattern in colorectal cancer (CRC): cyclin A1 (cyclin a1) (100%, 65/65), caudal type homeobox transcription factor 1 (CDX1) (100%, 65/65), RAR- (85%, 55/65), myogenic factor 3 (MYOD1) (69%, 45/65), cyclin-dependent kinase inhibitor 2B (p15, inhibits CDK4) (p15(INK4b)) (68%, 44/65), prostaglandin-endoperoxide synthase 2 (prostaglandin G/H synthase and cyclooxygenase) (COX2) (72%, 47/65), cadherin 13, H-cadherin (heart) (CDH13) (65%, 42/65), CAAX box 1 (CXX1) (58%, 38/65), tumor protein p73 (p73) (63%, 41/65) and Wilms tumor 1 (WT1) (58%, 38/65). However, no significant correlation of changes in methylation with any given clinical-pathological features was detected. Furthermore, the frequent changes in methylation appeared to be an early phase event of colon carcinogenesis. The in situ expression of 10 genes was assessed by the immunohistochemical approach at the protein level: CDH1, CDH13, COX2, cyclin A1, hMLH1, MGMT, p14(ARF), p73, RAR- , and TIMP3 genes in the context of the methylation status in colorectal cancer. No clear correlation between the hypermethylation of the promoter CpG islands and the negative expression of the genes was established.CONCLUSION: The methylation profile of 31 genes was established in patients with colon cancer and colorectal adenomas, which provides new insights into the DNA methylation mediated mechanisms underlying the carcinogenesis of colorectal cancer and may be of prognostic values for colorectal cancer."
"Natriuretic peptides (NPs) and their receptors (NPRs) are expressed in the heart, but their effects on myocyte function are poorly understood. Because NPRs are coupled to synthesis of cGMP, an activator of the sarcolemmal Na(+)-K(+) pump, we examined whether atrial natriuretic peptide (ANP) regulates the pump. We voltage clamped rabbit ventricular myocytes and identified electrogenic Na(+)-K(+) pump current (arising from the 3:2 Na(+):K(+) exchange and normalized for membrane capacitance) as the shift in membrane current induced by 100 micromol/l ouabain. Ten nanomoles per liter ANP stimulated the Na(+)-K(+) pump when the intracellular compartment was perfused with pipette solutions containing 10 mmol/l Na(+) but had no effect when the pump was at near maximal activation with 80 mmol/l Na(+) in the pipette solution. Stimulation was abolished by inhibition of cGMP-activated protein kinase with KT-5823, nitric oxide (NO)-activated guanylyl cyclase with 1H-[1,2,4]oxadiazole[4,3-a]quinoxalin-1-one (ODQ), or NO synthase with N(G)-nitro-L-arginine methyl ester (L-NAME). Since synthesis of cGMP by NPR-A and NPR-B is not NO dependent or ODQ sensitive, we exposed myocytes to AP-811, a highly selective ligand for the NPR-C ""clearance"" receptor. It abolished ANP-induced pump stimulation. Conversely, the selective NPR-C agonist ANP(4-23) reproduced stimulation. The stimulation was blocked by l-NAME. To examine NO production in response to ANP(4-23), we loaded myocytes with the NO-sensitive fluorescent dye diacetylated diaminofluorescein-2 and examined them by confocal microscopy. ANP(4-23) induced a significant increase in fluorescence, which was abolished by L-NAME. We conclude that NPs stimulate the Na(+)-K(+) pump via an NPR-C and NO-dependent pathway."
"Activation of human monocytes with MDP (N-acetylmuramyl-L-alanyl-D-isoglutamine) 1-100 micrograms per ml for 48 h in vitro enhanced the cytostatic activity against the target cell line K-562, while cytolysis remained unchanged. Catalase, 600 SU per ml, had no inhibitory effect on the cytostasis mediated by MDP-activated monocytes. The optimal MDP concentration for activation was in the range 3-10 micrograms per ml. Supernatants from monocytes activated with MDP 1-30 micrograms per ml for 48 h exerted no cytostatic activity. MDP 1-100 micrograms per ml had no direct cytostatic or cytolytic effect on the target cells in a 24 h assay. When added to monocytes cultured in vitro for four days immediately prior to the chemiluminescence (CL)-assay, MDP 10-100 micrograms per ml enhanced both the zymosan and phorbol myristate acetate-triggered lucigenin-dependent CL. Monocytes pre-activated with MDP for 48 h did not demonstrate any enhanced CL-response. MDP-activation 30 micrograms per ml for 48 h increased the zymosan-triggered generation of H2O2 moderately. The enhanced cytostatic activity induced by MDP-activation is probably not mediated by hydrogen peroxidase or production of cytostatic factors."
"Acute myeloid leukaemias induced by ionizing radiation in mouse are characterized by chromosome (chr) 2 aberrations. While it is known that chr 2 aberrations form early and in abundance post-irradiation, unequivocal evidence for hypersensitivity of chr 2 in the first post-irradiation mitoses is lacking. Here it is established that chromosomal aberrations detected in bone marrow cells by chromosome painting are induced in all mice at an approximately 2-fold greater frequency in chr 2 by comparison with chrs 1 and 3 at 24 and 48 h following in vivo whole-body X-irradiation. Long-term follow up studies (to 15 months post-irradiation) indicated that chromosomal hypersensitivity is accounted for largely by the existence of hot-spots for aberration formation on sensitive chromosomes. Analysis of clonal developments suggested that chr 2 aberrant clones are selected for entry into the proliferating bone marrow cell compartment in preference to cells with other aberrations and that these clones in general have a higher proliferative potential. However, neither the induction of chr 2 aberrations nor the presence of a chr 2 aberrant clone specifically predict the development of AML in an individual irradiated mouse. Nonetheless these events or sub-groups of these events are necessary for AML development."
"The possible vasopressor effect of cyclosporine (CS) on both the systemic and pulmonary vascular beds has been investigated during bicycle exercise in 12 heart transplant recipients (mean age, 41 years) using pulmonary artery catheter measurements. Eight patients were taking cyclosporine and six azathioprine and prednisolone (AzS) as immunosuppressive therapy. With exercise, CS recipients show a significantly larger rise in systemic pressure than AzS recipients (P less than 0.001), with persistently higher pulmonary pressures (P less than 0.001). This suggests a generalized vasopressor effect of CS on the vasculature."
"This research studied the occurrence of estrogens in the Upper Satilla watershed, Georgia, USA, which was impacted by poultry litter land application and discharge from a sewage treatment plant (STP) receiving poultry wastes. Over 14 months, four estrogens in stream water, sediment, suspended particles, and STP samples were quantified by LC/MS. Estrogens were consistently found in the STP influent with high concentrations while they were below the detection limits in the majority of stream water, suspended particles, and sediment. Estrone, 17â-estradiol, and estriol were found in 18% of stream water samples with concentrations up to 46.4, 67.2, and 125 ng L(-1), respectively. However, 17á-ethinylestradiol was only detected in STP samples. Estrogens were found in 14% of suspended particle samples with the median concentration being 27.5 ng g(-1) for estrone, 104.5 ng g(-1) for 17â-estradiol, and 93.9 ng g(-1) for estriol. The estrogen concentrations in sediment were <4.95 ng g(-1), indicating that sediment is not a major sink for estrogens in this watershed. The quantitative analysis of the temporal and spatial distribution of the estrogens suggests the occasional elevation of estrogens in the watershed above the predicted-no-effect-concentrations to fish likely to be associated with litter disposal and rainfall events."
"With mounting evidence of how neighborhood socioeconomic context influences individual behavior, investigation of neighborhood social context and sex/drug use risk behavior could help explain and provide insight into solutions to solve persistent racial disparities in HIV. Interviewer-administered surveys and HIV testing among street-recruited individuals who reported illicit drug use in New York City were conducted from 2000 to 2004. Individuals were geocoded to census tracts, and generalized estimating equations were used to determine correlates of being newly diagnosed with HIV at study enrollment. Analyses were completed in 2014. Of the 920 participants, 10.5 % were HIV-positive, and among those, 45 % were diagnosed at study enrollment. After restricting the sample to those who self-reported negative HIV status (n = 867), 72 % were male, 65 % Latino, and 5.1 % tested HIV-positive. After adjustment, those testing HIV-positive were more likely to report male same-sex partnership (p < 0.01) and less likely to be homeless compared with those confirmed HIV-negative (p < 0.01). Neighborhood-adjusted models indicated those from neighborhoods with less deprivation (p < 0.05), and a higher proportion of owner-occupied homes (p < 0.01) were more likely to test HIV-positive. Additionally, Black individuals who used drugs and were from neighborhoods with a higher proportion of Black residents were more likely to be newly diagnosed compared to Latino individuals who used drugs and were from neighborhoods with lower proportions of Black residents (p < 0.05). These data suggest that HIV prevention and treatment efforts should continue widening its reach to those unaware of their HIV infection, namely men who have sex with men, heavy, drug-involved Black communities, and both Black and Latino communities from relatively less disadvantaged neighborhoods."
"BACKGROUND: Articular cartilage contributes to transferring enormous loads as uniformly as possible from one skeletal segment to the next. Whether it manages this task when subjected to the high repetitive loading cycles occurring during long-distance running and can remain intact is still the topic of controversy.PURPOSE: To investigate the changes in cartilaginous volumes of the tibia, patella, and medial and lateral menisci after extreme dynamic loading as occurs in long-distance runners.STUDY DESIGN: Controlled laboratory study.METHODS: Forty-eight knees of male athletes were studied (38 +/- 14 years). The subjects ran around a predetermined and precisely measured course (5, 10, 20 km), the beginning and end of the run being in the magnetic resonance imaging investigation room. The scan protocol was 60-minute rest period, first measurement, run, 3-minute delay, and second measurement.RESULTS: Overall, there were significant reductions in volume (P < .05) for the patella, tibia, and menisci. There was evidence of significant change after a running distance of 5 km. A further statistical reduction of the volume could only be demonstrated for the medial meniscus after 10 and 20 km.CONCLUSION: Tibial, patellar, and meniscal cartilaginous volumes show not only load-dependent volume changes but also an asymptotic curve. This is the first time that meniscal volume changes due to loading have been used as an indicator of the important mechanical contribution that the menisci make to sustaining the knee during repetitive loading.CLINICAL RELEVANCE: On the basis of the results of this study, the authors assume that the cartilage is able to adapt well to the loads caused by running."
"OBJECTIVES: This research investigated the prognostic significance of radiographically detectable coronary calcific deposits.BACKGROUND: Coronary calcific deposits are almost always associated with coronary atherosclerosis. We investigated the association between fluoroscopically determined coronary calcium and coronary heart disease end points at 1 year of follow-up.METHODS: This prospective population-based cohort study was conducted in the suburbs of Los Angeles. Fourteen hundred sixty-one asymptomatic adults with an estimated > or = 10% risk of having a coronary heart disease event within 8 years underwent cardiac cinefluoroscopy for assessment of coronary calcium at initiation of the study. Clinical status including angina, documented myocardial infarction, myocardial revascularization and death from coronary heart disease were determined after 1 year.RESULTS: The prevalence of calcific deposits was high (47%). A follow-up examination at 1 year was successfully completed in 99.9% of subjects. Six subjects (0.4%) had died from coronary heart disease and 9 (0.6%) had had a nonfatal myocardial infarction. Thirty-seven subjects (2.5%) reported angina pectoris, and 13 (0.9%) had undergone myocardial revascularization. Fifty-three subjects had at least one event during the 1-year period. Radiographically detectable calcium was associated with the presence of at least one of these end points, with a risk ratio of 2.7 (confidence limits 1.4, 4.6). The presence of coronary calcium was an independent predictor of at least one end point when controlling for age, gender and risk factors. However, three deaths due to coronary heart disease and two nonfatal myocardial infarctions occurred in subjects without detectable coronary calcium.CONCLUSIONS: The presence of coronary calcific deposits incurs an increased risk of coronary heart disease events in asymptomatic high risk subjects at 1 year. This increased risk is independent of that incurred by standard risk factors."
"Look, feel, move is a simple and widely taught sequence to be followed when undertaking a clinical examination in orthopaedics (Maher et al., 1994; McRae, 1999; Solomon et al., 2010). The splinting of an acute tibial fracture with a posterior back-slab is also common practice; with the most commonly taught design involving covering the dorsum of the foot with bandaging (Charnley, 1950; Maher et al., 1994; McRae, 1989). We investigated the effect of the visual cues provided by exposing the dorsum of the foot and marking the dorsalis pedis pulse. We used a clinical simulation in which we compared the quality of the recorded clinical examination undertaken by 30 nurses. The nurses were randomly assigned to assess a patient with either a traditional back-slab or one in which the dorsal bandaging had been cut back and the dorsalis pedis pulse marked. We found that the quality of the recorded clinical examination was significantly better in the cut-back group. Previous studies have shown that the cut-back would not alter the effectiveness of the back-slab as a splint (Zagorski et al., 1993). We conclude that all tibial back-slabs should have the bandaging on the dorsum of the foot cut back and the location of the dorsalis pedis pulse marked. This simple adaptation will improve the subsequent clinical examinations undertaken and recorded without reducing the back-slab's effectiveness as a splint."
"As a study of grief resolution, 71 surviving spouses of patients who had died in a hospice or a hospital acute care oncology ward were interviewed in their homes 6 and 12 months following the death of their mate. It was hypothesized that hospice survivors would score significantly lower on measures of depression and anxiety, would be more involved socially, would be more involved in constructive social action, and would be less likely to use tranquilizers than hospital survivors. At 6 months there is only partial support for the hypotheses. At 12 months there is strong support for the hypotheses. Interpretations of these findings and comparisons with similar studies are discussed."
"Autophagy plays a key role during Salmonella infection, by eliminating these pathogens following escape into the cytosol. In this process, selective autophagy receptors, including the myosin VI adaptor proteins optineurin and NDP52, have been shown to recognize cytosolic pathogens. Here, we demonstrate that myosin VI and TAX1BP1 are recruited to ubiquitylated Salmonella and play a key role in xenophagy. The absence of TAX1BP1 causes an accumulation of ubiquitin-positive Salmonella, whereas loss of myosin VI leads to an increase in ubiquitylated and LC3-positive bacteria. Our structural studies demonstrate that the ubiquitin-binding site of TAX1BP1 overlaps with the myosin VI binding site and point mutations in the TAX1BP1 zinc finger domains that affect ubiquitin binding also ablate binding to myosin VI. This mutually exclusive binding and the association of TAX1BP1 with LC3 on the outer limiting membrane of autophagosomes may suggest a molecular mechanism for recruitment of this motor to autophagosomes. The predominant role of TAX1BP1, a paralogue of NDP52, in xenophagy is supported by our evolutionary analysis, which demonstrates that functionally intact NDP52 is missing in Xenopus and mice, whereas TAX1BP1 is expressed in all vertebrates analysed. In summary, this work highlights the importance of TAX1BP1 as a novel autophagy receptor in myosin VI-mediated xenophagy. Our study identifies essential new machinery for the autophagy-dependent clearance of Salmonella typhimurium and suggests modulation of myosin VI motor activity as a potential therapeutic target in cellular immunity."
"OBJECTIVE: To compare the postoperative survival rate of laryngeal carcinoma patient at stage III or IV whom accepted partial laryngectomy and total laryngectomy.METHOD: We performed a retrospective cohort follow-up study of 126 patients of stage III or IV who underwent operation for laryngeal carcinoma in Chinese PLA General Hospital between January, 2005 and December, 2009. Survival rates were calculated by product-limit method.RESULT: There were 80 patients at stage III and 46 patient at stage IV. Sixty five patients underwent partial laryngectomy and 61 patients underwent total laryngectomy. There were 24 patients in whole group died in the 5 years, 15 of them underwent partial and 9 accepted total laryngectomy. The 5-years survival rate of partial and total group were 62.58% and 68.74% respectively. The survival curve of both groups had no significant difference (P < 0.05).CONCLUSION: For laryngeal carcinoma patients at later stage, with suitable operative indication, the partial laryngectomy could achieve an acceptable effect as well as total laryngectomy."
This article provides a historical perspective for the patenting of gene sequences and describes the fundamentals and evolution of patent law. It summarizes federal technology transfer law and policy and assesses the impacts of patenting on academic research. The patentability of gene sequences is then considered along with potential impacts that published sequence data may have on obtaining patent protection for downstream products. Industry's position on gene patenting is summarized and perspectives from the emerging public record on these issues are presented. The article discussing points at which the filing of patent applications and the licensing of patents may be appropriate. It concludes that technology transfer policies for genome research must be adopted carefully so that they remain viable in a time of rapid technological change.
"We performed bedside testing for peripheral neuropathy in our systemic sclerosis (SSc) population to determine whether foot care guidelines should be developed for SSc. Twenty consecutive SSc patients and 20 healthy control (HC) patients were evaluated for peripheral neuropathy in both feet using the 10-g Semmes-Weinstein monofilament examination (SWME) and 128 Hz vibration sensation using the on-off method. Independent, blinded, vibratory sensation, and SWME evaluations were performed on each subject by two investigators who had completed a training session to standardize each exam. An additional consecutive 20 patients with type 2 diabetes mellitus (DM) were examined by a diabetologist to compare with peripheral neuropathy prevalence in SSc patients. We examined the inter-rater variability using Cohen's kappa. We compared SWME and vibratory sensation in SSc to HC using Fisher's exact. The t test was used to compare duration of disease and modified Rodnan skin score (mRSS) for those with abnormal SWME or vibratory sensation. Two of 20 SSc patients reported sensory foot symptoms consistent with peripheral neuropathy prior to the examination. Inter-rater agreement for both SWME and vibratory sensation was strong (kappa: 0.72 and 0.83, respectively). Two HC and 12 SSc patients demonstrated abnormal vibratory sense (one-sided Fishers' exact, p < 0.002). No HC and four SSc patients had abnormal monofilament exams (one-sided Fisher's exact, p = 0.053). Neither mRSS (p = 0.28) nor duration of non-Raynauds (p = 0.07) symptoms differed between those with peripheral neuropathy and those without. Duration of Raynaud's symptoms were clinically significantly associated with presence of peripheral neuropathy (p = 0.04). The prevalence of sensory loss to monofilament in SSc was identical to DM patients (4/20). SSc patients have a considerable prevalence of pedal peripheral neuropathy as detected by loss of vibratory sensation or inability to sense the 10-g SWME. Further studies are indicated to determine if routine screening for neuropathy and subsequent podiatric care for SSc patients with abnormalities can reduce pedal complications."
"The recent increase in immigration of people from areas endemic for Chagas disease (Trypanosoma cruzi) to the United States and Europe has raised concerns about the transmission via blood transfusion and organ transplants in these countries. Infection by these pathways occurs through blood trypomastigotes (BT), and these forms of T. cruzi are completely distinct of metacyclic trypomastigotes (MT), released by triatomine vector, in relation to parasite-host interaction. Thus, research comparing infection with these different infective forms is important for explaining the potential impacts on the disease course. Here, we investigated tissue parasitism and relative mRNA expression of cytokines, chemokines, and chemokine receptors in the heart during acute infection by MT or BT forms in dogs. BT-infected dogs presented a higher cardiac parasitism, increased relative mRNA expression of pro-inflammatory and immunomodulatory cytokines and of the chemokines CCL3/MIP-1á, CCL5/RANTES, and the chemokine receptor CCR5 during the acute phase of infection, as compared to MT-infected dogs. These results suggest that infection with BT forms may lead to an increased immune response, as revealed by the cytokines ratio, but this kind of immune response was not able to control the cardiac parasitism. Infection with the MT form presented an increase in the relative mRNA expression of IL-12p40 as compared to that of IL-10 or TGF-â1. Correlation analysis showed increased relative mRNA expression of IFN-ã as well as IL-10, which may be an immunomodulatory response, as well as an increase in the correlation of CCL5/RANTES and its CCR5 receptor. Our findings revealed a difference between inoculum sources of T. cruzi, as vectorial or transfusional routes of T. cruzi infection may trigger distinct parasite-host interactions during the acute phase, which may influence immunopathological aspects of Chagas disease."
"BACKGROUND: Healthcare workers particularly doctors are at high risk of being victims of verbal and physical violence perpetrated by patients or their relatives. There is a paucity of studies on work-related violence against doctors in India. We aimed to assess the exposure of workplace violence among doctors, its consequences among those who experienced it and its perceived risk factors.METHODS: This study was done among doctors working in a tertiary care hospital in Delhi. Data were collected by using a self-administered questionnaire containing items for assessment of workplace violence against doctors, its consequences among those who were assaulted, reporting mechanisms and perceived risk factors.RESULTS: Of the 169 respondents, 104 (61.4%) were men. The mean (SD) age of the study group was 28.6 (4.2) years. Sixty-nine doctors (40.8%) reported being exposed to violence at their workplace in the past 12 months. However, there was no gender-wise difference in the exposure to violence (p=0.86). The point of delivery of emergency services was reported as the most common place for experiencing violence. Verbal abuse was the most common form of violence reported (n=52; 75.4%). Anger, frustration and irritability were the most common symptoms experienced by the doctors who were subjected to violence at the workplace. Only 44.2% of doctors reported the event to the authorities. 'Poor communication skills' was considered to be the most common physician factor responsible for workplace violence against doctors.CONCLUSIONS: A large proportion of doctors are victims of violence by their patients or relatives. Violence is being under-reported. There is a need to encourage reporting of violence and prepare healthcare facilities to tackle this emerging issue for the safety of physicians."
"Changes in the cross-sectional shape, size, bone mass and amount of unmineralised osteoid tissue were studied in 17 dissected tibiae from spina-bifida babies who died with paralysis and foot deformities and in 14 tibiae from non-spina bifida controls of matching age. In addition, 12 tibiae from young experimental rats with myotomy of foot dorsiflexors and foot plantiflexors were double-labelled with bone-seeking markers and studied in order to find the role of experimental muscle imbalance in the dynamic remodelling of the developing long bones. It was found that in tibiae from spina-bifida children with paralysis the total area of cortical bone, its thickness, number of Haversian systems and number of large remodelling cavities are diminished. Significant changes in the cross-sectional shape of the midshaft of the tibia were found, ranging from the triangular shape seen in normal babies and in those with spina bifida and calcaneus-type foot deformity, to the circular shape of tibiae from babies with spina-bifida paralysis and no foot deformity or with spina bifida and equinovarus-type of deformity. Results of experimental myotomy on growing rats showed the direct influence of working muscles on the remodelling process of growing tibiae. On the side of myotomy the flat cortex resumed a bulging convex shape and the centre of gravity shifted towards the myotomised side. These principles cannot on their own explain the specific changes in the shape of human tibiae found during anatomical studies. There is, however, a common denominator in these apparently contradictory findings. This is the combined action of two factors previously reported: the combination of paralysis of the growing limb and mechanical intra-uterine pressure acting on it. The findings in the present study also indicate that they played a major role in the production of deformities. The total amount of osteoid tissue in spina-bifida paralysed bone is increased. This delay of mineralisation of newly laid-down bone matrix would lead to softening of the new bone matrix and osteoid-rich subepiphyseal and metaphyseal regions. This 'paralytic rickets', together with the diminished total bone mass found, could probably be the cause of the common spina-bifida fractures in these regions."
"The effect of social and environmental conditions on the epidemiology of Rheumatoid Factor (RF) in healthy population was studied. For this purpose 2 sample of 828 subject was studied using 5 tests for RF. Our sample included: 419 randomized subjects from a quarter at the outskirts of Bologna (high rate immigration, non homogeneous habits and health background; 409 randomized subjects from one rural small town on the Romagna's hills (no immigration, no change in residence or profession for generations, uniform, habits and health background). These tests included: three on slide with binded human (2) and rabbit (1) IgGs and sheep-cells tests-one on slide (Scat) and the other in tube (W.-Rose by Mizuoka). The most frequent positive test was Scat test, in urban population and one human IgG latex-test in the rural population. No urban subject reacted at the same time to 4 or 5 tests. Associated positive results for 2, 3 or 4 tests are usually observed in rural subjects. These results are the expression of the different and more uniform social and biological background acquired by rural people."
"The activity of oxidative enzymes of the Krebs cycle was examined in white rats during hypokinesia. On hypokinesia day 7 the cytosol activity of NAD-dependent isocitrate dehydrogenase (ICDH) increased and that of malic-enzyme decreased. On hypokinesia days 30 and 45 the activity of succinate dehydrogenase (SDH) and alpha-ketoglutarate dehydrogenase (alpha-KGDH) decreased, that of cytoplasmatic malate dehydrogenases (MDH) slightly increased, and that of NADP ICDH declined. On hypokinesia day 60 the total activity of mitochondrial dehydrogenases reduced due to a low protein content of the mitochondrial fraction, whereas the specific activity either remained unchanged (ICDH, NAD MDH, alpha-KGDH) or increased (SDH, NADP MDH). On recovery day 25 only the activity of mitochondrial NAD-dependent malate and isocitrate dehydrogenases returned to normal."
"Voluntary spatial attention concentrates neural resources at the attended location. Here, we examined the effects of spatial attention on spatial position selectivity in humans. We measured population receptive fields (pRFs) using high-field functional MRI (fMRI) (7T) while subjects performed an attention-demanding task at different locations. We show that spatial attention attracts pRF preferred positions across the entire visual field, not just at the attended location. This global change in pRF preferred positions systematically increases up the visual hierarchy. We model these pRF preferred position changes as an interaction between two components: an attention field and a pRF without the influence of attention. This computational model suggests that increasing effects of attention up the hierarchy result primarily from differences in pRF size and that the attention field is similar across the visual hierarchy. A similar attention field suggests that spatial attention transforms different neural response selectivities throughout the visual hierarchy in a similar manner."
"CONCLUSION: The Na-K-2Cl cotransporter-1 (NKCCl) may be essential for the maintenance and functioning of the vestibular morphology in mice and it is strongly expressed in human vestibular end organs.OBJECTIVE: NKCCl is a member of the cation-coupled chloride transporter which participates in salt transport and cell volume regulation in diverse tissues. NKCCl-deficient mice exhibit deafness, and show structural alterations in the cochlea. In addition to hearing loss, NKCCl-deficient mice show a shaker-waltzer behavior, which suggests a vestibular system defect. In this study we investigated the morphology of the vestibular system of NKCCl-deficient mice and also evaluated whether NKCCl mRNA and its protein are expressed in human vestibular end organs.MATERIAL AND METHODS: NKCCl-deficient and wild-type mice aged 4-5 weeks were sacrificed. Their heads were cut in the midsagittal plane, fixed and decalcified. For light microscopy, 5-microm sections were cut and stained with hematoxylin-eosin. Human vestibular end organs were harvested during acoustic tumor surgery via a translabyrinthine approach. Some of these end organs were used for total mRNA extraction and the remainder for immunostaining. Reverse transcriptase polymerase chain reaction and immunostaining were performed for NKCCl.RESULTS: The scala media of the cochleae of the NKCCl-deficient mice had collapsed but the bony labyrinth appeared unaffected. However, the semicircular canals (SCCs) were much smaller than those in the wild-type mice. Furthermore, the SCCs were completely missing in some NKCCl-deficient mice. NKCCl mRNA was expressed in both the human macula and crista ampullaris, and its protein was expressed mainly in the transitional and dark cell areas of the human crista ampullaris."
"Osteoarthritis (OA) is the most prevalent form of human arthritis which is characterized by the degradation of cartilage and inflammation. As a rare Sirt6 activator, cyanidin is the major component of anthocyanins commonly found in the Mediterranean diet, and increasing evidence has shown that cyanidin exhibits anti-inflammatory effects in a variety of diseases. However, the anti-inflammatory effects of cyanidin on OA have not been reported. In the present study, we identified that cyanidin treatment could strongly suppress the expression of NO, PGE2, TNF-á, IL-6, iNOs, COX-2, ADAMTS5 and MMP13, and reduce the degradation of aggrecan and collagen II in IL-1â-induced human OA chondrocytes, indicating the anti-inflammatory effect of cyanidin. Further investigation of the mechanism involved revealed that cyanidin could upregulate the Sirt6 level in a dose-dependent manner and Sirt6 silencing abolished the effect of cyanidin in IL-1â-stimulated human OA chondrocytes, indicating a stimulatory effect of cyanidin on Sirt6 activation. Meanwhile, we found that cyanidin could inhibit the NF-êB pathway in IL-1â-stimulated human OA chondrocytes and its effect may to some extent depend on Sirt6 activation, suggesting that cyanidin may exert a protective effect through regulating the Sirt6/NF-êB signaling axis. Moreover, the in vivo study also proved that cyanidin ameliorated the development of OA in surgical destabilization of the medial meniscus (DMM) mouse OA models. In conclusion, these results demonstrate that cyanidin may have therapeutic potential for the treatment of OA."
"Cigarette smoke contains irritants and vasoactive substances. We wanted to determine the effect of smoking a cigarette and of nasally or orally inhaled nicotine on airway blood flow (Q(aw)) and airflow in smokers. In ten healthy current smokers, Q(aw), FEV(1), and FEF(25-75) were measured before and at 5, 30, and 180 min after smoking a cigarette. The effects of systemic nicotine using a nicotine nasal spray and local nicotine using a nicotine inhaler were also studied. Mean (+/- SE) Q(aw) increased by 81% +/- 16% (p = 0.03) 5 min after smoking a cigarette and was no longer different from baseline at 30 and 180 min. Nicotine nasal spray and nicotine oral inhaler had no effect on Q(aw.) FEV(1) and FEF(25-75) remained unchanged after smoking a cigarette and after local or systemic nicotine administration. Smoking a cigarette is followed by a transient increase in airway blood flow but no changes in airflow. Nicotine, at the rate and dose provided by the nasal spray (systemic action) and oral inhaler (local and systemic action), does not appear to be involved in the Q(aw) change, suggesting a pharmacologic or nonspecific irritant effect of other cigarette smoke constituents."
"To assess abnormalities of sensory conduction in anorectal disease we have evaluated peripheral sensory perception and somatosensory evoked potentials produced by rectal stimulation in control subjects and patients with either constipation or idiopathic faecal incontinence. Evoked potentials were also recorded after posterior tibial and dorsal genital nerve stimulation. Rectal sensation was also assessed using electrical stimulation. Reproducible evoked potential recordings after anorectal stimulation were possible in only a minority of subjects and when recorded showed intersubject and intrasubject variation. In the constipated group there was a significant difference in rectal electrical sensation (P < 0.05) from controls. We conclude that peripheral sensory testing demonstrates an abnormality in severe constipation. However, cerebral evoked potentials cannot be reliably recorded after rectal stimulation, and when recorded the latencies are of too broad a range to discriminate between health and disease. This probably relates to the difference between somatic and visceral pathways."
"Plating on solid media is the standard technique used in most laboratories for the isolation of Helicobacter pylori from gastric biopsies. Recently, various selective media were developed for this purpose. We compared and evaluated three selective media, Skirrow's, Dent's CP, and modified Glupczynski's Brussels campylobacter charcoal media, and chocolate agar medium for the isolation of H. pylori. Gastric biopsies taken from a total of 203 patients were plated in parallel on all four media. An isolation rate of 51% (104 of 203) was obtained with a combination of all four media. Of the 104, 92 (88%) were positive with Dent's medium and with modified Glupczynski's medium. Skirrow's medium gave the highest isolation rate, 96% (100 of 104). However, growth of H. pylori was scant (only one to five colonies) when growth occurred on Skirrow's medium alone. Overall, modified Glupczynski's medium provided significantly heavier growth. Chocolate agar medium yielded a 76% (79 of 104) positivity rate. We recommend the use of a combination of two selective media for the maximum recovery of H. pylori from antral biopsies."
"Release of arachidonic acid (AA) and subsequent formation of a lipoxygenase (LOX) metabolite(s) is an obligatory signal to induce spreading of HeLa cells on a gelatin substratum (Chun and Jacobson, 1992). This study characterizes signaling pathways that follow the LOX metabolite(s) formation. Levels of diacylglycerol (DG) increase upon attachment and before cell spreading on a gelatin substratum. DG production and cell spreading are insignificant when phospholipase A2 (PLA2) or LOX is blocked. In contrast, when cells in suspension where PLA2 activity is not stimulated are treated with exogenous AA, DG production is turned on, and inhibition of LOX turns it off. This indicates that the formation of a LOX metabolite(s) from AA released during cell attachment induces the production of DG. Consistent with the DG production is the activation of protein kinase C (PKC) which, as with AA and DG, occurs upon attachment and before cell spreading. Inhibition of AA release and subsequent DG production blocks both PKC activation and cell spreading. Cell spreading is also blocked by the inhibition of PKC with calphostin C or sphingosine. The inhibition of cell spreading induced by blocking AA release is reversed by the direct activation of PKC with phorbol ester. However, the inhibition of cell spreading induced by PKC inhibition is not reversed by exogenously applied AA. In addition, inhibition of PKC does not block AA release and DG production. The data indicate that there is a sequence of events triggered by HeLa cell attachment to a gelatin substratum that leads to the initiation of cell spreading: AA release, a LOX metabolite(s) formation, DG production, and PKC activation. The data also provide evidence indicating that HeLa cell spreading is a cyclic feedback amplification process centered on the production of AA, which is the first messenger produced in the sequence of messengers initiating cell spreading. Both DG and PKC activity that are increased during HeLa cell attachment to a gelatin substratum appear to be involved. DG not only activates PKC, which is essential for cell spreading, but is also hydrolyzed to AA. PKC, which is initially activated as consequence of AA production, also increases more AA production by activating PLA2."
"Electroencephalography (EEG) records brain electrical activity at the scalp level. As a functional and non invasive witness of brain activity, EEG has long raised the interest of researchers and practitioners, notably in the domain of anesthesia. Thanks to technical advances, this complex signal can now be dissected, and a huge amount of information can be extracted from it. This information gives the opportunity to quantify theeffects of general anesthesia on the brain, and provides a better understanding of the underlying mechanisms."
"Studies of the pig muscle lactate dehydrogenase (LDH) activity were permormed after incubating the enzyme solution with extracts of loach (Misgurnus fossilis) eggs and embryos, which were subjected to the influence of insulin hydrocortisone as well as to insulin combination with actinomycin D, cycloheximide or puromycin. The insulin alone is established to decrease the inactivating ability of the investigated extracts on the lactate dehydrogenase activity, when antibiotics removed to a considerable extent the influence of hormone. In the eggs and embryos there are proteins activating and stabilizing the LDH molecule. The level of LDH activation under the influence of the eggs and embryos extracts subjected to the insulin action was decreased. The addition of hydrocortisone to the medium for the incubation of eggs and embryos does not affect significantly the LDH-inactivating ability of their extracts."
"BACKGROUND: EMODIN (1,3,8-trihydroxy-6-methylanthraquinone) is a Chinese herbal anthraquinone derivative from the rhizome of rhubarb (Rheum palmatum L.) that exhibits numerous biological activities, such as antitumor, antibacterial, antiinflammatory, and immunosuppressive. In the present studies, the anti-allergic activities of emodin were investigated to elucidate the underlying active mechanisms.METHODS: The inhibitory effects of emodin on the IgE-mediated allergic response in rat basophilic leukemia (RBL-2H3) cells were evaluated by measuring the release of granules and cytokines. The Ca2+ mobilization in RBL-2H3 cells loaded with the Ca(2+)-reactive fluorescent probe Fluo-4 AM was also measured by laser scanning confocal microscope.RESULTS: Emodin inhibited the release of â-hexosaminidase (â-HEX; IC50 = 5.5 ìM) and tumor necrosis factor (TNF)-á (IC50 = 11.5 ìM) from RBL-2H3 cells induced by 2,4-dinitrophenylated bovine serum albumin (DNP-BSA) and displayed stronger inhibition of â-HEX release than ketotifen fumarate salt (IC50 = 63.8 ìM). Emodin at a concentration of 12.5 ìM also inhibited the DNP-BSA-induced influx of extracellular Ca2+ in RBL-2H3 cells.CONCLUSIONS: These results suggested that emodin likely exhibits anti-allergic activities via increasing the stability of the cell membrane and inhibiting extracellular Ca2+ influx."
"BACKGROUND AND PURPOSE: To identify predictive factors for the development of gastroduodenal toxicity (GDT) in cirrhotic patients treated with three-dimensional conformal radiotherapy (3D-CRT) for hepatocellular carcinoma (HCC).MATERIALS AND METHODS: We retrospectively analyzed dose-volume histograms (DVHs) and clinical records of 73 cirrhotic patients treated with 3D-CRT for HCC. The median radiation dose was 36 Gy (range, 30-54 Gy) with a daily dose of 3 Gy. The grade of GDT was defined by the Common Toxicity Criteria Version 2. The predictive factors of grade 3 GDT were identified.RESULTS: Grade 3 GDT was found in 9 patients. Patient's age and the percentage of gastroduodenal volume receiving more than 35 Gy (V(35)) significantly affected the development of grade 3 GDT. Patients over 50 years of age developed grade 3 GDT more frequently than patients under 50 years of age. The risk of grade 3 GDT grew exponentially as V(35) increased. The 1-year actuarial rate of grade 3 GDT in patients with V(35)<5% is significantly lower than that in patients with a V(35)> or =5% (4% vs. 48%, p<.01).CONCLUSIONS: Patient's age and V(35) were the most predictive factors for the development of grade 3 GDT in patients treated with RT."
"INTRODUCTION: Strangulation of the intestine as the result of compression of its blood supply in a tightly closed gastroschisis defect is a very rare occurrence.CLINICAL CASES: We present the cases of two newborn patients who had extra-abdominal infarcted bowel and intra-abdominal jejunal atresia due to vascular compression for gastroschisis defect. One was associated with colonic, probably acquired aganglionosis. Both had similar clinical courses.CONCLUSIONS: This association is very uncommon. Prognosis of this complex is very poor."
"ARHGEF9 resides on Xq11.1 and encodes collybistin, which is crucial in gephyrin clustering and GABAA receptor localization. ARHGEF9 mutations have been identified in patients with heterogeneous phenotypes, including epilepsy of variable severity and intellectual disability. However, the mechanism underlying phenotype variation is unknown. Using next-generation sequencing, we identified a novel mutation, c.868C > T/p.R290C, which co-segregated with epileptic encephalopathy, and validated its association with epileptic encephalopathy. Further analysis revealed that all ARHGEF9 mutations were associated with intellectual disability, suggesting its critical role in psychomotor development. Three missense mutations in the PH domain were not associated with epilepsy, suggesting that the co-occurrence of epilepsy depends on the affected functional domains. Missense mutations with severe molecular alteration in the DH domain, or located in the DH-gephyrin binding region, or adjacent to the SH3-NL2 binding site were associated with severe epilepsy, implying that the clinical severity was potentially determined by alteration of molecular structure and location of mutations. Male patients with ARHGEF9 mutations presented more severe phenotypes than female patients, which suggests a gene-dose effect and supports the pathogenic role of ARHGEF9 mutations. This study highlights the role of molecular alteration in phenotype expression and facilitates evaluation of the pathogenicity of ARHGEF9 mutations in clinical practice."
"Centrioles are vital cellular structures that organise centrosomes and cilia. Due to their subresolutional size, centriole ultrastructural features have been traditionally analysed by electron microscopy. Here we present an adaptation of magnified analysis of the proteome expansion microscopy method, to be used for a robust analysis of centriole number, duplication status, length, structural abnormalities and ciliation by conventional optical microscopes. The method allows the analysis of centriole's structural features from large populations of adherent and nonadherent cells and multiciliated cultures. We validate the method using EM and superresolution microscopy and show that it can be used as an affordable and reliable alternative to electron microscopy in the analysis of centrioles and cilia in various cell cultures. LAY DESCRIPTION: Centrioles are microtubule-based structures organised as ninefold symmetrical cylinders which are, in human cells, ?500 nm long and ?230 nm wide. Centrioles assemble dozens of proteins around them forming centrosomes, which nucleate microtubules and organise spindle poles in mitosis. Centrioles, in addition, assemble cilia and flagella, two critically important organelles for signalling and motility. Due to centriole small size, electron microscopy has been a major imaging technique for the analysis of their ultrastructural features. However, being technically demanding, electron microscopy it is not easily available to the researchers and it is rarely used to collect large datasets. Expansion microscopy is an emerging approach in which biological specimens are embedded in a swellable polymer and isotopically expanded several fold. Physical separation of cellular structures allows the analysis of, otherwise unresolvable, structures by conventional optical microscopes. We present an adaptation of expansion microscopy approach, specifically developed for a robust analysis of centrioles and cilia. Our protocol can be used for the analysis of centriole number, duplication status, length, localisation of various centrosomal components and ciliation from large populations of cultured adherent and nonadherent cells and multiciliated cultures. We validate the method against electron microscopy and superresolution microscopy and demonstrate that it can be used as an accessible and reliable alternative to electron microscopy."
"A spectrofluorimetric method, involving alkaline degradation and formation of a magnesium complex, is described for the determination of tetracycline (TC) and anhydrotetracycline (ATC) in their mixed solution. Tetracycline is degraded and determined in alkaline solution. This treatment of ATC produces almost no fluorescence, but a fluorescent magnesium complex forms at pH 7.5. Several synthetic samples of TC and ATC, with TC:ATC ratios ranging from 50:1 to 1:50, were analysed. The recoveries of TC and ATC are about 71-76 and 61-63% in serum, respectively, and are all about 100% in urine."
"Background: Although consensus exists that malaria in pregnancy (MiP) increases the risk of malaria in infancy, and eventually nonmalarial fevers (NMFs), there is a lack of conclusive evidence of benefits of MiP preventive strategies in infants.Methods: In Burkina Faso, a birth cohort study was nested to a clinical trial assessing the effectiveness of a community-based scheduled screening and treatment of malaria in combination with intermittent preventive treatment with sulfadoxine-pyrimethamine (CSST/IPTp-SP) to prevent placental malaria. Clinical episodes and asymptomatic infections were monitored over 1 year of follow-up to compare the effect of CSST/IPTp-SP and standard IPTp-SP on malaria and NMFs.Results: Infants born during low-transmission season from mothers receiving CSST/IPTp-SP had a 26% decreased risk of experiencing a first clinical episode (hazard ratio, 0.74 [95% confidence interval, .55-0.99]; P = .047). CSST/IPTp-SP interacted with birth season and gravidity to reduce the incidence of NMFs. No significant effects of CSST/IPTp-SP on the incidence of clinical episodes, parasite density, and Plasmodium falciparum infections were observed.Conclusions: Our findings indicate that CSST/IPTp-SP strategy may provide additional protection against both malaria and NMFs in infants during the first year of life, and suggest that malaria control interventions during pregnancy could have long-term benefits in infants."
"The lymphatic drainage of diseased and normal bowel was studied in 21 patients undergoing surgery for Crohn's disease. Mesenteric lymphatic obstruction was a consistent feature, identified in areas of small bowel macroscopically affected by Crohn's disease. This finding was also observed in some areas of apparently unaffected small bowel. Subsequent examination of these areas confirmed the presence of early Crohn's disease. This method of study may prove to be of value for determining the extent of operative resection in patients with Crohn's disease. Experimental lymphatic obstruction in animals failed to produce pathological changes of Crohn's disease, suggesting that this feature is an epiphenomenon and is not of primary importance in the pathogenesis of regional enteritis."
"OBJECTIVE: To investigate the role of NF-êB inhibitor in occurence and development of AML.METHODS: AML and normal bone marrow samples were collected from 8 AML patients and 8 normal persons. The expression of NF-êB signaling pathway genes was detected by NF-êB PCR array. Then, AML mouse model was constructed to test the role of NF-êB inhibitor in AML.RESULTS: The NF-êB signal pathway was activated in AML patients. The up-regulated genes, EDARADD, TNFSF14, could activate the NF-êB signal pathway, IL6 could regulate the inflammatory signal. The down-regulated genes, TNFRSF 10B, TNFRSF1A, could lead to cell apoptosis. the AML mouse model was constructed successfully. Then administration of NF-êB inhibitor reduced the inhibition of leukemia niche to the normal hematopoietic stem cells (HSCs), promoted the HSC to enter into cell cycle.CONCLUSION: The NF-êB signal pathway is activated in AML cells. AML mouse model is constructed successfully. NF-êB inhibitor has a potential to treat AML and promotes the HSC to enrter into cell cycle."
"Fluorescence correlation spectroscopy (FCS) is a powerful technique for studying the diffusion of molecules within biological membranes with high spatial and temporal resolution. FCS can quantify the molecular concentration and diffusion coefficient of fluorescently labeled molecules in the cell membrane. This technique has the ability to explore the molecular diffusion characteristics of molecules in the plasma membrane of immune cells in steady state (i.e., without processes affecting the result during the actual measurement time). FCS is suitable for studying the diffusion of proteins that are expressed at levels typical for most endogenous proteins. Here, a straightforward and robust method to determine the diffusion rate of cell membrane proteins on primary lymphocytes is demonstrated. An effective way to perform measurements on antibody-stained live cells and commonly occurring observations after acquisition are described. The recent advancements in the development of photo-stable fluorescent dyes can be utilized by conjugating the antibodies of interest to appropriate dyes that do not bleach extensively during the measurements. Additionally, this allows for the detection of slowly diffusing entities, which is a common feature of proteins expressed in cell membranes. The analysis procedure to extract molecular concentration and diffusion parameters from the generated autocorrelation curves is highlighted. In summary, a basic protocol for FCS measurements is provided; it can be followed by immunologists with an understanding of confocal microscopy but with no other previous experience of techniques for measuring dynamic parameters, such as molecular diffusion rates."
"Dietary fish oil restores ovarian function in subfertile rats, which is thought to be associated with decreased transcription of follicle-stimulating hormone (FSH) â-subunit. We have previously demonstrated a reduction in early follicular serum FSH levels in normal weight but not obese women after treatment with omega-3 polyunsaturated fatty acids (PUFA). Herein, we report the effect of supplementation with omega-3 PUFA on urinary reproductive hormones across the whole menstrual cycle. This interventional study included 17 eumenorrheic women, aged 24-41 years. One month of daily morning urine was collected before and after 1 month of omega-3 PUFA supplementation with 4 g of eicosapentaenoic acid and docosahexaenoic acid daily. Measurements included urinary FSH, luteinizing hormone (LH) and estrogen and progesterone metabolites, plasma fatty acid composition, and markers of endoplasmic reticulum stress. Compliance with dietary supplementation was verified by significantly reduced ratios of omega-6 to omega-3 PUFA for all subjects after treatment (P < .01). After 1 month of omega-3 PUFA supplementation, urinary FSH was significantly decreased in normal weight, but not obese women, in both follicular and luteal phases (-28.4% and -12.6%, respectively, both P = .04). No significant changes were seen in LH or sex steroids for either weight group. The selective and specific decrease in FSH suggests that omega-3 PUFA supplementation merits further investigation in normal weight women with decreased fertility and/or diminished ovarian reserve."
"Hemoglobinuria occurs in rats, but not in mice, after i.p. application of Tropidechis carinatus venom. By gel filtration on Sephadex G-75 and G-50 and chromatography on SP-Sephadex C-25 and DEAE-cellulose, a coagulant factor and a phospholipase A were isolated from the venom which in combination only, produced hemoglobinuria."
A 13-year-old boy with a 4-year history of bipolar disorder and concomitant obsessive-compulsive disorder required four hospitalizations and two partial hospitalizations due to inadequate responses to combinations of neuroleptics and traditional treatments for bipolar disorder. The use of clozapine in combination with lithium and clomipramine facilitated successful discharge from the hospital and return to a structured school setting. Significant adverse affects from interactions between valproic acid and clozapine necessitated discontinuation of valproic acid.
"PURPOSE: Clobazam (CLB) has an important antiepileptic effect and is less expensive than the new antiepileptic drugs (AEDs), but still has not been considered as first-line drug in the treatment of epilepsy. We evaluated the efficacy of CLB as add-on therapy in patients with refractory partial epilepsy.METHODS: This was an open, retrospective study, conducted at the epilepsy clinic of our university hospital. All patients had chronic epilepsy and were being evaluated for epilepsy surgery. CLB was introduced as add-on therapy (starting with 10 mg/ day) in patients with previous failure of at least two AEDs. Information was obtained from clinical notes and follow-up visits.RESULTS: We evaluated 97 patients, 37 men and 60 women. Ages ranged from 15 to 70 years (mean, 35.8 years). Etiology of epilepsy was hippocampal atrophy in 67 (69%), cortical dysgenesis in nine (9.3%), and other etiologies in nine (9.3%). In 12 (12.3%) patients, the etiology of epilepsy was not identified despite clinical and neurologic investigation. Patients used CLB for a period ranging from 1 month to 7 years and 9 months (mean, 16.7 months) with doses ranging from 10 to 60 mg/day (mean, 29.7 mg/day). Seven (7.2%) patients were seizure free, 48 (49.4%) had > or =50% of improvement in seizure control, 39 (40.2%) had <50% of improvement in seizure control, and in three (3.1%), no data were available.CONCLUSIONS: We conclude that CLB may have efficacy equivalent to that of the new AEDs when used as add-on therapy in patients with refractory epilepsy. CLB should be considered an economic alternative in the treatment of patients with refractory epilepsy."
"PURPOSE: We report what is to our knowledge a novel approach that led to the rapid development of a 3-dimensional bladder model, including a differentiated urothelium reconstructed without a period of exposure to the air-liquid interface.MATERIALS AND METHODS: Bilayered bladder constructs were produced using anchored mesenchymal cell seeded collagen gels to create the mesenchymal layer. Gels were coated with urine for 20 minutes before urothelial cell seeding. The 3-dimensional bladder models were cultured under submerged conditions for 15 days.RESULTS: Pure urine coating of the collagen matrix surface combined with its intermittent presence during urothelial development was found to be best to maintain urothelial cell properties. Immunohistological and ultrastructural analyses showed the formation of a pseudostratified urothelium devoid of abnormal K14 expression, allowing for uroplakin trafficking and forming an asymmetrical unit membrane at the apical surface.CONCLUSIONS: Such tissues could be adapted for clinical applications, including bladder repair. In the context of basic science this model could serve as a good alternative to animal use for fundamental and pharmacological studies of normal or pathological bladder tissues."
"BACKGROUND: Bipolar disorder (BD) has been increasingly associated with abnormalities in neuroplasticity and cellular resilience. Brain Derived Neurotrophic Factor (BDNF) gene has been considered an important candidate marker for the development of bipolar disorder and this neurotrophin seems involved in intracellular pathways modulated by mood stabilizers. Also, previous studies demonstrated a role for BDNF in the pathophysiology and clinical presentation of mood disorders.METHODS: We investigated whether BDNF levels are altered during mania. Sixty subjects (14 M and 46 F) were selected and included in the study. Thirty patients meeting SCID-I criteria for manic episode were age and gender matched with thirty healthy controls. Young Mania Rating Scale (YMRS) evaluated the severity of manic episode and its possible association with the neurotrophin levels.RESULTS: Mean BDNF levels were significantly decreased in drug free/naive (224.8 +/- 76.5 pg/ml) compared to healthy controls (318.5 +/- 114.2), p < .001]. Severity of the manic episode presented a significant negatively correlation to plasma BDNF levels (r= .78; p < .001; Pearson test).CONCLUSIONS: Overall, these results suggest that the decreased plasma BDNF levels may be directly associated with the pathophysiology and severity of manic symptoms in BD. Further studies are necessary to clarify the role of BDNF as a putative biological marker in BD."
"Oligosaccharides perform a large number of biological roles, as dictated by their chemical structure and spatial arrangement. While conformational entropies are usually determined in vacuo by computer modeling, molecular recognition processes normally take place in solution. Here I show results of experiments using size-exclusion chromatography (SEC), an entropically driven solution technique. These clearly differentiate the individual contributions of the alpha and beta anomeric configurations and of the (1 --> 4) and (1 --> 6) glycosidic linkages to the solution conformational entropy of O-linked disaccharides. I also distinguish between the members of the epimeric disaccharide pair isomaltose-melibiose and trace the difference to that between their constituent monosaccharides, alpha-glucose and alpha-galactose."
"MagiProbe is a fluorescence quenching-based oligonucleotide probe and applicable to homogeneous nucleic acid assays. Further advantage of this probe is its inherent ability to discriminate between matched and mismatched duplex without reliance on the difference of thermal stability. To improve discriminating ability, the relationships between a position of mismatch in MagiProbe and a power of discrimination were studied. This resulted that intercalation of pyrene was destabilized by a mismatch but still stabilized by neighboring matched base pairs. To afford multiplexing capacity of nucleic acid detection assays to MagiProbe, a variety of differently colored fluorophores were tested in this system. Although each fluorophore showed a different response to hybridization, a couple of promising fluorophores were founded."
"We have synthesized three new antithrombin activating pentasaccharides displaying various sulfation patterns on the reducing end unit (H). We found that when L-iduronic acid stands in the 2S(0) conformation, the sulfate groups at positions 3 and 6 of the reducing end unit are practically devoid of influence on the activation of antithrombin. This suggests that the positive role of these sulfates is more related to their ability to shift the conformational equilibrium of L-iduronic acid towards 3S(0) than to directly interact with the protein."
"OBJECTIVE: To assess the reproducibility of a mobile non-contact camera-based digital imaging system (DIS) for measuring tooth colour under in vitro and in vivo conditions.METHODS: One in vitro and two in vivo studies were performed using a mobile non-contact camera-based digital imaging system. In vitro study: two operators used the DIS to image 10 dry tooth specimens in a randomised order on three occasions. In vivo study 1:25 subjects with two natural, normally aligned, upper central incisors had their teeth imaged using the DIS on four consecutive days by one operator to measure day-to-day variability. On one of the four test days, duplicate images were collected by three different operators to measure inter- and intra-operator variability. In vivo study 2:11 subjects with two natural, normally aligned, upper central incisors had their teeth imaged using the DIS twice daily over three days within the same week to assess day-to-day variability. Three operators collected images from subjects in a randomised order to measure inter- and intra-operator variability.RESULTS: Subject-to-subject variability was the largest source of variation within the data. Pairwise correlations and concordance coefficients were > 0.7 for each operator, demonstrating good precision and excellent operator agreement in each of the studies. Intraclass correlation coefficients (ICCs) for each operator indicate that day-to-day reliability was good to excellent, where all ICC's where > 0.75 for each operator.CONCLUSION: The mobile non-contact camera-based digital imaging system was shown to be a reproducible means of measuring tooth colour in both in vitro and in vivo experiments."
"In order to assist drinking water utilities with identifying the possible sources and causes of taste-and-odor conditions associated with materials used in distribution systems, we evaluated information from case studies and a database from the National Sanitation Foundation (NSF), International. This database identified chemicals that had leached from drinking water system components during testing of materials under ANSI/NSF Standard 61, which provides information to water utilities on potential taste-and-odor and health concerns from the use of new materials. The data were arranged to provide a process for locating the potential source of a taste-and-odor event. After a sensory analysis is conducted on the drinking water samples, the descriptor can be matched with categories on the ""Drinking Water Taste and Odor Wheel 2000"" in order to suggest the candidate material."
"The purpose of the present study is to evaluate the effects of low-level laser therapy on the osseointegration process by comparing resonance frequency analysis measurements performed at implant placement and after 30 days and micro-computed tomography images in irradiated vs nonirradiated rabbits. Fourteen male New Zealand rabbits were randomly divided into two groups of seven animals each, one control group (nonirradiated animals) and one experimental group that received low-level laser therapy (Thera Lase®, aluminum-gallium-arsenide laser diode, 10 J per spot, two spots per session, seven sessions, 830 nm, 50 mW, CW, ? 0.0028 cm2). The mandibular left incisor was surgically extracted in all animals, and one osseointegrated implant was placed immediately afterward (3.25? ? 11.5 mm; NanoTite, BIOMET 3i). Resonance frequency analysis was performed with the Osstell® device at implant placement and at 30 days (immediately before euthanasia). Micro-computed tomography analyses were then conducted using a high-resolution scanner (SkyScan 1172 X-ray Micro-CT) to evaluate the amount of newly formed bone around the implants. Irradiated animals showed significantly higher implant stability quotients at 30 days (64.286 ± 1.596; 95 % confidence interval (CI) 60.808-67.764) than controls (56.357 ± 1.596; 95 %CI 52.879-59.835) (P = .000). The percentage of newly formed bone around the implants was also significantly higher in irradiated animals (75.523 ± 8.510; 95 %CI 61.893-89.155) than in controls (55.012 ± 19.840; 95 %CI 41.380-68.643) (P = .027). Laser therapy, based on the irradiation protocol used in this study, was able to provide greater implant stability and increase the volume of peri-implant newly formed bone, indicating that laser irradiation effected an improvement in the osseointegration process."
"Hybrid corn seed is traditionally produced using either mechanical/hand detasseling or cytoplasmic male sterility, or a combination of both. In recent years, the development of transgenic systems to produce hybrid seed in several crops has attracted much attention. Here we describe a transgenic mechanism for production of hybrid corn, reversible male sterility (RMS), in which the action of the cytotoxic gene used to introduce male sterility is suppressed by the application of a chemical to the plant. Reversion of the sterility allows the RMS parent to be self-fertilized, a step which overcomes the need to remove fertile sib plants prior to making the hybrid cross. The key enabling technology in RMS is the use of a plant gene promoter which is specifically induced by chemical application. We have exemplified RMS in transgenic corn plants and believe that it provides specific benefits in the production of hybrid corn seed."
"Adenovirus (Ad) enters target cells by receptor-mediated endocytosis, escapes to the cytosol, and then delivers its DNA genome into the nucleus. Here we analyzed the trafficking of fluorophore-tagged viruses in HeLa and TC7 cells by time-lapse microscopy. Our results show that native or taxol-stabilized microtubules (MTs) support alternating minus- and plus end-directed movements of cytosolic virus with elementary speeds up to 2.6 micrometer/s. No directed movement was observed in nocodazole-treated cells. Switching between plus- and minus end-directed elementary speeds at frequencies up to 1 Hz was observed in the periphery and near the MT organizing center (MTOC) after recovery from nocodazole treatment. MT-dependent motilities allowed virus accumulation near the MTOC at population speeds of 1-10 micrometer/min, depending on the cell type. Overexpression of p50/dynamitin, which is known to affect dynein-dependent minus end-directed vesicular transport, significantly reduced the extent and the frequency of minus end-directed migration of cytosolic virus, and increased the frequency, but not the extent of plus end-directed motility. The data imply that a single cytosolic Ad particle engages with two types of MT-dependent motor activities, the minus end- directed cytoplasmic dynein and an unknown plus end- directed activity."
"In this study was compared the mental health status of 47 multiple sclerosis patients with silver/mercury tooth fillings (amalgams) to that of 50 patients with their fillings removed. On the Beck Depression Inventory the multiple sclerosis subjects with amalgams suffered significantly more depression while their scores on the State-Trait Anger Expression Inventory indicated the former group also exhibited significantly more anger. On the SCL-90 Revised, subjects with amalgam fillings had significantly more symptoms of depression, hostility, psychotism, and were more obsessive-compulsive than the patients with such fillings removed. On a questionnaire containing 18 mental health symptoms multiple sclerosis subjects with amalgam fillings reported a history of 43% more symptoms than those without amalgam fillings over the past 12 months. These data suggested that the poorer mental health status exhibited by multiple sclerosis subjects with dental amalgam fillings may be associated with mercury toxicity from the amalgam."
"High expression of the estrogen receptor-related receptor (ERR)-alpha in human tumors is correlated to a poor prognosis, suggesting an involvement of the receptor in cell proliferation. In this study, we show that a synthetic compound (XCT790) that modulates the activity of ERRalpha reduces the proliferation of various cell lines and blocks the G(1)/S transition of the cell cycle in an ERRalpha-dependent manner. XCT790 induces, in a p53-independent manner, the expression of the cell cycle inhibitor p21(waf/cip)(1) at the protein, mRNA, and promoter level, leading to an accumulation of hypophosphorylated Rb. Finally, XCT790 reduces cell tumorigenicity in Nude mice."
"The effect of prolactin (Prl) on oestrogen-induced gonadotrophin secretion was examined in vitro in a sequential double chamber perfusion system. As control groups, mediobasal hypothalamus (MBH)-pituitary pairs or pituitaries without the MBHs were perifused with Medium 199. As an experimental group, MBH-pituitary pairs were perifused with Medium 199 containing 1 micrograms/ml of rat Prl. These groups were stimulated with 10(-7) M oestradiol-17 beta (E2) for 30 min, and luteinizing hormone (LH) in the serial fractions of effluent was measured. In the control group of MBH-pituitary pairs perifused with medium without Prl, secretion of LH began to rise within 30 min after the beginning of stimulation, reached a peak 30 min after the end of stimulation and then remained at a plateau for the rest of the experimental period, whereas in the control group of pituitaries alone no significant response was observed. In the experimental group perifused with medium containing Prl, LH-secretion showed peaks 20 and 80 min after the end of E2-stimulation, respectively, and the first peak was significantly (P less than 0.01) less than the level in the control group. These data demonstrate that Prl at this concentration suppressed the rapid LH release induced by E2. Its site of action is suggested to be at the hypothalamic level, and its possible mechanism of action is discussed."
"STUDY DESIGN: Cross-sectional.OBJECTIVES: To describe the manual wheelchair (MWC) skill profiles of experienced MWC users with spinal cord injury and their wheeled mobility (distance and speed) while considering their level of injury and age.SETTING: Rehabilitation centers, participant's home and the community.METHODS: MWC skills were evaluated using the wheelchair skills test (WST) and wheeled mobility data were collected in the participants' own environment over a 7-day period, using a Cateye cycle computer (VELO 8). A total of 54 participants took part in the study.RESULTS: The mean total performance score of the sample on the WST was 80.7±11.8%, with a significant difference between participants with tetraplegia (C4-C8) and those with low-level paraplegia (T7-L2) (P<0.01). The average daily distance covered was 2.5±2.1 km at 1.7±0.9 km h(-1), with no significant difference between participants with paraplegia and those with tetraplegia (wheeled distance: P=0.70; speed: P=0.65). Significant relationships were found between MWC skills and daily wheeled distance (r=-0.32, P<0.05), but the correlation between these variables did not remain significant when controlling for age (partial r=0.26, P=0.07).CONCLUSION: These results suggest that the level of injury is related to MWC skills but not wheeled mobility. MWC skills are related to greater wheeled distance, but to a lesser extent when controlling for age."
"A Golgi impregnated, non-spiny multipolar cell whose soma occurred in layer V of the region of mouse SmI cortex containing the posteromedial barrel subfield (PMBSF) (Woolsey and Van der Loos, '70) was gold-toned and deimpregnated (Fairen et al., '77). Two of its dendrites, contained within a single PMBSF barrel, were serial thin-sectioned and then reconstructed in three dimensions. Dendrites of an unimpregnated, non-spiny layer IV bitufted cell, present within the same barrel, were also reconstructed in three dimensions from the series of thin sections. This approach permitted a comparison of the distribution of synapses along dendrites of the two non-spiny neurons. Results showed dendrites of the layer IV bitufted cell formed about twice as many synapses per unit length as those of the multipolar cell. Particularly striking was the contrast between the large number of synapses made by degenerating thalamocortical axon terminals with the dendrites of the bitufted cell and the rarity with which such synapses occur on dendrites of the multipolar cell. Furthermore, the proportion of the total number of synapses made by thalamocortical axons terminals onto dendrites of the bitufted cell was six times greater than the proportion of the thalamocortical synapses onto the multipolar cell dendrites."
"The biochemical basis for the regulation of fibre-type determination in skeletal muscle is not well understood. In addition to the expression of particular myofibrillar proteins, type I (slow-twitch) fibres are much higher in mitochondrial content and are more dependent on oxidative metabolism than type II (fast-twitch) fibres. We have previously identified a transcriptional co-activator, peroxisome-proliferator-activated receptor-gamma co-activator-1 (PGC-1 alpha), which is expressed in several tissues including brown fat and skeletal muscle, and that activates mitochondrial biogenesis and oxidative metabolism. We show here that PGC-1 alpha is expressed preferentially in muscle enriched in type I fibres. When PGC-1 alpha is expressed at physiological levels in transgenic mice driven by a muscle creatine kinase (MCK) promoter, a fibre type conversion is observed: muscles normally rich in type II fibres are redder and activate genes of mitochondrial oxidative metabolism. Notably, putative type II muscles from PGC-1 alpha transgenic mice also express proteins characteristic of type I fibres, such as troponin I (slow) and myoglobin, and show a much greater resistance to electrically stimulated fatigue. Using fibre-type-specific promoters, we show in cultured muscle cells that PGC-1 alpha activates transcription in cooperation with Mef2 proteins and serves as a target for calcineurin signalling, which has been implicated in slow fibre gene expression. These data indicate that PGC-1 alpha is a principal factor regulating muscle fibre type determination."
"Canis lupus familiaris (domestic dog) possess a high capacity to metabolize higher-chlorinated polychlorinated biphenyls (PCBs) to thyroid hormone (TH)-like hydroxylated PCB metabolites (OH-PCBs). As a result, the brain could be at high risk of toxicity caused by OH-PCBs. To evaluate the effect of OH-PCBs on dog brain, we analyzed OH-PCB levels in the brain and the metabolome of the frontal cortex following exposure to a mixture of PCBs (CB18, 28, 70, 77, 99, 101, 118, 138, 153, 180, 187, and 202). 4-OH-CB202 and 4-OH-CB107 were major OH-PCBs in the brain of PCB-exposed dogs. These OH-PCBs were associated with metabolites involved in urea cycle, proline-related compounds, and purine, pyrimidine, glutathione, and amino-acid metabolism in dog brain. Moreover, adenosine triphosphate levels in the PCBs exposure group were significantly lower than in the control group. These results suggest that OH-PCB exposure is associated with a disruption in TH homeostasis, generation of reactive oxygen species, and/or disruption of oxidative phosphorylation (OXPHOS) in brain cells. Among them, OXPHOS disturbance could be associated with both disruptions in cellular amino-acid metabolism and urea cycle. Therefore, an OXPHOS activity assay was performed to evaluate the disruption of OXPHOS by OH-PCBs. The results indicated that 4-OH-CB107 inhibits the function of Complexes III, IV, and V of the electron transport chain, suggesting that 4-OH-CB107 inhibit these complexes in OXPHOS. The neurotoxic effects of PCB exposure may be mediated through mitochondrial toxicity of OH-PCBs in the brain."
"BACKGROUND: Alcohol use has been consistently found to have a J-shaped association with coronary heart disease, with moderate drinkers exhibiting a decreased risk compared with both heavy drinkers and nondrinkers. However, results of studies of the association between alcohol use and subclinical coronary artery disease are conflicting.OBJECTIVE: The objective was to determine whether alcohol is associated with the presence, amount, or progression of coronary calcium over a 2- to 4-y period.DESIGN: The Multi-Ethnic Study of Atherosclerosis (MESA) is a prospective community-based cohort study of subclinical cardiovascular disease in a multi-ethnic cohort. In 2000-2002, 6814 participants free of clinical cardiovascular disease were enrolled at 6 participating centers.RESULTS: The subjects consisted of 3766 (55.5%) current drinkers, 1635 (24.1%) former drinkers, and 1390 (20.5%) never drinkers. Although light-to-moderate alcohol consumption was associated with lower coronary heart disease risk, we found no evidence of a protective or J-shaped association of alcohol and coronary artery calcium (CAC). In fact, there was evidence that heavy consumption of hard liquor was associated with greater CAC accumulation. Other alcoholic beverages were not associated with CAC prevalence, incidence, or progression.CONCLUSIONS: This was the first large study to evaluate the association of alcohol with CAC in 4 racial-ethnic groups and to evaluate the progression of calcification. These results suggest that the cardiovascular benefits that may be derived from light-to-moderate alcohol consumption are not mediated through reduced CAC accumulation."
"Anaerobic batch and flow-through experiments were performed to assess the capacity of two organic substrates to promote denitrification of nitrate-contaminated groundwater within managed artificial recharge systems (MAR) in arid or semi-arid regions. Denitrification in MAR systems can be achieved through artificial recharge ponds coupled with a permeable reactive barrier in the form of a reactive organic layer. In arid or semi-arid regions, short-term efficient organic substrates are required due to the short recharge periods. We examined the effectiveness of two low-cost, easily available and easily handled organic substrates, commercial plant-based compost and crushed palm tree leaves, to determine the feasibility of using them in these systems. Chemical and multi-isotopic monitoring (ä15NNO3, ä18ONO3, ä34SSO4, ä18OSO4) of the laboratory experiments confirmed that both organic substrates induced denitrification. Complete nitrate removal was achieved in all the experiments with a slight transient nitrite accumulation. In the flow-through experiments, ammonium release was observed at the beginning of both experiments and lasted longer for the experiment with palm tree leaves. Isotopic characterisation of the released ammonium suggested ammonium leaching from both organic substrates at the beginning of the experiments and pointed to ammonium production by DNRA for the palm tree leaves experiment, which would only account for a maximum of 15% of the nitrate attenuation. Sulphate reduction was achieved in both column experiments. The amount of organic carbon consumed during denitrification and sulphate reduction was 0.8‰ of the total organic carbon present in commercial compost and 4.4% for the palm tree leaves. The N and O isotopic fractionation values obtained (åN and åO) were -10.4‰ and -9.0‰ for the commercial compost (combining data from both batch and column experiments), and -9.9‰ and -8.6‰ for the palm tree column, respectively. Both materials showed a satisfactory capacity for denitrification, but the palm tree leaves gave a higher denitrification rate and yield (amount of nitrate consumed per amount of available C) than commercial compost."
"Among various bone tissue engineering strategies, selective cell retention (SCR) technology has been used as a practical clinical method for bone graft manufacturing in real time. The more mesenchymal stem cells (MSCs) are retained, the better the osteoinductive microenvironment provided by the scaffold, which in turn promotes the osteogenesis of the SCR-fabricated bone grafts. Integrin receptors are crucial to cell-matrix adhesion and signal transduction. We designed a collagen-binding domain (CBD)-containing IKVAV-cRGD peptide (CBD-IKVAV-cRGD peptide) to complement the collagen-based demineralized bone matrix (DBM) with a functionalized surface containing multiple integrin ligands, which correspond to the highly expressed integrin subtypes on MSCs. This DBM/CBD-IKVAV-cRGD composite exhibited superior in vitro adhesion capacity to cultured MSCs, as determined by oscillatory cell adhesion assay, centrifugal cell adhesion assay and mimetic SCR. Moreover, it promoted the retention of MSC-like CD271+ cells and MSC-like CD90+/CD105+ cells in the clinical SCR method. Furthermore, the DBM/CBD-IKVAV-cRGD composite induced robust MSC osteogenesis, coupled with the activation of the downstream FAK-ERK1/2 signaling pathway of integrins. The SCR-prepared DBM/CBD-IKVAV-cRGD composite displayed superior in vivo osteogenesis, indicating that it may be potentially utilized as a biomaterial in SCR-mediated bone transplantation. STATEMENT OF SIGNIFICANCE: Selective cell retention technology (SCR) has been utilized in clinical settings to manufacture bioactive bone grafts. Specifically, demineralized bone matrix (DBM) is a widely-used SCR clinical biomaterial but it displays poor adhesion performance and osteoinduction. Improvements of the DBM that promote cell adhesion and osteoinduction will benefit SCR-prepared implants. In this work, we developed a novel peptide that complements the DBM with a functionalized surface of multiple integrin ligands, which are corresponding to integrin subtypes available on human bone marrow-derived mesenchymal stem cells (MSCs). Our results indicate this novel functionalized bioscaffold greatly increases SCR-mediated MSC adhesion and in vivo osteogenesis. Overall, this novel material has promising SCR applications and may likely provide highly bioactive bone implants in clinical settings."
"Should older people eat more carrots, or at least increase their carotene intake to prevent loss of night vision? Participants in the Blue Mountains Eye Study were asked about their ability to see in the dark. Nutrient and food intake were estimated from a food frequency questionnaire. Associations between self-reported poor night vision and estimated nutrient intake were investigated using logistic regression. Poor night vision among women was associated with higher beta-carotene (P for trend = 0.03) and total vitamin A intake (P for trend = 0.048). Increased consumption of carrots, but no other food high in beta-carotene, was associated with significant increased reporting of poor night vision among women (P for trend = 0.04). While carrot intake may protect against difficulty in seeing at night, it is probable that people attributing poor driving ability to their vision may be eating more carrots in the hope of reversing this decline."
"One hundred unrelated individuals of French origin were screened for mtDNA variation as restriction fragment length polymorphisms (RFLPs) with the restriction enzymes HpaI, BamHI, HaeII, MspI, AvaII and HincII. Twenty enzyme morphs were detected, four of which (AvaII-37Fr, -38Fr, HincII-18Fr and -19Fr) are new. Of the 17 mitotypes detected, five are new and they were named 1-19Fr, 6-18Fr, 100Fr-2 (2-1-2-4-1-2), 101Fr-2 (2-1-1-1-38Fr-2) and 102Fr-2 (2-1-1-4-37Fr-2). All new morphs and mitotypes derive from those already known due to a single nucleotide substitution. The French population was compared with other European, Mediterranean and Caucasian populations. Calculation of the genetic distances showed close genetic affinity with European-Mediterranean populations and especially with Calabrians, Majorcans and northern Italians (at negative values)."
"BACKGROUND: Auditory verbal hallucinations (AVH) are experienced by 21-54% of patients diagnosed with a borderline personality disorder (BPD), and ensuing distress is often high. Little is known about the beliefs these patients foster about their voices, and the influence thereof on distress and need for hospitalisation.METHODS: In a convenience sample of 38 BPD outpatients with AVH, data were collected with the aid of the Psychotic Symptom Rating Scales (PSYRATS), Beliefs about Voices Questionnaire (BAVQ), Social Comparison Rating Scale (SCRS), and Voice Power Differential Scale (VPDS).RESULTS: The majority of patients with BPD who experience AVH rate their voices as malevolent and omnipotent, and higher in social rank than themselves. Moreover, their resistance against them tends to be high. These parameters correlate positively and significantly with high levels of distress experienced in relation to these AVH. The need for hospitalisation, in turn, is associated with high scores for omnipotence of the voices and distress due to AVH. However, these findings could not be confirmed in regression analyses.CONCLUSIONS: As negative beliefs can be altered with cognitive-behavioural therapy (CBT), we expect CBT to be beneficial in the treatment of AVH in BPD patients, whether or not in combination with antipsychotic medication."
"Image quality in B-mode ultrasound is important as it reflects the diagnostic accuracy and diagnostic information provided during clinical scanning. Quality assurance programs for B-mode ultrasound systems/components are comprised of initial quality acceptance testing and subsequent regularly scheduled quality control testing. The importance of quality assurance programs for B-mode ultrasound image quality using ultrasound phantoms is well documented in the human medical and medical physics literature. The purpose of this prospective, cross-sectional, survey study was to determine the prevalence and methodology of quality acceptance testing and quality control testing of image quality for ultrasound system/components among veterinary sonographers. An online electronic survey was sent to 1497 members of veterinary imaging organizations: the American College of Veterinary Radiology, the Veterinary Ultrasound Society, and the European Association of Veterinary Diagnostic Imaging, and a total of 167 responses were received. The results showed that the percentages of veterinary sonographers performing quality acceptance testing and quality control testing are 42% (64/151; 95% confidence interval 34-52%) and 26% (40/156: 95% confidence interval 19-33%) respectively. Of the respondents who claimed to have quality acceptance testing or quality control testing of image quality in place for their ultrasound system/components, 0% have performed quality acceptance testing or quality control testing correctly (quality acceptance testing 95% confidence interval: 0-6%, quality control testing 95% confidence interval: 0-11%). Further education and guidelines are recommended for veterinary sonographers in the area of quality acceptance testing and quality control testing for B-mode ultrasound equipment/components."
"Electrochemotherapy (EQT) is a local cancer treatment well established to cutaneous and subcutaneous tumors. Electric fields are applied to biological tissue in order to improve membrane permeability for cytotoxic drugs. This phenomenon is called electroporation or electropermeabilization. Studies have reported that tissue conductivity is electric field dependent. Electroporation numerical models of biological tissues are essential in treatment planning. Tumors of the mouth are very common in dogs. Inadequate EQT treatment of oral tumor may be caused by significant anatomic variations between dogs and tumor position. Numerical models of oral mucosa and tumor allow the treatment planning and optimization of electrodes for each patient. In this work, oral mucosa conductivity during electroporation was characterized by measuring applied voltage and current of ex vivo rats. This electroporation model was used with a spontaneous canine oral melanoma. The model outcomes of oral tumor EQT is applied in different parts of the oral cavity including near bones and the hard palate. The numerical modeling for treatment planning will help the development of new electrodes and increase the EQT effectiveness."
"INTRODUCTION: This is an observational study that examines the clinical features, the degrees of esophageal injury, physiological markers, and clinical outcomes after paraquat ingestion and seeks to determine what association, if any, may exist between these findings.METHODS: The study included 16 of 1410 paraquat subjects who underwent endoscopies at Chang Gung Memorial Hospital between 1980 and 2007.RESULTS: Corrosive esophageal injuries were classified as grade 1 in 8, 2a in 5, and 2b in 3 patients. No patients had grade 0, 3a, or 3b esophageal injuries. After paraquat ingestion, systemic toxicity occurred, with rapid development of hypoxia, hepatitis, and renal failure in many cases. Hypoxia occurred in 1 (12.5%), 5 (100%), and 3 (100%) patients with grades 1, 2a, and 2b esophageal injury, respectively. There were more hypoxic patients with grades 2a and 2b than those with grade 1 esophageal injury (P < .05). The nadir Pao(2) was lower in patients with grades 2a and 2b than those with grade 1 esophageal injury (P < .05). However, there were no significant differences in terms of acute hepatitis, peak serum alanine aminotransferase, acute renal failure, and peak serum creatinine between the 3 groups (P > .05). Kaplan-Meier analysis did not find any difference in survival between the groups (P > .05).CONCLUSION: Paraquat, a mild caustic agent, produces only grades 1, 2a, and 2b esophageal injury. Our findings showed a potential relationship between the degree of hypoxia, mortality, and degree of esophageal injury, although such a low number of study subjects limits the conclusions that can be made by this study."
"Malignant effusions because of renal-cell carcinoma (RCC) are an unusual event and occur in patients with papillary and clear cell tumors. We have studied a 65-year-old man who underwent right renal tumorectomy, diagnosed as chromophobe RCC (pT1). After 16 months, the patient presented cough and fever. Positron emission computed tomography demonstrated extensive mediastinal lymphadenopathy. Chest radiograph showed right pleural effusion. The cytological examination of the fluid showed malignant cells. Immunohistochemistry had been performed on primary renal tumor and on cell block of pleural effusion. The renal tumor showed positivity for parvalbumin, cytokeratin (CK) 7, C-kit (CD117), E-cadherin, and RCC marker. The neoplastic cells of pleural effusion showed positive immunohistochemical staining for parvalbumin, RCC marker, pancytokeratin, epithelial membrane antigen, CK7, C-kit (CD117), E-cadherin, and CD10. They were negative for thyroid transcription factor-1, CK20, calretinin, CK5, D2-40 podoplanin, CDX2, and Wilms' tumor suppressor gene. Malignant effusion secondary to RCC is rare. In several studies, RCC had been the cause of 1-2.2% of malignant pleural fluids. Chromophobe RCC tends to be localized into the kidney and to be of nuclear grade 2 at presentation, factors that probably explain its more favorable outlook. In our case, the chromophobe RCC was asymptomatic and was discovered because abdominal pain due to stone in the gallbladder. The tumor had an unusual aggressive clinical behavior. Immunohistochemistry performed on the cell block let to establish the renal origin and the chromophobe histotype of malignant cells found in the pleural fluid."
"Shoe-mounted inertial sensors are widespread deployed in satellite-denied scenarios because of the possibility to re-calibrate stepwise the estimated position. These re-calibrations, known as zero-velocity corrections, prevent an accumulated positioning error growth over time caused by the noise of current medium- and low-cost sensors. However, the error accumulated over time in the height estimation is still an issue under study. The objective of this article is to propose a height correction that is based on the dynamics of the foot. The presented algorithm analyzes the movement of the foot, which is different when walking on horizontal surfaces and stairs. The identification of horizontal surfaces and stairs is detailed in this article. For the assessment of the performance of the proposed height correction, a dataset of approximately 5 h recorded with 10 volunteers walking in a five-story building is employed. The error is evaluated using pre-defined ground truth points. We compare the height error estimated with and without applying the proposed correction and show that the height correction improves the vertical positioning accuracy up to 85."
"The expression and topography of some integrins and basement membrane proteins in cutaneous basal cell carcinomas (BCCs) and squamous cell carcinomas (SCCs) have been studied by immunohistochemistry and Western blotting. It has been shown that the typical cell-to-cell distribution of alpha 2 beta 1 and alpha 3 beta 1 found in normal epidermis is replaced by pericellular distribution in both BCC and SCC cells. BCC and SCC also showed different patterns of expression of alpha 6 beta 4, an integrin heterodimer normally lining the basal surface of basal epidermal keratinocytes: whereas SCC showed high expression and pericellular distribution of alpha 6 beta 4, BCC cells did not express this integrin at all. The absence of alpha 6 and beta 4 subunits from BCC extracts was confirmed by Western blotting. The molecular composition of the basement membrane was markedly different in the two types of epidermal tumors. Whereas laminin and collagen type IV were conserved in the basement membrane zone of both tumors, the molecular complex BM-600/nicein, which is recognized by the monoclonal antibody GB3 and is possibly identical to the previously described basement membrane glycoproteins kalinin and epiligrin, was absent from BCC cells. Then, the simultaneous loss of expression of alpha 6 beta 4 and BM-600/nicein in BCC cells but not in SCC cells indicates that alpha 6 beta 4 integrin and one of its potential ligands may be co-regulated in both BCC and SCC, thus suggesting a role for this phenomenon in the pathogenesis and clinical behavior of these epidermal tumors."
"Home apnea monitoring has been used increasingly for infants with or at high risk for apnea. This study examined mood states, perceived family functioning, and perceived support systems of mothers with infants on home apnea monitors from the first week on the monitor through three months after discharge. It also compared these factors in mothers receiving care through an established home monitor program or through their own physicians. Greatest discrepancies in all three areas of study occurred during the first week at home. During this time the group receiving care through private physicians reported higher discrepancy in professional support than that in the home monitoring program. No other significant differences were found between the two groups across time periods. Community health nurses have an important role in assessing and providing counseling concerning the mood disturbance and changes in family functioning that may take place when infants are receiving this therapy, particularly in the initial time at home."
"Fifty-six Holstein calves  were used to investigate effects of heat and cold stressors on mitogen-induced blastogenesis of isolated peripheral blood mononuclear cells and immunoglobulins G1 and M in blood plasma. Calves were exposed to constant hot (35 degrees C), constant cold (-5 degrees C), or thermoneutral (23 degrees C) ambient conditions in environmentally-controlled chambers. Immune responses were measured soon after introduction into environmental chambers (3 days) and after various degrees of adaptation (7 and 14 days). Mortality was greater among heat- and cold-exposed calves than among thermoneutral calves. Neither heat nor cold exposure had a direct effect on blastogenesis of peripheral blood mononuclear cells by phytohemagglutinin or concanavalin A. Plasma from heat- and cold-exposed calves then was incorporated into the culture medium at a final concentration of 5% and tested in a mitogenesis assay on peripheral blood mononuclear cells from a single healthy donor. Plasma from heat-exposed calves consistently enhanced tritiated thymidine incorporation into normal peripheral blood mononuclear cells by phytohemagglutinin and concanavalin A as compared to plasma from cold-exposed calves. After heat exposure for 3 to 14 days, immunoglobulin G1 averaged 27% less in heat-exposed calves than in calves that were held at thermoneutrality, but M was unaffected. Cold exposure did not have a consistent effect on G1 or M. These data demonstrate that chronic heat and cold stressors affect calves by altering both antibody- and cell-mediated immunity."
"Information theoretic measures to incorporate anatomical priors have been explored in the field of emission tomography, but not in transmission tomography. In this work, we apply the joint entropy prior to the case of limited angle transmission tomography. Due to the data insufficiency problem, the joint entropy prior is found to be very sensitive to local optima. Two methods for robust joint entropy minimization are proposed. The first approximates the joint probability density function by a single 2D Gaussian, and is found to be appropriate for reconstructions where the ground truth joint histogram is dominated by two clusters, or multiple clusters that are roughly aligned. The second method is an extension to the case of multiple Gaussians. The intended application for the single Gaussian approximation is digital breast tomosynthesis, where reconstructed volumes are approximately bimodal, consisting mainly of fatty and fibroglandular tissues."
"In genetics, pleiotropy describes the genetic effect of a single gene on multiple phenotypic traits. A common approach is to analyze the phenotypic traits separately using univariate analyses and combine the test results through multiple comparisons. This approach may lead to low power. Multivariate functional linear models are developed to connect genetic variant data to multiple quantitative traits adjusting for covariates for a unified analysis. Three types of approximate F-distribution tests based on Pillai-Bartlett trace, Hotelling-Lawley trace, and Wilks's Lambda are introduced to test for association between multiple quantitative traits and multiple genetic variants in one genetic region. The approximate F-distribution tests provide much more significant results than those of F-tests of univariate analysis and optimal sequence kernel association test (SKAT-O). Extensive simulations were performed to evaluate the false positive rates and power performance of the proposed models and tests. We show that the approximate F-distribution tests control the type I error rates very well. Overall, simultaneous analysis of multiple traits can increase power performance compared to an individual test of each trait. The proposed methods were applied to analyze (1) four lipid traits in eight European cohorts, and (2) three biochemical traits in the Trinity Students Study. The approximate F-distribution tests provide much more significant results than those of F-tests of univariate analysis and SKAT-O for the three biochemical traits. The approximate F-distribution tests of the proposed functional linear models are more sensitive than those of the traditional multivariate linear models that in turn are more sensitive than SKAT-O in the univariate case. The analysis of the four lipid traits and the three biochemical traits detects more association than SKAT-O in the univariate case."
"New data on genetics, including an extensive pedigree and certain aspects of natural history, have been compiled on Family S, which is characterized by a hereditary progressive atrioventricular (A-V) conduction defect. Concordance analysis of heart block in affected parents and their offspring suggests as a working hypothesis transmission of diathesis for the defect by means of a Mendelian autosomal dominant factor. Regression of the defect in several relatives, including reversion from first degree heart block to normal A-V conduction, defies explanation at this time. However, rapport established with the unusually large (1,067 members) and cooperative kindred will permit longitudinal evaluation of these findings."
"The common acute lymphoblastic leukemia antigen (CALLA) is a 749-amino acid type II integral membrane protein expressed by most acute lymphoblastic leukemias, certain other lymphoid malignancies with an immature phenotype, and normal lymphoid progenitors. A computer search against the most recent GenBank release (no. 56) indicates that human CALLA cDNA encodes a protein nearly identical to the rat and rabbit neutral endopeptidase 24.11 (""enkephalinase;"" EC 3.4.24.11). This zinc metalloendopeptidase, which has been shown to inactivate a variety of peptide hormones including enkephalin, chemotactic peptide, substance P, neurotensin, oxytocin, bradykinin, and angiotensins I and II, had not been identified in lymphoid cells. To determine whether CALLA cDNA derived from human acute lymphoblastic leukemia cells (Nalm-6 cell line) encodes functional neutral endopeptidase activity, we generated CALLA+ stable transfectants in the CALLA- murine myeloma cell line J558 and analyzed them for enzymatic activity in a fluorometric assay based upon cleavage of the substrate glutaryl-Ala-Ala-Phe 4-methoxy-2-naphthylamide at the Ala-Phe bond. Total lysates as well as whole-cell suspensions of the Nalm-6 line and of the CALLA+ transfectants, but not of the CALLA- J558 cells, possessed neutral endopeptidase activity. This enzymatic activity was associated with the cellular membrane fraction and was abrogated by the specific neutral endopeptidase inhibitor phosphoramidon. The unequivocal identification of CALLA as a functional neutral endopeptidase provides insight into its potential role in both normal and malignant lymphoid function."
"The angioarchitectural classification and distributive patterns were investigated in the filiform papillae (FiP) on the meso-dorsal surface of the rabbit tongue by using microvascular cast specimens (MVCS) and the scanning electron microscope (SEM). The author examined the microvascular network structures, which consisted of ascending and descending branches entering the FiP. They inclined in a posterior (pharynx) direction, and densely and geometrically covered the meso-dorsal surface, directing the spoon-like concave face in an anterior direction. FiPs could be classified into three types: a small filiform papilla (SfP) covering on the meso-dorsal surface except for the marginal part of the intermolar eminence (MIME) and the intermolar eminence itself (IME): a spoon-like concave structure facing in an anterior (apex) direction with an arrowhead-like top: a middle filiform papilla (MfP) on the MIME, made up of a long triangle-like concave structure with a sharp arrowhead-like top and inclined at right angles to the IME. A large filiform papilla (LfP) on the whole swelling dorsal area of the IME was formed by a long triangle-like concave structure with a sharper arrowhead-like top. LfPs are longer and larger than MfPs and inclined towards the pharynx."
"The aim of the present prospective study was to detect lactose malabsorption in subjects in northern India infected with Entamoeba histolytica and passing cysts. The study group included forty-one patients with E. histolytica cysts in at least one of three consecutive faecal samples. Lactose malabsorption was detected by a lactose H2 breath test. The results were compared with those of forty controls subjects. Thirty-two of forty-one (78.0 %) subjects passing E. histolytica cysts had lactose malabsorption compared with seventeen of forty (42.5 %) control subjects (P<0.01). In conclusion, the present study shows that lactose malabsorption is significantly more common in individuals infected with E. histolytica and passing cysts compared with control subjects."
"This study prepared three liposomal formulations coloaded with elacridar and tariquidar to overcome the P-glycoprotein-mediated efflux at the blood-brain barrier. Their pharmacokinetics, brain distribution, and impact on the model P-glycoprotein substrate, loperamide, were compared to those for the coadministration of free elacridar plus free tariquidar. After intravenous administration in rats, elacridar and tariquidar in conventional liposomes were rapidly cleared from the bloodstream. Their low levels in the brain did not improve the loperamide brain distribution. Although elacridar and tariquidar in PEGylated liposomes exhibited 2.6 and 1.9 longer half-lives than free elacridar and free tariquidar, respectively, neither their Kp for the brain nor the loperamide brain distribution was improved. However, the conjugation of OX26 F(ab')2 fragments to PEGylated liposomes increased the Kps for the brain of elacridar and tariquidar by 1.4- and 2.1-fold, respectively, in comparison to both free P-gp modulators. Consequently, the Kp for the brain of loperamide increased by 2.7-fold. Moreover, the plasma pharmacokinetic parameters and liver distribution of loperamide were not modified by the PEGylated OX26 F(ab')2 immunoliposomes. Thus, this formulation represents a promising tool for modulating the P-glycoprotein-mediated efflux at the blood-brain barrier and could improve the brain uptake of any P-glycoprotein substrate that is intended to treat central nervous system diseases."
"Apathy and depression are discriminable but related dimensions of behavior. The purpose of this study was to evaluate the source of the overlap between measures of apathy and depression. We evaluated the intercorrelations between the Apathy Evaluation Scale (AES) and the Hamilton Rating Scale for Depression (HamD) in 107 subjects, aged 53-85, who met research criteria for normal aging, left or right cerebral hemisphere stroke, probable Alzheimer's disease, or major depression. We determined the correlation between the individual items on the HamD and the total scores on the AES and the HamD. The HamD items having the strongest correlations with AES total score were diminished work/interest, psychomotor retardation, anergy, and lack of insight. The correlation between AES and HamD total scores was nonsignificant when major depression subjects and these variables most closely related to apathy were excluded from consideration. These findings indicate that the convergence between HamD and AES is attributable to (i) a subset of HamD items which are consistent with the syndrome of apathy and (ii) the fact that major depression is associated with both apathy and depression. Clinical and research applications of these results are discussed."
"Clustering molecules based on numeric data such as, gene-expression data, physiochemical properties, or theoretical data is very important in drug discovery and other life sciences. Most approaches use hierarchical clustering algorithms, non-hierarchical algorithms (for examples, K-mean and K-nearest neighbor), and other similar methods (for examples, the Self-Organization Mapping (SOM) and the Support Vector Machine (SVM)). These approaches are non-robust (results are not consistent) and, computationally expensive. This paper will report a new, non-hierarchical algorithm called the V-Cluster (V stands for vector) Algorithm. This algorithm produces rational, robust results while reducing computing complexity. Similarity measurement and data normalization rules are also discussed along with case studies. When molecules are represented in a set of numeric vectors, the V-Cluster Algorithm clusters the molecules in three steps: (1) ranking the vectors based upon their overall intensity levels, (2) computing cluster centers based upon neighboring density, and (3) assigning molecules to their nearest cluster center. The program is written in C/C++ language, and runs on Window95/NT and UNIX platforms. With the V-Cluster program, the user can quickly complete the clustering process and, easily examine the results by use of thumbnail graphs, superimposed intensity curves of vectors, and spreadsheets. Multi-functional query tools have also been implemented."
"The S. aureus 209P UF mutant dehydrogenase activity suppression method was used for detection of enterotoxin of Enterobacteriaceae opportunistic bacteria. Comparison of V. cholerae non 01, E coli and K. pneumonia toxigenic and nontoxigenic strains, neutralization test with antitoxic anticholera serum, comparative study of toxigenicities in paw edema test (according to Yu. P. Vartanyan) have shown that Escherichia and Klebsiella toxigenic strains can suppress S. aureus 209P UF mutant dehydrogenase activity, which fact permits the employment of this method for the detection of opportunistic bacteria enterotoxigenicity."
"Measuring PCO2 (partial pressure of carbon dioxide) in an organ can enable early detection of ischemia. However, there are few clinical applicable solutions for measuring PCO2. Based upon the requirement for clinical applications, a conductivity based PCO2 sensor is proposed. A conductivity based PCO2 sensor measures conductance in an aqueous solution separated from the measured object by a gas-permeable membrane. A bridge design with two cavities is favored for such a sensor. A planar and a cylindrical macro prototype based upon the bridge design were studied. The design criteria were based on the contribution from the electrode polarization, stray capacitances, contact area with the sample and design ability to miniaturize the sensor. The cylindrical sensor is favored because of its large contact area and advantages for miniaturization. Further investigation has to be done to confirm the functionality of such a design in a miniaturized form and its clinical performance."
"YiiP is a dimeric Zn(2+)/H(+) antiporter from Escherichia coli belonging to the cation diffusion facilitator family. We used cryoelectron microscopy to determine a 13-? resolution structure of a YiiP homolog from Shewanella oneidensis within a lipid bilayer in the absence of Zn(2+). Starting from the X-ray structure in the presence of Zn(2+), we used molecular dynamics flexible fitting to build a model consistent with our map. Comparison of the structures suggests a conformational change that involves pivoting of a transmembrane, four-helix bundle (M1, M2, M4, and M5) relative to the M3-M6 helix pair. Although accessibility of transport sites in the X-ray model indicates that it represents an outward-facing state, our model is consistent with an inward-facing state, suggesting that the conformational change is relevant to the alternating access mechanism for transport. Molecular dynamics simulation of YiiP in a lipid environment was used to address the feasibility of this conformational change. Association of the C-terminal domains is the same in both states, and we speculate that this association is responsible for stabilizing the dimer that, in turn, may coordinate the rearrangement of the transmembrane helices."
Formation of a mucosal collar from the inner surface of the prepuce offers the surgeon who performs hypospadias repairs the opportunity to create a cosmetically normal-appearing phallus. This technique results in transposition of mucosal membrane type of tissue to the subglandular area to complete the normal repair.
"OBJECTIVE: To evaluate the diagnostic significance of multiple detector-row spiral CT(MSCT) in patients with obstructive sleep apnea hypopnea syndrome (OSAHS).METHOD: Sixty-seven patients with OSAHS and 40 volunteers were scanned. The CT imagings from the nasopharyngeal floor to the glottis obtained. The relevant dimensions of area, diameter, thickness of retropharyngeal tissue were measured in some regions in imagings including nasopharynx, oral pharynx and hypopharynx, as well as the narrowest region in pharynx.RESULT: 1) The values of area, left-right diameter and front-back diameter of oral pharyngeal imagings of patients with OSAHS were narrowest regions which were (133.5 +/- 32. 9) mm2, (12.5 +/- 2.0) mm, (10.4 +/- 1.8) mm respectively. The value of above parameters of oral pharyngeal imagings of volunteers were (238.5 +/- 46.5) mm2, (20.4 +/- 3.1) mm, (21.1 +/- 4.0) mm respectively. The values of two groups had marked difference by statistics (P< 0.01). 2) The narrowest regions were located in oral pharynx in the imagings of 58 patients with OSAHS, which located in soft palate site in 19 patients, in oral pharynx site in 11 patients and in retro-lingua site in 28 patients. The narrowest regions were located in nasopharynx in the imagings of 3 patients. None of the narrowest region was found in hypopharynx. The narrowest regions, which all located in oral pharynx, were measured in the imagings of 24 volunteers. 3) The values of area, left-right and front-back diameter of the narrowest regions of imaging of 58 patients with OSAHS among 67 patients were (75.6 +/- 17.9) mm2, (10.6 +/- 2.1) mm, (6.9 +/- 1.0) mm respectively. The values of bove parameter of the most narrowest regions of imagings of volunteer were (187.3 +/- 35.6) mm2, (21.4 +/- 4.3) mm, (15.6 +/- 2.7) mm respectively. There were significant difference in statistics among the data of these groups (P < 0.01).CONCLUSION: The imagings of MSCT may provide accurate diagnosis in OSAHS. Patients with OSAHS always had anatomically narrow in pharynx, especially in oral pharynx."
"The long-term benefits of cognitive behaviour therapy (CBT) for trauma survivors with acute stress disorder were investigated by assessing patients 3 years after treatment. Civilian trauma survivors (n=87) were randomly allocated to six sessions of CBT, CBT combined with hypnosis, or supportive counselling (SC), 69 completed treatment, and 53 were assessed 2 years post-treatment for post-traumatic stress disorder (PTSD) with the Clinician-Administered PTSD Scale. In terms of treatment completers, 2 CBT patients (10%), 4 CBT/hypnosis patients (22%), and 10 SC patients (63%) met PTSD criteria at 2-years follow-up. Intent-to-treat analyses indicated that 12 CBT patients (36%), 14 CBT/hypnosis patients (46%), and 16 SC patients (67%) met PTSD criteria at 2-year follow-up. Patients who received CBT and CBT/hypnosis reported less re-experiencing and less avoidance symptoms than patients who received SC. These findings point to the long-term benefits of early provision of CBT in the initial month after trauma."
"PURPOSE: There are numerous constructs employed in the treatment of metacarpal fractures with varying degrees of success. While plate fixation commonly involves dorsal application of a bicortical non-locking plate, there has been recent exploration of other fixation options including unicortical locked plating. The purpose of this study was to evaluate the biomechanical integrity of a polyetheretherketone (PEEK) inset locking plate and, in doing so, compare it to standard plate fixation (utilizing a clinically proven bicortical non-locking titanium plate) in a simulated porcine metacarpal fracture model.METHODS: Reproducible mid-shaft fractures were created in porcine second metacarpals. The fractured specimens were reduced and plated with either a bicortical non-locking plate or a unicortical locking plate with a PEEK locking design. Constructs were then loaded to failure in the same fashion as performed to create the fracture. Peak load was measured as the apex on the load-to-failure deflection curve. Stiffness was calculated as the linear slope on the load-to-failure deflection curve. Data were analyzed via Student's t test.RESULTS: Unicortical locking constructs failed at 344 ± 119 N, while bicortical non-locking constructs were found to fail at 277 ± 101 N (p = 0.19). The unicortical locking constructs demonstrated a stiffness of 80 ± 36 N/mm compared with the bicortical non-locking constructs (69 ± 36 N/mm) although again the difference was not found to be statistically different (p = 0.49).CONCLUSION: Based on this study, a locked plating construct using a polymer mechanism provides an interesting new locking fixation method for small bone fractures and with our limited number of specimens tested, provided at least a similar strength and rigidity profile in comparison with bicortical fixation in the treatment of metacarpal fractures."
"BACKGROUND: Nateglinide is a D-phenylalanine derivative that stimulates fast insulin secretion with a short activity span. It has been suggested that the hypoglycemic effect of nateglinide is related to the glucose concentration, an aspect that still has not been completely evaluated in human beings.OBJECTIVE: The aim of this study is to evaluate the effect of nateglinide on the insulin secretion at two different concentrations of glucose level.PARTICIPANTS AND METHODS: A randomized, double-blind, cross-over, placebo-controlled clinical trial with two parallel groups was carried out; each group was made up by six healthy volunteers who were submitted to a hyperglycemic-hyperinsulinemic clamp technique on two different occasions, one of them prior to the administration of 120 mg nateglinide and the other one prior to the administration of an homologated placebo. One group was submitted to and maintained at a hyperglycemia of 6.9 mmol/l above the fasting glucose level and the other group at a hyperglycemia of 4.1 mmol/l above the baseline of fasting glucose level.RESULTS: In volunteers submitted to the clamp at 4.1 mmol/l above the baseline of glucose level, the insulin secretion in the early phase was 212.4+/-55.8 pmol/l in the placebo test versus 338.4+/-124.8 pmol/l in the nateglinide test (P<.05), whereas in the group submitted at 6.9 mmol/l over the baseline, no significant differences were observed.CONCLUSION: Nateglinide increased the early insulin secretion in healthy individuals submitted to a mild hyperglycemia, but not at high glucose concentrations."
"MicroRNAs (miRNAs) play an important role as regulators of tumor suppressors and oncogenes in cancer-related processes. Single nucleotide polymorphisms (SNPs) in miRNAs have been shown to be relevant to various different cancers, including breast cancer (BC). The aim of this study was to estimate the associations between miRNA-related gene polymorphisms (miR-196a2, miR-499, and miR-608) and the risk of BC in a Chinese population. Gene polymorphisms were analyzed in 1143 subjects (controls = 583; BC = 560). The 3 SNPs were genotyped using the Sequenom Mass-ARRAY platform. The associations between the SNP frequencies and BC were assessed by computing odds ratios (ORs) and 95% confidence intervals (95% CIs), as well as by applying Chi-square tests. The miR-196a2 (rs11614913) T allele was associated with a decreased risk of BC based on results from dominant (OR = 0.67, 95% CI = 0.52-0.86), recessive (OR = 0.65, 95% CI = 0.48-0.86), and allele models (OR = 0.73, 95% CI = 0.62-0.86). In contrast, the miR-499 (rs3746444) AG/GG genotypes were associated with an increased risk of BC (OR = 1.45, 95% CI = 1.10-1.91), and miR-608 (rs4919510) was not significantly associated with BC risk. Our study suggested that the polymorphisms of rs11614913 and rs3746444 may be associated with BC risk in Chinese individuals."
"The Barthel Index (BI) is the most commonly used scale for assessing impairment of activities of daily living (ADL). For a global view of patients' abilities and the care needed in everyday neurorehabilitation practice, additional information about basic psychological and cognitive functions is essential. We therefore designed a new disability scale comprised of assessments of consciousness, approachability, orientation, memory, behaviour, emotions, communication, problem solving, perception, and behaviour at night. The scale shows exactly the same inner structure as the BI, with ten items and a score of up to 20 in steps from 0-100% (or 0-20 points). By a careful weighing of the items, the final score of the neuromental index (NMI) should create a clearer picture of both the disabilities and the needed resources. A second aim was to cover a broad range of patients including those in coma and coma remission states and those with only slight neuropsychological or behavioural symptoms. The NMI was examined with a group of 179 neurorehabilitation inpatients and proved to be highly valid, reliable, and practicable. It was designed to enable a global assessment of disability as well as the care resources needed, even in patients with different disability levels in ADL and psychological and cognitive functions."
"With the rapid increase of the number of protein structures in the Protein Data Bank, it becomes urgent to develop algorithms for efficient protein structure comparisons. In this article, we present the mTM-align server, which consists of two closely related modules: one for structure database search and the other for multiple structure alignment. The database search is speeded up based on a heuristic algorithm and a hierarchical organization of the structures in the database. The multiple structure alignment is performed using the recently developed algorithm mTM-align. Benchmark tests demonstrate that our algorithms outperform other peering methods for both modules, in terms of speed and accuracy. One of the unique features for the server is the interplay between database search and multiple structure alignment. The server provides service not only for performing fast database search, but also for making accurate multiple structure alignment with the structures found by the search. For the database search, it takes about 2-5 min for a structure of a medium size (?300 residues). For the multiple structure alignment, it takes a few seconds for ?10 structures of medium sizes. The server is freely available at: http://yanglab.nankai.edu.cn/mTM-align/."
"Membrane proteins are aggregation-prone in aqueous environments, and their biogenesis poses acute challenges to cellular protein homeostasis. How the chaperone network effectively protects integral membrane proteins during their post-translational targeting is not well understood. Here, biochemical reconstitutions showed that the yeast cytosolic Hsp70 is responsible for capturing newly synthesized tail-anchored membrane proteins (TAs) in the soluble form. Moreover, direct interaction of Hsp70 with the cochaperone Sgt2 initiates a sequential series of TA relays to the dedicated TA targeting factor Get3. In contrast to direct loading of TAs to downstream chaperones, stepwise substrate loading via Hsp70 maintains the solubility and targeting competence of TAs, ensuring their efficient delivery to the endoplasmic reticulum (ER). Inactivation of cytosolic Hsp70 severely impairs TA translocation in vivo Our results demonstrate a new role of cytosolic Hsp70 in directly assisting the targeting of an essential class of integral membrane proteins and provide a paradigm for how ""substrate funneling"" through a chaperone cascade preserves the conformational quality of nascent membrane proteins during their biogenesis."
"Activation of endothelial cells and platelets is an initial step toward the development of cardiovascular disease. Erectile dysfunction (ED) may be an early manifestation of endotheliopathy. We evaluated the effects of tadalafil on cyclic nucleotides (cGMP and cAMP) and soluble adhesion molecules (E- and P-selectin [ES and PS]). The patients were divided into 2 groups on the basis of the presence (10 patients) or absence (9 patients) of cardiovascular risk factors (dyslipidemia, hypertension, and smoking). Nitric oxide (NO) was unmeasurable in all the patients. Tadalafil administration induced a significant increase in cGMP levels in both groups (P < .01). In contrast, cAMP significantly increased (P < .05) and PS decreased (P < .01) only in patients without cardiovascular risk factors. Tadalafil induced a beneficial effect on platelet activation in patients with ED without cardiovascular risk factors; this effect was not mediated by NO."
"The confidence interval approach to bioavailability assessment depends first on selection of the confidence level, usually 95%, and then determination of the confidence limits for the expected bioavailability ratio AUC(Test)/AUC(Reference). In practice, however, it is sometimes of greater interest to know the probability that the expected bioavailability will fall below a critical value, for example 0.75, or within a clinically set bioequivalence range, for example 0.80 to 1.25. Up to now, posterior probability distributions have been suggested, based on classical analysis of variance (ANOVA) with its rather restrictive assumptions, including that of a (logarithmic) normal distribution. In this report, a distribution-free confidence interval based on the Wilcoxon signed-rank statistic has been generalized so that confidence probabilities can be obtained for any given confidence limits. In the case of unimodal and almost symmetrical sampling distributions, the results obtained are very similar to those of the ANOVA-based posterior probability distribution. However, skewed or multimodal sampling distributions are better reflected by the proposed distribution-free method, and more valid information is obtained in these cases, as demonstrated by examples."
"Thirty-three cases of 1,500 spontaneously-aborted foetuses showed hepatic calcifications. The exact location of these calcifications were confirmed by contrast studies, anatomic dissection, and further histology when necessary. Of them, 18 were calcified hepatic vein thrombi (CHVT), 12 were calcified portal vein thrombi (CPVT), 2 were parenchymal calcifications, and one was mixed. Associated anomalies were high (85% of cases). No significant difference was found between the type and percentage of anomalies of those with CHVT and those with CPVT. The most common anomalies encountered in all cases were meconium intraluminal calcification (27%), cystic hygroma (18%), and metaphyseal defect (18%). In view of this, it is suggested that a variety of severe foetal illnesses predispose to CHVT and CPVT. At correlation with maternal factors, it was found that the highest incidence was in the third decade. A significant high percentage of mothers (33%) had been on contraceptive pills, and there was interesting inverse relationship of hepatic calcification with gravidity. Practically, it is also hoped that the awareness of the presence of various types of hepatic calcifications will help in their detection prenatally by ultrasound."
"OBJECTIVE: To investigate the feasibility of the preservation of the epiphysis and joint function of the distal femur in children with osteosarcoma with epiphyseal distraction by external fixator.METHODS: Between July 2007 and May 2011, 6 children with osteoblastic osteosarcoma of the distal femur underwent epiphyseal distraction by external fixator, combined with tumor resection and repair with massive allograft bone transplantation to preserve the epiphysis and joint function of the distal femur. There were 4 boys and 2 girls, aged from 9 to 14 years (mean, 10.5 years). According to Enneking clinical staging, 4 cases were in stage II A and 2 cases in stage II B. According to San-Julian et al. typing for metaphyseal tumor invasion, 3 cases were in type I and 3 cases in type II. The size of tumor ranged from 6 cm x 4 cm to 12 cm x 9 cm. All patients received 2 cycles of COSS 86 chemotherapy before operation and 4 cycles after operation.RESULTS: Poor healing of incision was observed in 1 case because of rejection of allograft bone and good healing was obtained after the symptomatic treatment, healing of incision by first intention was achieved in the other children. All 6 cases were followed up 11 to 56 months (mean, 37.5 months). One case died of lung metastasis at 2 years after operation. X-ray films showed no complication of internal fixator loosening and broken or bone nonunion. According to the functional evaluation criteria of International Society of Limb Salvage (ISOLS) at last follow-up, the results were excellent in 3 cases, good in 2 cases, and fair in 1 case; the excellent and good rate was 83.3%. The length of operated limb was (62.97 +/- 7.51) cm, showing significant difference when compared with that of normal limb [(64.03 +/- 7.47) cm] (t=0.246 6, P=0.813 4).CONCLUSION: On the premise of adaptable indication, effective chemotherapy, and thoroughly tumor resection, the epiphyseal distraction by external fixator can obtain satisfactory results in limb-length and limb function in children with osteoblastic osteosarcoma of the distal femur."
"Twenty automechanics possessing increased whole blood values of one or more of the following heavy metals; chromium, copper, lead, manganese and nickel, were studied for peripheral nerve affection by means of electromyography (both sensoric and motoric nerve potentials were recorded). The heavy metal contents were related to the findings of denervation, distal motor latency, distal sensory latency, motoric and sensoric conduction velocities. Apart from two workers, in whom only lead was assayed, the remaining group of 18 were assayed for all heavy metals under study. Six workers showed increased distal motor and/or sensory latency and seven decreased nerve conduction velocity (four motoric and three sensoric affections). Of the workers with nerve affection, three showed increased levels of lead (nickel and chromium also raised). Four workers showed increased lead, nickel and chromium and one of lead, chromium and manganese. All in all, 10 out of 20 workers (50 percent) with elevated lead levels showed definite signs of peripheral neuropathy and seven out of 14 with raised nickel values showed these signs but they could all be accounted for by the increased lead levels. All except seven workers with raised lead levels in the whole group showed values above the critical limit of 80.0 mug/100 ml in whole blood. The data argue for the highly toxic effect of lead and other heavy metals on the peripheral nervous system and stress the diverse toxic exposure which automechanics undergo during their work. The possibility of there being a synergistic action between heavy metals and components of mineral oil and petroleum is discussed."
"A 13-year-old Hispanic female presented with symptoms of abdominal pain, amenorrhea, and unintentional weight loss of 11 kg. Preliminary investigation yielded no immediate causes, and an initial differential included inflammatory bowel disease (IBD), celiac disease, as well as viral, bacterial, or parasitic gastrointestinal infection. Evaluation of these potential diagnoses yielded negative results; thus, the team thought that the patient may be suffering from anorexia nervosa. The patient was discharged to outpatient care, and was treated in our adolescent health clinic, where repeat laboratory testing yielded a positive Giardia-antigen test. The patient was placed on metronidazole, rapidly gained weight, and resumed menstruation soon after. The final diagnosis was chronic giardiasis. Chronic giardiasis is a rare and enigmatic disease that presents with many symptoms similar to chronic gastrointestinal disorders (e.g. IBD and celiac disease) and anorexia nervosa. Practitioners involved in the diagnosis and treatment of anorexia nervosa should be aware of this disorder and include it in differential diagnoses of patients presenting with anorexia nervosa symptoms."
"The authors investigated coupling passive sampling technologies with ultraviolet irradiation experiments to study polycyclic aromatic hydrocarbon (PAH) and oxygenated PAH transformation processes in real-world bioavailable mixtures. Passive sampling device (PSD) extracts were obtained from coastal waters impacted by the Deepwater Horizon oil spill and Superfund sites in Portland, Oregon, USA. Oxygenated PAHs were found in the contaminated waters with our PSDs. All mixtures were subsequently exposed to a mild dose of ultraviolet B (UVB). A reduction in PAH levels and simultaneous formation of several oxygenated PAHs were measured. Site-specific differences were observed with UVB-exposed PSD mixtures."
"We have studied the properties of kainic acid receptor-activated channels using domoic acid as an agonist. Similarities of the electrophysiological, pharmacological and noise properties of domoic acid and kainic acid-evoked currents confirm that domoate is a potent and specific agonist of the kainate receptor. Single-channel properties of domoic acid-evoked currents were directly determined from outside-out membrane patches for the first time, and results were compared with those obtained by fluctuation analysis of macroscopic currents. Small conductance cationic-selective channels of approximately 4 pS and a mean open time of 2 to 3 ms were detected using both methods."
"Low frequency (0.1-2 Hz) dynamic mechanical analysis on individual type I collagen fibrils has been carried out using atomic force microscopy (AFM). Both the elastic (static) and viscous (dynamic) responses are correlated to the characteristic axial banding, gap and overlap regions. The elastic modulus (?5 GPa) on the overlap region, where the density of tropocollagen is highest, is 160% that of the gap region. The amount of dissipation on each region is frequency dependent, with the gap region dissipating most energy at the lowest frequencies (0.1 Hz) and crossing over with the overlap region at ?0.75 Hz. This may reflect an ability of collagen fibrils to absorb energy over a range of frequencies using more than one mechanism, which is suggested as an evolutionary driver for the mechanical role of type I collagen in connective tissues and organs."
"Neurophysiological findings among patients with solvent poisoning and among groups with long-term occupational exposure to various solvents are reviewed. Hydrocarbons with six carbon atoms have been shown to cause peripheral neuropathy, which can be revealed with electroneurography and electromyography. Various mixtures of solvents and carbon disulfide have caused similar types of abnormalities. Abnormal electroencephalograms have been reported for patients with solvent poisoning and also connected to occupational exposure. Visual evoked potentials have rarely been applied to study of solvent effects, latency increases have been reported. Multiple lesions within the central and peripheral nervous system should arouse a thought of possible toxic etiology."
Dispersion curves of the longitudinal relaxation T1 of protons in healthy amniotic fluid in a meconium solution are distinct at low Larmor frequencies (V0 less than 100 kHz). We are thus able to distinguish these fluids by T1 measurements in this range.
"We examined the effects of tetragastrin on mucin (mucus glycoprotein) content and mucosal damage in the rat stomach and duodenum. Following an injection of tetragastrin (12, 120, or 400 microg/kg subcutaneously), no macroscopic damage was found to the gastric mucosa but an increase in corpus mucin content was noted, whereas mucosal lesions appeared and the mucin content decreased in the duodenum in a dose-related manner. In the groups with histamine (0.8, 8, or 80 mg/kg intraperitoneally) administration, the extent of mucosal damage and the decrease in mucin content were dose-related in both these regions. For assessment of the effect of tetragastrin on the protective action in gastroduodenal mucosa, changes in mucin content and mucosal damage with histamine (80 mg/kg) -induced injury were examined. Coadministration of tetragastrin prevented the gastric mucosal damage and inhibited the decrease in corpus mucin content. In the duodenum, tetragastrin aggravated the histamine-induced mucosal damage and did not inhibit the reduction of the mucin content. From the present results, the increase in gastric mucins induced by tetragastrin might be related to the protective effect of gastric mucosa against injury. Tetragastrin did not protect the duodenal mucosa, and histamine-induced injury occurring in this region would be aggravated by the increase in HCl secretion and the decrease in mucin content induced by tetragastrin."
"Degradation of the plant hormone cytokinin is controlled by cytokinin oxidase/dehydrogenase (CKX) enzymes. The molecular and cellular behavior of these proteins is still largely unknown. In this study, we show that CKX1 is a type II single-pass membrane protein that localizes predominantly to the endoplasmic reticulum (ER) in Arabidopsis (Arabidopsis thaliana). This indicates that this CKX isoform is a bona fide ER protein directly controlling the cytokinin, which triggers the signaling from the ER. By using various approaches, we demonstrate that CKX1 forms homodimers and homooligomers in vivo. The amino-terminal part of CKX1 was necessary and sufficient for the protein oligomerization as well as for targeting and retention in the ER. Moreover, we show that protein-protein interaction is largely facilitated by transmembrane helices and depends on a functional GxxxG-like interaction motif. Importantly, mutations rendering CKX1 monomeric interfere with its steady-state localization in the ER and cause a loss of the CKX1 biological activity by increasing its ER-associated degradation. Therefore, our study provides evidence that oligomerization is a crucial parameter regulating CKX1 biological activity and the cytokinin concentration in the ER. The work also lends strong support for the cytokinin signaling from the ER and for the functional relevance of the cytokinin pool in this compartment."
"Angiogenin (Ang) is a small basic protein which belongs to the pancreatic ribonuclease superfamily. It potently induces the formation of new blood vessels and has emerged as a promising anticancer target. Mice possess genes encoding one ortholog (mAng) and three homologs of Ang, designated angiogenin-related protein (mAngrp), angiogenin-3 (mAng-3), and angiogenin-4 (mAng-4). Structural and functional study of these homologs has been hampered by the low yield of protein from the existing heterologous expression system. In the experiments described, we used a pET expression vector to express these proteins in the cytoplasm of Escherichia coli BL21-CodonPlus(DE3)-RIL cells, whereupon substantial amounts of each accumulated in the form of insoluble aggregates. The proteins were renatured using an arginine-assisted procedure and subsequently purified by cation-exchange chromatography and reversed-phase HPLC; each purified protein was shown to be enzymatically active toward tRNA. The yields of pure mAngrp and mAng-3 were 7.6 and 12 mg/liter culture, respectively, representing substantial increases over previously reported experiments. This is also the first report of the expression and purification of mAng-4, obtained here in a yield of 30 mg/liter culture. The ready availability of milligram quantities of these proteins will enable further functional studies and high-resolution structural analyses to be conducted."
"Pharmaceutical services in surgery and anesthesiology should be the standard of practice in health care organizations across the United States. Although not essential, an onsite satellite pharmacy would help in the provision of these services. A majority of distribution-related activities should be done by technicians. The availability of automated devices and other technology may lessen, to some extent, the time devoted to drug distribution. It is important for the OR pharmacist to concentrate on the provision of clinical services, such as medication-use management, drug information services, medication-use evaluations, formulary management, and pharmacoeconomic analyses of anesthesia-related medications. These activities provide the best opportunity for the pharmacist to contribute to improving patient care and outcomes and containing costs. Finally, pharmaceutical services in surgery and anesthesiology should be periodically assessed for patient care and financial effectiveness."
A macromolecular-assembly of polypeptides constructs a network of anionic and cationic charges vital for recognizing and coassembling Ca(2+) and CO(3)(2-) ions to mineralize and stabilize different mineral forms of CaCO(3) with core-shell or solid morphologies in an aqueous solution.
"During the process of matrix-driven translocation, certain types of cells or polystyrene latex beads are transported between compositionally different regions of a collagen matrix. Under appropriate conditions this translocation depends on an interaction between the cell or particle surface and fibronectin. We now show that this interaction takes place at a site located within the first 31 kDa of the amino-terminal end of the fibronectin molecule. Using defined fibronectin fragments and monoclonal antibodies directed against specific fibronectin domains, this site is established as both necessary and sufficient for the promotion of matrix-driven translocation. Competition experiments using heparin, heparan sulfate, and other sulfated polysaccharides show that this fibronectin site interacts with heparin-like cell or particle surface components in promoting matrix-driven translocation. Treatment of cells with heparinase renders them unresponsive to the translocational effect. An antibody directed against the amino-terminal domain of fibronectin completely inhibits matrix-driven translocation without interfering with heparin binding, suggesting that a post-binding conformational change in fibronectin may be required for promotion of the effect."
"OBJECTIVE: Vitamin D deficiency is a common problem during pregnancy and might contribute to adverse birth outcomes. Vitamin D-binding protein plays a key role in regulating vitamin D metabolism. We investigated whether maternal genetic variation in GC, the gene encoding vitamin-D binding protein, modulates the relationship between 25-hydroxyvitamin D [25(OH)D] levels and infant birth weight.METHODS: We measured 25(OH)D concentrations in maternal and umbilical cord blood from 356 pregnant women and their infants by liquid chromatography tandem mass spectrometry. We extracted DNA from the maternal blood for genotyping GC single-nucleotide polymorphisms (SNPs).RESULTS: The 25(OH)D concentrations were significantly higher in the maternal blood than in the cord blood, although the concentrations from each source were positively correlated with one another among individuals. Maternal GC SNPs rs12512631 and rs7041 were not significantly associated with infant birth weight. On the other hand, the GC SNPs rs12512631 and rs7041 significantly modified the relationships between the maternal and cord-blood concentrations of 25(OH)D and birth weight. Low 25(OH)D levels in the maternal and cord blood were significantly associated with decreased birth weight among infants born to mothers carrying the rs12512631 'C' allele but not in those born to mothers homozygous for the 'T' allele (P-interaction = 0.043 and 0.0008 for the maternal and cord blood, respectively). Low 25(OH)D levels in the cord blood were significantly associated with decreased birth weight only among infants born to mothers carrying the rs7041 'G' allele (P-interaction = 0.009).CONCLUSIONS: Our findings suggest that the interaction between 25(OH)D status and some maternal GC variants influence the birth weight of infants."
"Fish surface mucin from Pampus argenteus was extracted with different organic solvents and the residue passed through Sephadex G-200. The major peak was purified by DEAE-Cellulose chromatography and five fractions were obtained. Carbohydrate and protein contents showed that major peak is a glycoprotein. Rechromatography of this component on the Sephadex G-200 column gave a single peak, with an estimated minimal molecular weight of 6.9 X 10(5). Analysis of individual sugar components revealed the presence of galactose, glucose, mannose, arabinose, N-acetyl glucosamine, N-acetyl galactosamine and sialic acid. The most represented amino acids are threonine, serine, proline, glutamic acid and glycine. The N-terminal amino acid end was blocked. Nearly 47% of sulphate was acid labile. Sialic acid and fucose were released rapidly by mild acid hydrolysis. The presence of blood group-A activity suggests that some kind of terminal alpha-Gal-NAC may be present."
"We studied the effects of stasis of gallbladder bile in a dog model. Three days after cystic duct ligation, all gallbladders contained sludge, and the mucosa was covered by densely adherent mucus with solid particles 1-4 mm in diameter (gravel). Thirty percent of the animals developed stones (greater than 4 mm), which appeared grossly like human pigment stones and microscopically like condensed biliary sludge. Centrifugation of bile yielded colorless pellets (3.8 +/- 3.2 mg/ml) at day 0 and pigmented pellets (33.1 +/- 11.0 mg/ml) at day 3 (p less than 0.05). Pellets contained 73 +/- 8% mucin by weight. Dissolved mucin in supernatant bile increased from 7.46 +/- 1.19 mg/ml (day 0) to 27.36 +/- 3.05 mg/ml (day 3) (p less than 0.001), while bilirubin concentration decreased from 127 +/- 12 mg/dl (day 0) to 71 +/- 16 mg/dl (day 3) (p less than 0.001). Cholesterol concentration increased but did not reach saturation, while the concentration of bile salt and phospholipid did not change. Mucin-bilirubin complexes formed and remained suspended as sludge initially. As bile mucin content increased, sludge particles coalesced, precipitated, and eventually formed gravel and stones. We suspect that stone formation in this setting occurs because of sequestration of biliary lipids by mucin."
"The pharmacological effects and metabolism of tiazofurin have been compared in the six transplantable tumors comprising the NCI rodent tumor panel, viz. the P388 leukemia (S); the L1210 leukemia (S); the Lewis lung carcinoma (S); the B16 melanoma (R); the colon 38 carcinoma (R); and the M5076 sarcoma (R), where (S) denotes sensitivity and (R) resistance to tiazofurin. In addition, a variant of the P388 leukemia rendered resistant to the drug in vitro, and maintaining stable resistance in vivo, P388/TR, was also studied. Intraperitoneal administration of tiazofurin (100 mg/kg) resulted in a 3- to 30-fold greater accumulation of thiazole-4-carboxamide adenine dinucleotide (TAD), the proposed active metabolite of the drug in S versus R lines. In general, levels of TAD, percent inhibition of IMP dehydrogenase (mean 40% in S versus 10% in R), depression in the concentration of guanosine nucleotides, (50% in S versus 20% in R) and percent elevation of levels of IMP (500% in S versus 60% in R) correlated well with sensitivity or resistance. However, the B16 melanoma, although resistant to tiazofurin treatment, showed certain biochemical features characteristic of an S line. The sensitive and resistant tumors displayed comparable abilities to phosphorylate tiazofurin, but there was significant depression only in the R lines of the pyrophosphorylase which converts tiazofurin-5'-monophosphate to TAD (mean 78 nmoles/mg protein/hr in S versus 22 nmoles/mg protein/hr in R). The naturally resistant tumors were also found to exhibit a greater ability to degrade synthetic TAD than the sensitive lines (mean 102 nmoles/mg protein/hr in R versus 29 nmoles/mg protein/hr in S lines). The state of sensitivity or resistance could not be attributed to the basal levels of IMP dehydrogenase, to the specific activities of the enzymes of purine salvage, or to the basal concentration of purine and pyrimidine nucleotides. Moreover, treatment with tiazofurin did not influence the enzymes of TAD synthesis or of purine salvage."
"OBJECTIVE: Methadone administration has increased in pediatric clinical settings. This review is an attempt to ascertain an equianalgesic dose ratio for methadone in the pediatric population using standard adult dose conversion guidelines.SETTING: US tertiary children's hospital.PATIENTS: Hospitalized pediatric patients, 0-18 years of age.MAIN OUTCOME MEASURES: A retrospective chart review was conducted for patients who were converted from their initial opioid therapy regimen (morphine, hydromorphone, and/or fentanyl) to methadone. The primary endpoint was whether or not a dose correction was needed for methadone in the 6 days following conversion using standard dose conversion charts for adults. Documented clinical signs of withdrawal, unrelieved pain, or oversedation were examined.RESULTS: The majority (53.7 percent) of the 199 children were converted to methadone on intensive care units prior extubation or postextubation. The mean conversion ratio was 23.7 mg of oral morphine to 1 mg of oral methadone (median, 18.8 mg:1 mg, SD=25.7). Most patients experienced an adequate conversion (n=115, 57.8 percent), while 83 (41.7 percent) appeared undermedicated, and one child was oversedated. There were no associations found with conversion ratios for initial morphine dose, days to conversion, or effect of withdrawal of concomitant agents with potential for withdrawal.CONCLUSIONS: Opioid conversion to methadone is commonly practiced at our institution; however, dosing was significantly lower compared to adult conversion ratios, and more than 40 percent of children were undermedicated. The majority of children in this study received opioids for sedation while intubated and ventilated; therefore, safe and efficacious pediatric methadone conversion rates remain unclear. Prospective studies are needed."
"OBJECTIVE: The goal of this study was to examine the role of endoplasmic reticulum (ER) stress signaling and the contribution of glycogen synthase kinase (GSK)-3â activation in hyperglycemic, hyperhomocysteinemic, and high-fat-fed apolipoprotein E-deficient (apoE(-/-)) mouse models of accelerated atherosclerosis.METHODS AND RESULTS: Female apoE(-/-) mice received multiple low-dose injections of streptozotocin (40 ìg/kg) to induce hyperglycemia, methionine-supplemented drinking water (0.5% wt/vol) to induce hyperhomocysteinemia, or a high-fat (21% milk fat+0.2% cholesterol) diet to induce relative dyslipidemia. A subset of mice from each group was supplemented with sodium valproate (625 mg/kg), a compound with GSK3 inhibitory activity. At 15 and 24 weeks of age, markers of ER stress, lipid accumulation, GSK3â phosphorylation, and GSK3â activity were analyzed in liver and aorta. Atherosclerotic lesions were examined and quantified. Hyperglycemia, hyperhomocysteinemia, and high-fat diet significantly enhanced GSK3â activity and also increased hepatic steatosis and atherosclerotic lesion volume compared with controls. Valproate supplementation blocked GSK3â activation and attenuated the development of atherosclerosis and the accumulation of hepatic lipids in each of the models examined. The mechanism by which GSK3â activity is regulated in these models likely involves alterations in phosphorylation at serine 9 and tyrosine 216.CONCLUSIONS: These findings support the existence of a common mechanism of accelerated atherosclerosis involving ER stress signaling through activation of GSK3â. Furthermore, our results suggest that atherosclerosis can be attenuated by modulating GSK3â phosphorylation."
"Formaldehyde, 0.5-4.5 mM, increased the threshold for electrical excitation of the nerve, and led to a partial and reversible inhibition of the compound action potential (cAP). The depression was not enhanced by high frequency stimulation. At 8.9 mM or higher, the depression of the nerve excitability could not be reversed. The inhibition of the nerve developed more slowly than that of the muscle, and the nerve was unaffected after 10 min exposure to 2.2 mM. Formaldehyde, 2.2 mM, caused an immediate depression of the indirectly (through the nerve) and directory (at the muscle) elicited twitch tension. After 10 min the tensions were reduced to respectively, 56% and 49% of control. However, the electromyogram was not changed, indicating that the effect was localized to the excitation-contraction coupling. Tetanic tension (100 Hz in 5 sec) was inhibited more than twitch tension during indirect stimulation, whereas the opposite was found during direct stimulation of the muscle. Thus, during high frequency stimulation, formaldehyde must have an additional effect on the neuromuscular transmission. This effect was localized presynaptically since a fall out of endplate potentials was observed in the formaldehyde-treated diaphragm. In 6.7 mM or higher concentrations the directly or indirectly induced contractions were irreversibly blocked. The resting membrane potential of the muscle cells was unchanged after exposure to formaldehyde. Formaldehyde caused myotonia-like contractions of the diaphragm, occasionally after exposure to low concentrations (2.2 mM), and always after exposure to higher concentrations.(ABSTRACT TRUNCATED AT 250 WORDS)"
"A cohort mortality study was conducted of all adult residents who ever lived in Uravan, Colorado, a company town built around a uranium mill. Vital status was determined through 2004 and standardised mortality analyses conducted for 1905 men and women alive after 1978 who lived for at least 6 months between 1936 and 1984 in Uravan. Overall, mortality from all causes (standardised mortality ratio (SMR) 0.90) and all cancers (SMR 1.00) was less than or as expected based on US mortality rates. Among the 459 residents who had worked in underground uranium mines, a significant increase in lung cancer was found (SMR 2.00; 95% CI 1.39-2.78). No significant elevation in lung cancer was seen among the 767 female residents of Uravan or the 622 uranium mill workers. No cause of death of a priori interest was significantly increased in any group, i.e. cancers of the kidney, liver, breast, lymphoma or leukaemia or non-malignant respiratory disease, renal disease or liver disease. This community cohort study revealed a significant excess of lung cancer among males who had been employed as underground miners. We attribute this excess to the historically high levels of radon in uranium mines of the Colorado Plateau, coupled with the heavy use of tobacco products. There was no evidence that environmental radiation exposures above natural background associated with the uranium mill operations increased the risk of cancer. Although the population studied was relatively small, the follow-up was long, extending up to 65 years after first residence in Uravan, and nearly half of the study subjects had died."
"Apolipoprotein A-II (apoA-II), the second major high-density lipoprotein (HDL) apolipoprotein, has been linked to familial combined hyperlipidemia. Human apoA-II transgenic mice constitute an animal model for this proatherogenic disease. We studied the ability of human apoA-II transgenic mice HDL to protect against oxidative modification of apoB-containing lipoproteins. When challenged with an atherogenic diet, antigens related to low-density lipoprotein (LDL) oxidation were markedly increased in the aorta of 11.1 transgenic mice (high human apoA-II expressor). HDL from control mice and 11.1 transgenic mice were coincubated with autologous very LDL (VLDL) or LDL, or with human LDL under oxidative conditions. The degree of oxidative modification of apoB lipoproteins was then evaluated by measuring relative electrophoretic mobility, dichlorofluorescein fluorescence, 9- and 13-hydroxyoctadecadienoic acid content, and conjugated diene kinetics. In all these different approaches, and in contrast to control mice, HDL from 11.1 transgenic mice failed to protect LDL from oxidative modification. A decreased content of apoA-I, paraoxonase (PON1), and platelet-activated factor acetyl-hydrolase activities was found in HDL of 11.1 transgenic mice. Liver gene expression of these HDL-associated proteins did not differ from that of control mice. In contrast, incubation of isolated human apoA-II with control mouse plasma at 37 degrees C decreased PON1 activity and displaced the enzyme from HDL. Thus, overexpression of human apoA-II in mice impairs the ability of HDL to protect apoB-containing lipoproteins from oxidation. Further, the displacement of PON1 by apoA-II could explain in part why PON1 is mostly found in HDL particles with apoA-I and without apoA-II, as well as the poor antiatherogenic properties of apoA-II-rich HDL."
"Metabolomics can be used to identify potential markers and discover new targets for future therapeutic interventions. Here, we developed a novel application of the metabonomics method based on gas chromatography-mass spectrometry (GC/MS) analysis and principal component analysis (PCA) for rapidly exploring the anticancer mechanism of physapubenolide (PB), a cytotoxic withanolide isolated from Physalis species. PB inhibited the proliferation of hepatocellular carcinoma cells in vitro and in vivo, accompanied by apoptosis-related biochemical events, including the cleavage of caspase-3/7/9 and PARP. Metabolic profiling analysis revealed that PB disturbed the metabolic pattern and significantly decreased lactate production. This suggests that the suppression of glycolysis plays an important role in the anti-tumour effects induced by PB, which is further supported by the decreased expression of glycolysis-related genes and proteins. Furthermore, the increased level of p53 and decreased expression of p-Akt were observed, and the attenuated glycolysis and enhanced apoptosis were reversed in the presence of Akt cDNA or p53 siRNA. These results confirm that PB exhibits anti-cancer activities through the Akt-p53 pathway. Our study not only reports for the first time the anti-tumour mechanism of PB, but also suggests that PB is a promising therapeutic agent for use in cancer treatments and that metabolomic approaches provide a new strategy to effectively explore the molecular mechanisms of promising anticancer compounds."
"BACKGROUND: Accurate quantification of DNA using quantitative real-time PCR at low levels is increasingly important for clinical, environmental and forensic applications. At low concentration levels (here referring to under 100 target copies) DNA quantification is sensitive to losses during preparation, and suffers from appreciable valid non-detection rates for sampling reasons. This paper reports studies on a real-time quantitative PCR assay targeting a region of the human SRY gene over a concentration range of 0.5 to 1000 target copies. The effects of different sample preparation and calibration methods on quantitative accuracy were investigated.RESULTS: At very low target concentrations of 0.5-10 genome equivalents (g.e.) eliminating any replicates within each DNA standard concentration with no measurable signal (non-detects) compromised calibration. Improved calibration could be achieved by eliminating all calibration replicates for any calibration standard concentration with non-detects ('elimination by sample'). Test samples also showed positive bias if non-detects were removed prior to averaging; less biased results were obtained by converting to concentration, including non-detects as zero concentration, and averaging all values. Tube plastic proved to have a strongly significant effect on DNA quantitation at low levels (p = 1.8 x 10(-4)). At low concentrations (under 10 g.e.), results for assays prepared in standard plastic were reduced by about 50% compared to the low-retention plastic. Preparation solution (carrier DNA or stabiliser) was not found to have a significant effect in this study.Detection probabilities were calculated using logistic regression. Logistic regression over large concentration ranges proved sensitive to non-detected replicate reactions due to amplification failure at high concentrations; the effect could be reduced by regression against log (concentration) or, better, by eliminating invalid responses.CONCLUSION: Use of low-retention plastic tubes is advised for quantification of DNA solutions at levels below 100 g.e. For low-level calibration using linear least squares, it is better to eliminate the entire replicate group for any standard that shows non-detects reasonably attributable to sampling effects than to either eliminate non-detects or to assign arbitrary high Ct values. In calculating concentrations for low-level test samples with non-detects, concentrations should be calculated for each replicate, zero concentration assigned to non-detects, and all resulting concentration values averaged. Logistic regression is a useful method of estimating detection probability at low DNA concentrations."
"We made a thorough observation of the morphology and course of the lingual nerve (LN) and inferior alveolar nerve (IAN) to clarify their topographical relationships in the infratemporal fossa and in the paralingual area. Thirty-two Korean hemi-sectioned heads were dissected macroscopically and microscopically from a clinical viewpoint. On the 32 tracings on the radiograph, the average distance between the retromolar portion and the LN was 7.8 mm, and no case was found where the LN ran above the alveolar crest as passing along the mandibular lingual plate. The bifurcation of the LN and IAN was located around the mandibular notch, inferior to the otic ganglion in 66% of the cases, and a plexiform branching pattern of the mandibular nerve was observed in only two cases. The bifurcation spot of the LN and IAN was located 14.3 mm inferior to the foramen ovale and 16.5 mm superior to the tip of hamulus. Collateral nerve twigs from the LN to the retromolar area were observed in 26 cases (81.2%), with an average of one nerve twig. We observed four types of variations in terms of communication pattern. In four specimens, the mylohyoid nerve passed through the mylohyoid muscle and connected with the LN. In other four specimens, the IAN communicated with the auriculotemporal nerve. We also observed another type of variational communication between the IAN and the nerve to the lateral pterygoid (LPt); this was observed in only one specimen, and it could be predicted that motor innervation from the nerve to the LPt was transmitted via the mental nerve to the depressor anguli oris. Another type was observed where the IAN divided into two branches with the posterior branch being partially entrapped by the LPt muscle fibers."
"We previously reported that the intronic tagSNP +357G/C in the metastasis suppressor HTPAP is associated with metastasis and prognosis of hepatocellular carcinoma (HCC). The aim of this study was to investigate whether SNPs in the HTPAP promoter modulate HTPAP expression and prognosis of HCC. Genomic DNA from 572 microdissected HCCs were genotyped by pyrosequencing and verified by direct sequencing. Haplotype blocks were analyzed. Reporter plasmids were constructed and transfected into HCC cell lines. Transcriptional activities of plasmids were analyzed by dual-luciferase reporter systems. HTPAP expression was measured by real-time quantitative PCR, western blots, and tissue microarrays. Invasion was assessed by Matrigel assays. The prognostic values of HTPAP promoter SNPs in HCC were evaluated by Kaplan-Meier and Cox regression analyses. We identified six SNPs, including -1053A/G and +64G/C, in the HTPAP promoter. The SNPs were in complete linkage disequilibrium, resulting in three promoter haplotypes (promoter I:-1053AA/+64GG, promoter II: -1053AG/+64GC, and promoter III: -1053GG/+64CC). Promoter I manifested the highest luciferase index (p<0.005). However, no significant difference was observed between promoters II and III. We consistently found that HTPAP mRNA and protein levels were significantly higher in promoter I than that of promoter II+III (p<0.001). Invasion was increased in HCC cells transfected with promoters II+III compared to those transfected with promoter I (p<0.05). The HTPAP promoter II+III haplotype was associated with significantly increased metastasis compared to that of promoter I (p = 0.023). The postoperative five-year overall survival of patients with promoters II+III was lower than that of patients with promoter I (p = 0.006). Multivariate analysis showed that the promoter II+III haplotype was an adverse prognostic marker in HCC. The genetic variants at loci -1053 and +64 of the HTPAP promoter affect the expression of HTPAP, which might be a novel determinant and target for HCC prognosis."
"Elderspeak is a form of patronizing speech that is sometimes used with older adults and can result in unintended negative consequences. Certified nursing assistants (CNAs) working in long-term care facilities may be particularly prone to using elderspeak because they frequently interact with vulnerable and frail older adults who require assistance with activities of daily living. The purpose of the current study was to assess contextual variables that may prompt the use of elderspeak by CNAs. One hundred thirty-four CNAs completed a 36-item questionnaire intended to determine their evaluations of the appropriateness of elderspeak in a variety of contexts. Results indicated that specific resident-related variables (e.g., age, cognitive impairment) and situational variables (e.g., the absence of others during a CNA-resident interaction) were associated with higher ratings of appropriateness of elderspeak. These findings may have implications for improving communication training for CNAs."
"The objective of the model experiment was to find out the improving effect of two selected enzymes, transglutaminase (i) and fermizyme (ii), added at different concentrations of 4.5 mg and 7.5 mg/300 g flour (i) and 15 mg and 60 mg/300 g flour (ii) to the pastry dough on the quality of end products. The investigation was aimed to observe some changes of the sensory parameters (sensory profile) of pastry produced from the freezer-stored dough (-18 +/- 2 degrees C/0, 1, 7 and 14 days), namely shape (camber), odour, taste, crust colour (thickness/hardness), crumb elasticity (porosity, colour, hardness), adhesiveness to palate, etc. It has been ascertained that the sensory quality is favourably affected by the addition of the lower concentration of both enzymes: transglutaminase, 4.5 mg/300 g flour and fermizyme, 15 mg/300 g flour in comparison to the control without enzymes. The cambering ratio values and other sensory profile parameters of pastry gradually decreased during the freezer storage of dough. The best sensory evaluation of both kinds of pastry was achieved after a one-day storage of doughs."
"Human solid tumors contain hypoxic regions that have considerably lower oxygen tension than normal tissues. These impart resistance to radiotherapy and anticancer chemotherapy, as well as predisposing to increased tumor metastases. To develop a potentially therapeutic protein drug highly specific for solid tumors, we constructed fusion proteins selectively stabilized in hypoxic tumor cells. A model fusion protein, oxygen-dependent degradation (ODD)-beta-galactosidase (beta-Gal), composed of a part of the ODD domain of hypoxia-inducible factor-1alpha fused to beta-Gal, showed increased stability in cultured cells under a hypoxia-mimic condition. When ODD-beta-Gal was further fused to the HIV-TAT protein transduction domain (TAT(47-57)) and i.p. injected to a tumor-bearing mouse, the biologically active fusion protein was specifically stabilized in solid tumors but was hardly detected in the normal tissue. Furthermore, when wild-type (WT) caspase-3 (Casp3(WT)) or its catalytically inactive mutant was fused to TAT-ODD and i.p. injected to a tumor-bearing mouse, the size of tumors was reduced by the administration of TAT-ODD-Casp3(WT) but not by TAT-ODD-mutant Casp3. TAT-ODD-Casp3(WT) did not cause any obvious side effects on tumor-bearing mice, suggesting specific stabilization and activation of the fusion protein in the hypoxic tumor cells. These results suggest that the combination of protein therapy using a cytotoxic TAT-ODD fusion protein with radiotherapy and chemotherapy may provide a new strategy for annihilating solid tumors."
"BACKGROUND: Longitudinal studies have revealed that the aortic segment proximal to an infrarenal abdominal aortic aneurysm (AAA) is at risk for continued enlargement after a standard aneurysm repair. Similarly, preliminary reports have shown expansion of one or both aortic necks after endovascular repair. Although some investigators have suggested that this may be a transient effect, continued dilatation at the endograft attachment site could effect the overall device stability.METHODS: As part of a multi-institutional trial of endovascular grafting for the treatment of AAA, 59 patients were successfully implanted with straight endografts between February 1993 and January 1995. A morphometric analysis of aortic neck size was undertaken with serial review of computed tomography scans available through April 1997. The neck sizes at both graft attachment sites were measured, with investigators blinded to patient identity and date of scan. Changes in minor diameter were defined, annual interval expansion rates were calculated, and the data were correlated with endoleak, device migration, aneurysm size change, endograft diameter, attachment system fractures, and initial preimplant neck size.RESULTS: Significant aortic neck enlargement, particularly at the level of the distal neck, was observed for at least 24 months after AAA repair. The annual interval dilation rates of the proximal aortic neck were 0.7 +/- 2.1 mm/year (P = .023) and 0.9 +/- 1.9 (P = .008) mm/year during the first and second years, respectively. Enlargement of the distal neck during the observation period was more marked, with corresponding annual expansion rates of 1.7 +/- 2.9 mm/year (P < .001) and 1.9 +/- 2.5 (P < .001) mm/year. In 5 patients (14%), the minor diameter of the distal neck was at least 6 mm larger than the preimplant diameter of the graft. Migration of the distal attachment system was observed in 3 of these 5 patients. Expansion rates did not have a statistically significant correlation with initial neck size, endograft dimensions, aneurysm size change, presence of endoleak, or attachment system fracture.CONCLUSIONS: Aortic neck enlargement was observed for at least 2 years after endovascular grafting. Close patient follow-up remains mandatory in lieu of the potential risk of late failure as a result of continued aortic expansion. The relative contribution of device design to this phenomenon will need to be defined."
"Commercial captive breeding and trade in body parts of threatened wild carnivores is an issue of significant concern to conservation scientists and policy-makers. Following a 2016 decision by Parties to the Convention on International Trade in Endangered Species of Wild Fauna and Flora, South Africa must establish an annual export quota for lion skeletons from captive sources, such that threats to wild lions are mitigated. As input to the quota-setting process, South Africa's Scientific Authority initiated interdisciplinary collaborative research on the captive lion industry and its potential links to wild lion conservation. A National Captive Lion Survey was conducted as one of the inputs to this research; the survey was launched in August 2017 and completed in May 2018. The structured semi-quantitative questionnaire elicited 117 usable responses, representing a substantial proportion of the industry. The survey results clearly illustrate the impact of a USA suspension on trophy imports from captive-bred South African lions, which affected 82% of respondents and economically destabilised the industry. Respondents are adapting in various ways, with many euthanizing lions and becoming increasingly reliant on income from skeleton export sales. With rising consumer demand for lion body parts, notably skulls, the export quota presents a further challenge to the industry, regulators and conservationists alike, with 52% of respondents indicating they would adapt by seeking 'alternative markets' for lion bones if the export quota allocation restricted their business. Recognizing that trade policy toward large carnivores represents a 'wicked problem', we anticipate that these results will inform future deliberations, which must nonetheless also be informed by challenging inclusive engagements with all relevant stakeholders."
"Sympathetic ophthalmia (SO) is a bilateral, granulomatous, intraocular inflammation that occurs following a penetrating injury to one eye, and has the potential to cause blindness of both eyes. The aim of this study was to examine the expression of á-crystallin and to detect apoptotic cells in the retina of human eyes with SO. Five globes, including three with SO and two age-matched normal appearing retinae, were examined. Formalin-fixed, paraffin-embedded tissue sections were submitted to hematoxylin and eosin staining and immuno-histochemistry with anti-áA and áB-crystallin antibodies. Apoptotic cells were detected using the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL) method, and double-staining immunohistochemistry was conducted together with the TUNEL reaction. In normal-appearing retina, áA-crystallin immunoreactivity was predominantly detected in the cytoplasm of photoreceptors, where áB-crystallin was less marked. In SO globes, granulomatous inflammation was noted in the choroid, whereas the retina and choriocapillaris were preserved. Immunoreactivity for áA-crystallin was detected in the retina, as well as in the cytoplasm and inner/outer photoreceptor segments. By contrast, áB-crystallin was weakly noted in the SO retina. Double-staining immuno-histochemistry revealed no TUNEL-positive photoreceptors in the retina displaying high immunoreactivity for áA-crystallin, but photoreceptor apoptosis was noted where expression of áA-crystallin was relatively low. The present study demonstrated that áA-crystallin was up-regulated in the cytoplasm of photoreceptors in the SO retina. This may play a protective role in the suppression of photoreceptor apoptosis associated with intraocular inflammation."
"OBJECTIVE: According to Orthodox Jewish law, abortion is only permitted before 40 days post conception. This evaluation was performed to determine the feasibility and safety of performing chorionic villus sampling (CVS) at 7 to 8 weeks' gestation so that genetic results would be useful for these patients.STUDY DESIGN: We evaluated a sequential series of 82 Orthodox Jewish patients who chose CVS at <63 days' gestation. Outcome measures included procedure success rates, laboratory success rates, pregnancy outcomes, and complications.RESULTS: CVS was successful in all cases. Ninety-one percent were performed transcervically, with 30% requiring 2 or more insertions. Abnormal results were found in 16 (20%). Of 61 cases with normal genetic and ultrasound results, spontaneous losses at less than 28 weeks occurred in 3 (5%). These rates are higher than the 2.3% loss rate and the 1.2% multiple insertion rate seen at our center when sampling is performed at the usual gestational ages of 10 to 12 weeks. One baby had a severe limb reduction defect (1.6%).CONCLUSION: In very experienced hands, CVS can be safely and reliably performed at very early gestational ages. The ability to obtain an early diagnosis may be associated with increased but acceptable complication rates, including a 1% to 2% risk of limb reduction defects. There are patients for whom the usual paradigms do not suffice, and obtaining an early disgnosis provides them the opportunity to trade increased risks for reproductive choice. The ethical issues are complex, but such decisions can be supported by extensive and detailed informed consent."
"IgG4 related thoracic aortitis is a recent addition to the differential diagnosis for inflammatory aortic disease - a condition which is often underappreciated until complications arise such as aneurysmal formation or aortic dissection. Currently, IgG4 aortitis remains a post-surgical diagnosis reliant on positive immunohistochemistry findings. Management is guided by the extent of disease involvement, which can be gauged by serum IgG4 levels and radiological findings. Options include surgical resection, corticosteroid therapy and steroid-sparing agents to prevent relapses."
"At our hospital, there is an At-Home Enteral Nutrition programme (NED in its Spanish acronym) with participation of the Clinical Nutrition Unit and the Pharmacy Service. The products and all necessary material are dispensed directly to the patient's home and nutritional follow-up is carried out. As a lack of information on various aspects of NED was detected among prescribing doctors, we decided to carry out a survey to assess the level of awareness and the opinion of doctors in the province of Tarragona with regard to NED. They were asked if they knew the indications and characteristics of the different enteral nutrition preparations, as well as their opinion on who should do the follow-up of the patients and on how dispensation should be organized. With the results obtained, we conclude that doctors rarely prescribe NED and are not familiar with the indications nor with enteral nutrition preparations (77.5% and 89%, respectively), although they are interested in the subject. They feel that dispensation should be done directly at the patient's home (43%) and that follow-up should be through a specialized team (57.6%)."
"Measurements of thoracic gas volume, airways resistance, and total respiratory resistance were measured in a group of babies with acute severe bronchiolitis. Assessments were made at convalescence, three to four months later, and after 12 months. Clinical histories were also taken 12 months after the acute episode. Results at this time showed that 35% of the infants had coughing attacks, 50% episodes of wheezing, 50% had dry skin or eczema, and that over 75% had lung function abnormality."
Transient receptor potential (TRP) receptors are ion channels that mediate pain and inflammation. We provide evidence for the distinct roles of TRPV1 and TRPA1 in arthritis.
"OBJECTIVE: To determine whether the experience of the specialist team was associated with adverse events following endovascular treatment of abdominal aortic aneurysms.METHODS: The EUROSTAR database is a voluntary registry of 2863 patients admitted to 93 hospitals in Europe with an abdominal aortic aneurysm treated with endovascular stenting. Mortality, rupture and the need for secondary interventions were the main outcomes.RESULTS: In patients who underwent endovascular stenting by the most experienced specialist teams the mortality rate was 40% lower than in those treated by the least experienced teams (adjusted hazard ratio 0.60, 95% confidence interval: 0.4-1.0; p = 0.05). Also patients treated by the most experienced specialist teams were 68% less likely to have adverse events necessitating a secondary intervention than those treated by the least experienced teams (adjusted hazard ratio 0.32, 95% confidence interval: 0.2-0.5; p < 0.001). The crude rupture rate was 0.1% among patients treated by the most experienced specialist teams and 0.8% among those treated by the least experienced teams (p = 0.74).CONCLUSIONS: Specialist teams with a high level of experience of endovascular abdominal aortic aneurysm stenting encounter lower mortality rates and fewer adverse events leading to secondary interventions."
"OBJECTIVES: A phase II study of weekly paclitaxel combined with S-1, a novel oral fluoropyrimidine, was performed to evaluate the efficacy and tolerability in unresectable or metastatic gastric cancer.PATIENTS AND METHODS: Twenty-nine patients with unresectable and/or metastatic gastric cancer were enrolled in the study. Paclitaxel 50 mg/m(2) was administered on days 1 and 8. S-1 was administered orally at 40 mg/m(2) b.i.d. for 14 consecutive days, followed by a 1-week rest. The primary endpoint was the response rate. Secondary endpoints were safety and overall survival.RESULTS: The overall response rate in 29 patients was 48.3%, differentiated 36.4% and undifferentiated 55.6%. The median survival time was 13.9 months. Grade 3 or higher toxicity was observed in neutropenia (3.4%), diarrhea (3.4%), bilirubin (3.4%) and neuropathy (3.4%).CONCLUSIONS: Combination chemotherapy of weekly paclitaxel and S-1 demonstrated tolerable toxicity and efficacy. This regimen will be one of the initial treatment options for unresectable or metastatic gastric cancer."
"Visual perceptual distortion (i.e., elongation) has been demonstrated in a single case study after several months of cortical deprivation after a stroke. Here we asked whether similar perceptual elongation can be observed in healthy participants after deprivation and, crucially, how soon after deprivation this elongation occurs. To answer this question, we patched one eye, thus noninvasively and reversibly depriving bottom-up input to the region of primary visual cortex (V1) corresponding to the blind spot (BS) in the unpatched eye, and tested whether and how quickly elongation occurs after the onset of deprivation. Within seconds of eye patching, participants perceived rectangles adjacent to the BS to be elongated toward the BS. We attribute this perceptual elongation to rapid receptive field expansion within the deprived V1 as reported in electrophysiological studies after retinal lesions and refer to it as ""referred visual sensations"" (RVS). This RVS is too fast to be the result of structural changes in the cortex (e.g., the growth of new connections), instead implicating unmasking of preexisting connections as the underlying neural mechanism. These findings may shed light on other reported perceptual distortions, as well as the phenomena of ""filling-in."""
"We found that the simple addition of L-methionine to the wild type of Corynebacterium glutamicum results in excretion of the cellular building block L-lysine up to rates of 2.5 nmol/min/mg (dry weight). Biochemical analyses revealed that L-methionine represses the homoserine dehydrogenase activity and reduces the intracellular L-threonine level from 7 to less than 2 mM. Since L-lysine synthesis is regulated mainly by L-threonine (plus L-lysine) availability, the result is enhanced flux towards L-lysine. This indicates a delicate and not well controlled type of flux control at the branch point of aspartate semialdehyde conversion to either L-lysine or L-threonine, probably due to the absence of isoenzymes in C. glutamicum. The inducible system of L-lysine excretion discovered was used to isolate mutants defective in the excretion of this amino acid. One such mutant characterized in detail accumulated 174 mM L-lysine in its cytosol without extracellular excretion of L-lysine, whereas the wild type accumulated 53 mM L-lysine in the cytosol and 5.9 mM L-lysine in the medium. The mutant was unaffected in L-lysine uptake or L-isoleucine or L-glutamate excretion, and also the membrane potential was unaltered. This mutant therefore represents a strain with a defect in an excretion system for the primary metabolite L-lysine."
"Lactobacillus helveticus CNRZ32 is used by the dairy industry to modulate cheese flavor. The compilation of a draft genome sequence for this strain allowed us to identify and completely sequence 168 genes potentially important for the growth of this organism in milk or for cheese flavor development. The primary aim of this study was to investigate the expression of these genes during growth in milk and MRS medium by using microarrays. Oligonucleotide probes against each of the completely sequenced genes were compiled on maskless photolithography-based DNA microarrays. Additionally, the entire draft genome sequence was used to produce tiled microarrays in which noninterrupted sequence contigs were covered by consecutive 24-mer probes and associated mismatch probe sets. Total RNA isolated from cells grown in skim milk or in MRS to mid-log phase was used as a template to synthesize cDNA, followed by Cy3 labeling and hybridization. An analysis of data from annotated gene probes identified 42 genes that were upregulated during the growth of CNRZ32 in milk (P < 0.05), and 25 of these genes showed upregulation after applying Bonferroni's adjustment. The tiled microarrays identified numerous additional genes that were upregulated in milk versus MRS. Collectively, array data showed the growth of CNRZ32 in milk-induced genes encoding cell-envelope proteinases, oligopeptide transporters, and endopeptidases as well as enzymes for lactose and cysteine pathways, de novo synthesis, and/or salvage pathways for purines and pyrimidines and other functions. Genes for a hypothetical phosphoserine utilization pathway were also differentially expressed. Preliminary experiments indicate that cheese-derived, phosphoserine-containing peptides increase growth rates of CNRZ32 in a chemically defined medium. These results suggest that phosphoserine is used as an energy source during the growth of L. helveticus CNRZ32."
"OBJECTIVE: To perform a detailed quantitative immunocytochemical study of the development of fetal rat pancreatic islet A cells.METHODS: Pancreases were obtained from 19 and 21-day- old fetal rats. Ten rats were used per each group. Non-fasting blood glucose levels were measured to confirm that the animals were normoglycemic. The pregnant rats were anesthetized by ether inhalation, and the fetuses were removed from their uteruses. They were fixed in buffered neutral formalin, dehydrated and embedded in paraplast and serially sectioned (5 microm). We examined 32-48 islets (8-12 per section) for each fetus. Sections were stained by avidin biotin complex technique. A quantitative study was performed on the pancreatic islet A cells. Carl Zeiss software from Zeiss was used in this study. This study was carried out at the Department of Anatomy, King Abdul-Aziz University, Jeddah, Kingdom of Saudi Arabia during the period January to December 2005.RESULTS: The volume density and the number of A cells showed a significant increase during the last days of gestation. All other parameters showed a significant increase during the last days of gestation. The A cell nuclear diameter and volume did not increase significantly during the last days of pregnancy. The A cells were well stained and occupied the peripheral part of the islets.CONCLUSION: The present study represented a detailed quantitative immunohistochemical study and demonstrated that the size of the endocrine tissue and the islet A cells increased significantly during the last days of gestation."
"We studied the success rates for tracheal intubation in 32 healthy, anaesthetised patients during simulated grade IIIa laryngoscopy, randomised to either the multiple-use or the single-use bougie. Success rates (primary end-point) and times taken (secondary end-point) to achieve tracheal intubation were recorded. The multiple-use bougie was more successful than the single-use one (15/16 successful intubations vs. 9/16; p = 0.03). With either device, median [range] total tracheal intubation times for successful attempts were < 54 [24-84] s and there were no clinically important differences between these times. We conclude that the multiple-use bougie is a more reliable aid to tracheal intubation than the single-use introducer in grade IIIa laryngoscopy."
We present an ultra-widely tunable non-collinear optical parametric oscillator with an average output power of more than 3 W and a repetition frequency of 34 MHz. The system is pumped by the second harmonic of a femtosecond Yb:KLu(WO4)2 thin-disk laser oscillator. The wavelength of the signal pulse can be rapidly tuned over a wide range from the visible to the NIR just by scanning the resonator length.
"Coexistence of host-specific herbivores on plants is believed to be governed by interspecific interactions, but few empirical studies have systematically unraveled these dynamics. We investigated the role of several factors in promoting coexistence among the aphids Aphis nerii, Aphis asclepiadis, and Myzocallis asclepiadis that all specialize on common milkweed (Asclepias syriaca). Competitive exclusion is thought to occur when interspecific competition is stronger than intraspecific competition. Consequently, we investigated whether predators, mutualists, or resource quality affected the strength of intra- vs. interspecific competition among aphids in factorial manipulations of competition with exposure to predation, ants, and variable plant genotypes in three separate experiments. In the predation x competition experiment, predators reduced aphid per capita growth by 66%, but the strength of intra- and interspecific competition did not depend on predators. In the ants x competition experiment, ants reduced per capita growth of A. nerii and M. asclepiadis (neither of which were mutualists with ants) by approximately one-half. In so doing, ants ameliorated the negative effects of these competitors on ant-tended A. asclepiadis by two-thirds, representing a novel benefit of ant-aphid mutualism. Nevertheless, ants alone did not explain the persistence of competitively inferior A. asclepiadis as, even in the presence of ants, interspecific competition remained stronger than intraspecific competition. In the plant genotype x competition experiment, both A. asclepiadis and M. asclepiadis were competitively inferior to A. nerii, with the strength of interspecific competition exceeding that of intraspecific competition by 83% and 23%, respectively. Yet these effects differed among milkweed genotypes, and there were one or more plant genotypes for each aphid species where coexistence was predicted. A synthesis of our results shows that predators play little or no role in preferentially suppressing competitively dominant A. nerii. Nonetheless, A. asclepiadis benefits from ants, and A. asclepiadis and M. asclepiadis may escape competitive exclusion by A. nerii on select milkweed genotypes. Taken as a whole, the coexistence of three host-specific aphid species sharing the same resource was promoted by the dual action of ants as antagonists and mutualists and by genetic diversity in the plant population itself."
"Rapid and reliable detection and identification of coccidian oocysts are essential for animal health and foodborne disease outbreak investigations. Traditional microscopy and morphological techniques can identify large and unique oocysts, but they are often subjective and require parasitological expertise. The objective of this study was to develop a real-time quantitative PCR (qPCR) assay using melting curve analysis (MCA) to detect, differentiate, and identify DNA from coccidian species of animal health, zoonotic, and food safety concern. A universal coccidia primer cocktail was designed and employed to amplify DNA from Cryptosporidium parvum, Toxoplasma gondii, Cyclospora cayetanensis, and several species of Eimeria, Sarcocystis, and Isospora using qPCR with SYBR Green detection. MCA was performed following amplification, and melting temperatures (T(m)) were determined for each species based on multiple replicates. A standard curve was constructed from DNA of serial dilutions of T. gondii oocysts to estimate assay sensitivity. The qPCR assay consistently detected DNA from as few as 10 T. gondii oocysts. T(m) data analysis showed that C. cayetanensis, C. parvum, Cryptosporidium muris, T. gondii, Eimeria bovis, Eimeria acervulina, Isospora suis, and Sarcocystis cruzi could each be identified by unique melting curves and could be differentiated based on T(m). DNA of coccidian oocysts in fecal, food, or clinical diagnostic samples could be sensitively detected, reliably differentiated, and identified using qPCR with MCA. This assay may also be used to detect other life-cycle stages of coccidia in tissues, fluids, and other matrices. MCA studies on multiple isolates of each species will further validate the assay and support its application as a routine parasitology screening tool."
"OBJECTIVES: To determine whether geriatric triage decisions made using a comprehensive geriatric assessment (CGA) performed online are less reliable than face-to-face (FTF) decisions.DESIGN: Multisite noninferiority prospective cohort study. Two specialist geriatricians assessed individuals sequentially referred for an acute care geriatric consultation. Participants were allocated to one FTF assessment and an additional assessment (FTF or online (OL)), creating two groups-two FTF (FTF-FTF, n = 81) or online and FTF (OL-FTF, n = 85).SETTING: Three acute care public hospitals in two Australian states.PARTICIPANTS: Admitted individuals referred for CGA.INTERVENTION: Nurse-administered CGA, based on the interRAI Acute Care assessment system accessed online and other online clinical data such as pathology results and imaging enabling geriatricians to review participants' information and provide input into their care from a distance.MEASUREMENTS: The primary decision subjected to this analysis was referral for permanent residential care. Geriatricians also recorded recommendations for referrals and variations for medication management and judgment regarding prognosis at discharge and after 3 months.RESULTS: Overall percentage agreement was 88% (n = 71) for the FTF-FTF group and 91% (n = 77) for the OL-FTF group. The difference in agreement between the FTF-FTF and OL-FTF groups was -3%, indicating that there was no difference between the methods of assessment. Judgements made regarding diagnoses of geriatric syndromes, medication management, and prognosis (with regard to hospital outcome and location at 3 months) were found to be equally reliable in each mode of consultation.CONCLUSION: Geriatric assessment performed online using a nurse-administered structured CGA system was no less reliable than conventional assessment in making clinical triage decisions."
"The stochastic process of long-distance dispersal is the exclusive means by which plants colonize oceanic islands. Baker's rule posits that self-incompatible plant lineages are unlikely to successfully colonize oceanic islands because they must achieve a coordinated long-distance dispersal of sufficiently numerous individuals to establish an outcrossing founder population. Here, we show for the first time that Mauritian Coffea species are self-incompatible and thus represent an exception to Baker's rule. The genus Coffea (Rubiaceae) is composed of approximately 124 species with a paleotropical distribution. Phylogenetic evidence strongly supports a single colonization of the oceanic island of Mauritius from either Madagascar or Africa. We employ Bayesian divergence time analyses to show that the colonization of Mauritius was not a recent event. We genotype S-RNase alleles from Mauritian endemic Coffea, and using S-allele gene genealogies, we show that the Mauritian allelic diversity is confined to just seven deeply divergent Coffea S-RNase allelic lineages. Based on these data, we developed an individual-based model and performed a simulation study to estimate the most likely number of founding individuals involved in the colonization of Mauritius. Our simulations show that to explain the observed S-RNase allelic diversity, the founding population was likely composed of fewer than 31 seeds that were likely synchronously dispersed from an ancestral mainland species."
"BACKGROUND: Conflicting reports exist regarding whether patients undergoing surgery on the weekend or later in the week experience worse outcomes.METHODS: We identified patients undergoing abdominal aortic aneurysm (AAA) repair in the Vascular Quality Initiative between 2009 and 2017 [n = 38,498; 30,537 endovascular aneurysm repair (EVAR) and 7961 open repair]. We utilized mixed effects logistic regression to compare adjusted rates of perioperative mortality based on the day of repair.RESULTS: Tuesday was the most common day for elective repair (22%), Friday for symptomatic repairs (20%), and ruptured aneurysms were evenly distributed. Patients with ruptured aneurysms experienced similar adjusted mortality whether they underwent repair during the week or on weekends. Transfers of ruptured AAA were more common over the weekend. However, patients transferred on the weekend experienced higher adjusted mortality than those transferred during the week (28% vs 21%, P = 0.02), despite the fact that during the week, transferred patients actually experienced lower adjusted mortality than patients treated at the index hospital (21% vs 31%, P < 0.01). Among symptomatic patients, adjusted mortality was higher for those undergoing repair over the weekend than those whose surgeries were delayed until a weekday (7.9% vs 3.1%, P = 0.02). Adjusted mortality in elective cases did not vary across the days of the week. Results were consistent between open and EVAR patients.CONCLUSION: We found no evidence of a weekend effect for ruptured or symptomatic AAA repair. However, patients with ruptured AAA transferred on the weekend experienced higher mortality than those transferred during the week, suggesting a need for improvement in weekend transfer processes."
"Transforming growth factor-beta1 (TGF-beta1) is widely regarded as a potent fibrogenic renal growth factor. In cell culture, TGF-beta1 has been shown to increase various extracellular matrix (ECM) proteins and tissue inhibitors of metalloproteinases (TIMP), while decreasing matrix metalloproteinases (MMP), providing the optimum environment for progressive ECM accumulation. This study, which uses the isolated perfused rat kidney (IPRK), describes for the first time in a whole kidney preparation the action of TGF-beta1 on factors associated with ECM processing. This model allows the study of the intact rat kidney with physiologic cell-cell interactions in the absence of confounding systemic influences. Left kidneys were removed from male Wistar rats by a nonischemic technique and perfused with a sterile, apyrogenic, endotoxin-free perfusate, based on the plasma volume expander Hemaccel (polygeline), at constant pressure in a recirculating IPRK system. Kidneys were perfused for 1 h either with (n = 3) or without (n = 3) recombinant human TGF-beta1 (20 ng/ml). The effects of perfusion were controlled by comparison with the nonperfused contralateral kidney (n = 6). TGF-beta1 was measured in the perfusate and urine, at the start and end of the experiment using an enzyme-linked immunosorbent assay to its biologically active form. After perfusion, sections of the kidneys were analyzed for changes in mRNA by Northern blotting. Significant increases in mRNA for fibronectin (7.5-fold, P < 0.01), heparan sulfate proteoglycan core protein (53-fold, P < 0.001), laminin beta1 (12-fold, P < 0.001), collagen alpha1(IV) (17-fold, P < 0.001), collagen alpha1(III) (fourfold, P < 0.001), and MMP9 (twofold, P < 0.05) were observed after perfusion with TGF-beta1. Measurement of TIMP1, TIMP2, TIMP3, MMP1, and MMP2 mRNA demonstrated no detectable change, whereas determination of mRNA for tissue transglutaminase, an enzyme capable of cross-linking many ECM components, showed an eightfold increase (P < 0.01). This study suggests that in the IPRK and in the absence of other exogenous growth factors, TGF-beta1 selectively increases the synthesis of ECM and tissue transglutaminase without changes that would result in the reduction of ECM degradation."
"The rsf12 mutation was isolated in a synthetic lethal screen for genes functionally interacting with Swi4. RSF12 is CLB5. The clb5 swi4 mutant cells arrest at G(2)/M due to the activation of the DNA-damage checkpoint. Defects in DNA integrity was confirmed by the increased rates of chromosome loss and mitotic recombination. Other results suggest the presence of additional defects related to morphogenesis. Interestingly, genes of the PKC pathway rescue the growth defect of clb5 swi4, and pkc1 and slt2 mutations are synthetic lethal with clb5, pointing to a connection between Clb5, the PKC pathway, and Swi4. Different observations suggest that like Clb5, the PKC pathway and Swi4 are involved in the control of DNA integrity: there is a synthetic interaction between pkc1 and slt2 with rad9; the pkc1, slt2, and swi4 mutants are hypersensitive to hydroxyurea; and the Slt2 kinase is activated by hydroxyurea. Reciprocally, we found that clb5 mutant is hypersensitive to SDS, CFW, latrunculin B, or zymolyase, which suggests that, like the PKC pathway and Swi4, Clb5 is related to cell integrity. In summary, we report numerous genetic interactions and phenotypic descriptions supporting a close functional relationship between the Clb5 cyclin, the PKC pathway, and the Swi4 transcription factor."
"A new class of mutations is described which induce mucoid growth of Escherichia coli K-12. Unlike classical capR and capS mutations, the mucoid phenotype of colonies of the cap forms obtained is determined by mutations in genomes of thermosensitive plasmids pEG1 and RP1-6Repts12, derivates of the RP1 Inc P1 Ap Tc Km factor and accompanied by a complete or partial loss of the thermosensitive character of maintenance. The morphological character induced by plasmids is not associated with changes in the sensitivity of bacteria to UV irradiation and is determined by superproduction of capsular polysaccharide differing in the chemical structure from colanic acid, a common capsular polysaccharide of E. coli K-12."
"Disordered swallowing, or dysphagia, is a common problem seen in patients undergoing treatment for cancer, stroke and neurodegenerative illnesses. This disease is associated with aspiration-induced chest infections. The methods currently used for diagnosis, however, are qualitative or based on expensive equipment. Swallowing accelerometry is a promising low-cost, quantitative and noninvasive tool for the evaluation of swallowing. This work describes the design and application of a bedside instrument able to evaluate swallowing mechanisms and to identify patients at risk of aspiration. Three-axis swallowing accelerometry was used to measure the neck vibrations associated with deglutition, providing analog signals to a virtual instrument developed in LabVIEW environment. In vivo tests in normal subjects as well as tests with disphagic patients showed that the system was able to easily and non-invasively detect changes in the swallowing acceleration pattern associated with increasing values of water volume (p < 0.02) and disphagia. We concluded that the developed system could be a useful tool for the objective bedside evaluation of patients at risk of aspiration."
Attenuated lines of Eimeria acervulina were isolated between 62 and 72 h post-infection from the Houghton (H) strain. The inoculation of small numbers of oocysts of precocious (HP) lines gave substantial protection to Light Sussex chicks kept on litter against challenge with the virulent H strain. The precocious trait of the 72 h HP line was shown to be stable because the kinetics of oocyst production remained unaltered after 9 consecutive passages through birds in which only late developing oocysts were used for passage. The precocious 62 h HP line was subjected to further selection for 5 consecutive passages. The resultant (62 A HP) line was shown to be more attenuated than the 72 h HP line but remained capable of immunizing chicks against challenge with the H strain.
"Purpose: Secondary to brain injury, many people develop eye movement disorders (oculomotor deficits). To clarify, optimize, and standardize the development of oculomotor rehabilitation programs, we systematically reviewed the literature on vision rehabilitation interventions for oculomotor deficits in brain injury, focusing on those with broad clinical feasibility.Materials and Methods: We searched MEDLINE (PubMed), CENTRAL, Scopus, and CINAHL databases for key title terms ""oculomotor"", ""rehabilitation"", or a related term, and ""brain injury"" or a related term in the title or abstract. We excluded case reports of a single patient, studies of non-oculomotor visual deficits, and articles in which the intervention and assessment methods were not explicitly identified.Results: Nine articles were included, six of which utilized computer-based training programs to elicit characteristic fixation, saccades, pursuit, vergence, and accommodative movements. Within the entire sample, interventions ranged from 3 to 10 weeks, and involved 2 to 5 training sessions per week.Conclusions: Oculomotor rehabilitation interventions showed some efficacy in treating patients with brain injury; however, there were very few studies overall. Several eye movement types - fixation, saccades, pursuit, vergence, and accommodation - can be elicited manually by therapists. We eagerly await the development and implementation of new intervention programs for broad-based clinical practice."
"BACKGROUND: Point mutations can have a strong impact on protein stability. A change in stability may subsequently lead to dysfunction and finally cause diseases. Moreover, protein engineering approaches aim to deliberately modify protein properties, where stability is a major constraint. In order to support basic research and protein design tasks, several computational tools for predicting the change in stability upon mutations have been developed. Comparative studies have shown the usefulness but also limitations of such programs.RESULTS: We aim to contribute a novel method for predicting changes in stability upon point mutation in proteins called MAESTRO. MAESTRO is structure based and distinguishes itself from similar approaches in the following points: (i) MAESTRO implements a multi-agent machine learning system. (ii) It also provides predicted free energy change (Ä ÄG) values and a corresponding prediction confidence estimation. (iii) It provides high throughput scanning for multi-point mutations where sites and types of mutation can be comprehensively controlled. (iv) Finally, the software provides a specific mode for the prediction of stabilizing disulfide bonds. The predictive power of MAESTRO for single point mutations and stabilizing disulfide bonds is comparable to similar methods.CONCLUSIONS: MAESTRO is a versatile tool in the field of stability change prediction upon point mutations. Executables for the Linux and Windows operating systems are freely available to non-commercial users from http://biwww.che.sbg.ac.at/MAESTRO."
Transitional cell carcinoma of the kidney with vena caval tumor thrombus is a rarity with 12 cases reported in the literature. We review in this article the elements of the diagnosis and possible treatment modalities.
"While it is known that precise dental epithelial-mesenchymal (DE-DM) cell interactions provide critical functions in tooth development, reliable methods to establish proper DE-DM cell interactions for tooth regeneration have yet to be established. To address this challenge, and to generate bioengineered teeth of predetermined size and shape, in this study, we characterize three dimensional (3D) pre-fabricated DE-DM cell constructs. Human dental pulp cell seeded Collagen gel layers were co-cultured with porcine DE cells suspended in Growth Factor Reduced (GFR) Matrigel. The resulting 3D DE-DM cell layers were cultured in vitro, or implanted and grown subcutaneously in vivo in nude rats. Molecular, histological and immunohistochemical (IHC) analyses of harvested implants revealed organized DE-DM cell interactions, the induced expression of dental tissue-specific markers Amelogenin (AM) and Dentin Sialophosphoprotein (DSPP), and basement membrane markers Laminin 5 and collagen IV, and irregular mineralized tissue formation after 4 weeks. We anticipate that these studies will facilitate the eventual establishment of reliable methods to elaborate dental tissues, and full sized teeth of specified sized and shape."
This work demonstrates that gp96 preparations isolated from cells infected with intracellular bacteria induce cytotoxic T-lymphocyte responses and confer protection. Our findings extend previous reports on the immunogenicity of gp96-associated peptides to antigens derived from intracellular bacteria. Immunization with gp96 may therefore represent a promising vaccination strategy against bacterial pathogens.
"The study evaluated the effect of DHA 625 mg in women who experience menopausal symptoms, on sexuality and quality of life (QoL), and on the auditory brainstem response (ABR). Forty-two perimenopausal women were enrolled. The Kupperman Index (KI) was used to evaluate menopause symptoms. The Short Form-36 (SF-36), Female Sexual Function Index (FSFI), and the Female Sexual Distress Scale (FSDS) were used to assess QoL, sexual function, and sexual distress, respectively. Auditory evoked potentials to measure the ABR. The study had one follow-up at 6 months. The women reported an improvement in the KI total score (p < .001). Moreover, women reported QoL improvements in all the psychological categories (p < .001), but not in physical categories (p = NS). FSFI and FSDS total scores increased (p < .01) and the FSDS score decreased (p < .01), mainly due to arousal (p < .03) and lubrication (p < .05) sexual aspects. The ABR wave latencies were lower than the baseline values (p < .05). DHA could be effective in modulating some perimenopausal symptoms in women and, consequently could contribute to improve their QoL and sexual life. Finally, DHA seems to have a direct activity on the neuronal conduction time into the audiological system."
"OBJECTIVE: To study the management of esophageal foreign bodies.MATERIAL AND METHODS: A retrospective study was made of all rigid esophagoscopies performed for suspected foreign bodies in the esophagus by an otolaryngology department for ten years.RESULTS: Rigid esophagoscopy was performed for suspected foreign bodies in 46 patients (27 females, 19 males); age range 22 months to 88 years. In 40 cases an impacted foreign body was found. The most frequent location was the upper third of the esophagus (33/82.5%). The most common type of foreign body was chicken bones in adults (17/42) and coins in children (2/4). Nine patients (all adults) had complications.CONCLUSIONS: Due to its low cost and morbility, flexible endoscopy is the first choice for managing esophageal foreign bodies. Rigid esophagoscopy is still an appropriate technique when flexible endoscopy fails or it is not possible."
"A case of fatal myocarditis in a 24-year-old otherwise healthy man is described. It was possible to cultivate Salmonella typhimurium from the alimentary tract, the blood, the liver and skeletal muscles. The possibility of a solitary myocarditis with fatal outcome due to Salmonella typhimurium infection is discussed. Such a case seems not to have been mentioned previously in the literature. The problems concerning the statistical registration of such a death are briefly discussed."
"The objective is to determine cardiovascular and insulin release effects under metoclopramide (MTC) and dopamine (DA) infusion by using an acute comparative design with the intravenous infusion of both drugs. We evaluated 15 normal (normotensive and normoglycemic) subjects, 13 hypertensive, and 15 type 2 diabetic subjects. Subjects were submitted to an experimental design in which we first gave them a 0.9% saline solution for 30 minutes, and then administered MTC at 7.5 microg kg min through an intravenous infusion during a period of 30 minutes. Although subjects were receiving MTC, we added an intravenous infusion of DA at 1-3 microg kg min during 30 minutes. Blood pressure, heart rate, serum lipid profile, and insulin levels were measured. Sympathetic reactivity by the cold pressor test was also measured. In normotensive subjects, there was a systolic blood pressure and heart rate increase during MTC plus DA infusion. In subjects with diabetes mellitus there was a heart rate increase without changes in blood pressure during the MTC plus DA infusion period. In hypertensive subjects, MTC induced a significant decrease of systolic and diastolic blood pressure. During MTC plus DA period there was an increase of heart rate but no significant changes in blood pressure. During cold pressor test in both diabetic and hypertensive subjects, there were significant increases of both blood pressure and heart rate. Insulin serum levels increased in normotensive and hypertensive subjects but were attenuated in subjects with diabetes mellitus. We conclude that there is a pharmacologic interaction between MTC and DA, that the pressor effects of DA are due to activation to beta and alpha adrenergic receptors, and that the cardiovascular effects of DA in type 2 diabetic subjects are attenuated by a probable defect in sympathetic system and to endothelial dysfunction."
"We studied the highly dynamic evolution of mitochondrial ribosomal proteins (MRPs) in Holozoa. Most major clades within Holozoa are characterized by gains and/or losses of MRPs. The usefulness of gains of MRPs as rare genomic changes in phylogenetics is undermined by the high frequency of secondary losses. However, phylogenetic analyses of the MRP sequences provide evidence for the Acrosomata hypothesis, a sister group relationship between Ctenophora and Bilateria. An extensive restructuring of the mitochondrial genome and, as a consequence, of the mitochondrial ribosomes occurred in the ancestor of metazoans. The last MRP genes encoded in the mitochondrial genome were either moved to the nuclear genome or were lost. The strong decrease in size of the mitochondrial genome was probably caused by selection for rapid replication of mitochondrial DNA during oogenesis in the metazoan ancestor. A phylogenetic analysis of MRPL56 sequences provided evidence for a horizontal gene transfer of the corresponding MRP gene between metazoans and Dictyostelidae (Amoebozoa). The hypothesis that the requisition of additional MRPs compensated for a loss of rRNA segments in the mitochondrial ribosomes is corroborated by a significant negative correlation between the number of MRPs and length of the rRNA. Newly acquired MRPs evolved faster than bacterial MRPs and positions in eukaryote-specific MRPs were more strongly affected by coevolution than positions in prokaryotic MRPs in accordance with the necessity to fit these proteins into the pre-existing structure of the mitoribosome."
"BACKGROUND AND OBJECTIVES: Abnormal ankle-brachial index (ABI) is associated with a high risk of cardiovascular disease. This study has aimed to investigate the association between low ABI and risk of cardiovascular death in a general population attended in a primary care center.PATIENTS AND METHODS: A total of 1,361 volunteers aged between 60 and 79 years without any evidence of peripheral artery disease who attended a primary care center participated in the study. They underwent a complete physical examination, together with standard blood tests and ABI was determined. The participants were contacted by telephone 4 years later and asked about any cardiovascular problems for that period. Causes of death and hospitalization were confirmed in the medical records in the primary care center and/or hospital.RESULTS: Information was obtained about the clinical evolution of 1,300 participants (mean age 69.9 years, 38.2% men). Mean follow-up was 49.8 months. There were 13 cardiovascular death and 49 major cardiovascular events. Low ABI (<0.9) was associated with a significant higher risk of cardiovascular death (adjusted relative risk 6.83; 95% confidence interval 1.36-34.30, P=.020), and with a higher risk of major cardiovascular events (adjusted relative risk 2.42; 95% confidence interval 0.99-5.91, P=.051). High or uncompressible ABI was not associated with higher cardiovascular risk.CONCLUSIONS: A low ABI was associated with higher risk of cardiovascular death in the general population followed-up in a primary care center."
"A reliable identification of cells on the basis of their surface markers is of great interest for diagnostic and therapeutic applications. We present a multiplexed labeling and detection strategy that is applied to four microparticle populations, each mimicking cellular or bacterial samples with varying surface concentrations of up to four epitopes, using four distinct biotags that are meant to be used in conjunction with surface enhanced Raman spectroscopy (SERS) instead of fluorescence, together with microfluidics. Four populations of 6 ìm polystyrene beads were incubated with different mixtures, ""cocktails"" of four SERS biotags (SBTs), simulating the approach that one would follow when seeking to identify multiple biomarkers encountered in biological applications. Populations were flowed in a microfluidic flow-focusing device and the SERS signal from individual beads was acquired during continuous flow. The spectrally rich SERS spectra enabled us to separate confidently the populations by utilizing principal component analysis (PCA). Also, using classical least squares (CLS), we were able to calculate the contributions of each SBT to the overall signal in each of the populations, and showed that the relative SBT contributions are consistent with the nominal percentage of each marker originally designed into that bead population, by functionalizing it with a given SBT cocktail. Our results demonstrate the multiplexing capability of SBTs in potential applications such as immunophenotyping."
"Elderly people are at high risk for influenza-related morbidity and mortality due to progressive immunosenescence. While toll-like receptor (TLR) agonist containing adjuvants, and other adjuvants, have been shown to enhance influenza vaccine-induced protective responses, the mechanisms underlying how these adjuvanted vaccines could benefit the elderly remain elusive. Here, we show that a split H1N1 influenza vaccine (sH1N1) combined with a TLR4 agonist, glucopyranosyl lipid adjuvant formulated in a stable oil-in-water emulsion (GLA-SE), boosts IgG2c:IgG1 ratios, enhances hemagglutination inhibition (HAI) titers, and increases protection in aged mice. We find that all adjuvanted sH1N1 vaccines tested were able to protect both young and aged mice from lethal A/H1N1/California/4/2009 virus challenge after two immunizations compared to vaccine alone. We show that GLA-SE combined with sH1N1, however, also provides enhanced protection from morbidity in aged mice given one immunization (based on change in weight percentage). While the GLA-SE-adjuvanted sH1N1 vaccine promotes the generation of cytokine-producing T helper 1 cells, germinal center B cells, and long-lived bone marrow plasma cells in young mice, these responses were muted in aged mice. Differential in vitro responses, dependent on age, were also observed from mouse-derived bone marrow-derived dendritic cells and lung homogenates following stimulation with adjuvants, including GLA-SE. Besides enhanced HAI titers, additional protective factors elicited with sH1N1 + GLA-SE in young mice were observed, including (a) rapid reduction of viral titers in the lung, (b) prevention of excessive lung inflammation, and (c) homeostatic maintenance of alveolar macrophages (AMs) following H1N1 infection. Collectively, our results provide insight into mechanisms of adjuvant-mediated immune protection in the young and elderly."
"Adipose tissue contributes to plasma levels of lipid transfer proteins and is also the major source of plasma adipokines. We hypothesized that plasma cholesteryl ester transfer protein (CETP) mass, phospholipid transfer protein (PLTP) activity and cholesteryl ester transfer (CET, a measure of CETP action) are determined by adipokine levels. In this study, relationships of plasma CETP mass, PLTP activity and CET with leptin, resistin and adiponectin were analyzed in type 2 diabetic patients and control subjects. Plasma PLTP activity (P<0.001), CET (P<0.001), leptin (P=0.003), resistin (P<0.001), high sensitive C-reactive protein (P=0.005), and insulin resistance (HOMA(ir)) (P<0.001) were higher, whereas HDL cholesterol (P<0.001) and plasma adiponectin (P<0.001) were lower in 83 type 2 diabetic patients (32 females) than in 83 sex-matched control subjects. Multiple linear regression analysis demonstrated that in diabetic patients plasma leptin levels were related to plasma CETP mass (P=0.018) and PLTP activity (P<0.001), but not to the other adipokines measured. Plasma CET was inversely correlated with adiponectin in univariate analysis, but this association disappeared in multivariate models that included plasma lipids and CETP. In conclusion, both plasma CETP mass and PLTP activity are associated with plasma leptin in type 2 diabetes. The elevated CET in these patients is not independently related to any of the measured plasma adipokines."
"1. Tachykinin-stimulated inositol phospholipid hydrolysis was examined in slices of rat parotid gland, hamster urinary bladder and guinea-pig ileum longitudinal muscle. 2. In the presence of lithium, substance P and other naturally-occurring and synthetic tachykinins induced large, dose-dependent increases in [3H]-inositol monophosphate accumulation. 3. In slices of rat parotid gland, [pGlu6,L-Pro9]SP(6-11) was considerably more potent in stimulating inositol phospholipid hydrolysis than [pGlu6,D-Pro9]SP(6-11). 4. In contrast, in slices of hamster urinary bladder, [pGlu6,D-Pro9]SP(6-11) exhibited greater potency in evoking inositol phospholipid breakdown than [pGlu6,L-Pro9]SP(6-11). 5. The differential selectivity of these C-terminal fragments of substance P suggests that they may be useful tools for distinguishing between NK1 and NK2 receptors. 6. L-659,837 and L-659,874 antagonized eledoisin-stimulated inositol phospholipid hydrolysis in slices of hamster urinary bladder. Neither compound significantly reduced substance-P evoked inositol phospholipid breakdown in slices of rat parotid gland, or senktide-induced inositol phospholipid hydrolysis in slices of guinea-pig ileum. 7. L-659,837 and L-659,874 had no effect on the atropine-sensitive, carbachol-stimulated inositol phospholipid hydrolysis in slices of rat parotid gland. 8. These data further support the notion that L-659,837 and L-659,874 are potent and selective NK2 receptor antagonists."
"The renal function of 113 patients undergoing cardiac surgery under ECC was studied. In 32 p. 100 of the cases renal involvement was noted which was moderate in 18 p. 100 of the cases, severe in 10 p. 100 of the cases and anuric in 4 p. 100 of the cases. Valvular surgery was complicated once in every three cases by renal involvement, the repair of congenital malformations once out of every two cases, and coronary surgery in 17 p. 100 of the cases. The fall in renal perfusion represents the essential factor in this renal involvement, which should be avoided by the maintenace during ECC of a high output level and a satisfactory perfusion pressure and by the recovery of correct hemodynamics after the intervention."
"Exploration of the diversity among primate lentiviruses is necessary to elucidate the origins and evolution of immunodeficiency viruses. During a serological survey in Cameroon, we screened 25 wild-born guereza colobus monkeys (Colobus guereza) and identified 7 with HIV/SIV cross-reactive antibodies. In this study, we describe a novel lentivirus, named SIVcol, prevalent in guereza colobus monkeys. Genetic analysis revealed that SIVcol was very distinct from all other known SIV/HIV isolates, with average amino acid identities of 40% for Gag, 50% for Pol, 28% for Env, and around 25% for proteins encoded by five other genes. Phylogenetic analyses confirmed that SIVcol is genetically distinct from other previously characterized primate lentiviruses and clusters independently, forming a novel lineage, the sixth in the current classification. Cercopithecidae monkeys (Old World monkeys) are subdivided into two subfamilies, the Colobinae and the Cercopithecinae, and, so far, all Cercopithecidae monkeys from which lentiviruses have been isolated belong to the Cercopithecinae subfamily. Therefore, SIVcol from guereza colobus monkeys (C. guereza) is the first primate lentivirus identified in the Colobinae subfamily and the divergence of SIVcol may reflect divergence of the host lineage."
"A field study was carried out in the south of the Iberian Peninsula in an industrial area in the neighbourhood of Huelva city, SW Spain, and in a natural area (Do?ana National Park) for comparison, to estimate the genetic risk induced by environmental pollution in wild mice. Genotoxic effects in a sentinel organism, the Algerian mice (Mus spretus) free living in the industrial area were compared with animals of the same species living in the natural protected area. The single cell gel electrophoresis, or Comet assay, was performed as a genotoxicity test in peripheral blood of mice. Our results clearly show that mice free living in the contaminated area bear a high burden of genetic damage as compared with control individuals. The results suggest that the assessing of genotoxicity levels by the Comet assay in wild mice can be used as a valuable test in pollution monitoring and environmental conservation."
"A bacteroidal disease of honeybee (Apis mellifera ) larvae was found in some regions of Zhejiang Province, China , in early spring 2005. The diseased larvae lost its shine, became yellow and rotted when serious. This symptom was different to any bacteroidal disease of honeybee larval been reported. So, it is considered to be a new bacteroidal disease of honeybee larval. Five pure cultures of bacteria were separated from ten collections of diseased honeybee larvae, named as L1, L2, L3, IA and L5. Among these five pure cultures, only L2 could make the healthy honeybee larvae become diseased in both field and lab test. The symptom caused by L2 was similar to the natural-infection. From the diseased larvae caused by L2 could isolate bacteria the same as L2. Thus 12 was determined as the causing agent of this bacteroidal disease of honeybee larval. L2 was identified according to the characteristics of morphology, physiological biochemical characteristics and 16S rRNA gene sequence. As a result, the morphology and physiological biochemical characteristics of L2 were similar to E. faecium. And its 16S rRNA sequences highly matched to E. faecium, the similarities between them were higher than 99%. The overall similarity values between L2 and the published 16S rRNA sequences of 41 typical species of Enterococcus were 93.9% - 99.5% , the top value was between 12 and E. faecium. In the phylogenetic tree, L2 and E. faecium were assembled in the same ramification. So 12 was identified as E. faecium. Although Enterococcus faecium was known as pathogen to many post, account for 12% of all nosocomial infections, only second to E. coli, but there is no report about this bacteria infects honeybee up to now. So it is a new pathogen to honeybee. The isolation and identification of pathogen of this new bacteroidal larvae disease, afford a good feasibility for available prevention and cure to this new disease."
"Interleukin 6 (IL6) plays an essential role in the regulation of immune response to chronic disease. In this study, the three known single nucleotide polymorphisms (SNPs) in the IL6 promoter region were genotyped in a large chronic hepatitis B cohort to evaluate the effects of IL6 promoter variants. The single base extension method was used for this genotyping. Haplotypes were constructed by the three SNPs in IL6. Allele frequencies were compared for; i) patients with chronic hepatitis (CH) and chronic carriers vs. chronic hepatis patients with clinical evidence of liver cirrhosis (LC) (i.e., portal hypertension), ii) cirrhotic patients with hepatocellular carcinoma (HCC) vs. without HCC by logistic regression, and iii) with respect to the time intervals from the onset of infection to HCC. Results were analyzed by Cox relative hazard analysis on the assumption that all the patients were infected during early infancy. The frequencies of each SNP were 0.002 (IL6-597 G>A), 0.25 (IL6-572 C>G) and 0.002 (IL6-174 G>C), respectively, in the Korean population (n = 1,046). No significant associations were detected between IL6-572 C>G and chronic hepatitis B outcome in this study; i.e., LC occurrence on CH (OR = 0.16-1.27, P = 0.13- 0.71) and HCC occurrence on LC (OR = 1.04-1.23, P = 0.89-0.60) of heterozygotes and homozygotes for G allele in referent comparison to homozygotes for common allele (C/C genotype), and time interval to HCC (RH = 0.67-1.00; P = 0.14-0.99). In conclusion, there appeared to be no significant associations between IL6 promoter variants and disease outcome in chronic hepatitis B."
"Amyloid angiopathy is characterized by amyloid beta-peptide (A beta) deposition and may contribute to the cerebrovascular abnormalities that precede the onset of Alzheimer's Disease (AD). That aberrant potassium (K+) channel function occurs in AD patients is supported by deleterious effects of A beta on normal fibroblast K+ channels and prevention of A beta-induced toxicity by potassium channel openers (KCOs) in neuronal cell culture. We report here that KCOs protect cerebral and peripheral vessels against the endothelial damage induced by A beta. Pressurized posterior cerebral artery and aortic ring segments from the rat were constricted and then relaxed with the endothelium-dependent vasodilator acetylcholine before and after incubation with A beta (10(-6) M), or pre-treatment with KCOs before the addition of beta-amyloid. Vessels treated with A beta exhibited features of endothelial dysfunction: enhanced vasoconstriction and diminished endothelium-dependent vasodilation. Pre-treatment with KCOs significantly antagonized the A beta effect in both cerebral and aortic vessel segments. This protection was provided by both KCa and KATP channel openers. Endothelial damage by A beta and protection by KCOs was verified by electron microscopy. The K+ channel blocker, TEA, reversed the protective effect of KCO. The results suggest that potassium channel openers protect against A beta induced endothelial dysfunction and that KCOs may have a role in the treatment of degenerative cerebrovascular disease as seen in stroke, AD and aging."
"BACKGROUND: Inhaled nitric oxide (INO) reduces extracorporeal membrane oxygenation (ECMO) use in term and near-term neonates with persistent pulmonary hypertension of the newborn; however, its overutilization is increasing. We hypothesized that implementing a shared baseline protocol would safely improve evidence-based INO use in a Level IV neonatal ICU.METHODS: Through several plan-do-study-act cycles, a shared baseline protocol for initiation and weaning of INO was developed and implemented starting in August 2014. Based on user feedback, the shared baseline protocol was amended and re-evaluated at regular intervals. Significant changes for process and outcome measures related to utilization of INO were detected using statistical process control, bivariate analyses using t test or nonparametric Wilcoxon rank-sum test as appropriate, and chi-square and Fisher exact testing as appropriate. Comparisons between the pre-plan-do-study-act group (January 2012 to July 2014) and post-plan-do-study-act group (August 2014 to October 2015) were made.RESULTS: One hundred sixteen INO courses in 95 subjects were administered during the pre-plan-do-study-act period, and 44 episodes were initiated in 39 subjects during the post-plan-do-study-act period. Process control charts demonstrate significant reductions in the percentage of INO doses > 20 ppm and the percentage of prolonged (>4-d) INO courses. Prolonged INO courses decreased from 67.9 to 40% (P = .032), whereas the median duration of INO per course decreased from 8 to 4 d (P < .001). The percentage of INO courses that exceeded the dose of 20 ppm decreased from 18.1 to 2.3% (P = .009). Very delayed INO weaning (weaning at FIO2 ? 0.40) decreased from 41.9 to 21.2% (P = .038). There were no differences in the percentage of INO courses administered to non-sedated subjects or the percentage of INO courses administered to preterm infants. There was no difference for death or ECMO between groups.CONCLUSIONS: Implementation of a shared baseline protocol to encourage appropriate INO initiation and weaning safely decreased INO exposures. Focused efforts on reducing unapproved INO use in preterm infants are warranted."
"Neuromuscular/neurodegenerative disorders, such as the death of spinal cord motor neurons in amyotrophic lateral sclerosis (ALS) or the degeneration of spinal cord motor neuron axons in certain peripheral neuropathies, present a unique opportunity for therapeutic intervention with neurotrophic proteins. We have found that in mixed rat embryonic spinal cord cultures or in purified motor neuron preparations, recombinant human insulin-like growth factor 1 (rhIGF-1) enhances the survival of motor neurons at EC50 concentrations of 2 nM, consistent with an interaction at the tyrosine kinase-coupled rhIGF-1 receptor. In a model of programmed cell death in ovo, administration of rhIGF-1 produces a marked survival of motor neurons. In a variety of models of predominantly motor neuron or nerve injury in rodents, administration of rhIGF-1 prevents the death of motor neurons in neonatal facial nerve lesions, attenuates the loss of cholinergic phenotype in adult hypoglossal nerve axotomy and hastens recovery from sciatic nerve crush in mice. In a genetic model of motor neuron compromise, the wobbler mouse, rhIGF-1 (1 mg/kg s.c. daily) delayed the deterioration of grip strength and provided for a more normal distribution of fibre types. In addition, rhIGF-1 (0.3-1.0 mg/kg s.c. daily) prevents the motor and/or sensory neuropathy in rodents caused by vincristine, cisplatinum or Taxol. These combined data indicate that rhIGF-1 has marked effects on the survival of compromised motor neurons and the maintenance of their axons and functional connections. They also suggest the potential utility of rhIGF-1 for the treatment of diseases such as ALS and certain neuropathies."
"AIMS/HYPOTHESIS: Genetic and environmental factors are believed to cause type 1 diabetes. The aim of this study was to investigate the influence of maternal BMI and gestational weight gain on the subsequent risk of childhood type 1 diabetes.METHODS: Children in the Swedish National Quality Register for Diabetes in Children were matched with control children from the Swedish Medical Birth Register. Children were included whose mothers had data available on BMI in early pregnancy and gestational weight gain, giving a total of 16,179 individuals: 3231 children with type 1 diabetes and 12,948 control children.RESULTS: Mothers of children with type 1 diabetes were more likely to be obese (9% [n = 292/3231] vs 7.7% [n = 991/12,948]; p = 0.02) and/or have diabetes themselves (2.8% [n = 90/3231] vs 0.8% [n = 108/12,948]; p < 0.001) compared with mothers of control children. Gestational weight gain did not differ significantly between the two groups of mothers. In mothers without diabetes, maternal obesity was a significant risk factor for type 1 diabetes in the offspring (p = 0.04). A child had an increased risk of developing type 1 diabetes if the mother had been obese in early pregnancy (crude OR 1.20; 95% CI 1.05, 1.38; adjusted OR 1.18; 95% CI 1.02, 1.36). Among children with type 1 diabetes (n = 3231) there was a difference (p < 0.001) in age at onset in relation to the mother's BMI. Among children in the oldest age group (15-19 years), there were more mothers who had been underweight during pregnancy, while in the youngest age group (0-4 years) the pattern was reversed.CONCLUSIONS/INTERPRETATION: Maternal obesity, in the absence of maternal diabetes, is a risk factor for type 1 diabetes in the offspring, and influences the age of onset of type 1 diabetes. This emphasises the importance of a normal maternal BMI to potentially decrease the incidence of type 1 diabetes."
"BACKGROUND: Evaluation of the efficacy and safety of a new 7F-atherectomy device (30-day endpoint) for the treatment of short and mid-length arterial lesions with a reference diameter of 2.5-7 mm.MATERIAL AND METHODS: Fifty-eight femoto-popliteal stenoses in 46 patients (67% male, mean age 66 +/- 9 years) with chronic peripheral occlusive disease of the lower limbs [Rutherford stage 2: n = 13 (28%); stage 3: n = 29 (63%), stage 4: 2 (4%), stage 5: n = 2 (4%)], were treated with directional atherectomy. Target lesion characteristics: Common femoral artery: n = 1 (2%), superficial femoral artery: n = 47 (81%); popliteal artery, n = 10 (17%); in stent n = 3 (5 %). Thirty (65 %) of the interventions were performed using an antegrade approach, 16 (35%) interventions in cross-over technique. Mean degree of stenosis was 83 +/- 11 mm, mean length of lesion was 37 +/- 37 mm.RESULTS: 6.5 +/- 2 (4-10) passes of the lesion were performed with the catheter. Three lesions were treated after predilatation, 55 (95%) interventions as primary atherectomy. In 31/58 lesions (53%) additional balloon angioplasty was performed, in 1 lesion (2%) additional stent placement was needed. The mean degree of stenosis after atherectomy was reduced to 29 +/- 20% (0-60%) after additional balloon angioplasty, it was 11 +/- 10% (0-30 %). A residual stenosis of < 50% after plain atherectomy was achieved in 55 (95%) lesions, of < 30% in 49 (84%).COMPLICATIONS: 3 (6.5%) cases of embolism of debris were detected and treated successfully by aspiration. The mean ankle-brachial index increased from 0.62 +/- 0.12 to 0.92 +/- 0.36 before discharge, and to 0.86 +/- 0.17 after 30 days. Rutherford stage after 30 days: stage 0: n = 038 (83%); Stage 1: n = 4 (8%); Stage 2: n = 3 (6%); Stage 5: n = 1 (2%).CONCLUSION: Lesions up to 8 cm in length of the femoropopliteal arteries can be treated successfully in most cases with the new atherectomy catheter. Embolism, the only complication that occurred, can be avoided by cleaning the nose cone after at least 4 passes of the lesion."
"Lower extremity fractures (LEFx) and pelvic fractures (PFx) are believed to increase the risk of lower extremity deep vein thrombosis (LEDVT). We studied trauma patients at high risk for LEDVT to determine whether an increased incidence of LEDVT was associated with LEFx and/or PFx. From January 1995 through December 1997 4163 trauma patients were admitted to our Level I trauma center. One thousand ninety-three patients at high risk for LEDVT were screened with serial lower extremity venous duplex ultrasound. Their medical records were retrospectively reviewed for demographics, mechanism of injury, and fracture data. The occurrence of LEDVT, pulmonary embolus, and LEDVT prophylaxis and treatment were noted. The incidence of LEDVT in the fracture group (Fx) was compared with that in the nonfracture group (NFx) using chi-square analysis and logistic regression. Statistical significance was set at < or = 0.05. Complete data were available for 1059 of 1093 patients. Five hundred sixty-nine (53.73%) patients had PFx and/or LEFx, 151 (14.26%) patients had PFx only, 317 (29.3%) patients had LEFx only, and 101 (9.54%) patients had both PFx and LEFx. Four hundred ninety (46.27%) patients had NFx. In 1059 patients LEDVT was detected in 125 (11.8%). Sixty-three patients in the Fx groups developed LEDVT (50.4%): 19 (15.2%) PFx patients, 15 (12.0%) PFx/LEFx patients, and 29 (23.2%) LEFx patients. Sixty-two (49.6%) NFx patients developed LEDVT. LEDVT incidence was not significantly different between the Fx and NFx groups or among the PFx, LEFx, and PFx/LEFx groups (P = 0.317). Nine patients developed pulmonary embolism: four NFx patients, two LEFx patients, two PFx patients, and one PFx/LEFx patient. Significant predictors of LEDVT were age and hospital length of stay. Mean age in patients with LEDVT was 47.58 years and in patients without LEDVT it was 40.89 years (P < 0.001). Mean hospital length of stay in patients with LEDVT was 29.81 days and in patients without LEDVT it was 16.84 days. The power of this study to detect differences representing medium effect sizes was greater than 90 per cent. We conclude that LEFx and/or PFx was not associated with an increased incidence of LEDVT in trauma patients at high risk for LEDVT. Lower extremity venous duplex ultrasound needs to be performed in both Fx and NFx groups to detect LEDVTs."
"INTRODUCTION: Immune dysfunction, promoted by pro-inflammatory cytokines, plays a pivotal role in neurodegeneration associated with Huntington's disease.AIMS: The aim of this study was to investigate the emerging immunoregulatory and antiinflammatory properties of Sertoli cells in Huntington's disease.METHODS: The experimental R6/2 mouse model of Huntington's disease was treated by a single intraperitoneal injection of microencapsulated prepubertal porcine Sertoli cells and lifespan, motor performance and striatal inflammatory pattern have been evaluated.RESULTS: The results of this study demonstrated that a single intraperitoneal injection of microencapsulated prepubertal porcine Sertoli cells uniquely improved performances and extended the life expectancy of R6/2 Huntington's disease mice, by immune dysfunction modulation in brain.CONCLUSIONS: This study highlights the immunomodulatory and trophic role of Sertoli cells that could be of help in the treatment of neurodegenerative disorders."
"Collagen-, arachidonate- and ADP-stimulated platelet thromboxane B2 (TXB2) formation was studied in platelet-rich plasma (PRP) of 14 alcoholics, 7 of whom had a biopsy-verified alcoholic fatty liver. On admission for detoxication, the alcoholics showed decreased platelet count and aggregability (p less than 0.001) as compared to nonalcoholic healthy controls. Platelet TXB2 formation was decreased (p less than 0.01), if PRP was stimulated by arachidonate, but not if it was stimulated by ADP or collagen. In contrast, 9-14 days after ethanol withdrawal platelet TXB2 formation had increased to markedly higher levels than those seen in nonalcoholic controls (p less than 0.01), if PRP was stimulated by ADP, but not if it was stimulated by arachidonate or collagen. Skin bleeding time was found to be prolonged (p less than 0.05) on admission in alcoholics having fatty liver, but it normalized within 2 weeks after ethanol withdrawal. We conclude that the effect of ethanol withdrawal in alcoholics on platelet TXB2 formation is influenced by platelet count, aggregability and the agonist used to induce platelet aggregation."
"The vertebrate brain is regionalized during development into forebrain, midbrain and hindbrain. Fibroblast growth factor 8 (FGF8) is expressed in the midbrain/hindbrain boundary (MHB) and functions as an organizer molecule. Previous studies demonstrated that the brain of basal chordates or ascidians is also regionalized at least into fore/midbrain and hindbrain. To better understand the ascidian brain regionalization, the expression of the Ciona Fgf8/17/18 gene was compared with the expression of Otx, En and Pax2/5/8 genes. The expression pattern of these genes resembled that of the genes in the vertebrate forebrain, midbrain, MHB and hindbrain, each of those domains being characterized by sole or combined expression of Otx, Pax2/5/8, En and Fgf8/17/18. In addition, the putative forebrain and midbrain expressed Ci-FgfL and Ci-Fgf9/16/20, respectively. Therefore, the regionalization of the ascidian larval central nervous system was also marked by the expression of Fgf genes."
"Many complicated postoperative vitreoretinal cases require reoperations. It is possible to perform some of these procedures outside the operating room. We describe examining room techniques for fluid-gas exchange, fluid and gas aspiration, and adherent vitreous strand removal. We also discuss a slit-lamp technique for using sodium hyaluronate (Healon) following fluid-gas exchange to eliminate optical distortion from endothelial striae."
"How the brain maintains an accurate and stable representation of visual target locations despite the occurrence of saccadic gaze shifts is a classical problem in oculomotor research. Here we test and dissociate the predictions of different conceptual models for head-unrestrained gaze-localization behavior of macaque monkeys. We adopted the double-step paradigm with rapid eye-head gaze shifts to measure localization accuracy in response to flashed visual stimuli in darkness. We presented the second target flash either before (static), or during (dynamic) the first gaze displacement. In the dynamic case the brief visual flash induced a small retinal streak of up to about 20 deg at an unpredictable moment and retinal location during the eye-head gaze shift, which provides serious challenges for the gaze-control system. However, for both stimulus conditions, monkeys localized the flashed targets with accurate gaze shifts, which rules out several models of visuomotor control. First, these findings exclude the possibility that gaze-shift programming relies on retinal inputs only. Instead, they support the notion that accurate eye-head motor feedback updates the gaze-saccade coordinates. Second, in dynamic trials the visuomotor system cannot rely on the coordinates of the planned first eye-head saccade either, which rules out remapping on the basis of a predictive corollary gaze-displacement signal. Finally, because gaze-related head movements were also goal-directed, requiring continuous access to eye-in-head position, we propose that our results best support a dynamic feedback scheme for spatial updating in which visuomotor control incorporates accurate signals about instantaneous eye- and head positions rather than relative eye- and head displacements."
"Image reconstruction in electrical impedance tomography (EIT) is a highly ill-posed, non-linear inverse problem. The modified Newton-Raphson (MNR) iteration algorithm is deduced from the strictest theoretic analysis. It is an optimization algorithm based on minimizing the object function. The MNR algorithm with regularization technique is usually not stable, due to the serious image reconstruction model error and measurement noise. So the reconstruction precision is not high when used in static EIT. A new static image reconstruction method for EIT based on genetic algorithm (GA-EIT) is proposed in this paper. The experimental results indicate that the performance (including stability, the precision and space resolution in reconstructing the static EIT image) of the GA-EIT algorithm is better than that of the MNR algorithm."
"BACKGROUND & AIMS: Prostaglandins are synthesized by cyclooxygenases (COX)-1 and -2. The expression and cellular localization of COX-1 and COX-2 in normal human colon and inflammatory bowel disease (IBD) surgical resections were studied.METHODS: COX-1 and COX-2 protein expression and cellular localization were assessed by Western blotting and immunohistochemistry.RESULTS: COX-1 protein was expressed at equal levels in normal, Crohn's disease, and ulcerative colitis colonic epithelial cells. COX-2 protein was not detected in normal epithelial cells but was detected in Crohn's disease and ulcerative colitis epithelial cells. Immunohistochemistry of normal, Crohn's colitis, and ulcerative colitis tissue showed equivalent COX-1 expression in epithelial cells in the lower half of the colonic crypts. COX-2 expression was absent from normal colon, whereas in Crohn's colitis and ulcerative colitis, COX-2 was observed in apical epithelial cells and in lamina propria mononuclear cells. In Crohn's ileitis, COX-2 was present in the villus epithelial cells. In ulcerative colitis, colonic epithelial cells expressing COX-2 also expressed inducible nitric oxide synthase.CONCLUSIONS: COX-1 was localized in the crypt epithelium of the normal ileum and colon, and its expression was unchanged in IBD. COX-2 was undetectable in normal ileum or colon, but it was induced in apical epithelial cells of inflamed foci in IBD."
"BACKGROUND: Low-energy-density foods with high satiating power may be useful tools for weight management. Energy density of yogurts can range from 0.4 to 1.8 kcal/g.OBJECTIVE: To test the effects of added inulin, a soluble fiber, on the satiating properties of low-energy-density and high-energy-density yogurt beverages (16 oz or 472 mL).DESIGN: The study followed a within-subject preload design with repeated measures. Each participant completed six conditions, presented in a counterbalanced order.SUBJECTS: Participants were 18 men and 20 women, aged 18 to 35 years.INTERVENTION: The experimental conditions were two high-energy-density yogurt beverages (440 kcal; 0.9 kcal/g) and two low-energy-density yogurt beverages (180 kcal; 0.4 kcal/g) with or without inulin (6 g) and an equal volume of orange juice (180 kcal). A no beverage control condition was used as well.MAIN OUTCOME MEASURES: Repeated ratings of hunger, fullness, and desire to eat and energy consumption at the lunch meal served 120 minutes post-ingestion were the main measures.STATISTICAL ANALYSES PERFORMED: Repeated measures analyses of variance were used to analyze motivational ratings and energy and nutrient intakes at the test meal.RESULTS: Yogurt beverages and liquid orange juice significantly suppressed appetite and promoted satiety relatively to the no beverage condition. Yogurt beverages had greater satiating power than did orange juice, as evidenced by higher satiety ratings and reduced energy intakes at lunch. The satiating power of low-energy-density yogurt with inulin was comparable to that of high-energy-density yogurt.CONCLUSIONS: Energy presented in liquid form can have satiating power. Added fiber can potentiate the satiating properties of low-energy-density liquid yogurts. Adding fiber to low-energy-density foods may be an effective way to suppress appetite and control food intake."
"It is unclear whether health risk assessment (HRA) or claims-based risk modeling is a superior indicator of the need for case management in Medicaid adults with disabilities. This is a prospective cohort study designed to compare the use of a claims-based Predictive Risk Report (PRR) to a HRA in a Medicaid Supplemental Security Income managed care population. Both the claims-based risk scores and HRAs proved to be significant predictors of case management placement and subsequent emergency department and hospital utilization. The PRR-derived risk scores, however, could be obtained on virtually all enrollees at the time of enrollment, while HRA scores were obtained on only 54% of enrollees by 210 days of enrollment. Furthermore, case management reduced the risk of emergency and hospital utilization. We conclude that the PRR and HRA are equally reliable predictors of need for case management. The PRR has the advantage of earlier availability and of being easier to obtain."
"Amphibian oocyte nucleoli are a particular suited object for research on nucleolar chromatin organization. By selective rDNA amplification each pachytene oocyte nucleus accumulates 30 pg of extrachromosomal rDNA, this amount corresponds to 2 million rDNA copies. Following the selective amplification stage, the amplified gene copies are finally distributed within more than thousand extrachromosomal nucleoli per individual oocyte nucleus. The aim of the present study was first to obtain a precise documentation of the fate of amplified rDNA during early Xenopus oogenesis until the final functional integration of these copies into individual oocyte nucleoli, and, second, a close correlation of the structural data with determination of rDNA transcription rates by S1 transcript analysis for the subsequent stages of oocyte differentiation. In order to investigate the structural complexity of the intranuclear rDNA translocation process in detail, a confocal laser scan microscope (CLSM) was used, equipped with an external UV-laser. This instrumentation unambiguously allowed (i) the detection of small clusters of rDNA copies and (ii) the precise spatial documentation of the intranuclear position of rDNA clusters in relation to the protein-free pre-nucleolar protein bodies, a specific characteristic of late pachytene/early diplotene amphibian oocyte nuclei. Our results indicate that the major rDNA translocation processes, e.g. the association of rDNA clusters with pre-nucleolar protein bodies, the formation of ribbon-like pre-nucleolar units sensu Van Gansen and Schramm (J. Cell Sci. 10: 339-367, 1972), and, finally, the translocation of fused rDNA units into the interior of pre-nucleolar protein bodies, occur--for the most part--in absence of massive rDNA transcription. As shown by the S1 transcript analysis, the onset of massive rDNA transcription starts concomitantly with an unraveling of the densely packed rDNA clusters into finely dispersed rDNA units, which were shown by CLSM analysis to be distributed throughout the entire nucleolar volume."
"OBJECTIVE: To investigate the epidemiological and molecular typing features of the pathogenic Yersinia enterocolitica strains isolated in China,using pulsed field gel electrophoresis(PFGE) and standardized PFGE method as well as typing database of Yersinia enterocolitica.METHODS: PFGE analysis was performed as Laboratory Directions for molecular subtyping of Salmonella by PFGE (PulseNet,USA) with some modifications and the results of PFGE were analyzed by BioNumerics soft (Version 4.0, Applied Maths BVBA, Belium).RESULTS: 114 O:3 Yersinia enterocolitica strains were typed by 25 patterns to have found that K6GN11C30012 (50 strains), K6GN11C30015(19 strains) and K6GN11C30016(10 strains) were the major patterns. K6GNllC30012 had 92.2% cluster similarity with K6GN11C30009-K6GN11C30023. This clone included 91.23% strains of 114 0:3 Yersinia enterocolitica strains. 51 0:9 Yersinia enterocolitica strains were typed by 14 patterns; K6GN11C90004 (22 strains) and K6GN11C90010 (13 strains)were the major patterns. K6GN11C90004 had 81.8% cluster similarity with K6GN11C90010 patterns. The major patterns of 0:3 and 0:9 serotypes were quite different.CONCLUSION: O:3 Yersinia enterocolitica strains might originate from the same clone and had very few variation in different years and provinces but O:9 Yersinia enterocolitica strains from two different clones with some changes."
"Individual variations in anatomic cerebral asymmetries have been linked with specific neurodevelopmental processes, with patterns of cognitive ability, and with recovery from focal brain damage. The present study investigated relationships between cerebral asymmetries and recovery from aphasia. Aphasic patients (N = 25) were assessed for language recovery for 1 year poststroke, and linear measurements of cerebral asymmetries were performed on CT scans. Increasing left occipital width asymmetry was associated with faster rate of language recovery and with higher final language scores during the first year poststroke. There was, moreover, a tendency for increasing left occipital width asymmetry to be associated with less initial impairment. It is hypothesized that those aspects of neural organization conferring better premorbid language skills are the same factors conferring greater recovery of language skills and that occipital width asymmetry serves as a marker for such individual differences in neural organization."
"The ultrastructure of ganglion cells and centrifugal fibers of the larval lamprey retinas were studied using horseradish peroxidase (HRP) as a marker. Larval ganglion cells were found both in the inner nuclear layer and the inner plexiform layer of the differentiated retina, and also were present in the undifferentiated retina. Direct photoreceptor-ganglion cell contacts and the presence of centrifugal fibers are described for the first time in the lamprey. The centrifugal fibers contact directly with ganglion cells in this species."
"BACKGROUND: Depression is the leading global cause of disability and often begins in adolescence. The genetic architecture and treatment response profiles for adults and adolescents differ even though identical criteria are used to diagnose depression across different age groups. There is no clear consensus on how these groups differ in their symptom profiles.METHODS: Using data from a two-generation family study, we compared the presentation of DSM-IV depressive symptoms in adolescents and adults with MDD (Major Depressive Disorder). We also compared DSM-IV depressive symptom counts using latent class analysis.RESULTS: Vegetative symptoms (appetite and weight change, loss of energy and insomnia) were more common in adolescent MDD than adult MDD. Anhedonia/loss of interest and concentration problems were more common in adults with MDD. When using latent class analysis to look at depressive symptoms, a vegetative symptom profile was also seen in adolescent depression only.LIMITATIONS: Adults and adolescents were recruited in different ways. Adolescent cases were more likely to be first-onset while adult cases were recurrences. It was not possible to examine how recurrence affected adolescent depression symptom profiles.CONCLUSION: Differences in how depression presents in adolescents and adults may be consistent with different pathophysiological mechanisms. For adolescents, we found that vegetative/physical disturbances were common (loss of energy, changes in weight, appetite and sleep changes). For adults, anhedonia/loss of interest and concentration difficulties were more common."
This paper demonstrates preliminary image processing with the aim of obtaining invariant signs for identification by colour. For this purpose we have used microscopic images of cell structures in coloured peripheral blood smears. The main parameter for the identification is the colouring of the respective cell structures on the basis of which we have created histograms by hue for the available cell types.
"A 20-year-old man with a posterior mediastinal tumor incidentally found on a chest X-ray was referred to our hospital. Chest computed tomography showed a 3 cm nodule located on the left side of the 10-11th thoracic vertebra, where the artery of Adamkiewicz is presumed to arise. He underwent left thoracotomy to remove the lesion. The tumor was safely resected with the assistance of intraoperative motor evoked potential(MEP) monitoring. The postoperative diagnosis was a benign schwannoma. In thoracic surgery for posterior mediastinal tumors, intraoperative MEP monitoring is useful for preventing paraplegia."
"BACKGROUND AND OBJECTIVE: To show that sclerostomy, a glaucoma filtering surgery, can be performed using an Er:YAG laser. Scarring at the filtering site, a recurrent problem, may be reduced through proper positioning of the sclerostomy by using an intraocular endoscope.MATERIALS AND METHODS: Ab interno full-thickness sclerostomies were performed on eye bank eyes with an Er:YAG laser through a custom made optical delivery system. The intraocular laser probe consisted of a low OH silica fiber inserted in a metallic tapered sheathing. A rigid intraocular endoscope based on gradient-index lenses allowed visualization of the filtration site.RESULTS: A clear view of the anterior chamber angle was obtained through the endoscope, allowing for precise location of the sclerostomy. Full-thickness sclerostomies could then be performed at the desired location. Histologic sections showed thermal necrosis less than 50 microm thick in tissue adjacent to the sclerostomy.CONCLUSIONS: A sclerostomy performed with a combined procedure using an Er:YAG laser and intraocular endoscopy increases the speed of the procedure. The use of a high-resolution intraocular endoscope may increase the success rate of ab interno laser glaucoma surgeries."
"Sixteen new 2-(2-phenylethyl)chromone dimers, including four pairs of enantiomers (1a/1b, 3a/3b, 6a/6b, and 8a/8b), along with eight optically pure analogues (2, 4, 5, 7, and 9-12) were isolated from the resinous wood of Aquilaria sinensis. Their structures were determined by extensive spectroscopic analysis (1D and 2D NMR, UV, IR, and HRMS) and experimental and computed ECD data. Compounds 1-10 feature an unusual 3,4-dihydro-2 H-pyran ring linkage connecting two 2-(2-phenylethyl)chromone monomeric units, while compounds 11 and 12 possess an unprecedented 6,7-dihydro-5 H-1,4-dioxepine moiety in their structures. A putative biosynthetic pathway of the representative structures via a diepoxy derivative of a chromone with a nonoxygenated A-ring is also proposed. Compounds 1a/1b, 2, 3a/3b, 5, 7, 8a/8b, and 10-12 exhibited significant inhibition of nitric oxide production in lipopolysaccharide-stimulated RAW264.7 cells with IC50 values in the range 7.0-12.0 ìM."
"A case-control study was conducted in Belgrade (about 320,000 inhabitants 0-16 years old) during the period 1994-97, comprising 68 diabetic children (cases) and 68 controls chosen from the siblings of the cases. Analysis using multivariable logistic regression analysis indicated the following independent risk factors for Type 1 diabetes: higher birth order, infections during the 6 months preceding the onset of the disease and stressful events. Out of individual stressful and psychological factors, 'other' stressful events (severe accident or hospitalisation or death of a close friend, conflict with a teacher, death of a pet, failure in competition, quarrel between parents, punishment, physical attack, war in republics of former Yugoslavia and near drowning in the pool) and learning problems were independent risk factor for Type 1 diabetes. The results obtained in this study of siblings supports the hypothesis that environmental factors play a role in the development of Type 1 diabetes."
"The quantity and distribution of transferrin and transferrin-binding sites in the placenta were investigated in rabbits on the 28th-29th days of pregnancy. The animals were injected intravenously with a mixture of 59Fe-125I-labelled rabbit diferric transferrin and 131I-labelled rabbit albumin. The binding of transferrin to placentas removed 3-75 min later was determined by using the 131I-labelled albumin values to correct for tissue content of plasma. Mean values for transferrin binding of 1460 and 560 micrograms/g tissue were obtained 3-15 and 45-75 min after injection, respectively. Gel filtration of placental extracts prepared with the non-ionic detergent, Teric 12A9, showed that the 125I-labelled transferrin bound to a large molecular weight component which had the properties of a specific receptor. The receptor had a higher affinity for diferric transferrin than for apotransferrin. The subcellular distribution of transferrin binding sites was determined by differential centrifugation of placental homogenates and by electron microscope autoradiography. The results with the former method indicated that the transferrin was bound to the microsomal fraction of the cells. Autoradiography showed that the majority of the transferrin molecules were at intracellular sites, mainly on the membrane of intracellular vesicles. It is concluded that iron-containing transferrin molecules enter the trophoblast cells by endocytosis or via a canalicular system after binding to cell membrane receptors. The higher affinity of the receptors for diferric transferrin than for apotransferrin explains the difference in amount of transferrin binding found within 15 min of injecting labelled diferric transferrin and that found 45-75 min later when much of the iron had been removed from the transferrin."
"This contribution to the centennial commemorative issue of the American Journal of Physiology: Gastrointestinal and Liver Physiology identifies some of the important studies of spontaneous electrical and motor activity in the gastrointestinal tract published in the Journal between 1898 and 1996. Emphasis is given to the contributions made by Walter B. Cannon, Walter C. Alvarez, Emil Bozler, C. Ladd Prosser, and James Christensen."
"Apparent values of Km and Vmax have been measured for catalysis of hydrolysis of unsonicated egg lecithin liposomes, activated through addition of 0.4 M n-hexanol, by phospholipases A2 from bee and snake venoms and by phospholipase C from Clostridium welchii as a function of the concentration of three surfactants: hexadecylamine, hexadecyltrimethylammonium bromide, and dihexadecyl phosphate. For all three enzymes, values of Km and Vmax show little or no dependence on the concentration of these ionic surfactants, demonstrating that the liposomal surface charge is not a crucial factor in determining susceptibility to phospholipase-catalyzed hydrolysis."
"HLA-DQA1 typing of the 4AOHW cell panel is presented using a novel strategy that exploits both intron and exon polymorphisms. Intron sequences adjacent to the variable HLA-DQA1 second exon exhibit stable polymorphisms that are specific for locus alleles and certain suballelic DR/DQ haplotypes. A PCR-RFLP method has been developed that is based on amplification of a 780-bp segment extending from intron 1 through exon 2 to intron 2. Stable sequence polymorphisms provide restriction enzyme sites and confer mobility variations detected on polyacrylamide minigel electrophoresis. Direct band comparison of amplified products and restriction fragments with known standards facilitates pattern comparison, obviating the requirement for accurate molecular weight determination. This method, using only two enzymes, identifies a total of 11 allelic and suballelic groups, including all eight DQA1 alleles encoded at the second exon."
"It has been studied the correlation of the mitotic activity of the chromosome aberrations and apoptosis, in the V-79 cells pre-exposure to an adapting dose of ionizing radiation from 14C-thymidine prior to an acute challenge dose of gamma-rays. In spite of that the incubation of the cells with isotope increased of the yield of the chromosome aberrations, but the cells became more resistant to following gamma-irradiation. Increasing the adaptive dose of the 14C on degree didn't influence on the present of the adaptive response. However, using concentrations of the 14C damaged metaphase/anaphase transition and cells blocked in this check-point by apoptotic death. The results suggest, that the cellular selection has been involved in 14C-induced adaptive response, estimated by level of asymmetric chromosome aberrations in V-79 cells."
"The influenza A M2 protein is an acid-activated proton channel responsible for acidification of the inside of the virus, a critical step in the viral life cycle. This channel has four central histidine residues that form an acid-activated gate, binding protons from the outside until an activated state allows proton transport to the inside. While previous work has focused on proton transport through the channel, the structural and dynamic changes that accompany proton flux and enable activation have yet to be resolved. In this study, extensive Multiscale Reactive Molecular Dynamics simulations with explicit Grotthuss-shuttling hydrated excess protons are used to explore detailed molecular-level interactions that accompany proton transport in the +0, + 1, and +2 histidine charge states. The results demonstrate how the hydrated excess proton strongly influences both the protein and water hydrogen-bonding network throughout the channel, providing further insight into the channel's acid-activation mechanism and rectification behavior. We find that the excess proton dynamically, as a function of location, shifts the protein structure away from its equilibrium distributions uniquely for different pH conditions consistent with acid-activation. The proton distribution in the xy-plane is also shown to be asymmetric about the channel's main axis, which has potentially important implications for the mechanism of proton conduction and future drug design efforts."
"Comparison of two group I intron sequences in the nucleolar genome of the myxomycete Physarum flavicomum to their homologs in the closely related Physarum polycephalum revealed insertion-like elements. One of the insertion-like elements consists of two repetitive sequence motifs of 11 and 101 bp in five and three copies, respectively. The smaller motif, which flanks the larger, resembles a target duplication and indicates a relationship to transposons or retroelements. The insertion-like elements are found in the peripheral loops of the RNA structure; the positions occupied by the ORFs of mobile nucleolar group I introns. The P. flavicomum introns are 1184 and 637 bp in size, located in the large subunit ribosomal RNA gene, and can be folded into group I intron structures at the RNA level. However, the intron 2s from both P. flavicomum and P. polycephalum contain an unusual core region that lacks the P8 segment. None of the introns are able to self-splice in vitro. Southern analysis of different isolates indicates that the introns are not optional in myxomycetes."
"The authors describe a clinical course that incorporates practical experience in continuous quality improvement, case management, managed care, and healthcare systems analysis. Increasingly, nurses need to know and be able to use these concepts; therefore, they were added to the nursing curriculum. Although nursing students are able to articulate leadership and management theories, their ability to apply them is limited by lack of experience. This course is designed to provide a practical experience base that students may draw from upon entry into practice."
"Methyl glyoxal (MG), a highly reactive dicarbonyl metabolite, causes a range of changes within the cell. It forms adducts with DNA and protein and contributes to the progression of several diseases as well as causing hepatic damage. In this study, we have used human hepatoma (HepG2) cells as a model to investigate the induction of protective enzymes in response to MG exposure. We have shown that treating HepG2 cells with sub-lethal concentrations of MG increases the level of NADPH:quinone oxidoreductase (NQO1) mRNA by 4.5-fold, AKR1C3 mRNA by 14-fold and AKR7A2 mRNA by 4-fold. Levels of AKR7A2 protein are increased by 2.1- and 1.8-fold following 9h and 24h exposure of cells to 50 ìM MG. The role of AKR7A2 in protecting HepG2 cells against MG toxicity was further investigated using specific siRNAs against AKR7A2 and Nrf2. Knockdown of AKR7A2 in HepG2 shows that AKR7A2 is responsible for up to 50% of the protection against MG toxicity in HepG2 cells. We have also shown that MG was able to induce the translocation of the transcription factor Nrf2 to the nucleus. HepG2 cells in which Nrf2 had been knocked down exhibited decreased NQO1 and AKR7A2 mRNA levels compared to control cells. In conclusion, these findings indicate that protective enzymes are significantly up-regulated in response to low concentrations of MG in HepG2 cells and that AKR7A2 contributes to protection against MG-induced toxicity. Nrf2 is critical in mediating MG induced expression of protective genes."
"Patterns of VOC and BTEX (Benzene, Toluene, Ethylvenzene, and Xylene) distribution at industrial emission sources, proximal residential areas of industrial estates, and ambient air were studied in Daegu, Korea. Daytime and night-time sampling was done at 12 sites and 9 emission sources to provide samples for analyses, using the TO-14 method. Measured BTEX component ratios B/T, T/EB, T/X and EB/X in ambient air were found to be 2.6 g, 11.3 g, 1.0 g and 1.2 g in the residential area; 2.2 g, 11.0 g, 1.0 g and 1.6 g in the commercial area; and 1.0 g, 14.9 g, 1.0 g and 1.3 g in the industrial area. The significant difference observed between the ratios for the residential and commercial areas implies that the two areas have different emission sources. This is also indicated by the significant differences observed between daytime and nighttime BTEX concentrations. Toluene and xylene were detected at very high concentrations, at the sampling sites. This pattern reflects the type of industrial processes and materials that are managed at the emission sources, as well as topographic/climatic factors that impact upon pollutant transport processes in the atmosphere. The BTEX distribution pattern in Daegu is observed to be similar to that of several Asian cities, particularly Hong Kong. These results are useful in the design of emission source control measures for VOCs and BTEX in Daegu."
"The microphthalmia with linear skin defects syndrome (MLS) is an X-linked dominant disorder with male lethality. In the majority of the patients reported, the MLS syndrome is caused by segmental monosomy of the Xp22.3 region. To date, five male patients with MLS and 46,XX karyotype (""XX males"") have been described. Here we report on the first male case with MLS and an XY complement. The patient showed agenesis of the corpus callosum, histiocytoid cardiomyopathy, and lactic acidosis but no microphthalmia, and carried a mosaic subtle inversion of the short arm of the X chromosome in 15% of his peripheral blood lymphocytes, 46,Y,inv(X)(p22.13 approximately 22.2p22.32 approximately 22.33)[49]/46,XY[271]. By fluorescence IN SITU hybridization (FISH), we showed that YAC 225H10 spans the breakpoint in Xp22.3. End-sequencing and database analysis revealed a YAC insert of at least 416 kb containing the genes HCCS and AMELX, and exons 2-16 of ARHGAP6. Molecular cytogenetic data suggest that the Xp22.3 inversion breakpoint is located in intron 1 of ARHGAP6, the gene encoding the Rho GTPase activating protein 6. Future molecular studies in karyotypically normal female MLS patients to detect submicroscopic rearrangements including the ARHGAP6 gene as well as mutation screening of ARHGAP6 in patients with no obvious chromosomal rearrangements will clarify the role of this gene in MLS syndrome."
"OBJECTIVE: This study aimed to determine if the inception of Early Intervention Services (EISs) is followed by an improvement in the prompt treatment of people with first episode psychosis.METHOD: A prospective cohort study of referrals to new and established EISs was conducted at 1, 2, 3, and 4 years after inception of new EIS. The study was conducted with 14 (seven new and seven established) secondary care EIS within geographically defined catchment areas in England between 2005 and 2009. Participants included 1027 consecutive referrals to EIS aged 14-35 with a first episode of psychosis. Duration of untreated psychosis (DUP) and number of participants treated adequately within 6 months of onset were the main outcome measures.RESULTS: A significant downward trend across yearly cohorts for DUP for new EIS (F1,549=8.4, p=0.004) but not for established EIS (F1,429=1.7, p=0.19) was observed. There was a significant upward trend across cohorts in the proportion of referrals treated within 6 months for new EIS (X(2)=8.0, df=1, p=0.005), but not for established EIS (X(2)=0.1, df=1, p=0.72).CONCLUSION: The introduction of new EIS was followed by a reduction in DUP and an increase in the proportion of patients treated within 6 months of onset. These trends were not present in the catchment areas of established services where DUP was initially lower, suggesting that there was no general tendency for DUP to fall over time. Hence, the introduction of an EIS was followed by an improvement in the prompt and proper treatment of first episode psychosis."
"The understanding of the factors that affect the real pore-network structure for a given bulk material due to different synthetic procedures is essential to develop the material with the best adsorption properties. In this work, we have deeply studied the influence of the crystallinity degree over the adsorption capacity on three new isostructural MOFs with the formula {[CdM(ì4-pmdc)2(H2O)2]?solv}n (in which, pmdc = pyrimidine-4,6-dicarboxylate; solv = corresponding solvent; M(II) = Cd (1), Mn (2), Zn (3)). Compared with other methods, the solvent-free synthesis stands as the most effective route because, apart from enabling the preparation of the heterometallic compounds 2 and 3, it also renders the adsorbents with the highest performance, which is indeed close to the expected one derived from Grand Canonical Monte Carlo (GCMC) calculations. The structural analysis of the as-synthesised and evacuated frameworks reveals the existence of a metal atom exposed to the pore. The accessibility of this site is limited due to its atomic environment, which is why it is considered as a pseudo-open-metal site. The chemical and physical characterisation confirms that this site can be modified as the metal atom is replaced in compounds 2 and 3. To assess the effect of the metal replacement on the adsorption behaviour, an exhaustive study of CO2 experimental isotherms has been performed. The affinity of the pseudo-open metal sites towards CO2 and the distribution of the preferred adsorption sites are discussed on the basis of DFT and GCMC calculations."
"This study investigates the adsorption and reactions of H(2)O(2) on TiO(2) anatase (101) and rutile (110) surfaces by first-principles calculations based on the density functional theory in conjunction with the projected augmented wave approach, using PW91, PBE, and revPBE functionals. Adsorption mechanisms of H(2)O(2) and its fragments on both surfaces are analyzed. It is found that H(2)O(2) , H(2)O, and HO preferentially adsorb at the Ti(5c) site, meanwhile HOO, O, and H preferentially adsorb at the (O(2c))(Ti(5c)), (Ti(5c))(2), and O(2c) sites, respectively. Potential energy profiles of the adsorption processes on both surfaces have been constructed using the nudged elastic band method. The two restructured surfaces, the 1/3 ML oxygen covered TiO(2) and the hydroxylated TiO(2), are produced with the H(2)O(2) dehydration and deoxidation, respectively. The formation of main products, H(2)O(g) and the 1/3 ML oxygen covered TiO(2) surface, is exothermic by 2.8 and 5.0 kcal/mol, requiring energy barriers of 0.8 and 1.1 kcal/mol on the rutile (110) and anatase (101) surface, respectively. The rate constants for the H(2)O(2) dehydration processes have been predicted to be 6.65 ? 10(-27) T(4.38) exp(-0.14 kcal mol(-1)/RT) and 3.18 ? 10(-23) T(5.60) exp(-2.92 kcal mol(-1)/RT) respectively, in units of cm(3) molecule(-1) s(-1)."
"The NAD-dependent glutamate dehydrogenase (GDH) gene from the halophilic archaeon Haloferax mediterranei has been cloned. The analysis of the nucleotide sequence revealed an open reading frame of 1323 bp that encodes a NAD-GDH. The amino acid sequence displayed high homology with those from other sources, especially the highly conserved residues involved in 2-oxoglutarate binding. The expression of this gene in Escherichia coli, the refolding and further characterization, yielded a fully active NAD-GDH with the same features than those found for the wild-type enzyme. This halophilic NAD-GDH showed a highly dependence on salts for both stability and activity, being essential for the refolding of the recombinant enzyme."
"This study was aimed to characterize the depression-like behaviour in the classical model of chronic inflammation induced by Complete Freund's Adjuvant (CFA). Male Swiss mice received an intraplantar (i.pl.) injection of CFA (50 µl/paw) or vehicle. Behavioural and inflammatory responses were measured at different time-points (1 to 4 weeks), and different pharmacological tools were tested. The brain levels of IL-1â and BDNF, or COX-2 expression were also determined. CFA elicited a time-dependent edema formation and mechanical allodynia, which was accompanied by a significant increase in the immobility time in the tail suspension (TST) or forced-swimming (FST) depression tests. Repeated administration of the antidepressants imipramine (10 mg/kg), fluoxetine (20 mg/kg) and bupropion (30 mg/kg) significantly reversed depression-like behaviour induced by CFA. Predictably, the anti-inflammatory drugs dexamethasone (0.5 mg/kg), indomethacin (10 mg/kg) and celecoxib (30 mg/kg) markedly reduced CFA-induced edema. The oral treatment with the analgesic drugs dipyrone (30 and 300 mg/kg) or pregabalin (30 mg/kg) significantly reversed the mechanical allodyinia induced by CFA. Otherwise, either dipyrone or pregabalin (both 30 mg/kg) did not significantly affect the paw edema or the depressive-like behaviour induced by CFA, whereas the oral treatment with dipyrone (300 mg/kg) was able to reduce the immobility time in TST. Noteworthy, CFA-induced edema was reduced by bupropion (30 mg/kg), and depression behaviour was prevented by celecoxib (30 mg/kg). The co-treatment with bupropion and celecoxib (3 mg/kg each) significantly inhibited both inflammation and depression elicited by CFA. The same combined treatment reduced the brain levels of IL-1â, as well as COX-2 immunopositivity, whilst it failed to affect the reduction of BDNF levels. We provide novel evidence on the relationship between chronic inflammation and depression, suggesting that combination of antidepressant and anti-inflammatory agents bupropion and celecoxib might represent an attractive therapeutic strategy for depression."
"Microparticles of chitosan (CHT) containing alendronate sodium (AL) were prepared in four drug:polymer ratios (1:1, 1:2, 1:4, 1:6) using the spray drying technique. The efficiency of the method was evaluated by determining production yield (about 70 %) and microencapsulation efficiency, which was almost 100 % in the case of all four of the formulations studied. Particles had a mean size of between 3.6 and 4.6 microm, and a near-spherical shape. The formulations with the highest content of AL (drug:polymer ratio 1:1 and 1:2) showed an asymmetrical distribution of particles, which were larger in size, and had a higher proportion of irregular particles than the other formulations. FT-IR analysis revealed an ionic interaction between AL and CHT. Differential scanning calorimetry and thermogravimetric analysis confirmed the microencapsulation of AL and the increased thermal stability of encapsulated AL. The dissolution profiles of AL from CHT microspheres, at pH values of 1.2 and 6.8, showed a delayed release of AL from microspheres, and the dissolution rate was dependent on the pH and the drug:polymer ratio. It can be concluded that spray drying is a suitable technique for preparing AL-loaded CHT microspheres, and that the drug:polymer ratio can be used to control the rate of AL release from microspheres."
"BACKGROUND: Access to facility delivery in India has significantly increased with the Janani Suraksha Yojana (JSY) cash transfer programme to promote facility births. However, a decline in maternal mortality has only followed secular trends as seen from the beginning of the decade well before the programme began. We, therefore, examined the quality of intrapartum care provided in facilities under the JSY programme to study whether it ensures skilled attendance at birth.DESIGN: 1) Non-participant observations (n=18) of intrapartum care during vaginal deliveries at a representative sample of 11 facilities in Madhya Pradesh to document what happens during intrapartum care. 2) Interviews (n=10) with providers to explore reasons for this care. Thematic framework analysis was used.RESULTS: Three themes emerged from the data: 1) delivery environment is chaotic: delivery rooms were not conducive to safe, women-friendly care provision, and coordination between providers was poor. 2) Staff do not provide skilled care routinely: this emerged from observations that monitoring was limited to assessment of cervical dilatation, lack of readiness to provide key elements of care, and the execution of harmful/unnecessary practices coupled with poor techniques. 3) Dominant staff, passive recipients: staff sometimes threatened, abused, or ignored women during delivery; women were passive and accepted dominance and disrespect. Attendants served as 'go-betweens' patients and providers. The interviews with providers revealed their awareness of the compromised quality of care, but they were constrained by structural problems. Positive practices were also observed, including companionship during childbirth and women mobilising in the early stages of labour.CONCLUSIONS: Our observational study did not suggest an adequate level of skilled birth attendance (SBA). The findings reveal insufficiencies in the health system and organisational structures to provide an 'enabling environment' for SBA. We highlight the need to ensure quality obstetric care prior to increasing coverage of facility births if cash transfer programmes like the JSY are to improve health outcomes."
"PURPOSE: This phase II study evaluated the efficacy and safety of a 7-day on/7-day off regimen of temozolomide before radiotherapy (RT) in patients with inoperable newly diagnosed glioblastoma.PATIENTS AND METHODS: Patients received temozolomide (150 mg/m2/d on days 1 to 7 and days 15 to 21 every 28 days; 7 days on/7 days off) for up to four cycles before conventional RT (2-Gy fractions to a total of 60 Gy) and for four cycles thereafter or until disease progression. The primary end point was tumor response. Tumor tissue from 25 patients was analyzed for O6-methylguanine-DNA methyltransferase (MGMT) expression.RESULTS: Twenty-nine patients with a median age of 60 years were treated, and 28 were assessable for response. Seven (24%) of 29 patients had a partial response, nine patients (31%) had stable disease, and 12 patients (41%) had progressive disease. Median progression-free survival (PFS) time was 3.8 months, and median overall survival (OS) time was 6.1 months. Patients with low MGMT expression, compared with patients with high MGMT expression, had a significantly higher response rate (55% v 7%, respectively; P = .004) and improved PFS (median, 5.5 v 1.9 months, respectively; P = .009) and OS (median, 16 v 5 months, respectively; P = .003). The most common grade 3 and 4 toxicities were thrombocytopenia (20%) and neutropenia (17%).CONCLUSION: This dose-dense temozolomide regimen resulted in modest antitumor activity with an acceptable safety profile in the neoadjuvant setting, and expression of MGMT correlated with response to temozolomide. However, this treatment approach seems to be inferior to standard concomitant RT plus temozolomide."
"Despite the limited use of irradiation for food preservation in the United States to date, the process provides an alternative to the use of some chemical pesticides and sprout inhibitors. The formation of random and varied radiolytic products (RPs) in foods that have been irradiated is the focus of criticism of the process, because RPs may affect the sensory and nutritive quality of foods processed with ionizing radiation. The FDA has deemed the process safe, within specified doses, for use on spices, some meats, fruits, and vegetables. Dietitians should be prepared to answer consumer questions related to irradiation as the process becomes more widespread."
"The aim of the present study was to develop a computational method aiding the design of dipeptidomimetic pro-moieties targeting the human intestinal di-/tripeptide transporter hPEPT1. First, the conformation in which substrates bind to hPEPT1 (the bioactive conformation) was identified by conformational analysis and 2D dihedral driving analysis of 15 hPEPT1 substrates, which suggested that psi(1) approximately 165 degrees , omega(1) approximately 180 degrees , and phi(2) approximately 280 degrees were descriptive of the bioactive conformation. Subsequently, the conformational energy required to change the peptide backbone conformation (DeltaE(bbone)) from the global energy minimum conformation to the identified bioactive conformation was calculated for 20 hPEPT1 targeted model prodrugs with known K(i) values. Quantitatively, an inverse linear relationship (r(2)=0.81, q(2)=0.80) was obtained between DeltaE(bbone) and log1/K(i), showing that DeltaE(bbone) contributes significantly to the experimentally observed affinity for hPEPT1 ligands. Qualitatively, the results revealed that compounds classified as high affinity ligands (K(i)<0.5 mM) all have a calculated DeltaE(bbone)<1 kcal/mol, whereas medium and low-affinity compounds (0.5 mM<K(i)<15 mM) have DeltaE(bbone) values in the range 1-3 kcal/mol. The findings also shed new light on the basis for the experimentally observed stereoselectivity of hPEPT1."
"Normalize the response of electronic portal imaging device (EPID) is the first step toward an EPID-based standardization of Linear Accelerator (linac) dosimetry quality assurance. In this study, we described an approach to generate two-dimensional (2D) pixel sensitivity maps (PSM) for EPIDs response normalization utilizing an alternative beam and dark-field (ABDF) image acquisition technique and large overlapping field irradiations. The automated image acquisition was performed by XML-controlled machine operation and the PSM was generated based on a recursive calculation algorithm for Varian linacs equipped with aS1000 and aS1200 imager panels. Cross-comparisons of normalized beam profiles and 1.5%/1.5 mm 1D Gamma analysis was adopted to quantify the improvement of beam profile matching before and after PSM corrections. PSMs were derived for both photon (6, 10, 15 MV) and electron (6, 20 MeV) beams via proposed method. The PSM-corrected images reproduced a horn-shaped profile for photon beams and a relative uniform profiles for electrons. For dosimetrically matched linacs equipped with aS1000 panels, PSM-corrected images showed increased 1D-Gamma passing rates for all energies, with an average 10.5% improvement for crossline and 37% for inline beam profiles. Similar improvements in the phantom study were observed with a maximum improvement of 32% for 15 MV and 22% for 20 MeV. The PSM value showed no significant change for all energies over a 3-month period. In conclusion, the proposed approach correct EPID response for both aS1000 and aS1200 panels. This strategy enables the possibility to standardize linac dosimetry QA and to benchmark linac performance utilizing EPID as the common detector."
"BACKGROUND: Coverage of roots exposed by gingival recession is one of the main objectives of periodontal reconstructive surgery. A large variety of mucogingival grafting procedures are available. However, the long-term effectiveness of this procedure is still not clear. This study compared the effectiveness of sub-pedicle acellular dermal matrix allografts with subepithelial connective tissue autografts in achieving root coverage 2 years postoperatively.METHODS: One hundred one (101) patients were treated with dermal matrix allografts (mean age, 28.4+/- 0.7 years; mean recession, 4.2 mm) and 65 patients treated with connective tissue graft (mean age, 30.1+/- 1.4 years; mean recession, 4.9 mm). All patients underwent full periodontal evaluation and presurgical preparation, including oral hygiene instruction and scaling and root planing. The exposed roots were thoroughly planed and covered by a graft without any further root treatment or conditioning. There were no differences in the average age, time of follow-up, or gender between the two groups. Patients were evaluated periodically between 1 and 2 years. Residual recession and defect coverage were assessed.RESULTS: Mean residual root recession after root coverage with acellular dermal matrix allograft was 0.2 +/- 0.04 mm, with defect coverage of 95.9% +/- 0.9%. Frequency of defect coverage was 82.2%. Root coverage was 98.8% +/- 0.2%, resulting in a frequency of root coverage of 100%. Gain in keratinized gingiva was 2.2+/- 0.04 mm and attachment gain was 4.5+/- 0.1 mm per patient. Connective tissue autografts resulted in mean residual root recession of 0.1+/- 0.04 mm, with percent defect coverage of 97.8%+/- 0.6% and frequency of defect coverage of 95.4%. Root coverage was 99.1%+/- 0.2%, and frequency of root coverage was 100%. Gain in keratinized gingiva was 3.0+/- 0.1 mm and attachment gain was 5.3+/- 0.2 mm per patient. No significant differences in final recession and root coverage between the two treatment methods were found. However, autografts resulted in significant increases in defect coverage, keratinized gingival gain, attachment gain, and residual probing depth. The clinical results were stable for the 2-year follow-up period.CONCLUSIONS: These results indicate that coverage of root by sub-pedicle acellular dermal matrix allografts or subepithelial connective tissue autografts is a very predictable procedure which is stable for 2 years postoperatively. However, subepithelial connective tissue autografts resulted in significant increases in defect coverage, keratinized gingival gain, attachment gain, and residual probing depth."
"A series of cyclic imides bearing a omega-(4-aryl and 4-heteroaryl-1-piperazinyl)alkyl moieties was synthesized and tested in vivo for anxiolytic activity. The in vitro binding affinities of these compounds were also examined for 5-HT1A receptor sites. Structure-activity relationships within these series are discussed. One of these compounds, (1R*,2S*,-3R*,4S*)-N-[4-[4-(2-pyrimidinyl)-1-piperazinyl]butyl]-2,3- bicyclo[2.2.1]heptanedicarboximide (1: tandospirone), was found to be equipotent with buspirone in its anxiolytic activity and more anxio-selective than buspirone and diazepam. Tandospirone (1) is currently undergoing clinical evaluation as a selective anxiolytic agent."
"Two hundred forty-seven healthy newborns were investigated in a prospective cohort descriptive study. Information on phenotype and obstetric and parental history was collected. A positive association was found between erythema toxicum neonatorum and season of birth (spring and summer), whereas parental history of any skin disease was related to a lower frequency of this eruption."
"Satellite DNA sequence evolution has been studied in several insect species from the genus Pimelia (Tenebrionidae, Coleoptera). Low-copy number homologs of the previously characterized major satellite DNA from P. monticola (PMON) have been cloned and sequenced from six congeneric species belonging to two species groups: Ibero-Balearic and Moroccan. Sequence analysis of a sample of low-copy number repeats revealed two subfamilies, differing on average 17.5% due to randomly spread single point mutations. Each subfamily is specific for a group of taxa in congruence with their biogeography. Within each group, there is no significant species-specific clustering of the sequences. These results suggest that the two satellite subfamilies arose after the split of an ancestral lineage into the North African and Ibero-Balearic Pimelia species-groups, but before their subsequent radiation. Rate heterogeneity tests suggest that PMON sequences have evolved faster in the lineage leading to the Moroccan group. Comparison of sequence divergences between minor PMON and the previously characterized major PIM357 satellite obtained from the same taxa, points to similar evolutionary dynamics. Both sequences are evolving in parallel accumulating mutations in a gradual manner irrespectively of significant differences in abundance. These data show that copy number of the sequence families does not necessarily affect the sequence change dynamics of satellite repeats."
"BACKGROUND: Hyperkalemia is a serious complication of rapid and massive blood transfusion due to high plasma potassium (K) in stored red blood cell (RBC) units. A potassium adsorption filter (PAF) was developed in Japan to remove K by exchanging with sodium (Na). We performed an in vitro evaluation of its efficacy and feasibility of use.STUDY DESIGN AND METHODS: Three AS-3 RBC units were filtered by each PAF using gravity; 10 PAFs were tested. Blood group, age, flow rate, and irradiation status were recorded. Total volume, K, Na, Cl, Mg, total Ca (tCa), RBC count, hemoglobin (Hb), hematocrit (Hct), and plasma Hb were measured before and after filtering each unit. Ionized Ca (iCa), pH, and glucose were measured for some units.RESULTS: After filtration, the mean decrease in K was 97.5% in the first RBC unit, 91.2% in the second unit, and 64.4% in the third unit. The mean increases in Na, Mg, and tCa were 33.0, 151.4, and 116.1%, respectively. iCa and pH remained low; glucose was unchanged. RBC count, Hb, and Hct decreased slightly after filtration of first units; plasma Hb was unchanged. After filtration, there was no visual evidence of increased hemolysis or clot formation.CONCLUSION: The PAF decreased K concentration in stored AS-3 RBC units to minimal levels in the first and second RBC units. Optimally, one filter could be used for 2 RBC units. Although Na increased, the level may not be clinically significant. PAF may be useful for at-risk patients receiving older units or blood that has been stored after gamma irradiation."
"The diagnostic and therapeutic problems faced during treatment of thirty-nine patients presenting biliopancreatitis (BP) are discussed. BP diagnosis is made on the ground of clinical picture, laboratory indicators, abdominal ultrasonography, intraoperative cholangio-pancreatography and cholangioscopy. Patients with cholelithiasis (ChL) of long-standing in the previous history are predominant, with the age group exceeding 50 years of age being most numerous, and the female gender prevailing. All patients undergo conservative pre- and postoperative treatment. Operative treatment in BP is performed as an emergency intervention. Cholecystectomy is done in all cases, and in 56.3 per cent of them it proves sufficient to promote a favourable outcome of the pathologic condition. External or internal drainage of the choledochus is necessitated in the presence of definite indications (obstruction of extrahepatic biliary ducts and pancreas documented by operative cholangiography, choledochoscopy and probing). Dilatation of the papillary sphincter is carried out in five patients (12.8 per cent) because of partial papillary stenosis. The destructive forms of pancreatitis are treated by gland draining in conjunction with necrectomy and drainage of the extrahepatic biliary ducts. A correlation is established between the incidence of destructive forms of pancreatitis and therapeutic results, on the one hand, and timing of the operative intervention, on the other. Operative management of BP is a method of choice insofar as it contributes to the complex and thorough treatment of the condition. Preoperative BP diagnosis is still a problem not well enough clarified which leads to delayed operation with an adverse impact on the prognosis of the disease."
"Virolex (acyclovir) used as 3% ointment in 50 patients (50 eyes) with herpetic keratitis was found highly effective in dendritic keratitis and sufficiently effective in keratoiridocyclitis with ulcerations (megaherpetic keratitis), providing cure in 92 and 75%, of cases, respectively. The drug is ineffective in the treatment of stromal herpetic keratitis not associated with corneal ulcers. 3% virolex ointment application in superficial forms of herpetic keratitis is more effective than instillations of 0.1% keracide."
"OBJECTIVE: To explore the effectiveness of the free bilobed medial sural artery perforator (BMSAP) flap to repair penetrating wound of the foot.METHODS: Between April 2012 and October 2014, 7 cases of foot penetrating wounds were treated with the BMSAP flap. There were 5 males and 2 females, aged from 21 to 43 years (mean, 31.5 years). The causes of injury included the crush injury (4 cases), blunt puncture (2 cases), and firearm injury (1 case). The wound was located at the left foot in 4 cases and at the right foot in 3 cases. There were longitudinal penetrating injury in 5 cases and transverse penetrating injury in 2 cases. The size of wound ranged from 4 cm x 3 cm to 9 cm x 7 cm. The interval between injury and admission was 0.5-5.5 hours (mean, 3.2 hours). The free BMSAP flap of 5 cm x 4 cm-10 cm x 8 cm in size was used to repair the wounds on both sides and to reconstruct the sensation. The donor site was sutured or repaired with skin graft.RESULTS: After operation, 1 case had distal flap necrosis, the flap survived after dressing change; 1 case had wound infection, and delayed healing was obtained after drainage; and the flap survived completely, and primary healing was obtained in the other 5 cases. The skin grafts survived and healing of incision by first intention was observed at donor sites. The patients were followed up from 7 to 24 months (mean, 12.5 months). The flap had soft texture and similar color to normal skin. According to the British Medical Research Council (BMRC) sensory function assessment system, 1 case was rated as S2, 4 cases as , and 2 cases as S₃+. The American Orthopaedic Foot and Ankle Society (AOFAS) score was 86-97 (mean, 93.6); the results were excellent in 6 cases and good in 1 case.CONCLUSION: The free BMSAP flap is very suitable to repair penetrating wound of the foot. The flap has the advantages of repairing the two wounds at the same time and reconstructing skin sensation as well."
"In the present study, we explored the expression and correlation of survivin with HIF-1á, TGF-â1 and TFE3 in adenoid cystic carcinoma (AdCC). The expression of survivin, HIF-1á, TGF-â1 and TFE3 was assessed by immunohistochemical staining of a tissue microarray containing tissue samples of normal salivary gland (NSG), pleomorphic adenoma (PA) and AdCC. Correlation analysis of these proteins revealed that increased survivin expression was associated with the overexpression of HIF-1á (P<0.001, r = 0.5599), TGF-â1 (P<0.001, r = 0.6616) and TFE3 (P<0.001, r = 0.7747). The expression of survivin, HIF-1á, TGF-â1 and TFE3 was not correlated with the pathological type of human AdCC (P>0.05). Selective inhibition of survivin by YM155 and siRNA significantly reduced human SACC-83 cell proliferation, with the corresponding decrease in expression of HIF-1á, TGF-â1 and TFE3. The data indicate that the overexpression of survivin in AdCC is related to HIF-1á, TGF-â1 and TFE3. We hypothesize from these findings that the inhibition of survivin may be a novel strategy for neoadjuvant chemotherapeutic and radiosensitive treatment of AdCC."
"The aim of this study was to characterize the physico-chemical properties and bone repair after implantation of zinc-containing nanostructured porous hydroxyapatite scaffold (nZnHA) in rabbits' calvaria. nZnHA powder containing 2% wt/wt zinc and stoichiometric nanostructured porous hydroxyapatite (nHA - control group) were shaped into disc (8 mm) and calcined at 550 °C. Two surgical defects were created in the calvaria of six rabbits (nZnHA and nHA). After 12 weeks, the animals were euthanized and the grafted area was removed, fixed in 10% formalin with 0.1 M phosphate buffered saline and embedded in paraffin (n=10) for histomorphometric evaluation. In addition, one sample from each group (n=2) was embedded in methylmethacrylate for the SEM and EDS analyses. The thermal treatment transformed the nZnHA disc into a biphasic implant composed of Zn-containing HA and Zn-containing â-tricalcium phosphate (ZnHA/âZnTCP). The XRD patterns for the nHA disc were highly crystalline compared to the ZnHA disc. Histological analysis revealed that both materials were biologically compatible and promoted osteoconduction. X-ray fluorescence and MEV-EDS of nZnHA confirmed zinc in the samples. Histomorphometric evaluation revealed the presence of new bone formation in both frameworks but without statistically significant differences (p>0.05), based on the Wilcoxon test. The current study confirmed that both biomaterials improve bone repair, are biocompatible and osteoconductive, and that zinc (2wt%) did not increase the bone repair. Additional in vivo studies are required to investigate the effect of doping hydroxyapatite with a higher Zn concentration."
"Pulmonary tumor embolism is an often missed antemortem diagnosis in patients with cancer and respiratory failure. Although rare, this complication is an important cause of additional morbidity. Referred for radionuclide pulmonary perfusion and ventilation scintigraphy, a typical pattern of multiple subsegmental peripheral defects on perfusion lung scanning without matching ventilation defects, suggesting a high probability of pulmonary thromboembolism, often leads to false conclusions. We present a case of bilateral multiple subsegmental mismatched defects in lung ventilation perfusion scintigraphy, where autopsy confirmed the diagnosis of pulmonary tumor embolism, secondary to an undifferentiated ductal type adenocarcinoma of the pancreas. Pulmonary tumor embolism is an entity to keep in mind in patients treated for carcinoma presenting with (sub) acute dyspnea."
"This article is devoted to the study of the double fertilization mechanism in plants, in particular of
the maize gamete membrane fusion genes. We detected and analyzed for the first time gamete-fusion genes
in the maize genome. Using the BLAST program, we searched for the hap2 gene (generative cell specific 1
(gcs1)) homologs from Arabidopsis in the maize genome. The ZM_BFb0162K03 maize transcript was found,
which had 67% identity to the Athap2 gene and contained a conserved region similar to the Athap2 gene fragment.
In mRNA samples from the haploid-inducing and control maize lines, an PCR was conducted by using
primers specific to the ZM_BFb0162K03 sequence fragment. Sequences of the PCR products from a fragment
(1467 bp) of the Zm_hap2 gene of the haploid-inducing and the control maize lines were identical and
also were identical to the maize sequences from the GenBank (ZM_BFb0162K03). PCR products (656 bp
region of Zm_hap2) for the ZM_BFb0162K03 (1925 bp) maize sequence were observed for the cDNA of pollen
grains, ovary, leaves, and roots of the haploid-inducing and control maize lines. Using the Blastx program,
we found significant homology of the maize translated proteins to the GEX2, TET11, and TET12 proteins,
involved in Arabidopsis gamete-fusion contacts."
"BACKGROUND: Using assessment scales in clinical and research practice is one of fundamental reference methods of evaluation in human pathological states. Pelvic vein varicosity is an independent nosological entity within the framework of chronic vein diseases. Currently, the clinical methods of assessment in the aspect of a patient-oriented approach in this type of disease are largely understudied and still not duly defined.AIM: The study was aimed at analyzing clinical outcomes of surgical treatment in the form of resection of the ovarian vein in female patients with pelvic varicose veins, based on the developed specialized scale of clinical assessment of disease severity.PATIENTS AND METHODS: We carried out an open prospective study of efficacy of resection of the ovarian vein in 37 women with pelvic varicose veins. The main criterion for assessment was a clinical method of determining manifestations of the disease by means of the Pelvic Venous Clinical Severity Score.RESULTS: According to the Pelvic Venous Clinical Severity Score, improvement of the condition was observed in 36 (97.3%) operated female patients and 1 (2.7%) woman turned out to have negative dynamics. The median of the composite score of the severity scale decreased form 11.78±5.06 points to 5.22±3.19 (p<0.05). The total positive gradient of the score amounted to 6.57±3 .65 points. A significant decrease in manifestations of severity was observed for 9 of the 10 clinical descriptors of the disease.CONCLUSION: The use of the suggested scale in practical assessment of the results made it possible to prove high efficacy of resection of the ovarian vein in women with pelvic varicose veins in the form of decreased intensity of the disease's symptomatology. The VCSS is an easy-to-fill-in tool, taking up little time, ensuring no influence of the physician's personality on the answers, presenting quantitative expression of therapeutic results."
"BACKGROUND: In perennial allergic rhinitis (PAR), the skin-prick test (SPT) is a good diagnostic tool to identify the specific allergens. A nasal provocation test (NPT) is used to identify allergens and to confirm the diagnosis. The aim of this study was to determine the optimal cutoff values of symptom and peak nasal inspiratory flow (PNIF) changes after dust-mite NPT for predicting PAR. We also studied the relationship of the changes of symptoms in NPT and the wheal size of SPT.METHODS: One hundred five patients with perennial rhinitis underwent the NPT to Dermatophagoides pteronyssinus and the SPT. The NPT was assessed by changes in symptoms and PNIF. The optimal cutoff values of the symptoms score and PNIF changes after the NPT for predicting the SPT were determined using a receiver operating characteristic (ROC) curve. The relationship of the wheal sizes of SPT and the changes from the NPT were analyzed.RESULTS: Forty-eight patients had a positive SPT to D. pteronyssinus, of whom 33 patients had a positive NPT by increases of the symptom score. Twenty patients had a positive NPT by decreases of PNIF. The area under the ROC curve was 0.85 for symptom score changes and it was 0.612 for PNIF changes. There was a significant correlation between the wheal size of the SPT and symptom changes in the NPT.CONCLUSION: Nasal provocation is a valuable test to confirm the diagnosis of D. pteronyssinus allergy, especially when the wheal from the SPT is small. The symptom change after the house-dust mite NPT is better than the PNIF change for predicting the PAR."
"BACKGROUND AND PURPOSE: The aim of the study was to determine the most frequent errors in medical treatment of craniocerebral injuries, based on materials reviewed by the Forensic Medicine Department, Medical University of ??d?, Poland.MATERIAL AND METHODS: Legal opinions in the area of craniocerebral injuries, elaborated by the Forensic Medicine Department, Medical University of ??d?, from 2000 to 2004, were assessed.RESULTS: Seven hundred ninety three opinions related to medical practice were given between 2000 and 2004; 30 cases referred to craniocerebral injuries. There were 19 opinions analyzed in which incorrectness of diagnostic and therapeutic process was found. Eight cases were related to disqualification from hospitalization, 4 cases referred to delay in diagnosis because of transportation to another place, and 4 cases were related to abandoned or misinterpreted imaging studies.CONCLUSIONS: Analyzed material comprised 17 errors during the decision-making process, including 4 diagnostic errors, as well as 1 therapeutic, 1 executive, and 1 organisational error. The most common error was disqualification from hospitalization of patients who should be observed in hospital. It was followed (in order of frequency) by errors related to transportation to the sobering chamber or to another hospital, and failure to perform or misinterpretation of imaging studies. The fewest errors referred to treatment. The main cause of craniocerebral injury was fall of a drunk person, and the alcohol intoxication made diagnosis difficult and delayed."
"BACKGROUND: To understand HIV-1 drug resistance in 11 prefectures of Hebei Province, China, we implemented a cross-sectional HIV-1 molecular epidemiological survey.METHODS: Blood samples were collected from 122 newly diagnosed drug-na?ve HIV-1-positive individuals and 229 antiretroviral therapy (ART)-failure individuals from 11 prefectures in Hebei Province, China. Patient demographic data were obtained via face-to-face interviews using a standardized questionnaire when blood samples were collected. Genotyping of HIV-1 drug resistance (DR) was implemented using an in-house assay.RESULTS: In this study, the overall prevalence of HIV-1 DR was 35.5%. The prevalence of HIV-1 DR in participants experiencing treatment failure and ART-na?ve participants was 51.9 and 5.9%, respectively. Mutations in protease inhibitors, nucleoside reverse transcriptase inhibitors (NRTIs), and non-NRTI (NNRTIs), as well as dual and multiple mutations were extensively seen in participants experiencing treatment failure. The proportions of NNRTI mutations (÷2 = 9.689, p = 0.002) and dual mutations in NRTIs and NNRTIs (÷2 = 39.958, p < 0.001) in participants experiencing treatment failure were significantly higher than those in ART-na?ve participants. The distributions of M184V/I and M41L mutations differed significantly among three main HIV-1 genotypes identified. Viral load, symptoms in the past 3 months, CD4 counts, transmission route, and the duration of ART were found to be associated with HIV-1 DR.CONCLUSIONS: Our results suggest that new prevention and control strategies should be formulated according to the epidemic characteristics of HIV-1-resistant strains in Hebei Province, where antiretroviral drugs are widely used."
"Emerging evidence suggests that schizophrenia is associated with brain dysconnectivity. Nonetheless, the implicit assumption of stationary functional connectivity (FC) adopted in most previous resting-state functional magnetic resonance imaging (fMRI) studies raises an open question of schizophrenia-related aberrations in dynamic properties of resting-state FC. This study introduces an empirical method to examine the dynamic functional dysconnectivity in patients with schizophrenia. Temporal brain networks were estimated from resting-state fMRI of 2 independent datasets (patients/controls = 18/19 and 53/57 for self-recorded dataset and a publicly available replication dataset, respectively) by the correlation of sliding time-windowed time courses among regions of a predefined atlas. Through the newly introduced temporal efficiency approach and temporal random network models, we examined, for the first time, the 3D spatiotemporal architecture of the temporal brain network. We found that although prominent temporal small-world properties were revealed in both groups, temporal brain networks of patients with schizophrenia in both datasets showed a significantly higher temporal global efficiency, which cannot be simply attributable to head motion and sampling error. Specifically, we found localized changes of temporal nodal properties in the left frontal, right medial parietal, and subcortical areas that were associated with clinical features of schizophrenia. Our findings demonstrate that altered dynamic FC may underlie abnormal brain function and clinical symptoms observed in schizophrenia. Moreover, we provide new evidence to extend the dysconnectivity hypothesis in schizophrenia from static to dynamic brain network and highlight the potential of aberrant brain dynamic FC in unraveling the pathophysiologic mechanisms of the disease."
"UNLABELLED: Transsexualism is one of the gender identity disorders where psychological sex is opposed to anatomical sex. This disorder leads to a discrepancy between the preferred social gender and the biological sex.AIM: The aim of this research is to compare knowledge and attitude toward transsexualism in student's opinion, coming from three universities in L?d?.METHOD: The questionnaire study was performed in the group of 300 students from three universities in L?d?: Technical University of L?d?, University of L?d?, Medical University of L?d?. The questionnaire contained 30 questions related to respondent's sex, birthplace, knowledge about definition and aetiology of transsexualism and also rights which students would grant to transsexuals.RESULTS: The right definition of transsexualism was pointed by 64% of students from Medical University, 57% from Technical University and 40% from University of L?d?. The right to surgical sex change for transsexuals would be granted by 87% of students from the Medical University, 69% from the University of L?d? and 40% from the Technical University. Majority of medical students (90%) and respectively 78% and 57% from the University of L?d? and Technical University would accept a transsexual as his/her co-worker.CONCLUSIONS: Student's knowledge about transsexualism is similar and does not differ from a foreign student's knowledge. Students from natural science studies (medicine and biology) are the most tolerant towards transsexuals."
