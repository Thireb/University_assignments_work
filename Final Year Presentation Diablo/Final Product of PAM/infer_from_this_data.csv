Abstract
"  We study the heavy path decomposition of conditional Galton-Watson trees. In
a standard Galton-Watson tree conditional on its size $n$, we order all
children by their subtree sizes, from large (heavy) to small. A node is marked
if it is among the $k$ heaviest nodes among its siblings. Unmarked nodes and
their subtrees are removed, leaving only a tree of marked nodes, which we call
the $k$-heavy tree. We study various properties of these trees, including their
size and the maximal distance from any original node to the $k$-heavy tree. In
particular, under some moment condition, the $2$-heavy tree is with high
probability larger than $cn$ for some constant $c > 0$, and the maximal
distance from the $k$-heavy tree is $O(n^{1/(k+1)})$ in probability. As a
consequence, for uniformly random Apollonian networks of size $n$, the expected
size of the longest simple path is $\Omega(n)$.
"
"  We consider a firm that sells a large number of products to its customers in
an online fashion. Each product is described by a high dimensional feature
vector, and the market value of a product is assumed to be linear in the values
of its features. Parameters of the valuation model are unknown and can change
over time. The firm sequentially observes a product's features and can use the
historical sales data (binary sale/no sale feedbacks) to set the price of
current product, with the objective of maximizing the collected revenue. We
measure the performance of a dynamic pricing policy via regret, which is the
expected revenue loss compared to a clairvoyant that knows the sequence of
model parameters in advance.
We propose a pricing policy based on projected stochastic gradient descent
(PSGD) and characterize its regret in terms of time $T$, features dimension
$d$, and the temporal variability in the model parameters, $\delta_t$. We
consider two settings. In the first one, feature vectors are chosen
antagonistically by nature and we prove that the regret of PSGD pricing policy
is of order $O(\sqrt{T} + \sum_{t=1}^T \sqrt{t}\delta_t)$. In the second
setting (referred to as stochastic features model), the feature vectors are
drawn independently from an unknown distribution. We show that in this case,
the regret of PSGD pricing policy is of order $O(d^2 \log T + \sum_{t=1}^T
t\delta_t/d)$.
"
"  We present a new algorithm which detects the maximal possible number of
matched disjoint pairs satisfying a given caliper when a bipartite matching is
done with respect to a scalar index (e.g., propensity score), and constructs a
corresponding matching. Variable width calipers are compatible with the
technique, provided that the width of the caliper is a Lipschitz function of
the index. If the observations are ordered with respect to the index then the
matching needs $O(N)$ operations, where $N$ is the total number of subjects to
be matched. The case of 1-to-$n$ matching is also considered.
We offer also a new fast algorithm for optimal complete one-to-one matching
on a scalar index when the treatment and control groups are of the same size.
This allows us to improve greedy nearest neighbor matching on a scalar index.
Keywords: propensity score matching, nearest neighbor matching, matching with
caliper, variable width caliper.
"
"  The ability to recognize objects is an essential skill for a robotic system
acting in human-populated environments. Despite decades of effort from the
robotic and vision research communities, robots are still missing good visual
perceptual systems, preventing the use of autonomous agents for real-world
applications. The progress is slowed down by the lack of a testbed able to
accurately represent the world perceived by the robot in-the-wild. In order to
fill this gap, we introduce a large-scale, multi-view object dataset collected
with an RGB-D camera mounted on a mobile robot. The dataset embeds the
challenges faced by a robot in a real-life application and provides a useful
tool for validating object recognition algorithms. Besides describing the
characteristics of the dataset, the paper evaluates the performance of a
collection of well-established deep convolutional networks on the new dataset
and analyzes the transferability of deep representations from Web images to
robotic data. Despite the promising results obtained with such representations,
the experiments demonstrate that object classification with real-life robotic
data is far from being solved. Finally, we provide a comparative study to
analyze and highlight the open challenges in robot vision, explaining the
discrepancies in the performance.
"
"  An Electronic Health Record (EHR) is designed to store diverse data
accurately from a range of health care providers and to capture the status of a
patient by a range of health care providers across time. Realising the numerous
benefits of the system, EHR adoption is growing globally and many countries
invest heavily in electronic health systems. In Australia, the Government
invested $467 million to build key components of the Personally Controlled
Electronic Health Record (PCEHR) system in July 2012. However, in the last
three years, the uptake from individuals and health care providers has not been
satisfactory. Unauthorised access of the PCEHR was one of the major barriers.
We propose an improved access control model for the PCEHR system to resolve the
unauthorised access issue. We discuss the unauthorised access issue with real
examples and present a potential solution to overcome the issue to make the
PCEHR system a success in Australia.
"
"  The least-squares support vector machine is a frequently used kernel method
for non-linear regression and classification tasks. Here we discuss several
approximation algorithms for the least-squares support vector machine
classifier. The proposed methods are based on randomized block kernel matrices,
and we show that they provide good accuracy and reliable scaling for
multi-class classification problems with relatively large data sets. Also, we
present several numerical experiments that illustrate the practical
applicability of the proposed methods.
"
"  A Bernoulli Mixture Model (BMM) is a finite mixture of random binary vectors
with independent Bernoulli dimensions. The problem of clustering BMM data
arises in a variety of real-world applications, ranging from population
genetics to activity analysis in social networks. In this paper, we have
analyzed the information-theoretic PAC-learnability of BMMs, when the number of
clusters is unknown. In particular, we stipulate certain conditions on both
sample complexity and the dimension of the model in order to guarantee the
Probably Approximately Correct (PAC)-clusterability of a given dataset. To the
best of our knowledge, these findings are the first non-asymptotic (PAC) bounds
on the sample complexity of learning BMMs.
"
"  In many societies alcohol is a legal and common recreational substance and
socially accepted. Alcohol consumption often comes along with social events as
it helps people to increase their sociability and to overcome their
inhibitions. On the other hand we know that increased alcohol consumption can
lead to serious health issues, such as cancer, cardiovascular diseases and
diseases of the digestive system, to mention a few. This work examines alcohol
consumption during the FIFA Football World Cup 2018, particularly the usage of
alcohol related information on Twitter. For this we analyse the tweeting
behaviour and show that the tournament strongly increases the interest in beer.
Furthermore we show that countries who had to leave the tournament at early
stage might have done something good to their fans as the interest in beer
decreased again.
"
"  We study Principal Component Analysis (PCA) in a setting where a part of the
corrupting noise is data-dependent and, as a result, the noise and the true
data are correlated. Under a bounded-ness assumption on the true data and the
noise, and a simple assumption on data-noise correlation, we obtain a nearly
optimal sample complexity bound for the most commonly used PCA solution,
singular value decomposition (SVD). This bound is a significant improvement
over the bound obtained by Vaswani and Guo in recent work (NIPS 2016) where
this ""correlated-PCA"" problem was first studied; and it holds under a
significantly weaker data-noise correlation assumption than the one used for
this earlier result.
"
"  The spot pricing scheme has been considered to be resource-efficient for
providers and cost-effective for consumers in the Cloud market. Nevertheless,
unlike the static and straightforward strategies of trading on-demand and
reserved Cloud services, the market-driven mechanism for trading spot service
would be complicated for both implementation and understanding. The largely
invisible market activities and their complex interactions could especially
make Cloud consumers hesitate to enter the spot market. To reduce the
complexity in understanding the Cloud spot market, we decided to reveal the
backend information behind spot price variations. Inspired by the methodology
of reverse engineering, we developed a Predator-Prey model that can simulate
the interactions between demand and resource based on the visible spot price
traces. The simulation results have shown some basic regular patterns of market
activities with respect to Amazon's spot instance type m3.large. Although the
findings of this study need further validation by using practical data, our
work essentially suggests a promising approach (i.e.~using a Predator-Prey
model) to investigate spot market activities.
"
"  Recent progress in deep learning for audio synthesis opens the way to models
that directly produce the waveform, shifting away from the traditional paradigm
of relying on vocoders or MIDI synthesizers for speech or music generation.
Despite their successes, current state-of-the-art neural audio synthesizers
such as WaveNet and SampleRNN suffer from prohibitive training and inference
times because they are based on autoregressive models that generate audio
samples one at a time at a rate of 16kHz. In this work, we study the more
computationally efficient alternative of generating the waveform frame-by-frame
with large strides. We present SING, a lightweight neural audio synthesizer for
the original task of generating musical notes given desired instrument, pitch
and velocity. Our model is trained end-to-end to generate notes from nearly
1000 instruments with a single decoder, thanks to a new loss function that
minimizes the distances between the log spectrograms of the generated and
target waveforms. On the generalization task of synthesizing notes for pairs of
pitch and instrument not seen during training, SING produces audio with
significantly improved perceptual quality compared to a state-of-the-art
autoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is
about 32 times faster for training and 2, 500 times faster for inference.
"
"  We present a novel end-to-end trainable neural network model for
task-oriented dialog systems. The model is able to track dialog state, issue
API calls to knowledge base (KB), and incorporate structured KB query results
into system responses to successfully complete task-oriented dialogs. The
proposed model produces well-structured system responses by jointly learning
belief tracking and KB result processing conditioning on the dialog history. We
evaluate the model in a restaurant search domain using a dataset that is
converted from the second Dialog State Tracking Challenge (DSTC2) corpus.
Experiment results show that the proposed model can robustly track dialog state
given the dialog history. Moreover, our model demonstrates promising results in
producing appropriate system responses, outperforming prior end-to-end
trainable neural network models using per-response accuracy evaluation metrics.
"
"  Recently, neural models for information retrieval are becoming increasingly
popular. They provide effective approaches for product search due to their
competitive advantages in semantic matching. However, it is challenging to use
graph-based features, though proved very useful in IR literature, in these
neural approaches. In this paper, we leverage the recent advances in graph
embedding techniques to enable neural retrieval models to exploit
graph-structured data for automatic feature extraction. The proposed approach
can not only help to overcome the long-tail problem of click-through data, but
also incorporate external heterogeneous information to improve search results.
Extensive experiments on a real-world e-commerce dataset demonstrate
significant improvement achieved by our proposed approach over multiple strong
baselines both as an individual retrieval model and as a feature used in
learning-to-rank frameworks.
"
"  Diamond Light Source is the UK's National Synchrotron Facility and as such
provides access to world class experimental services for UK and international
researchers. As a user facility, that is one that focuses on providing a good
user experience to our varied visitors, Diamond invests heavily in software
infrastructure and staff. Over 100 members of the 600 strong workforce consider
software development as a significant tool to help them achieve their primary
role. These staff work on a diverse number of different software packages,
providing support for installation and configuration, maintenance and bug
fixing, as well as additional research and development of software when
required.
This talk focuses on one of the software projects undertaken to unify and
improve the user experience of several experiments. The ""mapping project"" is a
large 2 year, multi group project targeting the collection and processing
experiments which involve scanning an X-ray beam over a sample and building up
an image of that sample, similar to the way that google maps bring together
small pieces of information to produce a full map of the world. The project
itself is divided into several work packages, ranging from teams of one to 5 or
6 in size, with varying levels of time commitment to the project. This paper
aims to explore one of these work packages as a case study, highlighting the
experiences of the project team, the methodologies employed, their outcomes,
and the lessons learnt from the experience.
"
"  By analyzing energy-efficient management of data centers, this paper proposes
and develops a class of interesting {\it Group-Server Queues}, and establishes
two representative group-server queues through loss networks and impatient
customers, respectively. Furthermore, such two group-server queues are given
model descriptions and necessary interpretation. Also, simple mathematical
discussion is provided, and simulations are made to study the expected queue
lengths, the expected sojourn times and the expected virtual service times. In
addition, this paper also shows that this class of group-server queues are
often encountered in many other practical areas including communication
networks, manufacturing systems, transportation networks, financial networks
and healthcare systems. Note that the group-server queues are always used to
design effectively dynamic control mechanisms through regrouping and
recombining such many servers in a large-scale service system by means of, for
example, bilateral threshold control, and customers transfer to the buffer or
server groups. This leads to the large-scale service system that is divided
into several adaptive and self-organizing subsystems through scheduling of
batch customers and regrouping of service resources, which make the middle
layer of this service system more effectively managed and strengthened under a
dynamic, real-time and even reward optimal framework. Based on this,
performance of such a large-scale service system may be improved greatly in
terms of introducing and analyzing such group-server queues. Therefore, not
only analysis of group-server queues is regarded as a new interesting research
direction, but there also exists many theoretical challenges, basic
difficulties and open problems in the area of queueing networks.
"
"  Mixed-Integer Second-Order Cone Programs (MISOCPs) form a nice class of
mixed-inter convex programs, which can be solved very efficiently due to the
recent advances in optimization solvers. Our paper bridges the gap between
modeling a class of optimization problems and using MISOCP solvers. It is shown
how various performance metrics of M/G/1 queues can be molded by different
MISOCPs. To motivate our method practically, it is first applied to a
challenging stochastic location problem with congestion, which is broadly used
to design socially optimal service networks. Four different MISOCPs are
developed and compared on sets of benchmark test problems. The new formulations
efficiently solve large-size test problems, which cannot be solved by the best
existing method. Then, the general applicability of our method is shown for
similar optimization problems that use queue-theoretic performance measures to
address customer satisfaction and service quality.
"
"  A Discriminative Deep Forest (DisDF) as a metric learning algorithm is
proposed in the paper. It is based on the Deep Forest or gcForest proposed by
Zhou and Feng and can be viewed as a gcForest modification. The case of the
fully supervised learning is studied when the class labels of individual
training examples are known. The main idea underlying the algorithm is to
assign weights to decision trees in random forest in order to reduce distances
between objects from the same class and to increase them between objects from
different classes. The weights are training parameters. A specific objective
function which combines Euclidean and Manhattan distances and simplifies the
optimization problem for training the DisDF is proposed. The numerical
experiments illustrate the proposed distance metric algorithm.
"
"  The task board is an essential artifact in many agile development approaches.
It provides a good overview of the project status. Teams often customize their
task boards according to the team members' needs. They modify the structure of
boards, define colored codings for different purposes, and introduce different
card sizes. Although the customizations are intended to improve the task
board's usability and effectiveness, they may also complicate its comprehension
and use. The increased effort impedes the work of both the team and team
externals. Hence, task board customization is in conflict with the agile
practice of fast and easy overview for everyone. In an eye tracking study with
30 participants, we compared an original task board design with three
customized ones to investigate which design shortened the required time to
identify a particular story card. Our findings yield that only the customized
task board design with modified structures reduces the required time. The
original task board design is more beneficial than individual colored codings
and changed card sizes. According to our findings, agile teams should rethink
their current task board design. They may be better served by focusing on the
original task board design and by applying only carefully selected adjustments.
In case of customization, a task board's structure should be adjusted since
this is the only beneficial kind of customization, that additionally complies
more precisely with the concept of fast and easy project overview.
"
"  Suszko's problem is the problem of finding the minimal number of truth values
needed to semantically characterize a syntactic consequence relation. Suszko
proved that every Tarskian consequence relation can be characterized using only
two truth values. Malinowski showed that this number can equal three if some of
Tarski's structural constraints are relaxed. By so doing, Malinowski introduced
a case of so-called mixed consequence, allowing the notion of a designated
value to vary between the premises and the conclusions of an argument. In this
paper we give a more systematic perspective on Suszko's problem and on mixed
consequence. First, we prove general representation theorems relating
structural properties of a consequence relation to their semantic
interpretation, uncovering the semantic counterpart of substitution-invariance,
and establishing that (intersective) mixed consequence is fundamentally the
semantic counterpart of the structural property of monotonicity. We use those
to derive maximum-rank results proved recently in a different setting by French
and Ripley, as well as by Blasio, Marcos and Wansing, for logics with various
structural properties (reflexivity, transitivity, none, or both). We strengthen
these results into exact rank results for non-permeable logics (roughly, those
which distinguish the role of premises and conclusions). We discuss the
underlying notion of rank, and the associated reduction proposed independently
by Scott and Suszko. As emphasized by Suszko, that reduction fails to preserve
compositionality in general, meaning that the resulting semantics is no longer
truth-functional. We propose a modification of that notion of reduction,
allowing us to prove that over compact logics with what we call regular
connectives, rank results are maintained even if we request the preservation of
truth-functionality and additional semantic properties.
"
"  In this paper we introduce a new classification algorithm called Optimization
of Distributions Differences (ODD). The algorithm aims to find a transformation
from the feature space to a new space where the instances in the same class are
as close as possible to one another while the gravity centers of these classes
are as far as possible from one another. This aim is formulated as a
multiobjective optimization problem that is solved by a hybrid of an
evolutionary strategy and the Quasi-Newton method. The choice of the
transformation function is flexible and could be any continuous space function.
We experiment with a linear and a non-linear transformation in this paper. We
show that the algorithm can outperform 6 other state-of-the-art classification
methods, namely naive Bayes, support vector machines, linear discriminant
analysis, multi-layer perceptrons, decision trees, and k-nearest neighbors, in
12 standard classification datasets. Our results show that the method is less
sensitive to the imbalanced number of instances comparing to these methods. We
also show that ODD maintains its performance better than other classification
methods in these datasets, hence, offers a better generalization ability.
"
"  Is perfect matching in NC? That is, is there a deterministic fast parallel
algorithm for it? This has been an outstanding open question in theoretical
computer science for over three decades, ever since the discovery of RNC
matching algorithms. Within this question, the case of planar graphs has
remained an enigma: On the one hand, counting the number of perfect matchings
is far harder than finding one (the former is #P-complete and the latter is in
P), and on the other, for planar graphs, counting has long been known to be in
NC whereas finding one has resisted a solution.
In this paper, we give an NC algorithm for finding a perfect matching in a
planar graph. Our algorithm uses the above-stated fact about counting matchings
in a crucial way. Our main new idea is an NC algorithm for finding a face of
the perfect matching polytope at which $\Omega(n)$ new conditions, involving
constraints of the polytope, are simultaneously satisfied. Several other ideas
are also needed, such as finding a point in the interior of the minimum weight
face of this polytope and finding a balanced tight odd set in NC.
"
"  In this paper, we investigate the common scenario where every candidate item
for recommendation is characterized by a maximum capacity, i.e., number of
seats in a Point-of-Interest (POI) or size of an item's inventory. Despite the
prevalence of the task of recommending items under capacity constraints in a
variety of settings, to the best of our knowledge, none of the known
recommender methods is designed to respect capacity constraints. To close this
gap, we extend three state-of-the art latent factor recommendation approaches:
probabilistic matrix factorization (PMF), geographical matrix factorization
(GeoMF), and bayesian personalized ranking (BPR), to optimize for both
recommendation accuracy and expected item usage that respects the capacity
constraints. We introduce the useful concepts of user propensity to listen and
item capacity. Our experimental results in real-world datasets, both for the
domain of item recommendation and POI recommendation, highlight the benefit of
our method for the setting of recommendation under capacity constraints.
"
"  Using deep reinforcement learning, we train control policies for autonomous
vehicles leading a platoon of vehicles onto a roundabout. Using Flow, a library
for deep reinforcement learning in micro-simulators, we train two policies, one
policy with noise injected into the state and action space and one without any
injected noise. In simulation, the autonomous vehicle learns an emergent
metering behavior for both policies in which it slows to allow for smoother
merging. We then directly transfer this policy without any tuning to the
University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for
connected and automated vehicles. We characterize the performance of both
policies on the scaled city. We show that the noise-free policy winds up
crashing and only occasionally metering. However, the noise-injected policy
consistently performs the metering behavior and remains collision-free,
suggesting that the noise helps with the zero-shot policy transfer.
Additionally, the transferred, noise-injected policy leads to a 5% reduction of
average travel time and a reduction of 22% in maximum travel time in the UDSSC.
Videos of the controllers can be found at
this https URL.
"
"  It is undeniable that the worldwide computer industry's center is the US,
specifically in Silicon Valley. Much of the reason for the success of Silicon
Valley had to do with Moore's Law: the observation by Intel co-founder Gordon
Moore that the number of transistors on a microchip doubled at a rate of
approximately every two years. According to the International Technology
Roadmap for Semiconductors, Moore's Law will end in 2021. How can we rethink
computing technology to restart the historic explosive performance growth?
Since 2012, the IEEE Rebooting Computing Initiative (IEEE RCI) has been working
with industry and the US government to find new computing approaches to answer
this question. In parallel, the CCC has held a number of workshops addressing
similar questions. This whitepaper summarizes some of the IEEE RCI and CCC
findings. The challenge for the US is to lead this new era of computing. Our
international competitors are not sitting still: China has invested
significantly in a variety of approaches such as neuromorphic computing, chip
fabrication facilities, computer architecture, and high-performance simulation
and data analytics computing, for example. We must act now, otherwise, the
center of the computer industry will move from Silicon Valley and likely move
off shore entirely.
"
"  This paper gives upper and lower bounds on the minimum error probability of
Bayesian $M$-ary hypothesis testing in terms of the Arimoto-Rényi conditional
entropy of an arbitrary order $\alpha$. The improved tightness of these bounds
over their specialized versions with the Shannon conditional entropy
($\alpha=1$) is demonstrated. In particular, in the case where $M$ is finite,
we show how to generalize Fano's inequality under both the conventional and
list-decision settings. As a counterpart to the generalized Fano's inequality,
allowing $M$ to be infinite, a lower bound on the Arimoto-Rényi conditional
entropy is derived as a function of the minimum error probability. Explicit
upper and lower bounds on the minimum error probability are obtained as a
function of the Arimoto-Rényi conditional entropy for both positive and
negative $\alpha$. Furthermore, we give upper bounds on the minimum error
probability as functions of the Rényi divergence. In the setup of discrete
memoryless channels, we analyze the exponentially vanishing decay of the
Arimoto-Rényi conditional entropy of the transmitted codeword given the
channel output when averaged over a random coding ensemble.
"
"  Improving endurance is crucial for extending the spatial and temporal
operation range of autonomous underwater vehicles (AUVs). Considering the
hardware constraints and the performance requirements, an intelligent energy
management system is required to extend the operation range of AUVs. This paper
presents a novel model predictive control (MPC) framework for energy-optimal
point-to-point motion control of an AUV. In this scheme, the energy management
problem of an AUV is reformulated as a surge motion optimization problem in two
stages. First, a system-level energy minimization problem is solved by managing
the trade-off between the energies required for overcoming the positive
buoyancy and surge drag force in static optimization. Next, an MPC with a
special cost function formulation is proposed to deal with transients and
system dynamics. A switching logic for handling the transition between the
static and dynamic stages is incorporated to reduce the computational efforts.
Simulation results show that the proposed method is able to achieve
near-optimal energy consumption with considerable lower computational
complexity.
"
"  The Surjective H-Colouring problem is to test if a given graph allows a
vertex-surjective homomorphism to a fixed graph H. The complexity of this
problem has been well studied for undirected (partially) reflexive graphs. We
introduce endo-triviality, the property of a structure that all of its
endomorphisms that do not have range of size 1 are automorphisms, as a means to
obtain complexity-theoretic classifications of Surjective H-Colouring in the
case of reflexive digraphs.
Chen [2014] proved, in the setting of constraint satisfaction problems, that
Surjective H-Colouring is NP-complete if H has the property that all of its
polymorphisms are essentially unary. We give the first concrete application of
his result by showing that every endo-trivial reflexive digraph H has this
property. We then use the concept of endo-triviality to prove, as our main
result, a dichotomy for Surjective H-Colouring when H is a reflexive
tournament: if H is transitive, then Surjective H-Colouring is in NL, otherwise
it is NP-complete.
By combining this result with some known and new results we obtain a
complexity classification for Surjective H-Colouring when H is a partially
reflexive digraph of size at most 3.
"
"  This paper proposes a modal typing system that enables us to handle
self-referential formulae, including ones with negative self-references, which
on one hand, would introduce a logical contradiction, namely Russell's paradox,
in the conventional setting, while on the other hand, are necessary to capture
a certain class of programs such as fixed-point combinators and objects with
so-called binary methods in object-oriented programming. The proposed system
provides a basis for axiomatic semantics of such a wider range of programs and
a new framework for natural construction of recursive programs in the
proofs-as-programs paradigm.
"
"  Recommender System research suffers currently from a disconnect between the
size of academic data sets and the scale of industrial production systems. In
order to bridge that gap we propose to generate more massive user/item
interaction data sets by expanding pre-existing public data sets. User/item
incidence matrices record interactions between users and items on a given
platform as a large sparse matrix whose rows correspond to users and whose
columns correspond to items. Our technique expands such matrices to larger
numbers of rows (users), columns (items) and non zero values (interactions)
while preserving key higher order statistical properties. We adapt the
Kronecker Graph Theory to user/item incidence matrices and show that the
corresponding fractal expansions preserve the fat-tailed distributions of user
engagements, item popularity and singular value spectra of user/item
interaction matrices. Preserving such properties is key to building large
realistic synthetic data sets which in turn can be employed reliably to
benchmark Recommender Systems and the systems employed to train them. We
provide algorithms to produce such expansions and apply them to the MovieLens
20 million data set comprising 20 million ratings of 27K movies by 138K users.
The resulting expanded data set has 10 billion ratings, 2 million items and
864K users in its smaller version and can be scaled up or down. A larger
version features 655 billion ratings, 7 million items and 17 million users.
"
"  This paper considers the problem of implementing a previously proposed
distributed direct coupling quantum observer for a closed linear quantum
system. By modifying the form of the previously proposed observer, the paper
proposes a possible experimental implementation of the observer plant system
using a non-degenerate parametric amplifier and a chain of optical cavities
which are coupled together via optical interconnections. It is shown that the
distributed observer converges to a consensus in a time averaged sense in which
an output of each element of the observer estimates the specified output of the
quantum plant.
"
"  Stacking is a general approach for combining multiple models toward greater
predictive accuracy. It has found various application across different domains,
ensuing from its meta-learning nature. Our understanding, nevertheless, on how
and why stacking works remains intuitive and lacking in theoretical insight. In
this paper, we use the stability of learning algorithms as an elemental
analysis framework suitable for addressing the issue. To this end, we analyze
the hypothesis stability of stacking, bag-stacking, and dag-stacking and
establish a connection between bag-stacking and weighted bagging. We show that
the hypothesis stability of stacking is a product of the hypothesis stability
of each of the base models and the combiner. Moreover, in bag-stacking and
dag-stacking, the hypothesis stability depends on the sampling strategy used to
generate the training set replicates. Our findings suggest that 1) subsampling
and bootstrap sampling improve the stability of stacking, and 2) stacking
improves the stability of both subbagging and bagging.
"
"  The two-dimensional discrete wavelet transform has a huge number of
applications in image-processing techniques. Until now, several papers compared
the performance of such transform on graphics processing units (GPUs). However,
all of them only dealt with lifting and convolution computation schemes. In
this paper, we show that corresponding horizontal and vertical lifting parts of
the lifting scheme can be merged into non-separable lifting units, which halves
the number of steps. We also discuss an optimization strategy leading to a
reduction in the number of arithmetic operations. The schemes were assessed
using the OpenCL and pixel shaders. The proposed non-separable lifting scheme
outperforms the existing schemes in many cases, irrespective of its higher
complexity.
"
"  In the last few years, we have seen the transformative impact of deep
learning in many applications, particularly in speech recognition and computer
vision. Inspired by Google's Inception-ResNet deep convolutional neural network
(CNN) for image classification, we have developed ""Chemception"", a deep CNN for
the prediction of chemical properties, using just the images of 2D drawings of
molecules. We develop Chemception without providing any additional explicit
chemistry knowledge, such as basic concepts like periodicity, or advanced
features like molecular descriptors and fingerprints. We then show how
Chemception can serve as a general-purpose neural network architecture for
predicting toxicity, activity, and solvation properties when trained on a
modest database of 600 to 40,000 compounds. When compared to multi-layer
perceptron (MLP) deep neural networks trained with ECFP fingerprints,
Chemception slightly outperforms in activity and solvation prediction and
slightly underperforms in toxicity prediction. Having matched the performance
of expert-developed QSAR/QSPR deep learning models, our work demonstrates the
plausibility of using deep neural networks to assist in computational chemistry
research, where the feature engineering process is performed primarily by a
deep learning algorithm.
"
"  We propose a general framework for entropy-regularized average-reward
reinforcement learning in Markov decision processes (MDPs). Our approach is
based on extending the linear-programming formulation of policy optimization in
MDPs to accommodate convex regularization functions. Our key result is showing
that using the conditional entropy of the joint state-action distributions as
regularization yields a dual optimization problem closely resembling the
Bellman optimality equations. This result enables us to formalize a number of
state-of-the-art entropy-regularized reinforcement learning algorithms as
approximate variants of Mirror Descent or Dual Averaging, and thus to argue
about the convergence properties of these methods. In particular, we show that
the exact version of the TRPO algorithm of Schulman et al. (2015) actually
converges to the optimal policy, while the entropy-regularized policy gradient
methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,
we illustrate empirically the effects of using various regularization
techniques on learning performance in a simple reinforcement learning setup.
"
"  We present some basic integer arithmetic quantum circuits, such as adders and
multipliers-accumulators of various forms, as well as diagonal operators, which
operate on multilevel qudits. The integers to be processed are represented in
an alternative basis after they have been Fourier transformed. Several
arithmetic circuits operating on Fourier transformed integers have appeared in
the literature for two level qubits. Here we extend these techniques on
multilevel qudits, as they may offer some advantages relative to qubits
implementations. The arithmetic circuits presented can be used as basic
building blocks for higher level algorithms such as quantum phase estimation,
quantum simulation, quantum optimization etc., but they can also be used in the
implementation of a quantum fractional Fourier transform as it is shown in a
companion work presented separately.
"
"  We analyzed the longitudinal activity of nearly 7,000 editors at the
mega-journal PLOS ONE over the 10-year period 2006-2015. Using the
article-editor associations, we develop editor-specific measures of power,
activity, article acceptance time, citation impact, and editorial renumeration
(an analogue to self-citation). We observe remarkably high levels of power
inequality among the PLOS ONE editors, with the top-10 editors responsible for
3,366 articles -- corresponding to 2.4% of the 141,986 articles we analyzed.
Such high inequality levels suggest the presence of unintended incentives,
which may reinforce unethical behavior in the form of decision-level biases at
the editorial level. Our results indicate that editors may become apathetic in
judging the quality of articles and susceptible to modes of power-driven
misconduct. We used the longitudinal dimension of editor activity to develop
two panel regression models which test and verify the presence of editor-level
bias. In the first model we analyzed the citation impact of articles, and in
the second model we modeled the decision time between an article being
submitted and ultimately accepted by the editor. We focused on two variables
that represent social factors that capture potential conflicts-of-interest: (i)
we accounted for the social ties between editors and authors by developing a
measure of repeat authorship among an editor's article set, and (ii) we
accounted for the rate of citations directed towards the editor's own
publications in the reference list of each article he/she oversaw. Our results
indicate that these two factors play a significant role in the editorial
decision process. Moreover, these two effects appear to increase with editor
age, which is consistent with behavioral studies concerning the evolution of
misbehavior and response to temptation in power-driven environments.
"
"  The beyond worst-case synthesis problem was introduced recently by Bruyère
et al. [BFRR14]: it aims at building system controllers that provide strict
worst-case performance guarantees against an antagonistic environment while
ensuring higher expected performance against a stochastic model of the
environment. Our work extends the framework of [BFRR14] and follow-up papers,
which focused on quantitative objectives, by addressing the case of
$\omega$-regular conditions encoded as parity objectives, a natural way to
represent functional requirements of systems.
We build strategies that satisfy a main parity objective on all plays, while
ensuring a secondary one with sufficient probability. This setting raises new
challenges in comparison to quantitative objectives, as one cannot easily mix
different strategies without endangering the functional properties of the
system. We establish that, for all variants of this problem, deciding the
existence of a strategy lies in ${\sf NP} \cap {\sf coNP}$, the same complexity
class as classical parity games. Hence, our framework provides additional
modeling power while staying in the same complexity class.
[BFRR14] Véronique Bruyère, Emmanuel Filiot, Mickael Randour, and
Jean-François Raskin. Meet your expectations with guarantees: Beyond
worst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha
Portier, editors, 31st International Symposium on Theoretical Aspects of
Computer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of
LIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik,
2014.
"
"  VAEs (Variational AutoEncoders) have proved to be powerful in the context of
density modeling and have been used in a variety of contexts for creative
purposes. In many settings, the data we model possesses continuous attributes
that we would like to take into account at generation time. We propose in this
paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational
AutoEncoder architecture and its generalizations which allows a fine control on
the embedding of the data into the latent space. When augmenting the VAE loss
with this regularization, changes in the learned latent space reflects changes
of the attributes of the data. This deeper understanding of the VAE latent
space structure offers the possibility to modulate the attributes of the
generated data in a continuous way. We demonstrate its efficiency on a
monophonic music generation task where we manage to generate variations of
discrete sequences in an intended and playful way.
"
"  In this work we perform outlier detection using ensembles of neural networks
obtained by variational approximation of the posterior in a Bayesian neural
network setting. The variational parameters are obtained by sampling from the
true posterior by gradient descent. We show our outlier detection results are
comparable to those obtained using other efficient ensembling methods.
"
"  We prove the unique assembly and unique shape verification problems,
benchmark measures of self-assembly model power, are
$\mathrm{coNP}^{\mathrm{NP}}$-hard and contained in $\mathrm{PSPACE}$ (and in
$\mathrm{\Pi}^\mathrm{P}_{2s}$ for staged systems with $s$ stages). En route,
we prove that unique shape verification problem in the 2HAM is
$\mathrm{coNP}^{\mathrm{NP}}$-complete.
"
"  This paper addresses the problem of large scale image retrieval, with the aim
of accurately ranking the similarity of a large number of images to a given
query image. To achieve this, we propose a novel Siamese network. This network
consists of two computational strands, each comprising of a CNN component
followed by a Fisher vector component. The CNN component produces dense, deep
convolutional descriptors that are then aggregated by the Fisher Vector method.
Crucially, we propose to simultaneously learn both the CNN filter weights and
Fisher Vector model parameters. This allows us to account for the evolving
distribution of deep descriptors over the course of the learning process. We
show that the proposed approach gives significant improvements over the
state-of-the-art methods on the Oxford and Paris image retrieval datasets.
Additionally, we provide a baseline performance measure for both these datasets
with the inclusion of 1 million distractors.
"
"  Scientific collaborations shape ideas as well as innovations and are both the
substrate for, and the outcome of, academic careers. Recent studies show that
gender inequality is still present in many scientific practices ranging from
hiring to peer-review processes and grant applications. In this work, we
investigate gender-specific differences in collaboration patterns of more than
one million computer scientists over the course of 47 years. We explore how
these patterns change over years and career ages and how they impact scientific
success. Our results highlight that successful male and female scientists
reveal the same collaboration patterns: compared to scientists in the same
career age, they tend to collaborate with more colleagues than other
scientists, seek innovations as brokers and establish longer-lasting and more
repetitive collaborations. However, women are on average less likely to adapt
the collaboration patterns that are related with success, more likely to embed
into ego networks devoid of structural holes, and they exhibit stronger gender
homophily as well as a consistently higher dropout rate than men in all career
ages.
"
"  Drone racing is becoming a popular sport where human pilots have to control
their drones to fly at high speed through complex environments and pass a
number of gates in a pre-defined sequence. In this paper, we develop an
autonomous system for drones to race fully autonomously using only onboard
resources. Instead of commonly used visual navigation methods, such as
simultaneous localization and mapping and visual inertial odometry, which are
computationally expensive for micro aerial vehicles (MAVs), we developed the
highly efficient snake gate detection algorithm for visual navigation, which
can detect the gate at 20HZ on a Parrot Bebop drone. Then, with the gate
detection result, we developed a robust pose estimation algorithm which has
better tolerance to detection noise than a state-of-the-art perspective-n-point
method. During the race, sometimes the gates are not in the drone's field of
view. For this case, a state prediction-based feed-forward control strategy is
developed to steer the drone to fly to the next gate. Experiments show that the
drone can fly a half-circle with 1.5m radius within 2 seconds with only 30cm
error at the end of the circle without any position feedback. Finally, the
whole system is tested in a complex environment (a showroom in the faculty of
Aerospace Engineering, TU Delft). The result shows that the drone can complete
the track of 15 gates with a speed of 1.5m/s which is faster than the speeds
exhibited at the 2016 and 2017 IROS autonomous drone races.
"
"  The graph Laplacian plays key roles in information processing of relational
data, and has analogies with the Laplacian in differential geometry. In this
paper, we generalize the analogy between graph Laplacian and differential
geometry to the hypergraph setting, and propose a novel hypergraph
$p$-Laplacian. Unlike the existing two-node graph Laplacians, this
generalization makes it possible to analyze hypergraphs, where the edges are
allowed to connect any number of nodes. Moreover, we propose a semi-supervised
learning method based on the proposed hypergraph $p$-Laplacian, and formalize
them as the analogue to the Dirichlet problem, which often appears in physics.
We further explore theoretical connections to normalized hypergraph cut on a
hypergraph, and propose normalized cut corresponding to hypergraph
$p$-Laplacian. The proposed $p$-Laplacian is shown to outperform standard
hypergraph Laplacians in the experiment on a hypergraph semi-supervised
learning and normalized cut setting.
"
"  Generative Adversarial Networks (GAN) have received wide attention in the
machine learning field for their potential to learn high-dimensional, complex
real data distribution. Specifically, they do not rely on any assumptions about
the distribution and can generate real-like samples from latent space in a
simple manner. This powerful property leads GAN to be applied to various
applications such as image synthesis, image attribute editing, image
translation, domain adaptation and other academic fields. In this paper, we aim
to discuss the details of GAN for those readers who are familiar with, but do
not comprehend GAN deeply or who wish to view GAN from various perspectives. In
addition, we explain how GAN operates and the fundamental meaning of various
objective functions that have been suggested recently. We then focus on how the
GAN can be combined with an autoencoder framework. Finally, we enumerate the
GAN variants that are applied to various tasks and other fields for those who
are interested in exploiting GAN for their research.
"
"  We revisit the classification problem and focus on nonlinear methods for
classification on manifolds. For multivariate datasets lying on an embedded
nonlinear Riemannian manifold within the higher-dimensional space, our aim is
to acquire a classification boundary between the classes with labels. Motivated
by the principal flow [Panaretos, Pham and Yao, 2014], a curve that moves along
a path of the maximum variation of the data, we introduce the principal
boundary. From the classification perspective, the principal boundary is
defined as an optimal curve that moves in between the principal flows traced
out from two classes of the data, and at any point on the boundary, it
maximizes the margin between the two classes. We estimate the boundary in
quality with its direction supervised by the two principal flows. We show that
the principal boundary yields the usual decision boundary found by the support
vector machine, in the sense that locally, the two boundaries coincide. By
means of examples, we illustrate how to find, use and interpret the principal
boundary.
"
"  The development of chemical reaction models aids understanding and prediction
in areas ranging from biology to electrochemistry and combustion. A systematic
approach to building reaction network models uses observational data not only
to estimate unknown parameters, but also to learn model structure. Bayesian
inference provides a natural approach to this data-driven construction of
models. Yet traditional Bayesian model inference methodologies that numerically
evaluate the evidence for each model are often infeasible for nonlinear
reaction network inference, as the number of plausible models can be
combinatorially large. Alternative approaches based on model-space sampling can
enable large-scale network inference, but their realization presents many
challenges. In this paper, we present new computational methods that make
large-scale nonlinear network inference tractable. First, we exploit the
topology of networks describing potential interactions among chemical species
to design improved ""between-model"" proposals for reversible-jump Markov chain
Monte Carlo. Second, we introduce a sensitivity-based determination of move
types which, when combined with network-aware proposals, yields significant
additional gains in sampling performance. These algorithms are demonstrated on
inference problems drawn from systems biology, with nonlinear differential
equation models of species interactions.
"
"  Inspired by the success of deep learning techniques in the physical and
chemical sciences, we apply a modification of an autoencoder type deep neural
network to the task of dimension reduction of molecular dynamics data. We can
show that our time-lagged autoencoder reliably finds low-dimensional embeddings
for high-dimensional feature spaces which capture the slow dynamics of the
underlying stochastic processes - beyond the capabilities of linear dimension
reduction techniques.
"
"  While bigger and deeper neural network architectures continue to advance the
state-of-the-art for many computer vision tasks, real-world adoption of these
networks is impeded by hardware and speed constraints. Conventional model
compression methods attempt to address this problem by modifying the
architecture manually or using pre-defined heuristics. Since the space of all
reduced architectures is very large, modifying the architecture of a deep
neural network in this way is a difficult task. In this paper, we tackle this
issue by introducing a principled method for learning reduced network
architectures in a data-driven way using reinforcement learning. Our approach
takes a larger `teacher' network as input and outputs a compressed `student'
network derived from the `teacher' network. In the first stage of our method, a
recurrent policy network aggressively removes layers from the large `teacher'
model. In the second stage, another recurrent policy network carefully reduces
the size of each remaining layer. The resulting network is then evaluated to
obtain a reward -- a score based on the accuracy and compression of the
network. Our approach uses this reward signal with policy gradients to train
the policies to find a locally optimal student network. Our experiments show
that we can achieve compression rates of more than 10x for models such as
ResNet-34 while maintaining similar performance to the input `teacher' network.
We also present a valuable transfer learning result which shows that policies
which are pre-trained on smaller `teacher' networks can be used to rapidly
speed up training on larger `teacher' networks.
"
"  We propose a method (TT-GP) for approximate inference in Gaussian Process
(GP) models. We build on previous scalable GP research including stochastic
variational inference based on inducing inputs, kernel interpolation, and
structure exploiting algebra. The key idea of our method is to use Tensor Train
decomposition for variational parameters, which allows us to train GPs with
billions of inducing inputs and achieve state-of-the-art results on several
benchmarks. Further, our approach allows for training kernels based on deep
neural networks without any modifications to the underlying GP model. A neural
network learns a multidimensional embedding for the data, which is used by the
GP to make the final prediction. We train GP and neural network parameters
end-to-end without pretraining, through maximization of GP marginal likelihood.
We show the efficiency of the proposed approach on several regression and
classification benchmark datasets including MNIST, CIFAR-10, and Airline.
"
"  Vasculature is known to be of key biological significance, especially in the
study of cancer. As such, considerable effort has been focused on the automated
measurement and analysis of vasculature in medical and pre-clinical images. In
tumors in particular, the vascular networks may be extremely irregular and the
appearance of the individual vessels may not conform to classical descriptions
of vascular appearance. Typically, vessels are extracted by either a
segmentation and thinning pipeline, or by direct tracking. Neither of these
methods are well suited to microscopy images of tumor vasculature. In order to
address this we propose a method to directly extract a medial representation of
the vessels using Convolutional Neural Networks. We then show that these
two-dimensional centerlines can be meaningfully extended into 3D in anisotropic
and complex microscopy images using the recently popularized Convolutional Long
Short-Term Memory units (ConvLSTM). We demonstrate the effectiveness of this
hybrid convolutional-recurrent architecture over both 2D and 3D convolutional
comparators.
"
"  In this paper, we propose a probabilistic parsing model, which defines a
proper conditional probability distribution over non-projective dependency
trees for a given sentence, using neural representations as inputs. The neural
network architecture is based on bi-directional LSTM-CNNs which benefits from
both word- and character-level representations automatically, by using
combination of bidirectional LSTM and CNN. On top of the neural network, we
introduce a probabilistic structured layer, defining a conditional log-linear
model over non-projective trees. We evaluate our model on 17 different
datasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree
Theorem (Tutte, 1984), the partition functions and marginals can be computed
efficiently, leading to a straight-forward end-to-end model training procedure
via back-propagation. Our parser achieves state-of-the-art parsing performance
on nine datasets.
"
"  In this letter, we propose a new identification criterion that guarantees the
recovery of the low-rank latent factors in the nonnegative matrix factorization
(NMF) model, under mild conditions. Specifically, using the proposed criterion,
it suffices to identify the latent factors if the rows of one factor are
\emph{sufficiently scattered} over the nonnegative orthant, while no structural
assumption is imposed on the other factor except being full-rank. This is by
far the mildest condition under which the latent factors are provably
identifiable from the NMF model.
"
"  Supervisory control synthesis encounters with computational complexity. This
can be reduced by decentralized supervisory control approach. In this paper, we
define intrinsic control consistency for a pair of states of the plant.
G-control consistency (GCC) is another concept which is defined for a natural
projection w.r.t. the plant. We prove that, if a natural projection is output
control consistent for the closed language of the plant, and is a natural
observer for the marked language of the plant, then it is G-control consistent.
Namely, we relax the conditions for synthesis the optimal non-blocking
decentralized supervisory control by substituting GCC property for L-OCC and
Lm-observer properties of a natural projection. We propose a method to
synthesize the optimal non-blocking decentralized supervisory control based on
GCC property for a natural projection. In fact, we change the approach from
language-based properties of a natural projection to DES-based property by
defining GCC property.
"
"  Agents vote to choose a fair mixture of public outcomes; each agent likes or
dislikes each outcome. We discuss three outstanding voting rules. The
Conditional Utilitarian rule, a variant of the random dictator, is
Strategyproof and guarantees to any group of like-minded agents an influence
proportional to its size. It is easier to compute and more efficient than the
familiar Random Priority rule. Its worst case (resp. average) inefficiency is
provably (resp. in numerical experiments) low if the number of agents is low.
The efficient Egalitarian rule protects similarly individual agents but not
coalitions. It is Excludable Strategyproof: I do not want to lie if I cannot
consume outcomes I claim to dislike. The efficient Nash Max Product rule offers
the strongest welfare guarantees to coalitions, who can force any outcome with
a probability proportional to their size. But it fails even the excludable form
of Strategyproofness.
"
"  Recent advances in learning Deep Neural Network (DNN) architectures have
received a great deal of attention due to their ability to outperform
state-of-the-art classifiers across a wide range of applications, with little
or no feature engineering. In this paper, we broadly study the applicability of
deep learning to website fingerprinting. We show that unsupervised DNNs can be
used to extract low-dimensional feature vectors that improve the performance of
state-of-the-art website fingerprinting attacks. When used as classifiers, we
show that they can match or exceed performance of existing attacks across a
range of application scenarios, including fingerprinting Tor website traces,
fingerprinting search engine queries over Tor, defeating fingerprinting
defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs
can be used to predict the fingerprintability of a website based on its
contents, achieving 99% accuracy on a data set of 4500 website downloads.
"
"  We consider generalizations of the familiar fifteen-piece sliding puzzle on
the 4 by 4 square grid. On larger grids with more pieces and more holes,
asymptotically how fast can we move the puzzle into the solved state? We also
give a variation with sliding hexagons. The square puzzles and the hexagon
puzzles are both discrete versions of configuration spaces of disks, which are
of interest in statistical mechanics and topological robotics. The
combinatorial theorems and proofs in this paper suggest followup questions in
both combinatorics and topology, and may turn out to be useful for proving
topological statements about configuration spaces.
"
"  Measuring gases for air quality monitoring is a challenging task that claims
a lot of time of observation and large numbers of sensors. The aim of this
project is to develop a partially autonomous unmanned aerial vehicle (UAV)
equipped with sensors, in order to monitor and collect air quality real time
data in designated areas and send it to the ground base. This project is
designed and implemented by a multidisciplinary team from electrical and
computer engineering departments. The electrical engineering team responsible
for implementing air quality sensors for detecting real time data and transmit
it from the plane to the ground. On the other hand, the computer engineering
team is in charge of Interface sensors and provide platform to view and
visualize air quality data and live video streaming. The proposed project
contains several sensors to measure Temperature, Humidity, Dust, CO, CO2 and
O3. The collected data is transmitted to a server over a wireless internet
connection and the server will store, and supply these data to any party who
has permission to access it through android phone or website in semi-real time.
The developed UAV has carried several field tests in Al Shamal airport in
Qatar, with interesting results and proof of concept outcomes.
"
"  In this paper, the problem of maximizing a black-box function $f:\mathcal{X}
\to \mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process
(GP) prior. In particular, a new algorithm for this problem is proposed, and
high probability bounds on its simple and cumulative regret are established.
The query point selection rule in most existing methods involves an exhaustive
search over an increasingly fine sequence of uniform discretizations of
$\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines
$\mathcal{X}$ which leads to a lower computational complexity, particularly
when $\mathcal{X}$ is a subset of a high dimensional Euclidean space. In
addition to the computational gains, sufficient conditions are identified under
which the regret bounds of the new algorithm improve upon the known results.
Finally an extension of the algorithm to the case of contextual bandits is
proposed, and high probability bounds on the contextual regret are presented.
"
"  Bias is a common problem in today's media, appearing frequently in text and
in visual imagery. Users on social media websites such as Twitter need better
methods for identifying bias. Additionally, activists --those who are motivated
to effect change related to some topic, need better methods to identify and
counteract bias that is contrary to their mission. With both of these use cases
in mind, in this paper we propose a novel tool called UnbiasedCrowd that
supports identification of, and action on bias in visual news media. In
particular, it addresses the following key challenges (1) identification of
bias; (2) aggregation and presentation of evidence to users; (3) enabling
activists to inform the public of bias and take action by engaging people in
conversation with bots. We describe a preliminary study on the Twitter platform
that explores the impressions that activists had of our tool, and how people
reacted and engaged with online bots that exposed visual bias. We conclude by
discussing design and implication of our findings for creating future systems
to identify and counteract the effects of news bias.
"
"  Following the presentation and proof of the hypothesis that image features
are particularly perceived at points where the Fourier components are maximally
in phase, the concept of phase congruency (PC) is introduced. Subsequently, a
two-dimensional multi-scale phase congruency (2D-MSPC) is developed, which has
been an important tool for detecting and evaluation of image features. However,
the 2D-MSPC requires many parameters to be appropriately tuned for optimal
image features detection. In this paper, we defined a criterion for parameter
optimization of the 2D-MSPC, which is a function of its maximum and minimum
moments. We formulated the problem in various optimal and suboptimal
frameworks, and discussed the conditions and features of the suboptimal
solutions. The effectiveness of the proposed method was verified through
several examples, ranging from natural objects to medical images from patients
with a neurological disease, multiple sclerosis.
"
"  Detecting and evaluating regions of brain under various circumstances is one
of the most interesting topics in computational neuroscience. However, the
majority of the studies on detecting communities of a functional connectivity
network of the brain is done on networks obtained from coherency attributes,
and not from correlation. This lack of studies, in part, is due to the fact
that many common methods for clustering graphs require the nodes of the network
to be `positively' linked together, a property that is guaranteed by a
coherency matrix, by definition. However, correlation matrices reveal more
information regarding how each pair of nodes are linked together. In this
study, for the first time we simultaneously examine four inherently different
network clustering methods (spectral, heuristic, and optimization methods)
applied to the functional connectivity networks of the CA1 region of the
hippocampus of an anaesthetized rat during pre-ictal and post-ictal states. The
networks are obtained from correlation matrices, and its results are compared
with the ones obtained by applying the same methods to coherency matrices. The
correlation matrices show a much finer community structure compared to the
coherency matrices. Furthermore, we examine the potential smoothing effect of
choosing various window sizes for computing the correlation/coherency matrices.
"
"  Ridesourcing platforms like Uber and Didi are getting more and more popular
around the world. However, unauthorized ridesourcing activities taking
advantages of the sharing economy can greatly impair the healthy development of
this emerging industry. As the first step to regulate on-demand ride services
and eliminate black market, we design a method to detect ridesourcing cars from
a pool of cars based on their trajectories. Since licensed ridesourcing car
traces are not openly available and may be completely missing in some cities
due to legal issues, we turn to transferring knowledge from public transport
open data, i.e, taxis and buses, to ridesourcing detection among ordinary
vehicles. We propose a two-stage transfer learning framework. In Stage 1, we
take taxi and bus data as input to learn a random forest (RF) classifier using
trajectory features shared by taxis/buses and ridesourcing/other cars. Then, we
use the RF to label all the candidate cars. In Stage 2, leveraging the subset
of high confident labels from the previous stage as input, we further learn a
convolutional neural network (CNN) classifier for ridesourcing detection, and
iteratively refine RF and CNN, as well as the feature set, via a co-training
process. Finally, we use the resulting ensemble of RF and CNN to identify the
ridesourcing cars in the candidate pool. Experiments on real car, taxi and bus
traces show that our transfer learning framework, with no need of a pre-labeled
ridesourcing dataset, can achieve similar accuracy as the supervised learning
methods.
"
"  Managing dynamic information in large multi-site, multi-species, and
multi-discipline consortia is a challenging task for data management
applications. Often in academic research studies the goals for informatics
teams are to build applications that provide extract-transform-load (ETL)
functionality to archive and catalog source data that has been collected by the
research teams. In consortia that cross species and methodological or
scientific domains, building interfaces that supply data in a usable fashion
and make intuitive sense to scientists from dramatically different backgrounds
increases the complexity for developers. Further, reusing source data from
outside one's scientific domain is fraught with ambiguities in understanding
the data types, analysis methodologies, and how to combine the data with those
from other research teams. We report on the design, implementation, and
performance of a semantic data management application to support the NIMH
funded Conte Center at the University of California, Irvine. The Center is
testing a theory of the consequences of ""fragmented"" (unpredictable, high
entropy) early-life experiences on adolescent cognitive and emotional outcomes
in both humans and rodents. It employs cross-species neuroimaging, epigenomic,
molecular, and neuroanatomical approaches in humans and rodents to assess the
potential consequences of fragmented unpredictable experience on brain
structure and circuitry. To address this multi-technology, multi-species
approach, the system uses semantic web techniques based on the Neuroimaging
Data Model (NIDM) to facilitate data ETL functionality. We find this approach
enables a low-cost, easy to maintain, and semantically meaningful information
management system, enabling the diverse research teams to access and use the
data.
"
"  We describe a fully data driven model that learns to perform a retrosynthetic
reaction prediction task, which is treated as a sequence-to-sequence mapping
problem. The end-to-end trained model has an encoder-decoder architecture that
consists of two recurrent neural networks, which has previously shown great
success in solving other sequence-to-sequence prediction tasks such as machine
translation. The model is trained on 50,000 experimental reaction examples from
the United States patent literature, which span 10 broad reaction types that
are commonly used by medicinal chemists. We find that our model performs
comparably with a rule-based expert system baseline model, and also overcomes
certain limitations associated with rule-based expert systems and with any
machine learning approach that contains a rule-based expert system component.
Our model provides an important first step towards solving the challenging
problem of computational retrosynthetic analysis.
"
"  Calcium imaging permits optical measurement of neural activity. Since
intracellular calcium concentration is an indirect measurement of neural
activity, computational tools are necessary to infer the true underlying
spiking activity from fluorescence measurements. Bayesian model inversion can
be used to solve this problem, but typically requires either computationally
expensive MCMC sampling, or faster but approximate maximum-a-posteriori
optimization. Here, we introduce a flexible algorithmic framework for fast,
efficient and accurate extraction of neural spikes from imaging data. Using the
framework of variational autoencoders, we propose to amortize inference by
training a deep neural network to perform model inversion efficiently. The
recognition network is trained to produce samples from the posterior
distribution over spike trains. Once trained, performing inference amounts to a
fast single forward pass through the network, without the need for iterative
optimization or sampling. We show that amortization can be applied flexibly to
a wide range of nonlinear generative models and significantly improves upon the
state of the art in computation time, while achieving competitive accuracy. Our
framework is also able to represent posterior distributions over spike-trains.
We demonstrate the generality of our method by proposing the first
probabilistic approach for separating backpropagating action potentials from
putative synaptic inputs in calcium imaging of dendritic spines.
"
"  The popular Alternating Least Squares (ALS) algorithm for tensor
decomposition is efficient and easy to implement, but often converges to poor
local optima---particularly when the weights of the factors are non-uniform. We
propose a modification of the ALS approach that is as efficient as standard
ALS, but provably recovers the true factors with random initialization under
standard incoherence assumptions on the factors of the tensor. We demonstrate
the significant practical superiority of our approach over traditional ALS for
a variety of tasks on synthetic data---including tensor factorization on exact,
noisy and over-complete tensors, as well as tensor completion---and for
computing word embeddings from a third-order word tri-occurrence tensor.
"
"  This paper proposes a data-driven approach, by means of an Artificial Neural
Network (ANN), to value financial options and to calculate implied volatilities
with the aim of accelerating the corresponding numerical methods. With ANNs
being universal function approximators, this method trains an optimized ANN on
a data set generated by a sophisticated financial model, and runs the trained
ANN as an agent of the original solver in a fast and efficient way. We test
this approach on three different types of solvers, including the analytic
solution for the Black-Scholes equation, the COS method for the Heston
stochastic volatility model and Brent's iterative root-finding method for the
calculation of implied volatilities. The numerical results show that the ANN
solver can reduce the computing time significantly.
"
"  We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis.
"
"  This work explores the feasibility of steering a drone with a (recurrent)
neural network, based on input from a forward looking camera, in the context of
a high-level navigation task. We set up a generic framework for training a
network to perform navigation tasks based on imitation learning. It can be
applied to both aerial and land vehicles. As a proof of concept we apply it to
a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a
room containing a number of obstacles. So far only feedforward neural networks
(FNNs) have been used to train UAV control. To cope with more complex tasks, we
propose the use of recurrent neural networks (RNN) instead and successfully
train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision
based control is a sequential prediction problem, known for its highly
correlated input data. The correlation makes training a network hard,
especially an RNN. To overcome this issue, we investigate an alternative
sampling method during training, namely window-wise truncated backpropagation
through time (WW-TBPTT). Further, end-to-end training requires a lot of data
which often is not available. Therefore, we compare the performance of
retraining only the Fully Connected (FC) and LSTM control layers with networks
which are trained end-to-end. Performing the relatively simple task of crossing
a room already reveals important guidelines and good practices for training
neural control networks. Different visualizations help to explain the behavior
learned.
"
"  Locality-sensitive hashing (LSH) is a fundamental technique for similarity
search and similarity estimation in high-dimensional spaces. The basic idea is
that similar objects should produce hash collisions with probability
significantly larger than objects with low similarity. We consider LSH for
objects that can be represented as point sets in either one or two dimensions.
To make the point sets finite size we consider the subset of points on a grid.
Directly applying LSH (e.g. min-wise hashing) to these point sets would require
time proportional to the number of points. We seek to achieve time that is much
lower than direct approaches.
Technically, we introduce new primitives for range-efficient consistent
sampling (of independent interest), and show how to turn such samples into LSH
values. Another application of our technique is a data structure for quickly
estimating the size of the intersection or union of a set of preprocessed
polygons. Curiously, our consistent sampling method uses transformation to a
geometric problem.
"
"  A commonly cited inefficiency of neural network training by back-propagation
is the update locking problem: each layer must wait for the signal to propagate
through the network before updating. We consider and analyze a training
procedure, Decoupled Greedy Learning (DGL), that addresses this problem more
effectively and at scales beyond those of previous solutions. It is based on a
greedy relaxation of the joint training objective, recently shown to be
effective in the context of Convolutional Neural Networks (CNNs) on large-scale
image classification. We consider an optimization of this objective that
permits us to decouple the layer training, allowing for layers or modules in
networks to be trained with a potentially linear parallelization in layers. We
show theoretically and empirically that this approach converges. In addition,
we empirically find that it can lead to better generalization than sequential
greedy optimization and even standard end-to-end back-propagation. We show that
an extension of this approach to asynchronous settings, where modules can
operate with large communication delays, is possible with the use of a replay
buffer. We demonstrate the effectiveness of DGL on the CIFAR-10 datasets
against alternatives and on the large-scale ImageNet dataset, where we are able
to effectively train VGG and ResNet-152 models.
"
"  We establish a Pontryagin maximum principle for discrete time optimal control
problems under the following three types of constraints: a) constraints on the
states pointwise in time, b) constraints on the control actions pointwise in
time, and c) constraints on the frequency spectrum of the optimal control
trajectories. While the first two types of constraints are already included in
the existing versions of the Pontryagin maximum principle, it turns out that
the third type of constraints cannot be recast in any of the standard forms of
the existing results for the original control system. We provide two different
proofs of our Pontryagin maximum principle in this article, and include several
special cases fine-tuned to control-affine nonlinear and linear system models.
In particular, for minimization of quadratic cost functions and linear time
invariant control systems, we provide tight conditions under which the optimal
controls under frequency constraints are either normal or abnormal.
"
"  It is well established that neural networks with deep architectures perform
better than shallow networks for many tasks in machine learning. In statistical
physics, while there has been recent interest in representing physical data
with generative modelling, the focus has been on shallow neural networks. A
natural question to ask is whether deep neural networks hold any advantage over
shallow networks in representing such data. We investigate this question by
using unsupervised, generative graphical models to learn the probability
distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep
belief networks, and deep restricted Boltzmann networks are trained on thermal
spin configurations from this system, and compared to the shallow architecture
of the restricted Boltzmann machine. We benchmark the models, focussing on the
accuracy of generating energetic observables near the phase transition, where
these quantities are most difficult to approximate. Interestingly, after
training the generative networks, we observe that the accuracy essentially
depends only on the number of neurons in the first hidden layer of the network,
and not on other model details such as network depth or model type. This is
evidence that shallow networks are more efficient than deep networks at
representing physical probability distributions associated with Ising systems
near criticality.
"
"  For any stream of time-stamped edges that form a dynamic network, an
important choice is the aggregation granularity that an analyst uses to bin the
data. Picking such a windowing of the data is often done by hand, or left up to
the technology that is collecting the data. However, the choice can make a big
difference in the properties of the dynamic network. This is the time scale
detection problem. In previous work, this problem is often solved with a
heuristic as an unsupervised task. As an unsupervised problem, it is difficult
to measure how well a given algorithm performs. In addition, we show that the
quality of the windowing is dependent on which task an analyst wants to perform
on the network after windowing. Therefore the time scale detection problem
should not be handled independently from the rest of the analysis of the
network.
We introduce a framework that tackles both of these issues: By measuring the
performance of the time scale detection algorithm based on how well a given
task is accomplished on the resulting network, we are for the first time able
to directly compare different time scale detection algorithms to each other.
Using this framework, we introduce time scale detection algorithms that take a
supervised approach: they leverage ground truth on training data to find a good
windowing of the test data. We compare the supervised approach to previous
approaches and several baselines on real data.
"
"  We discover a population of short-period, Neptune-size planets sharing key
similarities with hot Jupiters: both populations are preferentially hosted by
metal-rich stars, and both are preferentially found in Kepler systems with
single transiting planets. We use accurate LAMOST DR4 stellar parameters for
main-sequence stars to study the distributions of short-period 1d < P < 10d
Kepler planets as a function of host star metallicity. The radius distribution
of planets around metal-rich stars is more ""puffed up"" as compared to that
around metal-poor hosts. In two period-radius regimes, planets preferentially
reside around metal-rich stars, while there are hardly any planets around
metal-poor stars. One is the well-known hot Jupiters, and the other is a
population of Neptune-size planets (2 R_Earth <~ R_p <~ 6 R_Earth), dubbed as
""Hoptunes"". Also like hot Jupiters, Hoptunes occur more frequently in systems
with single transiting planets though the fraction of Hoptunes occurring in
multiples is larger than that of hot Jupiters. About 1% of solar-type stars
host ""Hoptunes"", and the frequencies of Hoptunes and hot Jupiters increase with
consistent trends as a function of [Fe/H]. In the planet radius distribution,
hot Jupiters and Hoptunes are separated by a ""valley"" at approximately Saturn
size (in the range of 6 R_Earth <~ R_p <~ 10 R_Earth), and this ""hot-Saturn
valley"" represents approximately an order-of-magnitude decrease in planet
frequency compared to hot Jupiters and Hoptunes. The empirical ""kinship""
between Hoptunes and hot Jupiters suggests likely common processes (migration
and/or formation) responsible for their existence.
"
"  We investigate the accuracy and robustness of one of the most common methods
used in glaciology for the discretization of the $\mathfrak{p}$-Stokes
equations: equal order finite elements with Galerkin Least-Squares (GLS)
stabilization. Furthermore we compare the results to other stabilized methods.
We find that the vertical velocity component is more sensitive to the choice of
GLS stabilization parameter than horizontal velocity. Additionally, the
accuracy of the vertical velocity component is especially important since
errors in this component can cause ice surface instabilities and propagate into
future ice volume predictions. If the element cell size is set to the minimum
edge length and the stabilization parameter is allowed to vary non-linearly
with viscosity, the GLS stabilization parameter found in literature is a good
choice on simple domains. However, near ice margins the standard parameter
choice may result in significant oscillations in the vertical component of the
surface velocity. For these cases, other stabilization techniques, such as the
interior penalty method, result in better accuracy and are less sensitive to
the choice of the stabilization parameter. During this work we also discovered
that the manufactured solutions often used to evaluate errors in glaciology are
not reliable due to high artificial surface forces at singularities. We perform
our numerical experiments in both FEniCS and Elmer/Ice.
"
"  The use of computers in statistical physics is common because the sheer
number of equations that describe the behavior of an entire system particle by
particle often makes it impossible to solve them exactly. Monte Carlo methods
form a particularly important class of numerical methods for solving problems
in statistical physics. Although these methods are simple in principle, their
proper use requires a good command of statistical mechanics, as well as
considerable computational resources. The aim of this paper is to demonstrate
how the usage of widely accessible graphics cards on personal computers can
elevate the computing power in Monte Carlo simulations by orders of magnitude,
thus allowing live classroom demonstration of phenomena that would otherwise be
out of reach. As an example, we use the public goods game on a square lattice
where two strategies compete for common resources in a social dilemma
situation. We show that the second-order phase transition to an absorbing phase
in the system belongs to the directed percolation universality class, and we
compare the time needed to arrive at this result by means of the main processor
and by means of a suitable graphics card. Parallel computing on graphics
processing units has been developed actively during the last decade, to the
point where today the learning curve for entry is anything but steep for those
familiar with programming. The subject is thus ripe for inclusion in graduate
and advanced undergraduate curricula, and we hope that this paper will
facilitate this process in the realm of physics education. To that end, we
provide a documented source code for an easy reproduction of presented results
and for further development of Monte Carlo simulations of similar systems.
"
"  We consider a helical system of fermions with a generic spin (or pseudospin)
orbit coupling. Using the equation of motion approach for the single-particle
distribution functions, and a mean-field decoupling of the higher order
distribution functions, we find a closed form for the charge and spin density
fluctuations in terms of the charge and spin density linear response functions.
Approximating the nonlocal exchange term with a Hubbard-like local-field
factor, we obtain coupled spin and charge density response matrix beyond the
random phase approximation, whose poles give the dispersion of four collective
spin-charge modes. We apply our generic technique to the well-explored
two-dimensional system with Rashba spin-orbit coupling and illustrate how it
gives results for the collective modes, Drude weight, and spin-Hall
conductivity which are in very good agreement with the results obtained from
other more sophisticated approaches.
"
"  We study correlations in fermionic lattice systems with long-range
interactions in thermal equilibrium. We prove a bound on the correlation decay
between anti-commuting operators and generalize a long-range Lieb-Robinson type
bound. Our results show that in these systems of spatial dimension $D$ with,
not necessarily translation invariant, two-site interactions decaying
algebraically with the distance with an exponent $\alpha \geq 2\,D$,
correlations between such operators decay at least algebraically with an
exponent arbitrarily close to $\alpha$ at any non-zero temperature. Our bound
is asymptotically tight, which we demonstrate by a high temperature expansion
and by numerically analyzing density-density correlations in the 1D quadratic
(free, exactly solvable) Kitaev chain with long-range pairing.
"
"  We present an integrated microsimulation framework to estimate the pedestrian
movement over time and space with limited data on directional counts. Using the
activity-based approach, simulation can compute the overall demand and
trajectory of each agent, which are in accordance with the available partial
observations and are in response to the initial and evolving supply conditions
and schedules. This simulation contains a chain of processes including:
activities generation, decision point choices, and assignment. They are
considered in an iteratively updating loop so that the simulation can
dynamically correct its estimates of demand. A Markov chain is constructed for
this loop. These considerations transform the problem into a convergence
problem. A Metropolitan Hasting algorithm is then adapted to identify the
optimal solution. This framework can be used to fill the lack of data or to
model the reactions of demand to exogenous changes in the scenario. Finally, we
present a case study on Montreal Central Station, on which we tested the
developed framework and calibrated the models. We then applied it to a possible
future scenario for the same station.
"
"  We present an approach to testing the gravitational redshift effect using the
RadioAstron satellite. The experiment is based on a modification of the Gravity
Probe A scheme of nonrelativistic Doppler compensation and benefits from the
highly eccentric orbit and ultra-stable atomic hydrogen maser frequency
standard of the RadioAstron satellite. Using the presented techniques we expect
to reach an accuracy of the gravitational redshift test of order $10^{-5}$, a
magnitude better than that of Gravity Probe A. Data processing is ongoing, our
preliminary results agree with the validity of the Einstein Equivalence
Principle.
"
"  Artificial Spin Ice (ASI), consisting of a two dimensional array of nanoscale
magnetic elements, provides a fascinating opportunity to observe the physics of
out of equilibrium systems. Initial studies concentrated on the static, frozen
state, whilst more recent studies have accessed the out-of-equilibrium dynamic,
fluctuating state. This opens up exciting possibilities such as the observation
of systems exploring their energy landscape through monopole quasiparticle
creation, potentially leading to ASI magnetricity, and to directly observe
unconventional phase transitions. In this work we have measured and analysed
the magnetic relaxation of thermally active ASI systems by means of SQUID
magnetometry. We have investigated the effect of the interaction strength on
the magnetization dynamics at different temperatures in the range where the
nanomagnets are thermally active and have observed that they follow an
Arrhenius-type Néel-Brown behaviour. An unexpected negative correlation of
the average blocking temperature with the interaction strength is also
observed, which is supported by Monte Carlo simulations. The magnetization
relaxation measurements show faster relaxation for more strongly coupled
nanoelements with similar dimensions. The analysis of the stretching exponents
obtained from the measurements suggest 1-D chain-like magnetization dynamics.
This indicates that the nature of the interactions between nanoelements lowers
the dimensionality of the ASI from 2-D to 1-D. Finally, we present a way to
quantify the effective interaction energy of a square ASI system, and compare
it to the interaction energy calculated from a simple dipole model and also to
the magnetostatic energy computed with micromagnetic simulations.
"
"  We theoretically study a scheme to develop an atomic based MW interferometry
using the Rydberg states in Rb. Unlike the traditional MW interferometry, this
scheme is not based upon the electrical circuits, hence the sensitivity of the
phase and the amplitude/strength of the MW field is not limited by the Nyquist
thermal noise. Further this system has great advantage due to its very high
bandwidth, ranging from radio frequency (RF), micro wave (MW) to terahertz
regime. In addition, this is \textbf{orders of magnitude} more sensitive to
field strength as compared to the prior demonstrations on the MW electrometry
using the Rydberg atomic states. However previously studied atomic systems are
only sensitive to the field strength but not to the phase and hence this scheme
provides a great opportunity to characterize the MW completely including the
propagation direction and the wavefront. This study opens up a new dimension in
the Radar technology such as in synthetic aperture radar interferometry. The MW
interferometry is based upon a six-level loopy ladder system involving the
Rydberg states in which two sub-systems interfere constructively or
destructively depending upon the phase between the MW electric fields closing
the loop.
"
"  We present a new method for the separation of superimposed, independent,
auto-correlated components from noisy multi-channel measurement. The presented
method simultaneously reconstructs and separates the components, taking all
channels into account and thereby increases the effective signal-to-noise ratio
considerably, allowing separations even in the high noise regime.
Characteristics of the measurement instruments can be included, allowing for
application in complex measurement situations. Independent posterior samples
can be provided, permitting error estimates on all desired quantities. Using
the concept of information field theory, the algorithm is not restricted to any
dimensionality of the underlying space or discretization scheme thereof.
"
"  GC-1 and GC-2 are two globular clusters (GCs) in the remote halo of M81 and
M82 in the M81 group discovered by Jang et al. using the {\it Hubble Space
Telescope} ({\it HST}) images. These two GCs were observed as part of the
Beijing--Arizona--Taiwan--Connecticut (BATC) Multicolor Sky Survey, using 14
intermediate-band filters covering a wavelength range of 4000--10000 \AA. We
accurately determine these two clusters' ages and masses by comparing their
spectral energy distributions (from 2267 to 20000~{\AA}, comprising photometric
data in the near-ultraviolet of the {\it Galaxy Evolution Explorer}, 14 BATC
intermediate-band, and Two Micron All Sky Survey near-infrared $JHK_{\rm s}$
filters) with theoretical stellar population-synthesis models, resulting in
ages of $15.50\pm3.20$ for GC-1 and $15.10\pm2.70$ Gyr for GC-2. The masses of
GC-1 and GC-2 obtained here are $1.77-2.04\times 10^6$ and $5.20-7.11\times
10^6 \rm~M_\odot$, respectively. In addition, the deep observations with the
Advanced Camera for Surveys and Wide Field Camera 3 on the {\it HST} are used
to provide the surface brightness profiles of GC-1 and GC-2. The structural and
dynamical parameters are derived from fitting the profiles to three different
models; in particular, the internal velocity dispersions of GC-1 and GC-2 are
derived, which can be compared with ones obtained based on spectral
observations in the future. For the first time, in this paper, the $r_h$ versus
$M_V$ diagram shows that GC-2 is an ultra-compact dwarf in the M81 group.
"
"  These notes aim at presenting an overview of Bayesian statistics, the
underlying concepts and application methodology that will be useful to
astronomers seeking to analyse and interpret a wide variety of data about the
Universe. The level starts from elementary notions, without assuming any
previous knowledge of statistical methods, and then progresses to more
advanced, research-level topics. After an introduction to the importance of
statistical inference for the physical sciences, elementary notions of
probability theory and inference are introduced and explained. Bayesian methods
are then presented, starting from the meaning of Bayes Theorem and its use as
inferential engine, including a discussion on priors and posterior
distributions. Numerical methods for generating samples from arbitrary
posteriors (including Markov Chain Monte Carlo and Nested Sampling) are then
covered. The last section deals with the topic of Bayesian model selection and
how it is used to assess the performance of models, and contrasts it with the
classical p-value approach. A series of exercises of various levels of
difficulty are designed to further the understanding of the theoretical
material, including fully worked out solutions for most of them.
"
"  Following the selection of The Gravitational Universe by ESA, and the
successful flight of LISA Pathfinder, the LISA Consortium now proposes a 4 year
mission in response to ESA's call for missions for L3. The observatory will be
based on three arms with six active laser links, between three identical
spacecraft in a triangular formation separated by 2.5 million km.
LISA is an all-sky monitor and will offer a wide view of a dynamic cosmos
using Gravitational Waves as new and unique messengers to unveil The
Gravitational Universe. It provides the closest ever view of the infant
Universe at TeV energy scales, has known sources in the form of verification
binaries in the Milky Way, and can probe the entire Universe, from its smallest
scales near the horizons of black holes, all the way to cosmological scales.
The LISA mission will scan the entire sky as it follows behind the Earth in its
orbit, obtaining both polarisations of the Gravitational Waves simultaneously,
and will measure source parameters with astrophysically relevant sensitivity in
a band from below $10^{-4}\,$Hz to above $10^{-1}\,$Hz.
"
"  Binary mixtures of dry grains avalanching down a slope are experimentally
studied in order to determine the interaction among coarse and fine grains and
their effect on the deposit morphology. The distance travelled by the massive
front of the avalanche over the horizontal plane of deposition area is measured
as a function of mass content of fine particles in the mixture, grain-size
ratio, and flume tilt. A sudden transition of the runout is detected at a
critical content of fine particles, with a dependence on the grain-size ratio
and flume tilt. This transition is explained as two simultaneous avalanches in
different flowing regimes (a viscous-like one and an inertial one) competing
against each other and provoking a full segregation and a split-off of the
deposit into two well-defined, separated deposits. The formation of the distal
deposit, in turn, depends on a critical amount of coarse particles. This allows
the condensation of the pure coarse deposit around a small, initial seed
cluster, which grows rapidly by braking and capturing subsequent colliding
coarse particles. For different grain-size ratios and keeping a constant total
mass, the change in the amount of fines needed for the transition to occur is
found to be always less than 7%. For avalanches with a total mass of 4 kg we
find that, most of the time, the runout of a binary avalanche is larger than
the runout of monodisperse avalanches of corresponding constituent particles,
due to lubrication on the coarse-dominated side or to drag by inertial
particles on the fine-dominated side.
"
"  We study the dynamics of an isotropic spin-1/2 Heisenberg chain starting in a
domain-wall initial condition, where the spins are initially up on the left
half-line and down on the right half-line. We focus on the long-time behavior
of the magnetization profile. We perform extensive time-dependent
density-matrix renormalization group simulations (up to t=350) and find that
the data are compatible with a diffusive behavior. Subleading corrections decay
slowly blurring the emergence of the diffusive behavior. We also compare our
results with two alternative scenarios: superdiffusive behavior and enhanced
diffusion with a logarithmic correction. We finally discuss the evolution of
the entanglement entropy.
"
"  We devise a new high order local absorbing boundary condition (ABC) for
radiating problems and scattering of time-harmonic acoustic waves from
obstacles of arbitrary shape. By introducing an artificial boundary $S$
enclosing the scatterer, the original unbounded domain $\Omega$ is decomposed
into a bounded computational domain $\Omega^{-}$ and an exterior unbounded
domain $\Omega^{+}$. Then, we define interface conditions at the artificial
boundary $S$, from truncated versions of the well-known Wilcox and Karp
farfield expansion representations of the exact solution in the exterior region
$\Omega^{+}$. As a result, we obtain a new local absorbing boundary condition
(ABC) for a bounded problem on $\Omega^{-}$, which effectively accounts for the
outgoing behavior of the scattered field. Contrary to the low order absorbing
conditions previously defined, the order of the error induced by this ABC can
easily match the order of the numerical method in $\Omega^{-}$. We accomplish
this by simply adding as many terms as needed to the truncated farfield
expansions of Wilcox or Karp. The convergence of these expansions guarantees
that the order of approximation of the new ABC can be increased arbitrarily
without having to enlarge the radius of the artificial boundary. We include
numerical results in two and three dimensions which demonstrate the improved
accuracy and simplicity of this new formulation when compared to other
absorbing boundary conditions.
"
"  We present $\texttt{BHM}$, a tool for restoring a smooth function from a
sampled histogram using the bin hierarchy method. The theoretical background of
the method is presented in [arXiv:1707.07625]. The code automatically generates
a smooth polynomial spline with the minimal acceptable number of knots from the
input data. It works universally for any sufficiently regular shaped
distribution and any level of data quality, requiring almost no external
parameter specification. It is particularly useful for large-scale numerical
data analysis. This paper explains the details of the implementation and the
use of the program.
"
"  We study the seasonal evolution of Titan's lower stratosphere (around
15~mbar) in order to better understand the atmospheric dynamics and chemistry
in this part of the atmosphere. We analysed Cassini/CIRS far-IR observations
from 2006 to 2016 in order to measure the seasonal variations of three
photochemical by-products: $\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and
$\mathrm{C_2N_2}$. We show that the abundances of these three gases have
evolved significantly at northern and southern high latitudes since 2006. We
measure a sudden and steep increase of the volume mixing ratios of
$\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and $\mathrm{C_2N_2}$ at the south pole
from 2012 to 2013, whereas the abundances of these gases remained approximately
constant at the north pole over the same period. At northern mid-latitudes,
$\mathrm{C_2N_2}$ and $\mathrm{C_4H_2}$ abundances decrease after 2012 while
$\mathrm{C_3H_4}$ abundances stay constant. The comparison of these volume
mixing ratio variations with the predictions of photochemical and dynamical
models provides constraints on the seasonal evolution of atmospheric
circulation and chemical processes at play.
"
"  The Percus-Yevick theory for monodisperse hard spheres gives very good
results for the pressure and structure factor of the system in a whole range of
densities that lie within the liquid phase. However, the equation seems to lead
to a very unacceptable result beyond that region. Namely, the Percus-Yevick
theory predicts a smooth behavior of the pressure that diverges only when the
volume fraction $\eta$ approaches unity. Thus, within the theory there seems to
be no indication for the termination of the liquid phase and the transition to
a solid or to a glass. In the present article we study the Percus-Yevick hard
sphere pair distribution function, $g_2(r)$, for various spatial dimensions. We
find that beyond a certain critical volume fraction $\eta_c$, the pair
distribution function, $g_2(r)$, which should be positive definite, becomes
negative at some distances. We also present an intriguing observation that the
critical $\eta_c$ values we find are consistent with volume fractions where
onsets of random close packing (or maximally random jammed states) are reported
in the literature for various dimensions. That observation is supported by an
intuitive argument. This work may have important implications for other systems
for which a Percus-Yevick theory exists.
"
"  We test the $\mathbb{C}P^{N-1}$ sigma models for the Painlevé property.
While the construction of finite action solutions ensures their meromorphicity,
the general case requires testing. The test is performed for the equations in
the homogeneous variables, with their first component normalised to one. No
constraints are imposed on the dimensionality of the model or the values of the
initial exponents. This makes the test nontrivial, as the number of equations
and dependent variables are indefinite. A $\mathbb{C}P^{N-1}$ system proves to
have a $(4N-5)$-parameter family of solutions whose movable singularities are
only poles, while the order of the investigated system is $4N-4$. The remaining
degree of freedom, connected with an extra negative resonance, may correspond
to a branching movable essential singularity. An example of such a solution is
provided.
"
"  The intersecting pedestrian flow on the 2D lattice with random update rule is
studied. Each pedestrian has three moving directions without the back step.
Under periodic boundary conditions, an intermediate phase has been found at
which some pedestrians could move along the border of jamming stripes. We have
performed mean field analysis for the moving and intermediate phase
respectively. The analytical results agree with the simulation results well.
The empty site moves along the interface of jamming stripes when the system
only has one empty site. The average movement of empty site in one Monte Carlo
step (MCS) has been analyzed through the master equation. Under open boundary
conditions, the system exhibits moving and jamming phases. The critical
injection probability $\alpha_c$ shows nontrivially against the forward moving
probability $q$. The analytical results of average velocity, the density and
the flow rate against the injection probability in the moving phase also agree
with simulation results well.
"
"  LiOsO$_3$ is the first example of a new class of material called a
ferroelectric metal. We performed zero-field and longitudinal-field $\mu$SR,
along with a combination of electronic structure and dipole field calculations,
to determine the magnetic ground state of LiOsO$_3$. We find that the sample
contains both static Li nuclear moments and dynamic Os electronic moments.
Below $\approx 0.7\,$K, the fluctuations of the Os moments slow down, though
remain dynamic down to 0.08$\,$K. We expect this could result in a frozen-out,
disordered ground state at even lower temperatures.
"
"  We consider a Josephson junction consisting of superconductor/ferromagnetic
insulator (S/FI) bilayers as electrodes which proximizes a nearby 2D electron
gas. By starting from a generic Josephson hybrid planar setup we present an
exhaustive analysis of the the interplay between the superconducting and
magnetic proximity effects and the conditions under which the structure
undergoes transitions to a non-trivial topological phase. We address the 2D
bound state problem using a general transfer matrix approach that reduces the
problem to an effective 1D Hamiltonian. This allows for straightforward study
of topological properties in different symmetry classes. As an example we
consider a narrow channel coupled with multiple ferromagnetic superconducting
fingers, and discuss how the Majorana bound states can be spatially controlled
by tuning the superconducting phases. Following our approach we also show the
energy spectrum, the free energy and finally the multiterminal Josephson
current of the setup.
"
"  Rashba spin orbit coupling in topological insulators has attracted much
interest due to its exotic properties closely related to spintronic devices.
The coexistence of nontrivial topology and giant Rashba splitting, however, has
rare been observed in two-dimensional films, limiting severely its potential
applications at room temperature. Here, we propose a series of inversion
asymmetric group IV films, ABZ2, whose stability are confirmed by phonon
spectrum calculations. The analyses of electronic structures reveal that they
are intrinsic 2D TIs with a bulk gap as large as 0.74 eV, except for GeSiF2,
SnSiCl2, GeSiCl2 and GeSiBr2 monolayers which can transform from normal to
topological phases under appropriate tensile strains. Another prominent
intriguing feature is the giant Rashba spin splitting with a magnitude reaching
0.15 eV, the largest value reported in 2D films. These results present a
platform to explore 2D TIs for room temperature device applications.
"
"  A Floquet systems is a periodically driven quantum system. It can be
described by a Floquet operator. If this unitary operator has a gap in the
spectrum, then one can define associated topological bulk invariants which can
either only depend on the bands of the Floquet operator or also on the time as
a variable. It is shown how a K-theoretic result combined with the
bulk-boundary correspondence leads to edge invariants for the half-space
Floquet operators. These results also apply to topological quantum walks.
"
"  Uranium beryllium-13 is a heavy fermion system whose anomalous behavior may
be explained by its poorly understood internal magnetic structure. Here,
uranium beryllium-13's magnetic distribution is probed via muon spin
spectroscopy ($\mu$SR)-a process where positive muons localize at magnetically
unique sites in the crystal lattice and precess at characteristic Larmor
frequencies, providing measurements of the internal field. Muon spin
experiments using the transverse-field technique conducted at varying
temperatures and external magnetic field strengths are analyzed via statistical
methods on ROOT. Two precession frequencies are observed at low temperatures
with an amplitude ratio in the Fourier transform of 2:1, enabling muon stopping
sites to be traced at the geometric centers of the edges of the crystal
lattice. Characteristic strong and weak magnetic sites are deduced,
additionally verified by mathematical relationships. Results can readily be
applied to other heavy fermion systems, and recent identification of quantum
critical points in a host of heavy fermion compounds show a promising future
for the application of these systems in quantum technology. Note that this
paper is an analysis of data, and all experiments mentioned here are conducted
by a third party.
"
"  We revisit the study of the phenomenology associated to a burst of particle
production of a field whose mass is controlled by the inflaton field and
vanishes at one given instance during inflation. This generates a bump in the
correlators of the primordial scalar curvature. We provide a unified formalism
to compute various effects that have been obtained in the literature and
confirm that the dominant effects are due to the rescattering of the produced
particles on the inflaton condensate. We improve over existing results (based
on numerical fits) by providing exact analytic expressions for the shape and
height of the bump, both in the power spectrum and the equilateral bispectrum.
We then study the regime of validity of the perturbative computations of this
signature. Finally, we extend these computations to the case of a burst of
particle production in a sector coupled only gravitationally to the inflaton.
"
"  A systematic experimental study of Gilbert damping is performed via
ferromagnetic resonance for the disordered crystalline binary 3d transition
metal alloys Ni-Co, Ni-Fe and Co-Fe over the full range of alloy compositions.
After accounting for inhomogeneous linewidth broadening, the damping shows
clear evidence of both interfacial damping enhancement (by spin pumping) and
radiative damping. We quantify these two extrinsic contributions and thereby
determine the intrinsic damping. The comparison of the intrinsic damping to
multiple theoretical calculations yields good qualitative and quantitative
agreement in most cases. Furthermore, the values of the damping obtained in
this study are in good agreement with a wide range of published experimental
and theoretical values. Additionally, we find a compositional dependence of the
spin mixing conductance.
"
"  We show, in the case of a special dipolar source, that electromagnetic fields
in fractional quantum mechanics have an unexpected space dependence:
propagating fields may have non-transverse components, and the distinction
between near-field zone and wave zone is blurred. We employ an extension of
Maxwell theory, Aharonov-Bohm electrodynamics, which is compatible with
currents $j^\nu$ conserved globally but not locally, we have derived in another
work the field equation $\partial_\mu F^{\mu \nu}=j^\nu+i^\nu$, where $i^\nu$
is a non-local function of $j^\nu$, called ""secondary current"". Y.\ Wei has
recently proved that the probability current in fractional quantum mechanics is
in general not locally conserved. We compute this current for a Gaussian wave
packet with fractional parameter $a=3/2$ and find that in a suitable limit it
can be approximated by our simplified dipolar source. Currents which are not
locally conserved may be present also in other quantum systems whose wave
functions satisfy non-local equations. The combined electromagnetic effects of
such sources and their secondary currents are very interesting both
theoretically and for potential applications.
"
"  We present a general form of Renormalization operator $\mathcal{R}$ acting on
potentials $V:\{0,1\}^\mathbb{N} \to \mathbb{R}$. We exhibit the analytical
expression of the fixed point potential $V$ for such operator $\mathcal{R}$.
This potential can be expressed in a naturally way in terms of a certain
integral over the Hausdorff probability on a Cantor type set on the interval
$[0,1]$. This result generalizes a previous one by A. Baraviera, R. Leplaideur
and A. Lopes where the fixed point potential $V$ was of Hofbauer type.
For the potentials of Hofbauer type (a well known case of phase transition)
the decay is like $n^{-\gamma}$, $\gamma>0$.
Among other things we present the estimation of the decay of correlation of
the equilibrium probability associated to the fixed potential $V$ of our
general renormalization procedure. In some cases we get polynomial decay like
$n^{-\gamma}$, $\gamma>0$, and in others a decay faster than $n \,e^{ -\,
\sqrt{n}}$, when $n \to \infty$.
The potentials $g$ we consider here are elements of the so called family of
Walters potentials on $\{0,1\}^\mathbb{N} $ which generalizes the potentials
considered initially by F. Hofbauer. For these potentials some explicit
expressions for the eigenfunctions are known.
In a final section we also show that given any choice $d_n \to 0$ of real
numbers varying with $n \in \mathbb{N}$ there exist a potential $g$ on the
class defined by Walters which has a invariant probability with such numbers as
the coefficients of correlation (for a certain explicit observable function).
"
"  Cosmic ray muons with the average energy of 280 GeV and neutrons produced by
muons are detected with the Large Volume Detector at LNGS. We present an
analysis of the seasonal variation of the neutron flux on the basis of the data
obtained during 15 years. The measurement of the seasonal variation of the
specific number of neutrons generated by muons allows to obtaine the variation
magnitude of of the average energy of the muon flux at the depth of the LVD
location. The source of the seasonal variation of the total neutron flux is a
change of the intensity and the average energy of the muon flux.
"
"  When two identical (coherent) beams are injected at a semi-infinite
non-Hermitian medium from left and right, we show that both reflection
$(r_L,r_R)$ and transmission $(t_L,t_R)$ amplitudes are non-reciprocal. In a
parametric domain, there exists Spectral Singularity (SS) at a real energy
$E=E_*$ and the determinant of the time-reversed two port S-matrix i.e.,
$|\det(S)|=|t_L t_R-r_L r_R|$ vanishes sharply at $E=E_*$ displaying the
phenomenon of Coherent Perfect Absorption (CPA). In the complimentary
parametric domain, the potential becomes either left or right reflectionless at
$E=E_z$. But we rule out the existence of Invisibility despite $r_R(E_i)=0$ and
$t_R(E_i)=1$ in these new models. We present two simple exactly solvable models
where the expressions for $E_*$, $E_z$, $E_i$ and the parametric conditions on
the potential have been obtained in explicit and simple forms. Earlier, the
novel phenomena of SS and CPA have been found to occur only in the scattering
complex potentials which are spatially localized (vanish asymptotically) and
having $t_L=t_R$.
"
"  A long range corrected range separated hybrid functional is developed based
on the density matrix expansion (DME) based semilocal exchange hole with
Lee-Yang-Parr (LYP) correlation. An extensive study involving the proposed
range separated hybrid for thermodynamic as well as properties related to the
fractional occupation number is compared with different BECKE88 family
semilocal, hybrid and range separated hybrids. It has been observed that using
Kohn-Sham kinetic energy dependent exchange hole several properties related to
the fractional occupation number can be improved without hindering the
thermochemical accuracy. The newly constructed range separated hybrid
accurately describe the hydrogen and non-hydrogen reaction barrier heights. The
present range separated functional has been constructed using full semilocal
meta-GGA type exchange hole having exact properties related to exchange hole
therefore, it has a strong physical basis.
"
"  The Sagnac effect has been shown in inertial frames as well as rotating
frames. We solve the problem of the generalized Sagnac effect in the standard
synchronization of clocks. The speed of a light beam that traverses an optical
fiber loop is measured with respect to the proper time of the light detector,
and is shown to be other than the constant c, though it appears to be c if
measured by the time standard-synchronized. The fiber loop, which can have an
arbitrary shape, is described by an infinite number of straight lines such that
it can be handled by the general framework of Mansouri and Sexl (MS). For a
complete analysis of the Sagnac effect, the motion of the laboratory should be
taken into account. The MS framework is introduced to deal with its motion
relative to a preferred reference frame. Though the one-way speed of light is
other than c, its two-way speed is shown to be c with respect to the proper
time. The theoretical analysis of the generalized Sagnac effect corresponds to
the experimental results, and shows the usefulness of the standard
synchronization. The introduction of the standard synchrony can make
mathematical manipulation easy and can allow us to deal with relative motions
between inertial frames without information on their velocities relative to the
preferred frame.
"
"  Anisotropic displacement parameters (ADPs) are commonly used in
crystallography, chemistry and related fields to describe and quantify thermal
motion of atoms. Within the very recent years, these ADPs have become
predictable by lattice dynamics in combination with first-principles theory.
Here, we study four very different molecular crystals, namely urea,
bromomalonic aldehyde, pentachloropyridine, and naphthalene, by
first-principles theory to assess the quality of ADPs calculated in the
quasi-harmonic approximation. In addition, we predict both thermal expansion
and thermal motion within the quasi-harmonic approximation and compare the
predictions with experimental data. Very reliable ADPs are calculated within
the quasi-harmonic approximation for all four cases up to at least 200 K, and
they turn out to be in better agreement with experiment than the harmonic ones.
In one particular case, ADPs can even reliably be predicted up to room
temperature. Our results also hint at the importance of normal-mode
anharmonicity in the calculation of ADPs.
"
"  Neural network (NN) model chemistries (MCs) promise to facilitate the
accurate exploration of chemical space and simulation of large reactive
systems. One important path to improving these models is to add layers of
physical detail, especially long-range forces. At short range, however, these
models are data driven and data limited. Little is systematically known about
how data should be sampled, and `test data' chosen randomly from some sampling
techniques can provide poor information about generality. If the sampling
method is narrow `test error' can appear encouragingly tiny while the model
fails catastrophically elsewhere. In this manuscript we competitively evaluate
two common sampling methods: molecular dynamics (MD), normal-mode sampling
(NMS) and one uncommon alternative, Metadynamics (MetaMD), for preparing
training geometries. We show that MD is an inefficient sampling method in the
sense that additional samples do not improve generality. We also show MetaMD is
easily implemented in any NNMC software package with cost that scales linearly
with the number of atoms in a sample molecule. MetaMD is a black-box way to
ensure samples always reach out to new regions of chemical space, while
remaining relevant to chemistry near $k_bT$. It is one cheap tool to address
the issue of generalization.
"
"  We study the photoinduced breakdown of a two-orbital Mott insulator and
resulting metallic state. Using time-dependent density matrix renormalization
group, we scrutinize the real-time dynamics of the half-filled two-orbital
Hubbard model interacting with a resonant radiation field pulse. The breakdown,
caused by production of doublon-holon pairs, is enhanced by Hund's exchange,
which dynamically activates large orbital fluctuations. The melting of the Mott
insulator is accompanied by a high to low spin transition with a concomitant
reduction of antiferromagnetic spin fluctuations. Most notably, the overall
time response is driven by the photogeneration of excitons with orbital
character that are stabilized by Hund's coupling. These unconventional ""Hund
excitons"" correspond to bound spin-singlet orbital-triplet doublon-holon pairs.
We study exciton properties such as bandwidth, binding potential, and size
within a semiclassical approach. The photometallic state results from a
coexistence of Hund excitons and doublon-holon plasma.
"
"  The non-linear response of entangled polymers to shear flow is complicated.
Its current understanding is framed mainly as a rheological description in
terms of the complex viscosity. However, the full picture requires an
assessment of the dynamical structure of individual polymer chains which give
rise to the macroscopic observables. Here we shed new light on this problem,
using a computer simulation based on a blob model, extended to describe shear
flow in polymer melts and semi-dilute solutions. We examine the diffusion and
the intermediate scattering spectra during a steady shear flow. The relaxation
dynamics are found to speed up along the flow direction, but slow down along
the shear gradient direction. The third axis, vorticity, shows a slowdown at
the short scale of a tube, but reaches a net speedup at the large scale of the
chain radius of gyration.
"
"  We demonstrate the generation of higher-order modulation formats using
silicon-based inphase/quadrature (IQ) modulators at symbol rates of up to 100
GBd. Our devices exploit the advantages of silicon-organic hybrid (SOH)
integration, which combines silicon-on-insulator waveguides with highly
efficient organic electro-optic (EO) cladding materials to enable small drive
voltages and sub-millimeter device lengths. In our experiments, we use an SOH
IQ modulator with a {\pi}-voltage of 1.6 V to generate 100 GBd 16QAM signals.
This is the first time that the 100 GBd mark is reached with an IQ modulator
realized on a semiconductor substrate, leading to a single-polarization line
rate of 400 Gbit/s. The peak-to-peak drive voltages amount to 1.5 Vpp,
corresponding to an electrical energy dissipation in the modulator of only 25
fJ/bit.
"
"  Magnetic trilayers having large perpendicular magnetic anisotropy (PMA) and
high spin-orbit torques (SOTs) efficiency are the key to fabricate nonvolatile
magnetic memory and logic devices. In this work, PMA and SOTs are
systematically studied in Pt/Co/Cr stacks as a function of Cr thickness. An
enhanced perpendicular anisotropy field around 10189 Oe is obtained and is
related to the interface between Co and Cr layers. In addition, an effective
spin Hall angle up to 0.19 is observed due to the improved antidamping-like
torque by employing dissimilar metals Pt and Cr with opposite signs of spin
Hall angles on opposite sides of Co layer. Finally, we observed a nearly linear
dependence between spin Hall angle and longitudinal resistivity from their
temperature dependent properties, suggesting that the spin Hall effect may
arise from extrinsic skew scattering mechanism. Our results indicate that 3d
transition metal Cr with a large negative spin Hall angle could be used to
engineer the interfaces of trilayers to enhance PMA and SOTs.
"
"  Grain boundary diffusion in severely deformed Al-based AA5024 alloy is
investigated. Different states are prepared by combination of equal channel
angular processing and heat treatments, with the radioisotope $^{57}$Co being
employed as a sensitive probe of a given grain boundary state. Its diffusion
rates near room temperature (320~K) are utilized to quantify the effects of
severe plastic deformation and a presumed formation of a previously reported
deformation-modified state of grain boundaries, solute segregation at the
interfaces, increased dislocation content after deformation and of the
precipitation behavior on the transport phenomena along grain boundaries. The
dominant effect of nano-sized Al$_3$Sc-based precipitates is evaluated using
density functional theory and the Eshelby model for the determination of
elastic stresses around the precipitates.
"
"  The quantum Schrodinger-Newton equation is solved for a self-gravitating Bose
gas at zero temperature. It is derived that the density is non-uniform and a
central hollow cavity exists. The radial distribution of the particle momentum
is uniform. It is shown that a quantum black hole can be formed only above a
certain critical mass. The temperature effect is accounted for via the
Schrodinger-Poisson-Boltzmann equation, where low and high temperature
solutions are obtained. The theoretical analysis is extended to a strong
interacting gas via the Schrodinger-Yukawa equation, showing that the atomic
nuclei are also hollow. Hollow self-gravitating Fermi gases are described by
the Thomas-Fermi equation.
"
"  Many brown dwarfs exhibit photometric variability at levels from tenths to
tens of percents. The photometric variability is related to magnetic activity
or patchy cloud coverage, characteristic of brown dwarfs near the L-T
transition. Time-resolved spectral monitoring of brown dwarfs provides
diagnostics of cloud distribution and condensate properties. However, current
time-resolved spectral studies of brown dwarfs are limited to low spectral
resolution (R$\sim$100) with the exception of the study of Luhman 16 AB at
resolution of 100,000 using the VLT$+$CRIRES. This work yielded the first map
of brown dwarf surface inhomogeneity, highlighting the importance and unique
contribution of high spectral resolution observations. Here, we report on the
time-resolved high spectral resolution observations of a nearby brown dwarf
binary, 2MASSW J0746425+200032AB. We find no coherent spectral variability that
is modulated with rotation. Based on simulations we conclude that the coverage
of a single spot on 2MASSW J0746425+200032AB is smaller than 1\% or 6.25\% if
spot contrast is 50\% or 80\% of its surrounding flux, respectively. Future
high spectral resolution observations aided by adaptive optics systems can put
tighter constraints on the spectral variability of 2MASSW J0746425+200032AB and
other nearby brown dwarfs.
"
"  We report $T=0$ diffusion Monte Carlo results for the ground-state and vortex
excitation of unpolarized spin-1/2 fermions in a two-dimensional disk. We
investigate how vortex core structure properties behave over the BEC-BCS
crossover. We calculate the vortex excitation energy, density profiles, and
vortex core properties related to the current. We find a density suppression at
the vortex core on the BCS side of the crossover, and a depleted core on the
BEC limit. Size-effect dependencies in the disk geometry were carefully
studied.
"
"  When it comes to searches for extensions to general relativity, large efforts
are being dedicated to accurate predictions for the power spectrum of density
perturbations. While this observable is known to be sensitive to the
gravitational theory, its efficiency as a diagnostic for gravity is
significantly reduced when Solar System constraints are strictly adhered to. We
show that this problem can be overcome by studying weigthed density fields. We
propose a transformation of the density field for which the impact of modified
gravity on the power spectrum can be increased by more than a factor of three.
The signal is not only amplified, but the modified gravity features are shifted
to larger scales which are less affected by baryonic physics. Furthermore, the
overall signal-to-noise increases, which in principle makes identifying
signatures of modified gravity with future galaxy surveys more feasible. While
our analysis is focused on modified gravity, the technique can be applied to
other problems in cosmology, such as the detection of neutrinos, the effects of
baryons or baryon acoustic oscillations.
"
"  Access to the transverse spin of light has unlocked new regimes in
topological photonics and optomechanics. To achieve the transverse spin of
nonzero longitudinal fields, various platforms that derive transversely
confined waves based on focusing, interference, or evanescent waves have been
suggested. Nonetheless, because of the transverse confinement inherently
accompanying sign reversal of the field derivative, the resulting transverse
spin handedness experiences spatial inversion, which leads to a mismatch
between the densities of the wavefunction and its spin component and hinders
the global observation of the transverse spin. Here, we reveal a globally pure
transverse spin in which the wavefunction density signifies the spin
distribution, by employing inverse molding of the eigenmode in the spin basis.
Starting from the target spin profile, we analytically obtain the potential
landscape and then show that the elliptic-hyperbolic transition around the
epsilon-near-zero permittivity allows for the global conservation of transverse
spin handedness across the topological interface between anisotropic
metamaterials. Extending to the non-Hermitian regime, we also develop
annihilated transverse spin modes to cover the entire Poincare sphere of the
meridional plane. Our results enable the complete transfer of optical energy to
transverse spinning motions and realize the classical analogy of 3-dimensional
quantum spin states.
"
"  We present a strongly interacting quadruple system associated with the K2
target EPIC 220204960. The K2 target itself is a Kp = 12.7 magnitude star at
Teff ~ 6100 K which we designate as ""B-N"" (blue northerly image). The host of
the quadruple system, however, is a Kp = 17 magnitude star with a composite
M-star spectrum, which we designate as ""R-S"" (red southerly image). With a 3.2""
separation and similar radial velocities and photometric distances, 'B-N' is
likely physically associated with 'R-S', making this a quintuple system, but
that is incidental to our main claim of a strongly interacting quadruple system
in 'R-S'. The two binaries in 'R-S' have orbital periods of 13.27 d and 14.41
d, respectively, and each has an inclination angle of >89 degrees. From our
analysis of radial velocity measurements, and of the photometric lightcurve, we
conclude that all four stars are very similar with masses close to 0.4 Msun.
Both of the binaries exhibit significant ETVs where those of the primary and
secondary eclipses 'diverge' by 0.05 days over the course of the 80-day
observations. Via a systematic set of numerical simulations of quadruple
systems consisting of two interacting binaries, we conclude that the outer
orbital period is very likely to be between 300 and 500 days. If sufficient
time is devoted to RV studies of this faint target, the outer orbit should be
measurable within a year.
"
"  The CALICE collaboration is developing highly granular calorimeters for
experiments at a future lepton collider primarily to establish technologies for
particle flow event reconstruction. These technologies also find applications
elsewhere, such as detector upgrades for the LHC. Meanwhile, the large data
sets collected in an extensive series of beam tests have enabled detailed
studies of the properties of hadronic showers in calorimeter systems, resulting
in improved simulation models and development of sophisticated reconstruction
techniques. In this proceeding, highlights are included from studies of the
structure of hadronic showers and results on reconstruction techniques for
imaging calorimetry. In addition, current R&D activities within CALICE are
summarized, focusing on technological prototypes that address challenges from
full detector system integration and production techniques amenable to mass
production for electromagnetic and hadronic calorimeters based on silicon,
scintillator, and gas techniques.
"
"  We demonstrate electro-mechanical control of an on-chip GaAs optical beam
splitter containing a quantum dot single-photon source. The beam splitter
consists of two nanobeam waveguides, which form a directional coupler (DC). The
splitting ratio of the DC is controlled by varying the out-of-plane separation
of the two waveguides using electro-mechanical actuation. We reversibly tune
the beam splitter between an initial state, with emission into both output
arms, and a final state with photons emitted into a single output arm. The
device represents a compact and scalable tuning approach for use in III-V
semiconductor integrated quantum optical circuits.
"
"  In classical mechanics well-known cryptographic algorithms and protocols can
be very useful for construction canonical transformations preserving form of
Hamiltonians. We consider application of a standard generic divisor doubling
for construction of new auto Bäcklund transformations for the Lagrange top
and Hénon-Heiles system separable in parabolic coordinates.
"
"  A systematic first-principles study has been performed to understand the
magnetism of thin film SrRuO$_3$ which lots of research efforts have been
devoted to but no clear consensus has been reached about its ground state
properties. The relative t$_{2g}$ level difference, lattice distortion as well
as the layer thickness play together in determining the spin order. In
particular, it is important to understand the difference between two standard
approximations, namely LDA and GGA, in describing this metallic magnetism.
Landau free energy analysis and the magnetization-energy-ratio plot clearly
show the different tendency of favoring the magnetic moment formation, and it
is magnified when applied to the thin film limit where the experimental
information is severely limited. As a result, LDA gives a qualitatively
different prediction from GGA in the experimentally relevant region of strain
whereas both approximations give reasonable results for the bulk phase. We
discuss the origin of this difference and the applicability of standard methods
to the correlated oxide and the metallic magnetic systems.
"
"  Gravitinos are a fundamental prediction of supergravity, their mass ($m_{G}$)
is informative of the value of the SUSY breaking scale, and, if produced during
reheating, their number density is a function of the reheating temperature
($T_{\text{rh}}$). As a result, constraining their parameter space provides in
turn significant constraints on particles physics and cosmology. We have
previously shown that for gravitinos decaying into photons or charged particles
during the ($\mu$ and $y$) distortion eras, upcoming CMB spectral distortions
bounds are highly effective in constraining the $T_{\text{rh}}-m_{G}$ space.
For heavier gravitinos (with lifetimes shorter than a few $\times10^6$ sec),
distortions are quickly thermalized and energy injections cause a temperature
rise for the CMB bath. If the decay occurs after neutrino decoupling, its
overall effect is a suppression of the effective number of relativistic degrees
of freedom ($N_{\text{eff}}$). In this paper, we utilize the observational
bounds on $N_{\text{eff}}$ to constrain gravitino decays, and hence provide new
constaints on gravitinos and reheating. For gravitino masses less than $\approx
10^5$ GeV, current observations give an upper limit on the reheating scale in
the range of $\approx 5 \times 10^{10}- 5 \times 10^{11}$GeV. For masses
greater than $\approx 4 \times 10^3$ GeV they are more stringent than previous
bounds from BBN constraints, coming from photodissociation of deuterium, by
almost 2 orders of magnitude.
"
"  Membrane proteins constitute a large portion of the human proteome and
perform a variety of important functions as membrane receptors, transport
proteins, enzymes, signaling proteins, and more. The computational studies of
membrane proteins are usually much more complicated than those of globular
proteins. Here we propose a new continuum model for Poisson-Boltzmann
calculations of membrane channel proteins. Major improvements over the existing
continuum slab model are as follows: 1) The location and thickness of the slab
model are fine-tuned based on explicit-solvent MD simulations. 2) The highly
different accessibility in the membrane and water regions are addressed with a
two-step, two-probe grid labeling procedure, and 3) The water pores/channels
are automatically identified. The new continuum membrane model is optimized (by
adjusting the membrane probe, as well as the slab thickness and center) to best
reproduce the distributions of buried water molecules in the membrane region as
sampled in explicit water simulations. Our optimization also shows that the
widely adopted water probe of 1.4 {\AA} for globular proteins is a very
reasonable default value for membrane protein simulations. It gives an overall
minimum number of inconsistencies between the continuum and explicit
representations of water distributions in membrane channel proteins, at least
in the water accessible pore/channel regions that we focus on. Finally, we
validate the new membrane model by carrying out binding affinity calculations
for a potassium channel, and we observe a good agreement with experiment
results.
"
"  We present spectroscopic redshifts of S(870)>2mJy submillimetre galaxies
(SMGs) which have been identified from the ALMA follow-up observations of 870um
detected sources in the Extended Chandra Deep Field South (the ALMA-LESS
survey). We derive spectroscopic redshifts for 52 SMGs, with a median of
z=2.4+/-0.1. However, the distribution features a high redshift tail, with ~25%
of the SMGs at z>3. Spectral diagnostics suggest that the SMGs are young
starbursts, and the velocity offsets between the nebular emission and UV ISM
absorption lines suggest that many are driving winds, with velocity offsets up
to 2000km/s. Using the spectroscopic redshifts and the extensive UV-to-radio
photometry in this field, we produce optimised spectral energy distributions
(SEDs) using Magphys, and use the SEDs to infer a median stellar mass of
M*=(6+/-1)x10^{10}Msol for our SMGs with spectroscopic redshifts. By combining
these stellar masses with the star-formation rates (measured from the
far-infrared SEDs), we show that SMGs (on average) lie a factor ~5 above the
main-sequence at z~2. We provide this library of 52 template fits with robust
and well-sampled SEDs available as a resource for future studies of SMGs, and
also release the spectroscopic catalog of ~2000 (mostly infrared-selected)
galaxies targeted as part of the spectroscopic campaign.
"
"  We give a survey of recent results on weak-strong uniqueness for compressible
and incompressible Euler and Navier-Stokes equations, and also make some new
observations. The importance of the weak-strong uniqueness principle stems, on
the one hand, from the instances of non-uniqueness for the Euler equations
exhibited in the past years; and on the other hand from the question of
convergence of singular limits, for which weak-strong uniqueness represents an
elegant tool.
"
"  The prospect of pileup induced backgrounds at the High Luminosity LHC
(HL-LHC) has stimulated intense interest in technology for charged particle
timing at high rates. In contrast to the role of timing for particle
identification, which has driven incremental improvements in timing, the LHC
timing challenge dictates a specific level of timing performance- roughly 20-30
picoseconds. Since the elapsed time for an LHC bunch crossing (with standard
design book parameters) has an rms spread of 170 picoseconds, the $\sim50-100$
picosecond resolution now commonly achieved in TOF systems would be
insufficient to resolve multiple ""in-time"" pileup. Here we present a MicroMegas
based structure which achieves the required time precision (ie 24 picoseconds
for 150 GeV $\mu$'s) and could potentially offer an inexpensive solution
covering large areas with $\sim 1$ cm$^2$ pixel size. We present here a
proof-of-principle which motivates further work in our group toward realizing a
practical design capable of long-term survival in a high rate experiment.
"
"  We report the first result on Ge-76 neutrinoless double beta decay from
CDEX-1 experiment at China Jinping Underground Laboratory. A mass of 994 g
p-type point-contact high purity germanium detector has been installed to
search the neutrinoless double beta decay events, as well as to directly detect
dark matter particles. An exposure of 304 kg*day has been analyzed. The
wideband spectrum from 500 keV to 3 MeV was obtained and the average event rate
at the 2.039 MeV energy range is about 0.012 count per keV per kg per day. The
half-life of Ge-76 neutrinoless double beta decay has been derived based on
this result as: T 1/2 > 6.4*10^22 yr (90% C.L.). An upper limit on the
effective Majorana-neutrino mass of 5.0 eV has been achieved. The possible
methods to further decrease the background level have been discussed and will
be pursued in the next stage of CDEX experiment.
"
"  We use superconducting rings with asymmetric link-up of current leads for
experimental investigation of winding number change at magnetic field
corresponding to the half of the flux quantum inside the ring. According to the
conventional theory, the critical current of such rings should change by jump
due to this change. Experimental data obtained at measurements of aluminum
rings agree with theoretical prediction in magnetic flux region close to
integer numbers of the flux quantum and disagree in the region close to the
half of the one, where a smooth change is observed instead of the jump. First
measurements of tantalum ring give a hope for the jump. Investigation of this
problem may have both fundamental and practical importance.
"
"  (349) Dembowska, a large, bright main-belt asteroid, has a fast rotation and
oblique spin axis. It may have experienced partial melting and differentiation.
We constrain Dembowska's thermophysical properties, e.g., thermal inertia,
roughness fraction, geometric albedo and effective diameter within 3$\sigma$
uncertainty of $\Gamma=20^{+12}_{-7}\rm~Jm^{-2}s^{-0.5}K^{-1}$, $f_{\rm
r}=0.25^{+0.60}_{-0.25}$, $p_{\rm v}=0.309^{+0.026}_{-0.038}$, and $D_{\rm
eff}=155.8^{+7.5}_{-6.2}\rm~km$, by utilizing the Advanced Thermophysical Model
(ATPM) to analyse four sets of thermal infrared data obtained by IRAS, AKARI,
WISE and Subaru/COMICS at different epochs. In addition, by modeling the
thermal lightcurve observed by WISE, we obtain the rotational phases of each
dataset. These rotationally resolved data do not reveal significant variations
of thermal inertia and roughness across the surface, indicating the surface of
Dembowska should be covered by a dusty regolith layer with few rocks or
boulders. Besides, the low thermal inertia of Dembowska show no significant
difference with other asteroids larger than 100 km, indicating the dynamical
lives of these large asteroids are long enough to make the surface to have
sufficiently low thermal inertia. Furthermore, based on the derived surface
thermophysical properties, as well as the known orbital and rotational
parameters, we can simulate Dembowska's surface and subsurface temperature
throughout its orbital period. The surface temperature varies from $\sim40$ K
to $\sim220$ K, showing significant seasonal variation, whereas the subsurface
temperature achieves equilibrium temperature about $120\sim160$ K below
$30\sim50$ cm depth.
"
"  The complex electric modulus and the ac conductivity of carbon
nanoonion/polyaniline composites were studied from 1 mHz to 1 MHz at isothermal
conditions ranging from 15 K to room temperature. The temperature dependence of
the electric modulus and the dc conductivity analyses indicate a couple of
hopping mechanisms. The distinction between thermally activated processes and
the determination of cross-over temperature were achieved by exploring the
temperature dependence of the fractional exponent of the dispersive ac
conductivity and the bifurcation of the scaled ac conductivity isotherms. The
results are analyzed by combining the granular metal model(inter-grain charge
tunneling of extended electron states located within mesoscopic highly
conducting polyaniline grains) and a 3D Mott variable range hopping model
(phonon assisted tunneling within the carbon nano-onions and clusters).
"
"  Predicting Arctic sea ice extent is a notoriously difficult forecasting
problem, even for lead times as short as one month. Motivated by Arctic
intraannual variability phenomena such as reemergence of sea surface
temperature and sea ice anomalies, we use a prediction approach for sea ice
anomalies based on analog forecasting. Traditional analog forecasting relies on
identifying a single analog in a historical record, usually by minimizing
Euclidean distance, and forming a forecast from the analog's historical
trajectory. Here an ensemble of analogs are used to make forecasts, where the
ensemble weights are determined by a dynamics-adapted similarity kernel, which
takes into account the nonlinear geometry on the underlying data manifold. We
apply this method for forecasting pan-Arctic and regional sea ice area and
volume anomalies from multi-century climate model data, and in many cases find
improvement over the benchmark damped persistence forecast. Examples of success
include the 3--6 month lead time prediction of pan-Arctic area, the winter sea
ice area prediction of some marginal ice zone seas, and the 3--12 month lead
time prediction of sea ice volume anomalies in many central Arctic basins. We
discuss possible connections between KAF success and sea ice reemergence, and
find KAF to be successful in regions and seasons exhibiting high interannual
variability.
"
"  Space out of a topological defect of the Abrikosov-Nielsen-Olesen vortex type
is locally flat but non-Euclidean. If a spinor field is quantized in such a
space, then a variety of quantum effects is induced in the vacuum. Basing on
the continuum model for long-wavelength electronic excitations, originating in
the tight-binding approximation for the nearest neighbor interaction of atoms
in the crystal lattice, we consider quantum ground state effects in monolayer
structures warped into nanocones by a disclination; the nonzero size of the
disclination is taken into account, and a boundary condition at the edge of the
disclination is chosen to ensure self-adjointness of the Dirac-Weyl Hamiltonian
operator. In the case of carbon nanocones, we find circumstances when the
quantum ground state effects are independent of the boundary parameter and the
disclination size.
"
"  We formulate part I of a rigorous theory of ground states for classical,
finite, Heisenberg spin systems. The main result is that all ground states can
be constructed from the eigenvectors of a real, symmetric matrix with entries
comprising the coupling constants of the spin system as well as certain
Lagrange parameters. The eigenvectors correspond to the unique maximum of the
minimal eigenvalue considered as a function of the Lagrange parameters.
However, there are rare cases where all ground states obtained in this way have
unphysical dimensions $M>3$ and the theory would have to be extended. Further
results concern the degree of additional degeneracy, additional to the trivial
degeneracy of ground states due to rotations or reflections. The theory is
illustrated by a couple of elementary examples.
"
"  We use Monte Carlo simulations to explore the statistical challenges of
constraining the characteristic mass ($m_c$) and width ($\sigma$) of a
lognormal sub-solar initial mass function (IMF) in Local Group dwarf galaxies
using direct star counts. For a typical Milky Way (MW) satellite ($M_{V} =
-8$), jointly constraining $m_c$ and $\sigma$ to a precision of $\lesssim 20\%$
requires that observations be complete to $\lesssim 0.2 M_{\odot}$, if the IMF
is similar to the MW IMF. A similar statistical precision can be obtained if
observations are only complete down to $0.4M_{\odot}$, but this requires
measurement of nearly 100$\times$ more stars, and thus, a significantly more
massive satellite ($M_{V} \sim -12$). In the absence of sufficiently deep data
to constrain the low-mass turnover, it is common practice to fit a
single-sloped power law to the low-mass IMF, or to fit $m_c$ for a lognormal
while holding $\sigma$ fixed. We show that the former approximation leads to
best-fit power law slopes that vary with the mass range observed and can
largely explain existing claims of low-mass IMF variations in MW satellites,
even if satellite galaxies have the same IMF as the MW. In addition, fixing
$\sigma$ during fitting leads to substantially underestimated uncertainties in
the recovered value of $m_c$ (by a factor of $\sim 4$ for typical
observations). If the IMFs of nearby dwarf galaxies are lognormal and do vary,
observations must reach down to $\sim m_c$ in order to robustly detect these
variations. The high-sensitivity, near-infrared capabilities of JWST and WFIRST
have the potential to dramatically improve constraints on the low-mass IMF. We
present an efficient observational strategy for using these facilities to
measure the IMFs of Local Group dwarf galaxies.
"
"  All previous experiments in open turbulent flows (e.g. downstream of grids,
jet and atmospheric boundary layer) have produced quantitatively consistent
values for the scaling exponents of velocity structure functions. The only
measurement in closed turbulent flow (von Kármán swirling flow) using
Taylor-hypothesis, however, produced scaling exponents that are significantly
smaller, suggesting that the universality of these exponents are broken with
respect to change of large scale geometry of the flow. Here, we report
measurements of longitudinal structure functions of velocity in a von
Kármán setup without the use of Taylor-hypothesis. The measurements are
made using Stereo Particle Image Velocimetry at 4 different ranges of spatial
scales, in order to observe a combined inertial subrange spanning roughly one
and a half order of magnitude. We found scaling exponents (up to 9th order)
that are consistent with values from open turbulent flows, suggesting that they
might be in fact universal.
"
"  We propose a method inspired from discrete light cone quantization (DLCQ) to
determine the heat kernel for a Schrödinger field theory (Galilean boost
invariant with $z=2$ anisotropic scaling symmetry) living in $d+1$ dimensions,
coupled to a curved Newton-Cartan background starting from a heat kernel of a
relativistic conformal field theory ($z=1$) living in $d+2$ dimensions. We use
this method to show the Schrödinger field theory of a complex scalar field
cannot have any Weyl anomalies. To be precise, we show that the Weyl anomaly
$\mathcal{A}^{G}_{d+1}$ for Schrödinger theory is related to the Weyl anomaly
of a free relativistic scalar CFT $\mathcal{A}^{R}_{d+2}$ via
$\mathcal{A}^{G}_{d+1}= 2\pi \delta (m) \mathcal{A}^{R}_{d+2}$ where $m$ is the
charge of the scalar field under particle number symmetry. We provide further
evidence of vanishing anomaly by evaluating Feynman diagrams in all orders of
perturbation theory. We present an explicit calculation of the anomaly using a
regulated Schrödinger operator, without using the null cone reduction
technique. We generalise our method to show that a similar result holds for one
time derivative theories with even $z>2$.
"
"  Argo floats measure seawater temperature and salinity in the upper 2,000 m of
the global ocean. Statistical analysis of the resulting spatio-temporal dataset
is challenging due to its nonstationary structure and large size. We propose
mapping these data using locally stationary Gaussian process regression where
covariance parameter estimation and spatio-temporal prediction are carried out
in a moving-window fashion. This yields computationally tractable nonstationary
anomaly fields without the need to explicitly model the nonstationary
covariance structure. We also investigate Student-$t$ distributed fine-scale
variation as a means to account for non-Gaussian heavy tails in ocean
temperature data. Cross-validation studies comparing the proposed approach with
the existing state-of-the-art demonstrate clear improvements in point
predictions and show that accounting for the nonstationarity and
non-Gaussianity is crucial for obtaining well-calibrated uncertainties. This
approach also provides data-driven local estimates of the spatial and temporal
dependence scales for the global ocean which are of scientific interest in
their own right.
"
"  Dielectronic recombination (DR) is the dominant mode of recombination in
magnetically confined fusion plasmas for intermediate to low-charged ions of W.
Complete, final-state resolved partial isonuclear W DR rate coefficient data is
required for detailed collisional-radiative modelling for such plasmas in
preparation for the upcoming fusion experiment ITER. To realize this
requirement, we continue {\it The Tungsten Project} by presenting our
calculations for tungsten ions W$^{55+}$ to W$^{38+}$. As per our prior
calculations for W$^{73+}$ to W$^{56+}$, we use the collision package {\sc
autostructure} to calculate partial and total DR rate coefficients for all
relevant core-excitations in intermediate coupling (IC) and configuration
average (CA) using $\kappa$-averaged relativistic wavefunctions. Radiative
recombination (RR) rate coefficients are also calculated for the purpose of
evaluating ionization fractions. Comparison of our DR rate coefficients for
W$^{46+}$ with other authors yields agreement to within 7-19\% at peak
abundance verifying the reliability of our method. Comparison of partial DR
rate coefficients calculated in IC and CA yield differences of a factor
$\sim{2}$ at peak abundance temperature, highlighting the importance of
relativistic configuration mixing. Large differences are observed between
ionization fractions calculated using our recombination rate coefficient data
and that of Pütterich~\etal [Plasma Phys. and Control. Fusion 50 085016,
(2008)]. These differences are attributed to deficiencies in the average-atom
method used by the former to calculate their data.
"
"  We present a many-body theory that explains and reproduces recent
observations of population polarization dynamics, is supported by controlled
human experiments, and addresses the controversy surrounding the Internet's
impact. It predicts that whether and how a population becomes polarized is
dictated by the nature of the underlying competition, rather than the validity
of the information that individuals receive or their online bubbles. Building
on this framework, we show that next-generation social media algorithms aimed
at pulling people together, will instead likely lead to an explosive
percolation process that generates new pockets of extremes.
"
"  Pillared Graphene Frameworks are a novel class of microporous materials made
by graphene sheets separated by organic spacers. One of their main features is
that the pillar type and density can be chosen to tune the material properties.
In this work, we present a computer simulation study of adsorption and dynamics
of H$_{4}$, CH$_{2}$, CO$_{2}$, N$_{2}$ and O$_{2}$ and binary mixtures
thereof, in Pillared Graphene Frameworks with nitrogen-containing organic
spacers. In general, we find that pillar density plays the most important role
in determining gas adsorption. In the low-pressure regime (< 10 bar) the amount
of gas adsorbed is an increasing function of pillar density. At higher
pressures the opposite trend is observed. Diffusion coefficients were computed
for representative structures taking into account the framework flexibility
that is essential in assessing the dynamical properties of the adsorbed gases.
Good performance for the gas separation in CH$_{4}$/H$_{2}$, CO$_{2}$/H$_{2}$
and CO$_{2}$/N$_{2}$ mixtures was found with values comparable to those of
metal-organic frameworks and zeolites.
"
"  We theoretically address spin chain analogs of the Kitaev quantum spin model
on the honeycomb lattice. The emergent quantum spin liquid phases or Anderson
resonating valence bond (RVB) states can be understood, as an effective model,
in terms of p-wave superconductivity and Majorana fermions. We derive a
generalized phase diagram for the two-leg ladder system with tunable
interaction strengths between chains allowing us to vary the shape of the
lattice (from square to honeycomb ribbon or brickwall ladder). We evaluate the
winding number associated with possible emergent (topological) gapless modes at
the edges. In the Az phase, as a result of the emergent Z2 gauge fields and
pi-flux ground state, one may build spin-1/2 (loop) qubit operators by analogy
to the toric code. In addition, we show how the intermediate gapless B phase
evolves in the generalized ladder model. For the brickwall ladder, the $B$
phase is reduced to one line, which is analyzed through perturbation theory in
a rung tensor product states representation and bosonization. Finally, we show
that doping with a few holes can result in the formation of hole pairs and
leads to a mapping with the Su-Schrieffer-Heeger model in polyacetylene; a
superconducting-insulating quantum phase transition for these hole pairs is
accessible, as well as related topological properties.
"
"  Weyl semimetals (WSMs) have recently attracted a great deal of attention as
they provide condensed matter realization of chiral anomaly, feature
topologically protected Fermi arc surface states and sustain sharp chiral Weyl
quasiparticles up to a critical disorder at which a continuous quantum phase
transition (QPT) drives the system into a metallic phase. We here numerically
demonstrate that with increasing strength of disorder the Fermi arc gradually
looses its sharpness, and close to the WSM-metal QPT it completely dissolves
into the metallic bath of the bulk. Predicted topological nature of the
WSM-metal QPT and the resulting bulk-boundary correspondence across this
transition can directly be observed in
angle-resolved-photo-emmision-spectroscopy (ARPES) and Fourier transformed
scanning-tunneling-microscopy (STM) measurements by following the continuous
deformation of the Fermi arcs with increasing disorder in recently discovered
Weyl materials.
"
"  This paper presents the first estimate of the seasonal cycle of ocean and sea
ice net heat and freshwater (FW) fluxes around the boundary of the Arctic
Ocean. The ocean transports are estimated primarily using 138 moored
instruments deployed in September 2005 to August 2006 across the four main
Arctic gateways: Davis, Fram and Bering Straits, and the Barents Sea Opening
(BSO). Sea ice transports are estimated from a sea ice assimilation product.
Monthly velocity fields are calculated with a box inverse model that enforces
volume and salinity conservation. The resulting net ocean and sea ice heat and
FW fluxes (annual mean $\pm$ 1 standard deviation) are 175 $\pm$48 TW and 204
$\pm$85 mSv (respectively; 1 Sv = 10$^{6} m^{3} s^{-1}$). These boundary fluxes
accurately represent the annual means of the relevant surface fluxes. Oceanic
net heat transport variability is driven by temperature variability in upper
part of the water column and by volume transport variability in the Atlantic
Water layer. Oceanic net FW transport variability is dominated by Bering Strait
velocity variability. The net water mass transformation in the Arctic entails a
freshening and cooling of inflowing waters by 0.62$\pm$0.23 in salinity and
3.74$\pm$0.76C in temperature, respectively, and a reduction in density by
0.23$\pm$0.20 kg m$^{-3}$. The volume transport into the Arctic of waters
associated with this water mass transformation is 11.3$\pm$1.2 Sv, and the
export is -11.4$\pm$1.1 Sv. The boundary heat and FW fluxes provide a benchmark
data set for the validation of numerical models and atmospheric re-analyses
products.
"
"  Binary stars can interact via mass transfer when one member (the primary)
ascends onto a giant branch. The amount of gas ejected by the binary and the
amount of gas accreted by the secondary over the lifetime of the primary
influence the subsequent binary phenomenology. Some of the gas ejected by the
binary will remain gravitationally bound and its distribution will be closely
related to the formation of planetary nebulae. We investigate the nature of
mass transfer in binary systems containing an AGB star by adding radiative
transfer to the AstroBEAR AMR Hydro/MHD code.
"
"  Ultrafast X-ray imaging provides high resolution information on individual
fragile specimens such as aerosols, metastable particles, superfluid quantum
systems and live biospecimen, which is inaccessible with conventional imaging
techniques. Coherent X-ray diffractive imaging, however, suffers from intrinsic
loss of phase, and therefore structure recovery is often complicated and not
always uniquely-defined. Here, we introduce the method of in-flight holography,
where we use nanoclusters as reference X-ray scatterers in order to encode
relative phase information into diffraction patterns of a virus. The resulting
hologram contains an unambiguous three-dimensional map of a virus and two
nanoclusters with the highest lat- eral resolution so far achieved via single
shot X-ray holography. Our approach unlocks the benefits of holography for
ultrafast X-ray imaging of nanoscale, non-periodic systems and paves the way to
direct observation of complex electron dynamics down to the attosecond time
scale.
"
"  We study the existence and uniqueness of minimal right determiners in various
categories. Particularly in a Hom-finite hereditary abelian category with
enough projectives, we prove that the Auslander-Reiten-Smal{\o}-Ringel formula
of the minimal right determiner still holds. As an application, we give a
formula of minimal right determiners in the category of finitely presented
representations of strongly locally finite quivers.
"
"  Estimating covariances between financial assets plays an important role in
risk management and optimal portfolio allocation. In practice, when the sample
size is small compared to the number of variables, i.e. when considering a wide
universe of assets over just a few years, this poses considerable challenges
and the empirical estimate is known to be very unstable.
Here, we propose a novel covariance estimator based on the Gaussian Process
Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear
extension of standard factor models with readily interpretable parameters
reminiscent of market betas. Furthermore, our Bayesian treatment naturally
shrinks the sample covariance matrix towards a more structured matrix given by
the prior and thereby systematically reduces estimation errors.
"
"  This paper studies the role of dg-Lie algebroids in derived deformation
theory. More precisely, we provide an equivalence between the homotopy theories
of formal moduli problems and dg-Lie algebroids over a commutative dg-algebra
of characteristic zero. At the level of linear objects, we show that the
category of representations of a dg-Lie algebroid is an extension of the
category of quasi-coherent sheaves on the corresponding formal moduli problem.
We describe this extension geometrically in terms of pro-coherent sheaves.
"
"  Recently $S_{b}$-metric spaces have been introduced as the generalizations of
metric and $S$-metric spaces. In this paper we investigate some basic
properties of this new space. We generalize the classical Banach's contraction
principle using the theory of a complete $S_{b}$-metric space. Also we give an
application to linear equation systems using the $S_{b}$-metric which is
generated by a metric.
"
"  In this paper we propose and analyze a finite element method for both the
harmonic map heat and Landau--Lifshitz--Gilbert equation, the time variable
remaining continuous. Our starting point is to set out a unified saddle point
approach for both problems in order to impose the unit sphere constraint at the
nodes since the only polynomial function satisfying the unit sphere constraint
everywhere are constants. A proper inf-sup condition is proved for the Lagrange
multiplier leading to the well-posedness of the unified formulation. \emph{A
priori} energy estimates are shown for the proposed method.
When time integrations are combined with the saddle point finite element
approximation some extra elaborations are required in order to ensure both
\emph{a priori} energy estimates for the director or magnetization vector
depending on the model and an inf-sup condition for the Lagrange multiplier.
This is due to the fact that the unit length at the nodes is not satisfied in
general when a time integration is performed. We will carry out a linear Euler
time-stepping method and a non-linear Crank--Nicolson method. The latter is
solved by using the former as a non-linear solver.
"
"  We introduce and study ternary $f$-distributive structures, Ternary
$f$-quandles and more generally their higher $n$-ary analogues. A
classification of ternary $f$-quandles is provided in low dimensions. Moreover,
we study extension theory and introduce a cohomology theory for ternary, and
more generally $n$-ary, $f$-quandles. Furthermore, we give some computational
examples.
"
"  The Reidemeister number of an endomorphism of a group is the number of
twisted conjugacy classes determined by that endomorphism. The collection of
all Reidemeister numbers of all automorphisms of a group $G$ is called the
Reidemeister spectrum of $G$. In this paper, we determine the Reidemeister
spectra of all fundamental groups of solvmanifolds up to Hirsch length 4.
"
"  A functional version of the Kato one-parametric regularisation for the
construction of a dynamical semigroup generator of a relative bound one
perturbation is introduced. It does not require that the minus generator of the
unperturbed semigroup is a positivity preserving operator. The regularisation
is illustrated by an example of a boson-number cut-off regularisation.
"
"  Let $X\rightarrow {\mathbb P}^1$ be an elliptically fibered $K3$ surface with
a section, admitting a sequence of Ricci-flat metrics collapsing the fibers.
Let $\mathcal E$ be a generic, holomoprhic $SU(n)$ bundle over $X$ such that
the restriction of $\mathcal E$ to each fiber is semi-stable. Given a sequence
$\Xi_i$ of Hermitian-Yang-Mills connections on $\mathcal E$ corresponding to
this degeneration, we prove that, if $E$ is a given fiber away from a finite
set, the restricted sequence $\Xi_i|_{E}$ converges to a flat connection
uniquely determined by the holomorphic structure on $\mathcal E$.
"
"  Several kinds of differential relations for polynomial components of almost
Belyi maps are presented. Saito's theory of free divisors give particularly
interesting (yet conjectural) logarithmic action of vector fields. The
differential relations implied by Kitaev's construction of algebraic Painleve
VI solutions through pull-back transformations are used to compute almost Belyi
maps for the pull-backs giving all genus 0 and 1 Painleve VI solutions in the
Lisovyy-Tykhyy classification.
"
"  This paper studies holomorphic semicocycles over semigroups in the unit disk,
which take values in an arbitrary unital Banach algebra. We prove that every
such semicocycle is a solution to a corresponding evolution problem. We then
investigate the linearization problem: which semicocycles are cohomologous to
constant semicocycles? In contrast with the case of commutative semicocycles,
in the non-commutative case non-linearizable semicocycles are shown to exist.
Simple conditions for linearizability are derived and are shown to be sharp.
"
"  In this paper, we consider the cubic fourth-order nonlinear Schrödinger
equation (4NLS) under the periodic boundary condition. We prove two results.
One is the local well-posedness in $H^s$ with $-1/3 \le s < 0$ for the Cauchy
problem of the Wick ordered 4NLS. The other one is the non-squeezing property
for the flow map of 4NLS in the symplectic phase space $L^2(\mathbb{T})$. To
prove the former we used the ideas introduced in [Takaoka and Tsutsumi 2004]
and [Nakanish et al 2010], and to prove the latter we used the ideas in
[Colliander et al 2005].
"
"  We introduce new boundary integral operators which are the exact inverses of
the weakly singular and hypersingular operators for the Laplacian on flat
disks. Moreover, we provide explicit closed forms for them and prove the
continuity and ellipticity of their corresponding bilinear forms in the natural
Sobolev trace spaces. This permit us to derive new Calderón-type identities
that can provide the foundation for optimal operator preconditioning in
Galerkin boundary element methods.
"
"  The present paper shows that warped Riemannian metrics, a class of Riemannian
metrics which play a prominent role in Riemannian geometry, are also of
fundamental importance in information geometry. Precisely, the paper features a
new theorem, which states that the Rao-Fisher information metric of any
location-scale model, defined on a Riemannian manifold, is a warped Riemannian
metric, whenever this model is invariant under the action of some Lie group.
This theorem is a valuable tool in finding the expression of the Rao-Fisher
information metric of location-scale models defined on high-dimensional
Riemannian manifolds. Indeed, a warped Riemannian metric is fully determined by
only two functions of a single variable, irrespective of the dimension of the
underlying Riemannian manifold. Starting from this theorem, several original
contributions are made. The expression of the Rao-Fisher information metric of
the Riemannian Gaussian model is provided, for the first time in the
literature. A generalised definition of the Mahalanobis distance is introduced,
which is applicable to any location-scale model defined on a Riemannian
manifold. The solution of the geodesic equation is obtained, for any Rao-Fisher
information metric defined in terms of warped Riemannian metrics. Finally,
using a mixture of analytical and numerical computations, it is shown that the
parameter space of the von Mises-Fisher model of $n$-dimensional directional
data, when equipped with its Rao-Fisher information metric, becomes a Hadamard
manifold, a simply-connected complete Riemannian manifold of negative sectional
curvature, for $n = 2,\ldots,8$. Hopefully, in upcoming work, this will be
proved for any value of $n$.
"
"  We look at stochastic optimization problems through the lens of statistical
decision theory. In particular, we address admissibility, in the statistical
decision theory sense, of the natural sample average estimator for a stochastic
optimization problem (which is also known as the empirical risk minimization
(ERM) rule in learning literature). It is well known that for general
stochastic optimization problems, the sample average estimator may not be
admissible. This is known as Stein's paradox in the statistics literature. We
show in this paper that for optimizing stochastic linear functions over compact
sets, the sample average estimator is admissible.
"
"  Given a substitution tiling $T$ of the plane with subdivision operator
$\tau$, we study the conformal tilings $\mathcal{T}_n$ associated with $\tau^n
T$. We prove that aggregate tiles within $\mathcal{T}_n$ converge in shape as
$n\rightarrow \infty$ to their associated Euclidean tiles in $T$.
"
"  We study the convergence of the parameter family of series
$$V_{\alpha,\beta}(t)=\sum_{p}p^{-\alpha}\exp(2\pi i p^{\beta}t),\quad
\alpha,\beta \in \mathbb{R}_{>0},\; t \in [0,1)$$ defined over prime numbers
$p$, and subsequently, their differentiability properties. The visible fractal
nature of the graphs as a function of $\alpha,\beta$ is analyzed in terms of
Hölder continuity, self similarity and fractal dimension, backed with
numerical results. We also discuss the link of this series to random walks and
consequently, explore numerically its random properties.
"
"  We consider smooth, complex quasi-projective varieties $U$ which admit a
compactification with a boundary which is an arrangement of smooth algebraic
hypersurfaces. If the hypersurfaces intersect locally like hyperplanes, and the
relative interiors of the hypersurfaces are Stein manifolds, we prove that the
cohomology of certain local systems on $U$ vanishes. As an application, we show
that complements of linear, toric, and elliptic arrangements are both duality
and abelian duality spaces.
"
"  We proposed a semi-parametric estimation procedure in order to estimate the
parameters of a max-mixture model and also of a max-stable model (inverse
max-stable model) as an alternative to composite likelihood. A good estimation
by the proposed estimator required the dependence measure to detect all
dependence structures in the model, especially when dealing with the
max-mixture model. We overcame this challenge by using the F-madogram. The
semi-parametric estimation was then based on a quasi least square method, by
minimizing the square difference between the theoretical F-madogram and an
empirical one. We evaluated the performance of this estimator through a
simulation study. It was shown that on an average, the estimation is performed
well, although in some cases, it encountered some difficulties. We apply our
estimation procedure to model the daily rainfalls over the East Australia.
"
"  In this article, we consider the equivariant Schrödinger map from $\Bbb
H^2$ to $\Bbb S^2$ which converges to the north pole of $\Bbb S^2$ at the
origin and spatial infinity of the hyperbolic space. If the energy of the data
is less than $4\pi$, we show that the local existence of Schrödinger map.
Furthermore, if the energy of the data sufficiently small, we prove the
solutions are global in time.
"
"  A collection of arbitrarily-shaped solid objects, each moving at a constant
speed, can be used to mix or stir ideal fluid, and can give rise to interesting
flow patterns. Assuming these systems of fluid stirrers are two-dimensional,
the mathematical problem of resolving the flow field - given a particular
distribution of any finite number of stirrers of specified shape and speed -
can be formulated as a Riemann-Hilbert problem. We show that this
Riemann-Hilbert problem can be solved numerically using a fast and accurate
algorithm for any finite number of stirrers based around a boundary integral
equation with the generalized Neumann kernel. Various systems of fluid stirrers
are considered, and our numerical scheme is shown to handle highly multiply
connected domains (i.e. systems of many fluid stirrers) with minimal
computational expense.
"
"  In their previous work, S. Koenig, S. Ovsienko and the second author showed
that every quasi-hereditary algebra is Morita equivalent to the right algebra,
i.e. the opposite algebra of the left dual, of a coring. Let $A$ be an
associative algebra and $V$ an $A$-coring whose right algebra $R$ is
quasi-hereditary. In this paper, we give a combinatorial description of an
associative algebra $B$ and a $B$-coring $W$ whose right algebra is the Ringel
dual of $R$. We apply our results in small examples to obtain restrictions on
the $A_\infty$-structure of the $\textrm{Ext}$-algebra of standard modules over
a class of quasi-hereditary algebras related to birational morphisms of smooth
surfaces.
"
"  The goal of scenario reduction is to approximate a given discrete
distribution with another discrete distribution that has fewer atoms. We
distinguish continuous scenario reduction, where the new atoms may be chosen
freely, and discrete scenario reduction, where the new atoms must be chosen
from among the existing ones. Using the Wasserstein distance as measure of
proximity between distributions, we identify those $n$-point distributions on
the unit ball that are least susceptible to scenario reduction, i.e., that have
maximum Wasserstein distance to their closest $m$-point distributions for some
prescribed $m<n$. We also provide sharp bounds on the added benefit of
continuous over discrete scenario reduction. Finally, to our best knowledge, we
propose the first polynomial-time constant-factor approximations for both
discrete and continuous scenario reduction as well as the first exact
exponential-time algorithms for continuous scenario reduction.
"
"  Recovering low-rank structures via eigenvector perturbation analysis is a
common problem in statistical machine learning, such as in factor analysis,
community detection, ranking, matrix completion, among others. While a large
variety of results provide tight bounds on the average errors between empirical
and population statistics of eigenvectors, fewer results are tight for
entrywise analyses, which are critical for a number of problems such as
community detection and ranking.
This paper investigates the entrywise perturbation analysis for a large class
of random matrices whose expectations are low-rank, including community
detection, synchronization ($\mathbb{Z}_2$-spiked Wigner model) and matrix
completion models. Denoting by $\{u_k\}$, respectively $\{u_k^*\}$, the
eigenvectors of a random matrix $A$, respectively $\mathbb{E} A$, the paper
characterizes cases for which $$u_k \approx \frac{A u_k^*}{\lambda_k^*}$$
serves as a first-order approximation under the $\ell_\infty$ norm. The fact
that the approximation is both tight and linear in the random matrix $A$ allows
for sharp comparisons of $u_k$ and $u_k^*$. In particular, it allows to compare
the signs of $u_k$ and $u_k^*$ even when $\| u_k - u_k^*\|_{\infty}$ is large,
which in turn allows to settle the conjecture in Abbe et al. (2016) that the
spectral algorithm achieves exact recovery in the stochastic block model
without any trimming or cleaning steps. The results are further extended to the
perturbation of eigenspaces, providing new bounds for $\ell_\infty$-type errors
in noisy matrix completion.
"
"  We present the use of the fitted Q iteration in algorithmic trading. We show
that the fitted Q iteration helps alleviate the dimension problem that the
basic Q-learning algorithm faces in application to trading. Furthermore, we
introduce a procedure including model fitting and data simulation to enrich
training data as the lack of data is often a problem in realistic application.
We experiment our method on both simulated environment that permits arbitrage
opportunity and real-world environment by using prices of 450 stocks. In the
former environment, the method performs well, implying that our method works in
theory. To perform well in the real-world environment, the agents trained might
require more training (iteration) and more meaningful variables with predictive
value.
"
"  Finite rank median spaces are a simultaneous generalisation of finite
dimensional CAT(0) cube complexes and real trees. If $\Gamma$ is an irreducible
lattice in a product of rank one simple Lie groups, we show that every action
of $\Gamma$ on a complete, finite rank median space has a global fixed point.
This is in sharp contrast with the behaviour of actions on infinite rank median
spaces.
The fixed point property is obtained as corollary to a superrigidity result;
the latter holds for irreducible lattices in arbitrary products of compactly
generated groups.
In previous work, we introduced ""Roller compactifications"" of median spaces;
these generalise a well-known construction in the case of cube complexes. We
provide a reduced $1$-cohomology class that detects group actions with a finite
orbit in the Roller compactification. Even for CAT(0) cube complexes, only
second bounded cohomology classes were known with this property, due to
Chatterji-Fernós-Iozzi. As a corollary, we observe that, in Gromov's density
model, random groups at low density do not have Shalom's property $H_{FD}$.
"
"  We consider the spatially homogeneous Boltzmann equation for hard potentials
with angular cutoff. This equation has a unique conservative weak solution
$(f_t)_{t\geq 0}$, once the initial condition $f_0$ with finite mass and energy
is fixed. Taking advantage of the energy conservation, we propose a recursive
algorithm that produces a $(0,\infty)\times\mathbb{R}^3$ random variable
$(M_t,V_t)$ such that $E[M_t {\bf 1}_{\{V_t \in \cdot\}}]=f_t$. We also write
down a series expansion of $f_t$. Although both the algorithm and the series
expansion might be theoretically interesting in that they explicitly express
$f_t$ in terms of $f_0$, we believe that the algorithm is not very efficient in
practice and that the series expansion is rather intractable. This is a tedious
extension to non-Maxwellian molecules of Wild's sum and of its interpretation
by McKean.
"
"  Materials science has adopted the term of auxetic behavior for structural
deformations where stretching in some direction entails lateral widening,
rather than lateral shrinking. Most studies, in the last three decades, have
explored repetitive or cellular structures and used the notion of negative
Poisson's ratio as the hallmark of auxetic behavior. However, no general
auxetic principle has been established from this perspective. In the present
paper, we show that a purely geometric approach to periodic auxetics is apt to
identify essential characteristics of frameworks with auxetic deformations and
can generate a systematic and endless series of periodic auxetic designs. The
critical features refer to convexity properties expressed through families of
homothetic ellipsoids.
"
"  Asymptotics of maximum likelihood estimation for $\alpha$-stable law are
analytically investigated with $(M)$ parameterization. The consistency and
asymptotic normality are shown on the interior of the whole parameter space.
Although these asymptotics have been proved with $(B)$ parameterization, there
are several gaps between. Especially in the latter, the density, so that scores
and their derivatives are discontinuous at $\alpha=1$ for $\beta\neq 0$ and
usual asymptotics are impossible, whereas in $(M)$ form these quantities are
shown to be continuous on the interior of the parameter space. We fill these
gaps and provide a convenient theory for applied people. We numerically
approximate the Fisher information matrix around the Cauchy law
$(\alpha,\beta)=(1,0)$. The results exhibit continuity at $\alpha=1,\,\beta\neq
0$ and this secures the accuracy of our calculations.
"
"  We introduce a new critical value $c_\infty(L)$ for Tonelli Lagrangians $L$
on the tangent bundle of the 2-sphere without minimizing measures supported on
a point. We show that $c_\infty(L)$ is strictly larger than the Mañé
critical value $c(L)$, and on every energy level $e\in(c(L),c_\infty(L))$ there
exist infinitely many periodic orbits of the Lagrangian system of $L$, one of
which is a local minimizer of the free-period action functional. This has
applications to Finsler metrics of Randers type on the 2-sphere. We show that,
under a suitable criticality assumption on a given Randers metric, after
rescaling its magnetic part with a sufficiently large multiplicative constant,
the new metric admits infinitely many closed geodesics, one of which is a
waist. Examples of critical Randers metrics include the celebrated Katok
metric.
"
"  Egbert Brieskorn died on July 11, 2013, a few days after his 77th birthday.
He was an impressive personality who has left a lasting impression on all who
knew him, whether inside or outside of mathematics. Brieskorn was a great
mathematician, but his interests, his knowledge, and activities ranged far
beyond mathematics. In this contribution, which is strongly influenced by many
years of personal connectedness of the authors with Brieskorn, we try to give a
deeper insight into the life and work of Brieskorn. We illuminate both his
personal commitment to peace and the environment as well as his long-term study
of the life and work of Felix Hausdorff and the publication of Hausdorff's
collected works. However, the main focus of the article is on the presentation
of his remarkable and influential mathematical work.
"
"  Aggressive incentive schemes that allow individuals to impose economic
punishment on themselves if they fail to meet health goals present a promising
approach for encouraging healthier behavior. However, the element of choice
inherent in these schemes introduces concerns that only non-representative
sectors of the population will select aggressive incentives, leaving value on
the table for those who don't opt in. In a field experiment conducted over a 29
week period on individuals wearing Fitbit activity trackers, we find modest and
short lived increases in physical activity for those provided the choice of
aggressive incentives. In contrast, we find significant and persistent
increases for those assigned (oftentimes against their stated preference) to
the same aggressive incentives. The modest benefits for those provided a choice
seems to emerge because those who benefited most from the aggressive incentives
were the least likely to choose them, and it was those who did not need them
who opted in. These results are confirmed in a follow up lab experiment. We
also find that benefits to individuals assigned to aggressive incentives were
pronounced if they also updated their step target in the Fitbit mobile
application to match the new activity goal we provided them. Our findings have
important implications for incentive based interventions to improve health
behavior. For firms and policy makers, our results suggest that one effective
strategy for encouraging sustained healthy behavior combines exposure to
aggressive incentive schemes to jolt individuals out of their comfort zones
with technology decision aids that help individuals sustain this behavior after
incentives end.
"
"  This paper establishes convergence rate bounds for a variant of the proximal
alternating direction method of multipliers (ADMM) for solving nonconvex
linearly constrained optimization problems. The variant of the proximal ADMM
allows the inclusion of an over-relaxation stepsize parameter belonging to the
interval $(0,2)$. To the best of our knowledge, all related papers in the
literature only consider the case where the over-relaxation parameter lies in
the interval $(0,(1+\sqrt{5})/2)$.
"
"  We introduce a new method for building models of CH, together with $\Pi_2$
statements over $H(\omega_2)$, by forcing over a model of CH. Unlike similar
constructions in the literature, our construction adds new reals, but only
$\aleph_1$-many of them. Using this approach, we prove that a very strong form
of the negation of Club Guessing at $\omega_1$ known as Measuring is consistent
together with CH, thereby answering a well-known question of Moore. The
construction works over any model of ZFC + CH and can be described as a finite
support forcing construction with finite systems of countable models with
markers as side conditions and with strong symmetry constraints on both side
conditions and working parts.
"
"  In this note we show that a mutation theory of species with potential can be
defined so that a certain class of skew-symmetrizable integer matrices have a
species realization admitting a non-degenerate potential. This gives a partial
affirmative answer to a question raised by Jan Geuenich and Daniel
Labardini-Fragoso. We also provide an example of a class of skew-symmetrizable
$4 \times 4$ integer matrices, which are not globally unfoldable nor strongly
primitive, and that have a species realization admitting a non-degenerate
potential.
"
"  This work builds on earlier results. We define universal elliptic Gau{\ss}
sums for Atkin primes in Schoof's algorithm for counting points on elliptic
curves. Subsequently, we show these quantities admit an efficiently computable
representation in terms of the $j$-invariant and two other modular functions.
We analyse the necessary computations in detail and derive an alternative
approach for determining the trace of the Frobenius homomorphism for Atkin
primes using these pre-computations. A rough run-time analysis shows, however,
that this new method is not competitive with existing ones.
"
"  Correlated random fields are a common way to model dependence struc- tures in
high-dimensional data, especially for data collected in imaging. One important
parameter characterizing the degree of dependence is the asymp- totic variance
which adds up all autocovariances in the temporal and spatial domain.
Especially, it arises in the standardization of test statistics based on
partial sums of random fields and thus the construction of tests requires its
estimation. In this paper we propose consistent estimators for this parameter
for strictly stationary {\phi}-mixing random fields with arbitrary dimension of
the domain and taking values in a Euclidean space of arbitrary dimension, thus
allowing for multivariate random fields. We establish consistency, provide cen-
tral limit theorems and show that distributional approximations of related test
statistics based on sample autocovariances of random fields can be obtained by
the subsampling approach. As in applications the spatial-temporal correlations
are often quite local, such that a large number of autocovariances vanish or
are negligible, we also investigate a thresholding approach where sample
autocovariances of small magnitude are omitted. Extensive simulation studies
show that the proposed estimators work well in practice and, when used to
standardize image test statistics, can provide highly accurate image testing
procedures.
"
"  We consider the problem of undirected graphical model inference. In many
applications, instead of perfectly recovering the unknown graph structure, a
more realistic goal is to infer some graph invariants (e.g., the maximum
degree, the number of connected subgraphs, the number of isolated nodes). In
this paper, we propose a new inferential framework for testing nested multiple
hypotheses and constructing confidence intervals of the unknown graph
invariants under undirected graphical models. Compared to perfect graph
recovery, our methods require significantly weaker conditions. This paper makes
two major contributions: (i) Methodologically, for testing nested multiple
hypotheses, we propose a skip-down algorithm on the whole family of monotone
graph invariants (The invariants which are non-decreasing under addition of
edges). We further show that the same skip-down algorithm also provides valid
confidence intervals for the targeted graph invariants. (ii) Theoretically, we
prove that the length of the obtained confidence intervals are optimal and
adaptive to the unknown signal strength. We also prove generic lower bounds for
the confidence interval length for various invariants. Numerical results on
both synthetic simulations and a brain imaging dataset are provided to
illustrate the usefulness of the proposed method.
"
"  Carmesin, Federici, and Georgakopoulos [arXiv:1603.06712] constructed a
transient hyperbolic graph that has no transient subtrees and that has the
Liouville property for harmonic functions. We modify their construction to get
a unimodular random graph with the same properties.
"
"  We investigate the holonomy group of singular Kähler-Einstein metrics on
klt varieties with numerically trivial canonical divisor. Finiteness of the
number of connected components, a Bochner principle for holomorphic tensors,
and a connection between irreducibility of holonomy representations and
stability of the tangent sheaf are established. As a consequence, known
decompositions for tangent sheaves of varieties with trivial canonical divisor
are refined. In particular, we show that up to finite quasi-étale covers,
varieties with strongly stable tangent sheaf are either Calabi-Yau or
irreducible holomorphic symplectic. These results form one building block for
Höring-Peternell's recent proof of a singular version of the
Beauville-Bogomolov Decomposition Theorem.
"
"  Motivated by the model- independent pricing of derivatives calibrated to the
real market, we consider an optimization problem similar to the optimal
Skorokhod embedding problem, where the embedded Brownian motion needs only to
reproduce a finite number of prices of Vanilla options. We derive in this paper
the corresponding dualities and the geometric characterization of optimizers.
Then we show a stability result, i.e. when more and more Vanilla options are
given, the optimization problem converges to an optimal Skorokhod embedding
problem, which constitutes the basis of the numerical computation in practice.
In addition, by means of different metrics on the space of probability
measures, a convergence rate analysis is provided under suitable conditions.
"
"  We prove that any cyclic quadrilateral can be inscribed in any closed convex
$C^1$-curve. The smoothness condition is not required if the quadrilateral is a
rectangle.
"
"  We prove that for a free noncyclic group $F$, $H_2(\hat F_\mathbb Q, \mathbb
Q)$ is an uncountable $\mathbb Q$-vector space. Here $\hat F_\mathbb Q$ is the
$\mathbb Q$-completion of $F$. This answers a problem of A.K. Bousfield for the
case of rational coefficients. As a direct consequence of this result it
follows that, a wedge of circles is $\mathbb Q$-bad in the sense of
Bousfield-Kan. The same methods as used in the proof of the above results allow
to show that, the homology $H_2(\hat F_\mathbb Z,\mathbb Z)$ is not divisible
group, where $\hat F_\mathbb Z$ is the integral pronilpotent completion of $F$.
"
"  This is the second in a series of papers where we construct an invariant of a
four-dimensional piecewise linear manifold $M$ with a given middle cohomology
class $h\in H^2(M,\mathbb C)$. This invariant is the square root of the torsion
of unusual chain complex introduced in Part I (arXiv:1605.06498) of our work,
multiplied by a correcting factor. Here we find this factor by studying the
behavior of our construction under all four-dimensional Pachner moves, and show
that it can be represented in a multiplicative form: a product of same-type
multipliers over all 2-faces, multiplied by a product of same-type multipliers
over all pentachora.
"
"  The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal
rates for kernel ridge regression in a very general setting. Its proof,
however, contains an error in its bound on the effective dimensionality. In
this note, we explain the mistake, provide a correct bound, and show that the
main theorem remains true.
"
"  We introduce a model of anonymous games with the player dependent action
sets. We propose several learning procedures based on the well-known Fictitious
Play and Online Mirror Descent and prove their convergence to equilibrium under
the classical monotonicity condition. Typical examples are first-order mean
field games.
"
"  We show that after forming a connected sum with a homotopy sphere, all
(2j-1)-connected 2j-parallelisable manifolds in dimension 4j+1, j > 0, can be
equipped with Riemannian metrics of 2-positive Ricci curvature. When j=1 we
extend the above to certain classes of simply-connected non-spin 5-manifolds.
The condition of 2-positive Ricci curvature is defined to mean that the sum of
the two smallest eigenvalues of the Ricci tensor is positive at every point.
This result is a counterpart to a previous result of the authors concerning the
existence of positive Ricci curvature on highly connected manifolds in
dimensions 4j-1 for j > 1, and in dimensions 4j+1 for j > 0 with torsion-free
cohomology.
"
"  In this paper, we introduce two new non-singular kernel fractional
derivatives and present a class of other fractional derivatives derived from
the new formulations. We present some important results of uniformly convergent
sequences of continuous functions, in particular the Comparison's principle,
and others that allow, the study of the limitation of fractional nonlinear
differential equations.
"
"  The multi-armed restless bandit problem is studied in the case where the
pay-off distributions are stationary $\varphi$-mixing. This version of the
problem provides a more realistic model for most real-world applications, but
cannot be optimally solved in practice, since it is known to be PSPACE-hard.
The objective of this paper is to characterize a sub-class of the problem where
{\em good} approximate solutions can be found using tractable approaches.
Specifically, it is shown that under some conditions on the $\varphi$-mixing
coefficients, a modified version of UCB can prove effective. The main challenge
is that, unlike in the i.i.d. setting, the distributions of the sampled
pay-offs may not have the same characteristics as those of the original bandit
arms. In particular, the $\varphi$-mixing property does not necessarily carry
over. This is overcome by carefully controlling the effect of a sampling policy
on the pay-off distributions. Some of the proof techniques developed in this
paper can be more generally used in the context of online sampling under
dependence. Proposed algorithms are accompanied with corresponding regret
analysis.
"
"  An explicit description of the virtualization map for the (modified) Nakajima
monomial model for crystals is given. We give an explicit description of the
Lusztig data for modified Nakajima monomials in type $A_n$.
"
"  We define compactifications of vector spaces which are functorial with
respect to certain linear maps. These ""many-body"" compactifications are
manifolds with corners, and the linear maps lift to b-maps in the sense of
Melrose. We derive a simple criterion under which the lifted maps are in fact
b-fibrations, and identify how these restrict to boundary hypersurfaces. This
theory is an application of a general result on the iterated blow-up of cleanly
intersecting submanifolds which extends related results in the literature.
"
"  We show that the m-fold connected sum $m\#\mathbb{C}\mathbb{P}^{2n}$ admits
an almost complex structure if and only if m is odd.
"
"  Skoda's 1972 result on ideal generation is a crucial ingredient in the
analytic approach to the finite generation of the canonical ring and the
abundance conjecture. Special analytic techniques developed by Skoda, other
than applications of the usual vanishing theorems and L2 estimates for the
d-bar equation, are required for its proof. This note (which is part of a
lecture given in the 60th birthday conference for Lawrence Ein) gives a
simpler, more straightforward proof of Skoda's result, which makes it a natural
consequence of the standard techniques in vanishing theorems and solving d-bar
equation with L2 estimates. The proof involves the following three ingredients:
(i) one particular Cauchy-Schwarz inequality for tensors with a special factor
which accounts for the exponent of the denominator in the formulation of the
integral condition for Skoda's ideal generation, (ii) the nonnegativity of
Nakano curvature of the induced metric of a special co-rank-1 subbundle of a
trivial vector bundle twisted by a special scalar weight function, and (iii)
the vanishing theorem and solvability of d-bar equation with L2 estimates for
vector bundles of nonnegative Nakano curvature on a strictly pseudoconvex
domain. Our proof gives readily other similar results on ideal generation.
"
"  We present a self-contained proof of Uhlenbeck's decomposition theorem for
$\Omega\in L^p(\mathbb{B}^n,so(m)\otimes\Lambda^1\mathbb{R}^n)$ for $p\in
(1,n)$ with Sobolev type estimates in the case $p \in[n/2,n)$ and
Morrey-Sobolev type estimates in the case $p\in (1,n/2)$. We also prove an
analogous theorem in the case when $\Omega\in L^p( \mathbb{B}^n, TCO_{+}(m)
\otimes \Lambda^1\mathbb{R}^n)$, which corresponds to Uhlenbeck's theorem with
conformal gauge group.
"
"  Composition and lattice join (transitive closure of a union) of equivalence
relations are operations taking pairs of decidable equivalence relations to
relations that are semi-decidable, but not necessarily decidable. This article
addresses the question, is every semi-decidable equivalence relation obtainable
in those ways from a pair of decidable equivalence relations? It is shown that
every semi-decidable equivalence relation, of which every equivalence class is
infinite, is obtainable as both a composition and a lattice join of decidable
equivalence relations having infinite equivalence classes. An example is
constructed of a semi-decidable, but not decidable, equivalence relation having
finite equivalence classes that can be obtained from decidable equivalence
relations, both by composition and also by lattice join. Another example is
constructed, in which such a relation cannot be obtained from decidable
equivalence relations in either of the two ways.
"
"  In this paper we study the category LCA(2) of certain non-locally compact
abelian topological groups, and extend the notion of Weil index. As
applications we deduce some product formulas for curves over local fields and
arithmetic surfaces.
"
"  We study how to sample paths of a random walk up to the first time it crosses
a fixed barrier, in the setting where the step sizes are iid with negative mean
and have a regularly varying right tail. We introduce a desirable property for
a change of measure to be suitable for exact simulation. We study whether the
change of measure of Blanchet and Glynn (2008) satisfies this property and show
that it does so if and only if the tail index $\alpha$ of the right tail lies
in the interval $(1, \, 3/2)$.
"
"  We define nearest-neighbour point processes on graphs with Euclidean edges
and linear networks. They can be seen as the analogues of renewal processes on
the real line. We show that the Delaunay neighbourhood relation on a tree
satisfies the Baddeley--M{\o}ller consistency conditions and provide a
characterisation of Markov functions with respect to this relation. We show
that a modified relation defined in terms of the local geometry of the graph
satisfies the consistency conditions for all graphs with Euclidean edges.
"
"  Let $X$ be a normal, connected and projective variety over an algebraically
closed field $k$. It is known that a vector bundle $V$ on $X$ is essentially
finite if and only if it is trivialized by a proper surjective morphism $f:Y\to
X$. In this paper we introduce a different approach to this problem which
allows to extend the results to normal, connected and strongly pseudo-proper
algebraic stack of finite type over an arbitrary field $k$.
"
"  In this paper we establish the characterization of the weighted BMO via two
weight commutators in the settings of the Neumann Laplacian $\Delta_{N_+}$ on
the upper half space $\mathbb{R}^n_+$ and the reflection Neumann Laplacian
$\Delta_N$ on $\mathbb{R}^n$ with respect to the weights associated to
$\Delta_{N_+}$ and $\Delta_{N}$ respectively. This in turn yields a weak
factorization for the corresponding weighted Hardy spaces, where in particular,
the weighted class associated to $\Delta_{N}$ is strictly larger than the
Muckenhoupt weighted class and contains non-doubling weights. In our study, we
also make contributions to the classical Muckenhoupt--Wheeden weighted Hardy
space (BMO space respectively) by showing that it can be characterized via area
function (Carleson measure respectively) involving the semigroup generated by
the Laplacian on $\mathbb{R}^n$ and that the duality of these weighted Hardy
and BMO spaces holds for Muckenhoupt $A^p$ weights with $p\in (1,2]$ while the
previously known related results cover only $p\in (1,{n+1\over n}]$. We also
point out that this two weight commutator theorem might not be true in the
setting of general operators $L$, and in particular we show that it is not true
when $L$ is the Dirichlet Laplacian $\Delta_{D_+}$ on $\mathbb{R}^n_+$.
"
"  We represent Matérn functions in terms of Schoenberg's integrals which
ensure the positive definiteness and prove the systems of translates of
Matérn functions form Riesz sequences in $L^2(\R^n)$ or Sobolev spaces. Our
approach is based on a new class of integral transforms that generalize Fourier
transforms for radial functions. We also consider inverse multi-quadrics and
obtain similar results.
"
"  The Hasse-Witt matrix of a hypersurface in ${\mathbb P}^n$ over a finite
field of characteristic $p$ gives essentially complete mod $p$ information
about the zeta function of the hypersurface. But if the degree $d$ of the
hypersurface is $\leq n$, the zeta function is trivial mod $p$ and the
Hasse-Witt matrix is zero-by-zero. We generalize a classical formula for the
Hasse-Witt matrix to obtain a matrix that gives a nontrivial congruence for the
zeta function for all $d$. We also describe the differential equations
satisfied by this matrix and prove that it is generically invertible.
"
"  Assume that $T$ is a self-adjoint operator on a Hilbert space $\mathcal{H}$
and that the spectrum of $T$ is confined in the union $\bigcup_{j\in
J}\Delta_j$, $J\subseteq\mathbb{Z}$, of segments $\Delta_j=[\alpha_j,
\beta_j]\subset\mathbb{R}$ such that $\alpha_{j+1}>\beta_j$ and $$ \inf_{j}
\left(\alpha_{j+1}-\beta_j\right) = d > 0. $$ If $B$ is a bounded (in general
non-self-adjoint) perturbation of $T$ with $\|B\|=:b<d/2$ then the spectrum of
the perturbed operator $A=T+B$ lies in the union $\bigcup_{j\in J}
U_{b}(\Delta_j)$ of the mutually disjoint closed $b$-neighborhoods
$U_{b}(\Delta_j)$ of the segments $\Delta_j$ in $\mathbb{C}$. Let $Q_j$ be the
Riesz projection onto the invariant subspace of $A$ corresponding to the part
of the spectrum of $A$ lying in $U_{b}\left(\Delta_j\right)$, $j\in J$. Our
main result is as follows: The subspaces $\mathcal{L}_j=Q_j(\mathcal H)$, $j\in
J$, form an unconditional basis in the whole space $\mathcal H$.
"
"  Given a combinatorial design $\mathcal{D}$ with block set $\mathcal{B}$, the
block-intersection graph (BIG) of $\mathcal{D}$ is the graph that has
$\mathcal{B}$ as its vertex set, where two vertices $B_{1} \in \mathcal{B}$ and
$B_{2} \in \mathcal{B} $ are adjacent if and only if $|B_{1} \cap B_{2}| > 0$.
The $i$-block-intersection graph ($i$-BIG) of $\mathcal{D}$ is the graph that
has $\mathcal{B}$ as its vertex set, where two vertices $B_{1} \in \mathcal{B}$
and $B_{2} \in \mathcal{B}$ are adjacent if and only if $|B_{1} \cap B_{2}| =
i$. In this paper several constructions are obtained that start with twofold
triple systems (TTSs) with Hamiltonian $2$-BIGs and result in larger TTSs that
also have Hamiltonian $2$-BIGs. These constructions collectively enable us to
determine the complete spectrum of TTSs with Hamiltonian $2$-BIGs (equivalently
TTSs with cyclic $2$-intersecting Gray codes) as well as the complete spectrum
for TTSs with $2$-BIGs that have Hamilton paths (i.e., for TTSs with
$2$-intersecting Gray codes).
In order to prove these spectrum results, we sometimes require ingredient
TTSs that have large partial parallel classes; we prove lower bounds on the
sizes of partial parallel clasess in arbitrary TTSs, and then construct larger
TTSs with both cyclic $2$-intersecting Gray codes and parallel classes.
"
"  A weak-strong uniqueness result is proved for measure-valued solutions to the
system of conservation laws arising in elastodynamics. The main novelty brought
forward by the present work is that the underlying stored-energy function of
the material is assumed strongly quasiconvex. The proof employs tools from the
calculus of variations to establish general convexity-type bounds on
quasiconvex functions and recasts them in order to adapt the relative entropy
method to quasiconvex elastodynamics.
"
"  On one hand, consider the problem of finding global solutions to a polynomial
optimization problem and, on the other hand, consider the problem of
interpolating a set of points with a complex exponential function. This paper
proposes a single algorithm to address both problems. It draws on the notion of
hyponormality in operator theory. Concerning optimization, it seems to be the
first algorithm that is capable of extracting global solutions from a
polynomial optimization problem where the variables and data are complex
numbers. It also applies to real polynomial optimization, a special case of
complex polynomial optimization, and thus extends the work of Henrion and
Lasserre implemented in GloptiPoly. Concerning interpolation, the algorithm
provides an alternative to Prony's method based on the Autonne-Takagi
factorization and it avoids solving a Vandermonde system. The algorithm and its
proof are based exclusively on linear algebra. They are devoid of notions from
algebraic geometry, contrary to existing methods for interpolation. The
algorithm is tested on a series of examples, each illustrating a different
facet of the approach. One of the examples demonstrates that hyponormality can
be enforced numerically to strenghten a convex relaxation and to force its
solution to have rank one.
"
"  There are many hard conjectures in graph theory, like Tutte's 5-flow
conjecture, and the 5-cycle double cover conjecture, which would be true in
general if they would be true for cubic graphs. Since most of them are
trivially true for 3-edge-colorable cubic graphs, cubic graphs which are not
3-edge-colorable, often called {\em snarks}, play a key role in this context.
Here, we survey parameters measuring how far apart a non 3-edge-colorable graph
is from being 3-edge-colorable. We study their interrelation and prove some new
results. Besides getting new insight into the structure of snarks, we show that
such measures give partial results with respect to these important conjectures.
The paper closes with a list of open problems and conjectures.
"
"  Let $p$ be a prime. A $p$-group $G$ is defined to be semi-extraspecial if for
every maximal subgroup $N$ in $Z(G)$ the quotient $G/N$ is a an extraspecial
group. In addition, we say that $G$ is ultraspecial if $G$ is semi-extraspecial
and $|G:G'| = |G'|^2$. In this paper, we prove that every $p$-group of
nilpotence class $2$ is isomorphic to a subgroup of some ultraspecial group.
Given a prime $p$ and a positive integer $n$, we provide a framework to
construct of all the ultraspecial groups order $p^{3n}$ that contain an abelian
subgroup of order $p^{2n}$. In the literature, it has been proved that every
ultraspecial group $G$ order $p^{3n}$ with at least two abelian subgroups of
order $p^{2n}$ can be associated to a semifield. We provide a generalization of
semifield, and then we show that every semi-extraspecial group $G$ that is the
product of two abelian subgroups can be associated with this generalization of
semifield.
"
"  We prove Cherlin's conjecture, concerning binary primitive permutation
groups, for those groups with socle isomorphic to $\mathrm{PSL}_2(q)$,
${^2\mathrm{B}_2}(q)$, ${^2\mathrm{G}_2}(q)$ or $\mathrm{PSU}_3(q)$. Our method
uses the notion of a ""strongly non-binary action"".
"
"  The concept of a C-class of differential equations goes back to E. Cartan
with the upshot that generic equations in a C-class can be solved without
integration. While Cartan's definition was in terms of differential invariants
being first integrals, all results exhibiting C-classes that we are aware of
are based on the fact that a canonical Cartan geometry associated to the
equations in the class descends to the space of solutions. For sufficiently low
orders, these geometries belong to the class of parabolic geometries and the
results follow from the general characterization of geometries descending to a
twistor space.
In this article we answer the question of whether a canonical Cartan geometry
descends to the space of solutions in the case of scalar ODEs of order at least
four and of systems of ODEs of order at least three. As in the lower order
cases, this is characterized by the vanishing of the generalized Wilczynski
invariants, which are defined via the linearization at a solution. The
canonical Cartan geometries (which are not parabolic geometries) are a slight
variation of those available in the literature based on a recent general
construction. All the verifications needed to apply this construction for the
classes of ODEs we study are carried out in the article, which thus also
provides a complete alternative proof for the existence of canonical Cartan
connections associated to higher order (systems of) ODEs.
"
"  We study the instability of standing wave solutions for nonlinear
Schrödinger equations with a one-dimensional harmonic potential in
dimension $N\ge 2$. We prove that if the nonlinearity is $L^2$-critical or
supercritical in dimension $N-1$, then any ground states are strongly unstable
by blowup.
"
"  This paper proposes the matrix-weighted consensus algorithm, which is a
generalization of the consensus algorithm in the literature. Given a networked
dynamical system where the interconnections between agents are weighted by
nonnegative definite matrices instead of nonnegative scalars, consensus and
clustering phenomena naturally exist. We examine algebraic and algebraic graph
conditions for achieving a consensus, and provide an algorithm for finding all
clusters of a given system. Finally, we illustrate two applications of the
proposed consensus algorithm in clustered consensus and in bearing-based
formation control.
"
"  In this paper we study the behavior of the fractions of a factorial design
under permutations of the factor levels. We focus on the notion of regular
fraction and we introduce methods to check whether a given symmetric orthogonal
array can or can not be transformed into a regular fraction by means of
suitable permutations of the factor levels. The proposed techniques take
advantage of the complex coding of the factor levels and of some tools from
polynomial algebra. Several examples are described, mainly involving factors
with five levels.
"
"  We propose new smoothed median and the Wilcoxon's rank sum test. As is
pointed out by Maesono et al.(2016), some nonparametric discrete tests have a
problem with their significance probability. Because of this problem, the
selection of the median and the Wilcoxon's test can be biased too, however, we
show new smoothed tests are free from the problem. Significance probabilities
and local asymptotic powers of the new tests are studied, and we show that they
inherit good properties of the discrete tests.
"
"  We define the notion of hom-Batalin-Vilkovisky algebras and strong
differential hom-Gerstenhaber algebras as a special class of hom-Gerstenhaber
algebras and provide canonical examples associated to some well-known
hom-structures. Representations of a hom-Lie algebroid on a hom-bundle are
defined and a cohomology of a regular hom-Lie algebroid with coefficients in a
representation is studied. We discuss about relationship between these classes
of hom-Gerstenhaber algebras and geometric structures on a vector bundle. As an
application, we associate a homology to a regular hom-Lie algebroid and then
define a hom-Poisson homology associated to a hom-Poisson manifold.
"
"The great pretender: Phosphorus mimics quite closely the chemistry of carbon in its low coordination states (that is, one and two). The picture illustrates some of the molecules which highlight this analogy: clockwise from the top: phosphaacetylene, phosphirane, phosphinine, the transient phosphinidene complex [HP-W(CO)5], phosphaferrocene, phosphole, and phosphaethene. Since the beginning of the seventies, organophosphorus chemistry has been completely rejuvenated by the discovery of stable derivatives in which phosphorus has the coordination numbers one or two. The chemistry of these compounds mimics the chemistry of their all-carbon analogues. In this Review article this analogy is discussed for the phosphorus counterparts of alkenes, alkynes, and carbenes. In each case, the synthesis, reactivity, and coordination modes are briefly examined. Some special electronic configurations are also discussed, which include one-electron PP bonds, strained bonds, and aromatic systems. To conclude, some potential applications of this chemistry in the areas of molecular materials and homogeneous catalysis are presented.
"
"Water has emerged as a versatile solvent for organic chemistry in recent years. Water as a solvent is not only inexpensive and environmentally benign, but also gives completely new reactivity. The types of organic reactions in water are broad including pericyclic reactions, reactions of carbanion equivalent, reactions of carbocation equivalent, reactions of radicals and carbenes, transition-metal catalysis, oxidations-reductions, which we discuss in this tutorial review. Aqueous organic reactions have broad applications such as synthesis of biological compounds from carbohydrates and chemical modification of biomolecules.

"
"Over the past decade, it has become clear that aqueous chemical processes occurring in cloud droplets and wet atmospheric particles are an important source of organic atmospheric particulate matter. Reactions of water-soluble volatile (or semivolatile) organic gases (VOCs or SVOCs) in these aqueous media lead to the formation of highly oxidized organic particulate matter (secondary organic aerosol; SOA) and key tracer species, such as organosulfates. These processes are often driven by a combination of anthropogenic and biogenic emissions, and therefore their accurate representation in models is important for effective air quality management. Despite considerable progress, mechanistic understanding of some key aqueous processes is still lacking, and these pathways are incompletely represented in 3D atmospheric chemistry and air quality models. In this article, the concepts, historical context, and current state of the science of aqueous pathways of SOA formation are discussed.

"
"Until recently, repetitive solid-phase synthesis procedures were used predominantly for the preparation of oligomers such as peptides, oligosaccharides, peptoids, oligocarbamates, peptide vinylogues, oligomers of pyrrolin-4-one, peptide phosphates, and peptide nucleic acids. However, the oligomers thus produced have a limited range of possible backbone structures due to the restricted number of building blocks and synthetic techniques available. Biologically active compounds of this type are generally not suitable as therapeutic agents but can serve as lead structures for optimization. “Combinatorial organic synthesis” has been developed with the aim of obtaining low molecular weight compounds by pathways other than those of oligomer synthesis. This concept was first described in 1971 by Ugi.[56f,g,59c] Combinatorial synthesis offers new strategies for preparing diverse molecules, which can then be screened to provide lead structures. Combinatorial chemistry is compatible with both solution-phase and solid-phase synthesis. Moreover, this approach is conducive to automation, as proven by recent successes in the synthesis of peptide libraries. These developments have led to a renaissance in solid-phase organic synthesis (SPOS), which has been in use since the 1970s. Fully automated combinatorial chemistry relies not only on the testing and optimization of known chemical reactions on solid supports, but also on the development of highly efficient techniques for simultaneous multiple syntheses. Almost all of the standard reactions in organic chemistry can be carried out using suitable supports, anchors, and protecting groups with all the advantages of solid-phase synthesis, which until now have been exploited only sporadically by synthetic organic chemists. Among the reported organic reactions developed on solid supports are Diels–Alder reactions, 1,3-dipolar cycloadditions, Wittig and Wittig–Horner reactions, Michael additions, oxidations, reductions, and Pd-catalyzed CC bond formation. In this article we present a comprehensive review of the previously published solid-phase syntheses of nonpeptidic organic compounds. The advantages of solid-phase synthesis have been recognized by organic chemists, who have adapted numerous well-known reactions for application with immobilized substrates. For example, a polymeric support may act as a protecting group for one functional group of a compound while a second functional group is derivatized. In other applications combinatorial syntheses on supports provide access to libraries of nonpeptides from collections of building blocks. This strategy is essential for supplying new compounds for modern automated screening assays used in the search and optimization of lead structures for drugs.

"
"The nucleophilicity N index (J. Org. Chem.2008, 73, 4615), the inverse of the electrophilicity, Image ID:c1ob05856h-t1.gif, and the recently proposed inverse of the electrodonating power,, (J. Org. Chem.2010, 75, 4957) have been checked toward (i) a series of single 5-substituted indoles for which rate constants are available, (ii) a series of para-substituted phenols, and for (iii) a series of 2,5-disubstituted bicyclic[2.2.1]hepta-2,5-dienes which display concurrently electrophilic and nucleophilic behaviors. While all considered indices account well for the nucleophilic behavior of organic molecules having a single substitution, the nucleophilicity N index works better for more complex molecules. Unlike, the inverse of the electrophilicity, Image ID:c1ob05856h-t3.gif, (R2 = 0.71), and the inverse of the electrodonating power, Image ID:c1ob05856h-t4.gif (R2 = 0.83), a very good correlation of the nucleophilicity N index of twelve 2-substituted-6-methoxy-bicyclic[2.2.1]hepta-2,5-dienes versus the activation energy associated with the nucleophilic attack on 1,1-dicyanoethylene is found (R2 = 0.99). This comparative study allows to assert that the nucleophilicity N index is a measure of the nucleophilicity of complex organic molecules displaying concurrently electrophilic and nucleophilic behaviors.

"
"One of the main goals of 21st century chemistry is to replace environmentally hazardous processes with energy efficient routes allowing to totally avoid the use and production of harmful chemicals and to maximise the quantity of raw material that ends up in the final product. Selective photocatalytic conversions will play a major role in this evolution and this account shows how photocatalysis is offering an alternative green route for the production of organics.

"
"The ring-strain theory, which Adolf von Baeyer formulated one hundred years ago, has been expanded in many directions; today, strain is discussed in terms of bond-length and bond-angle distortions as well as nonbonding interactions. Only in such terms can the stability of such highly strained compounds as tetra-tert-butyltetrahedrane and [1.1.1]propellane be understood. “Die Ringschließung ist offenbar diejenige Erscheinung, welche am meisten über die räumliche Anordnung der Atome Auskunft geben kann. Wenn eine Kette von 5 und 6 Gliedern sich leicht, eine von weniger oder mehr Gliedern sich schwierig oder auch gar nicht schließen läßt, so müssen dafür offenbar räumliche Gründe vorhanden sein.… Die vier Valenzen des Kohlenstoffatoms wirken in den Richtungen, welche den Mittelpunkt der Kugel mit den Tetraederecken verbinden, und welche miteinander einen Winkel von 109°28′ machen. Die Richtung der Anziehung kann eine Ablenkung erfahren, die jedoch eine mit der Größe der Letzteren wachsende Spannung zur Folge hat,”[

1 Ring formation is evidently the reaction that can provide the most important information on the spatial arrangement of atoms. Steric reasons should explain the observation that 5- or 6-membered chain undergoes ready closure, whereas a shorter or longer chain undergoes difficult or no closure.… The four valences of the carbon atom act in the directions that connect the center of a sphere with the corners of a tetrahedron and form angles of 109°28′ with one another. The direction of the attraction can deviate, but this in increasing strain as the sizes of the latter increase. ] This is the quintessence of the “ring-strain theory” formulated by Adolf von Baeyer over one hundred years ago. Although it is today only one facet of the many aspects of strain theory, it has repeatedly stimulated experimental and theoretical chemists. Among the most spectacular of the recent successes in synthetic chemistry are the syntheses of tetra-tert-butyltetrahedrane and [1.1.1]propellane. The reasons for the great stability of these two highly strained compounds are completely different. The experimental findings as well as the results of theoretical analysis by means of molecular mechanics and ab initio calculations have contributed decisively to our present state of knowledge of the structure, energy, and reactivity of organic compounds.\"
"The modes of formation of carbonaceous deposits (“coke”) during the transformation of organic compounds over acid and over bifunctional noble metal-acid catalysts are described. At low reaction temperatures, (<200°C) “coke” formation involves mainly condensation and rearrangement steps. Therefore, the deposits are not polyaromatic and their composition depends very much on the reactant. The retention of the “coke” molecules on the catalysts is mainly due to their strong adsorption and to their low volatility (gas-phase reactions) or to their low solubility (liquid-phase reactions). At high temperatures (>350°C), the coke components are polyaromatic. Their formation involves hydrogen transfer (acid catalysts) and dehydrogenation (bifunctional catalysts) steps in addition to condensation and rearrangement steps. On microporous catalysts, the retention of coke molecules is due to their steric blockage within the micropores.

"
"Microwave-assisted organic synthesis (MAOS) is rapidly becoming recognized as a valuable tool for easing some of the bottlenecks in the drug discovery process. This article outlines the basic principles behind the technology and summarizes the areas in which microwave technology has made an impact, to date.

"
"Chemistry on solid surfaces is central to many areas of practical interest such as heterogeneous catalysis, tribology, electrochemistry, and materials processing. With the development of many surface-sensitive analytical techniques in the past decades, great advances have been possible in our understanding of such surface chemistry at the molecular level. Earlier studies with model systems, single crystals in particular, have provided rich information about the adsorption and reaction kinetics of simple inorganic molecules. More recently, the same approach has been expanded to the study of the surface chemistry of relatively complex organic molecules, in large measure in connection with the selective synthesis of fine chemicals and pharmaceuticals. In this report, the chemical reactions of organic molecules and fragments on solid surfaces, mainly on single crystals of metals but also on crystals of metal oxides, carbides, nitrides, phosphides, sulfides and semiconductors as well as on more complex models such as bimetallics, alloys, and supported particles, are reviewed. A scheme borrowed from the organometallic and organic chemistry literature is followed in which key examples of representative reactions are cited first, and general reactivity trends in terms of both the reactants and the nature of the surface are then identified to highlight important mechanistic details. An attempt has been made to emphasize recent advances, but key earlier examples are cited as needed. Finally, correlations between surface and organometallic and organic chemistry, the relevance of surface reactions to applied catalysis and materials functionalization, and some promising future directions in this area are briefly discussed."
"Over the last decade, organic electrosynthesis has become recognized as one of the methodologies that can fulfill several important criteria that are needed if society is to develop environmentally compatible processes. It can be used to replace toxic or dangerous oxidizing or reducing reagents, reduce energy consumption, and can be used for the in situ production of unstable and hazardous reagents. These are just a few of the most important attributes that render electrochemistry environmentally useful. In this review the main characteristics of electrochemistry as a promising green methodology for organic synthesis are described and exemplified. Herein we provide basic information concerning the nature of electrosynthetic processes, paired electrochemical reactions, electrocatalytic reactions, reactions carried out in ionic liquids, electrogeneration of reactants, electrochemical reactions that use renewable starting materials (biomass), green organic electrosynthesis in micro- and nano-emulsions, the synthesis of complex molecules using an electrosynthetic key step, and conclude with some insights concerning the future. Throughout the review the “green aspects” of these topics are highlighted and their relationship with the twelve green chemistry principles is described.

"
"High-power ultrasound can generate cavitation within a liquid and through cavitation provide a source of energy which can be used to enhance a wide range of chemical processes. Such uses of ultrasound have been grouped under the general name sonochemistry. This review will concentrate on applications in organic synthesis where ultrasound seems to provide a distinct alternative to other, more traditional, techniques of improving reaction rates and product yields. In some cases it has also provided new synthesic pathways.

"
"Microwave-assisted organic chemistry is reviewed in the context of the methods employed. A range of technical difficulties indicated that specifically designed reactors were required. Hence, the CSIRO continuous microwave reactor (CMR) and microwave batch reactor (MBR) were developed for organic synthesis. On the laboratory scale, they operated at temperatures (pressures) up to 200°C (1400 kPa) and 260°C (10 MPa), respectively. Advantages and applications of the units are discussed, along with safety issues. Features include the capability for rapid, controlled heating and cooling of reaction mixtures, and elimination of wall effects. Concurrent heating and cooling, and differential heating were unique methodologies introduced to organic synthesis through the MBR. Applications of the microwave reactors for optimizing high-temperature preparations, e.g, the Willgerodt reaction and the Fischer indole synthesis, were demonstrated. Water was a useful pseudo-organic solvent, applicable to environmentally benign synthetic chemistry.

"
"In recent years, photoredox catalysis has come to the forefront in organic chemistry as a powerful strategy for the activation of small molecules. In a general sense, these approaches rely on the ability of metal complexes and organic dyes to convert visible light into chemical energy by engaging in single-electron transfer with organic substrates, thereby generating reactive intermediates. In this Perspective, we highlight the unique ability of photoredox catalysis to expedite the development of completely new reaction mechanisms, with particular emphasis placed on multicatalytic strategies that enable the construction of challenging carbon–carbon and carbon–heteroatom bonds.

"
"Enzymes as catalysts in synthetic organic chemistry gained importance in the latter half of the 20th century, but nevertheless suffered from two major limitations. First, many enzymes were not accessible in large enough quantities for practical applications. The advent of recombinant DNA technology changed this dramatically in the late 1970s. Second, many enzymes showed a narrow substrate scope, often poor stereo- and/or regioselectivity and/or insufficient stability under operating conditions. With the development of directed evolution beginning in the 1990s and continuing to the present day, all of these problems can be addressed and generally solved. The present Perspective focuses on these and other developments which have popularized enzymes as part of the toolkit of synthetic organic chemists and biotechnologists. Included is a discussion of the scope and limitation of cascade reactions using enzyme mixtures in vitro and of metabolic engineering of pathways in cells as factories for the production of simple compounds such as biofuels and complex natural products. Future trends and problems are also highlighted, as is the discussion concerning biocatalysis versus nonbiological catalysis in synthetic organic chemistry. This Perspective does not constitute a comprehensive review, and therefore the author apologizes to those researchers whose work is not specifically treated here.

"
"Robert B. Woodward, a supreme patterner of chaos, was one of my teachers. I dedicate this lecture to him, for it is our collaboration on orbital symmetry conservation, the electronic factors which govern the course of chemical reactions, which is recognized by half of the 1981 Nobel Prize in Chemistry. From Woodward I learned much: the significance of the experimental stimulus to theory, the craft of constructing explanations, and the importance of asethetics in science. I will try to show you how these characteristics of chemical theory may be applied to the construction of conceptual bridges between inorganic and organic chemistry.

"
"Methods for the incorporation of organic functionality onto semiconductor surfaces have seen immense progress in recent years. Of the multiple methods developed, the direct, covalent attachment of organic moieties is valuable because it allows for excellent control of the interfacial properties. This review article will focus on a number of synthetic strategies that have been developed to exploit the unique reactivity of group-IV surfaces under vacuum. A picture of the semiconductor surface and its reactions will be developed within the standard framework of organic chemistry with emphasis on the importance of combined experimental and theoretical approaches. Three broad areas of organic chemistry will be highlighted, including nucleophilic/electrophilic, pericyclic, and aromatic reactions. The concept of nucleophilicity and electrophilicity will be discussed within the context of dative bonding and proton transfer of amines and alcohols. Pericyclic reactions cover the [4 + 2] or Diels–Alder cycloaddition, [2 + 2] cycloaddition, dipolar, and ene reactions. Examples include the reactions of alkenes, dienes, ketones, nitriles, and related multifunctional molecules at the interface. Aromaticity and the use of directing groups to influence the distribution of surface products will be illustrated with benzene, xylene, and heteroaromatic compounds. Finally, multifunctional molecules are used to describe the competition and selectively observed among different surface reactions.

"
"The relationship between spatial ability and performance in organic chemistry was studied in four organic chemistry courses designed for students with a variety of majors including agriculture, biology, health sciences, pre-med, pre-vet, pharmacy, medicinal chemistry, chemistry, and chemical engineering.

Students with high spatial scores did significantly better on questions which required problem solving skills, such as completing a reaction or outlining a multi-step synthesis, and questions which required students to mentally manipulate two-dimensional representations of a molecule. Spatial ability was not significant, however, for questions which could be answered by rote memory or by the application of simple algorithms.

Students who drew preliminary figures or extra figures when answering questions were more likely to get the correct answer. High spatial ability students were more likely to draw preliminary figures, even for questions that did not explicitly require these drawings. When questions required preliminary or extra figures, low spatial ability students were more likely to draw figures that were incorrect. Low spatial ability students were also more likely to draw structures that were lopsided, ill-proportioned, and nonsymmetric.

The results of this study are interpreted in terms of a model which argues that high spatial ability students are better at the early stages of problem solving described as “understanding” the problem. A model is also discussed which explains why students who draw preliminary or extra figures for questions are more likely to get correct answers."
"The flipped classroom is a pedagogical approach that moves course content from the classroom to homework, and uses class time for engaging activities and instructor-guided problem solving. The course content in a sophomore level Organic Chemistry I course was assigned as homework using video lectures, followed by a short online quiz. In class, students' misconceptions were addressed, the concepts from the video lectures were applied to problems, and students were challenged to think beyond given examples. Students showed increased comprehension of the material and appeared to improve their performance on summative assessments (exams). Students reported feeling more comfortable with the subject of organic chemistry, and became noticeably passionate about the subject. In addition to being an effective tool for teaching Organic Chemistry I at a small college, flipping the organic chemistry classroom may help students take more ownership of their learning.

"
"Understanding the mechanisms of chemical reactions, especially catalysis, has been an important and active area of computational organic chemistry, and close collaborations between experimentalists and theorists represent a growing trend. This Perspective provides examples of such productive collaborations. The understanding of various reaction mechanisms and the insight gained from these studies are emphasized. The applications of various experimental techniques in elucidation of reaction details as well as the development of various computational techniques to meet the demand of emerging synthetic methods, e.g., C–H activation, organocatalysis, and single electron transfer, are presented along with some conventional developments of mechanistic aspects. Examples of applications are selected to demonstrate the advantages and limitations of these techniques. Some challenges in the mechanistic studies and predictions of reactions are also analyzed.

"
"Tricoordinate, I(III), and pentacoordinate, I(V), polyvalent iodine compounds have been known for over a century. In the last twenty years, new polyvalent iodine reagents have been introduced along with synthetic methodologies, based on these and derived reagents, that play an ever increasing role in contemporary organic chemistry. In this Perspective, an overview of these developments is provided with emphasis on the chemistry and uses of aryl-, alkenyl-, and alkynyliodonium salts in preparative and synthetic organic chemistry. It is hoped that this brief overview, along with recent more comprehensive reviews of the field, will stimulate further developments and applications of this useful class of compounds across a broad spectrum of organic chemistry.

"
"While organic electrochemistry can look quite different to a chemist not familiar with the technique, the reactions are at their core organic reactions. As such, they are developed and optimized using the same physical organic chemistry principles employed during the development of any other organic reaction. Certainly, the electron transfer that triggers the reactions can require a consideration of new “wrinkles” to those principles, but those considerations are typically minimal relative to the more traditional approaches needed to manipulate the pathways available to the reactive intermediates formed downstream of that electron transfer. In this review, three very different synthetic challenges—the generation and trapping of radical cations, the development of site-selective reactions on microelectrode arrays, and the optimization of current in a paired electrolysis—are used to illustrate this point.

"
"Theoretical reactivity indices based on the conceptual Density Functional Theory (DFT) have become a powerful tool for the semiquantitative study of organic reactivity. A large number of reactivity indices have been proposed in the literature. Herein, global quantities like the electronic chemical potential μ, the electrophilicity ω and the nucleophilicity N indices, and local condensed indices like the electrophilic and nucleophilic Parr functions, as the most relevant indices for the study of organic reactivity, are discussed. View Full-Text
"
"Gold! Gold! Gold from the American River! The seminal reports on asymmetric catalysis with secondary amines attracted waves of researchers to the “gold mine” of organocatalysis. Particular challenges, milestones, and future directions of asymmetric aminocatalysis are discussed in this Review.Catalysis with chiral secondary amines (asymmetric aminocatalysis) has become a well-established and powerful synthetic tool for the chemo- and enantioselective functionalization of carbonyl compounds. In the last eight years alone, this field has grown at such an extraordinary pace that it is now recognized as an independent area of synthetic chemistry, where the goal is the preparation of any chiral molecule in an efficient, rapid, and stereoselective manner. This has been made possible by the impressive level of scientific competition and high quality research generated in this area. This Review describes this “Asymmetric Aminocatalysis Gold Rush” and charts the milestones in its development. As in all areas of science, progress depends on human effort.

"
"A student-centered learning technique, process-oriented, guided-inquiry learning (POGIL), has been developed as a pedagogical technique that facilitates collaborative and cooperative learning in the chemistry classroom. With the use of this technique, students enhance their higher-order thinking skills and process skills synergistically. In addition, they develop positive relationships with other students in the course. POGIL was recently implemented at a mid-sized, comprehensive public institution and used in the organic chemistry sequence. Comparisons of the ACS exam percentile rankings and incoming proficiency (ACS scores and grade point averages) data were made to determine the extent of the effect that POGIL had on student learning when compared to students who had been taught using traditional methods. Overall, the data provide evidence to suggest that students learning by the POGIL method have a greater grasp of content knowledge than students who learned by the traditional lecture approach, as evidenced by higher final exam scores for POGIL students. The POGIL experience positively impacted students of all levels of proficiency. Difficulties associated with the implementation and perceptions of reform-based learning methods are addressed.

"
"Garlic and onion, the best known representatives of the Allium genus form the basis of this review on organosulfur compounds. It covers the biosynthesis of the S-alk(en)yl-L-cysteine-S-oxides, the precursors of the compounds responsible for flavor and odor, the enzymatic transformations that occur when the bulbs are crushed, and the elucidation of structures and reaction sequences, as well as the physiological effects caused by the plants' chemical components. Two of the many interesting compounds are singled out here: the lacrimatory factor in onion (1) and the antithrombotic agent, Ajoen (2), in garlic. A Cook's tour is presented of the organosulfur chemistry of the genus Allium, as represented, inter alia, by garlic (Allium sativum L.) and onion (Allium cepa L.). We report on the biosynthesis of the S-alk(en)yl-L-cysteine S-oxides (aroma and flavor precursors) in intact plants and on how upon cutting or crushing the plants these precursors are cleaved by allinase enzymes, giving sulfenic acids—highly reactive organosulfur intermediates. In garlic, 2-propenesulfenic acid gives allicin, a thiosulfinate with antibiotic properties, while in onion 1-propenesulfenic acid rearranges to the sulfine (Z)-propanethial S-oxide, the lachrymatory factor (LF) of onion. Highlights of onion chemistry include the assignment of stereochemistry to the LF and determination of the mechanism of its dimerization; the isolation, characterization, and synthesis of thiosulfinates which most closely duplicate the taste and aroma of the freshly cut bulb, and additional unusual compounds such as zwiebelanes (dithiabicyclo[2.1.1]hexanes), a bis-sulfine (a 1,4-butanedithial S,S′-dioxide), antithrombotic and antiasthmatic cepaenes (α-sulfinyl disulfides), and vic-disulfoxides. Especially noteworthy in the chemistry of garlic are the discovery of ajoene, a potent antithrombotic agent from garlic, and the elucidation of the unique sequence of reactions that occur when diallyl disulfide, which is present in steam-distilled garlic oil, is heated. Reaction mechanisms under discussion include [3, 3]- and [2, 3]-sigma-tropic rearrangements involving sulfur (e.g. sulfoxide-accelerated thio- and dithio-Claisen rearrangements) and cycloadditions involving thiocarbonyl systems. In view of the culinary importance of alliaceous plants as well as the unique history of their use in folk medicine, this survey concludes with a discussion of the physiological activity of the components of these plants: cancer prevention, antimicrobial activity, insect and animal attractive/repulsive activity, olfactory–gustatory–lachrymatory properties, effect on lipid metabolism, platelet aggregation inhibitory activity and properties associated with ajoene. And naturally, comments about onion and garlic induced bad breath and heartburn may not be overlooked."
"Transition metal catalyzed CC bond formations belong to the most important reactions in organic synthesis. One particularly interesting reaction is olefin metathesis, a metal-catalyzed exchange of alkylidene moieties between alkenes. Olefin metathesis can induce both cleavage and formation of CC double bonds. Special functional groups are not necessary. Although this reaction—which can be catalyzed by numerous transition metals—is used in industry, its potential in organic synthesis was not recognized for many years. The recent abrupt end to this Sleeping-Beauty slumber has several reasons. Novel catalysts can effect the conversion of highly fictionalized and sterically demanding olefins under mild reaction conditions and in high yields. Improved understanding of substrate–catalyst interaction has greatly contributed to the recent establishment of olefin metathesis as a synthetic method. In addition to the preparation of polymers with fine-tuned characteristics, the metathesis today also provides new routes to compounds of low molecular weight. The highly developed ring-closing metathesis has been proven to be key step in the synthesis of a growing number of natural products. At the same time interesting applications can be envisioned for newly developed variants of bimolecular metathesis. Improvements in the selective cross-metathesis of acyclic olefins as well as promising attempts to include alkynes as viable substrates provide for a vivid development of the metathesis chemistry. Tailor-made polymers and complex compounds of low molecular weight are accessible for olefin metathesis with modern Ru–and Mo–alkylidene complexes. Ring-closing and ring-opening metatheses, selective cross-metathesis of alkenes, and stereoselective olefin metatheses, which proceed “waste-free” and atom economically, are just a few of the reactions that have joined the arsenal of methods in organic chemistry, or will do so soon. New generations of catalysts make this over 40-year-old reaction principle feasible, even for the future.
"
"Labels of the reconstruction: Chemical modification of proteins with synthetic probes is a powerful means of elucidating protein functions in live cells and of influencing these functions. New reactions that can be successfully applied in living systems represent a worthy challenge to organic chemistry, especially as the labeling and manipulation of endogenous proteins in their natural habitats is currently at an early stage. The modification of proteins with synthetic probes is a powerful means of elucidating and engineering the functions of proteins both in vitro and in live cells or in vivo. Herein we review recent progress in chemistry-based protein modification methods and their application in protein engineering, with particular emphasis on the following four strategies: 1) the bioconjugation reactions of amino acids on the surfaces of natural proteins, mainly applied in test-tube settings; 2) the bioorthogonal reactions of proteins with non-natural functional groups; 3) the coupling of recognition and reactive sites using an enzyme or short peptide tag–probe pair for labeling natural amino acids; and 4) ligand-directed labeling chemistries for the selective labeling of endogenous proteins in living systems. Overall, these techniques represent a useful set of tools for application in chemical biology, with the methods 2–4 in particular being applicable to crude (living) habitats. Although still in its infancy, the use of organic chemistry for the manipulation of endogenous proteins, with subsequent applications in living systems, represents a worthy challenge for many chemists.

"
"Enzymes have great potential as catalysts for use in synthetic organic chemistry. Applications of enzymes in synthesis have so far been limited to a relatively small number of largescale hydrolytic processes used in industry, and to a large number of small-scale syntheses of materials used in analytical procedures and in research. Changes in the technology for production of enzymes (in part attributable to improved methods from classical microbiology, and in part to the promise of genetic engineering) and for their stabilization and manipulation now make these catalysts practical for wider use in large-scale synthetic organic chemistry. This paper reviews the status of the rapidly developing field of enzyme-catalyzed organic synthesis, and outlines both present opportunities and probable future developments in this field.

The use of esterases, lipases, and other enzymes in organic synthesis is becoming increasingly common. In addition to the obvious advantages of enzymes is the fact that the intermediates in multistep reactions often do not need to be purified since the enzyme transforms only a specific substrate. Thus, the treatment of a reaction mixture with an enzyme is also a form of form of purification.

"
"When an arenesulfonyl azide, particularly p-toluenesulfonyl azide, reacts, in the presence of a base, with a compound containing an active methylene group, the two hydrogen atoms of the active methylene group are replaced by a diazo group to form a diazo compound and an arenesulfonamide. The method may be used for the synthesis of the diazo derivatives of cyclopentadienes, cyclohexadienes, 1,3-dicarbonyl, 1,3-disulfonyl, and 1,3-ketosulfonyl compounds, ketones, carbonic acid esters, and β-iminoketones. Secondary reactions can lead to azo compounds and heterocycles such as 1,2,3-triazoles, 1,2,3-thiadiazoles, and pyrazolin-4-ones. Azidinium salts react in the same way, but in this case an acidic reaction medium is necessary, a fact that is sometimes advantageous.

"
"The use of elemental fluorine as a reagent over the period 1997–2006 for carbon–fluorine bond formation in organic synthesis is reviewed. Recent advances in the exhaustive fluorination of ethers and esters to give perfluorinated systems, selective direct fluorination of aliphatic, aromatic, heterocyclic and carbonyl systems and the application of microreactor techniques to direct fluorination are discussed.

"
"Evolution in the test tube: With the help of the error-prone polymerase chain reaction as a method for random mutagenesis, an efficient gene-expression system, and a screening test for the rapid identification of enantioselective catalysts, it is possible to increase sequentially the ee value of an unselective lipase-catalyzed ester hydrolysis (see picture on the right).

"
"Since the days of the “Manhattan Project” our knowledge of the chemistry of fluorine-con-taining compounds has increased tremendously. At first only considered out of curiosity, this area of research bordering between inorganic and organic chemistry has developed from modest beginnings to such a complexity of interesting ramifications that it would be very difficult to review all aspects in detail. The motivating force behind the developments in this field were, above all, the unusual properties the numerous compounds acquire on introduction of fluorine substituents. The spectrum of unusual modes of behavior extend from extreme stabilization in fluorine-containing polymers and blood-substitutes to drastic increases in reactivity in pharmacologically and phytomedicinally active substances and in dyes. Moreover, one physical peculiarity of the fluorine atom, its magnetic moment, opens a new route to the decyphering of metabolic processes.—Parallel to the ever increasing number of fluorinated compounds there has been a concomitant increase in the variety of methods of synthesis and reagents, which is frequently difficult to overview in entirety, even for the specialists. Older, simple methods such as halogen metathesis with metal fluorides and electrofluorination are being refined as far as higher selectivity is concerned, and completely new, very reactive substances such as hypofluorites and noble-gas fluorides enable fascinating reactions on complex substrates, e. g. steroids and nucleobases. The present review is an attempted documentation and classification of the methods as well as a critical appraisal, not least from the standpoint of efficiency and economic aspects.

The spectrum of changes the fluorine substituents can bring about in organic molecules extends from extreme stabilization (e. g. in polymers and blood-substitutes) to drastic increases in reactivity (e. g. in Pharmaceuticals).—A selection of fluorinating agents is given below.

"
"First described more than two decades ago, microwave-assisted organic synthesis has matured from a laboratory curiosity to an established technique that today is heavily used in both academia and industry. One of the most valuable advantages of using controlled microwave dielectric heating for chemical synthesis is the dramatic reduction in reaction times: from days and hours to minutes and seconds. As will be explained in this tutorial review, there are many more good reasons why organic chemists are nowadays incorporating dedicated microwave reactors into their daily work routine.

"
"Organocopper reagents provide the most general synthetic tools in organic chemistry for nucleophilic delivery of hard carbanions to electrophilic carbon centers. A number of structural and mechanistic studies have been reported and have led to a wide variety of mechanistic proposals, some of which might even be contradictory to others. With the recent advent of physical and theoretical methodologies, the accumulated knowledge on organocopper chemistry is being put together into a few major mechanistic principles. This review will summarize first the general structural features of organocopper compounds and the previous mechanistic arguments, and then describe the most recent mechanistic pictures obtained through high-level quantum mechanical calculations for three typical organocuprate reactions, carbocupration, conjugate addition, and SN2 alkylation. The unified view on the nucleophilic reactivities of metal organocuprate clusters thus obtained has indicated that organocuprate chemistry represents an intricate example of molecular recognition and supramolecular chemistry, which chemists have long exploited without knowing it. Reasoning about the uniqueness of the copper atom among neighboring metal elements in the periodic table will be presented. Controversy and contradictions have dominated the mechanistic studies of organocuprate(I) reactions. Now, more than half a century since the discovery of these uniquely effective synthetic reagents, the mechanisms of the reactions have been revealed through experimental and theoretical by studies using the most advanced instrumentation available (see scheme).
"
"The hetero Diels-Alder reaction is one of the most important methods for the synthesis of heterocycles. In this article an overview is given for the period since 1989 describing the reaction of heterobutadienes such as oxabutadienes, thiabutadienes, azabutadienes, diaazabutadienes, nitroso-alkenes and nitroalkenes as well as of heterodienophiles such as carbonyls, thiocarbonyls, imines, iminium salts, azo- and nitroso compounds. In addition, several other less common hetero Diels-Alder reactions such as cycloadditions of thiaazabutadienes, oxaazabutadienes, dioxabutadienes, dithiabutadienes, oxathiabutadienes, diazaoxabutadienes as well as the use of N-sulfinyl-phosphaalkynes and other dienophiles are mentioned. A main point of discussion is the stereoselectivity of the reactions and the preparation of enantiopure compounds either using dienes and dienophiles carrying a chiral auxiliary or employing chiral Lewis acids. A point stressed is the synthesis of natural products using hetero Diels-Alder reactions leading to carbohydrates, alkaloids, terpenes, antibiotics, mycotoxins, cytochalasans, antitumor agents and several other classes of natural products.

Another topic is the use of high pressure in hetero Diels-Alder reactions discussing the influence on the rate constants and the stereoselectivity. Finally, modern developments such as reactions on solid phase, the use of catalytic monoclonal antibodies, transformations in aqueous solution and the microwave activation are described."
"The development in the uses of polymer-bound catalysts and reagens in organic synthesis is outlined. Literature dating from 1983 to July, 1997 is discussed.

The development in the uses of polymer-bound catalysts and reagens in organic synthesis is outlined. Literature dating from 1983 to July, 1997 is discussed"
"100 years ago Pictet and Spengler discovered the most important method for the synthesis of alkaloid scaffolds—the condensation of aryl ethylamines and aldehydes. Today, efficient enzymatic and non-enzymatic methods are both available for the Pictet–Spengler reaction.Alkaloids are an important class of natural products that are widely distributed in nature and produced by a large variety of organisms. They have a wide spectrum of biological activity and for many years were used in folk medicine. These days, alkaloids also have numerous applications in medicine as therapeutic agents. The importance of these natural products in inspiring drug discovery programs is proven and, therefore, their continued synthesis is of significant interest. The condensation discovered by Pictet and Spengler is the most important method for the synthesis of alkaloid scaffolds. The power of this synthesis method has been convincingly proven in the construction of stereochemicaly and structurally complex alkaloids."
"Lipases trapped in hydrophobic organic/inorganic materials result in chemically and thermally stable heterogeneous catalysts that display unusually high catalytic activities. The entrapment of lipases and other biocatalysts by sol-gel encapsulation is an interesting approach to enzyme immobilization, which has the aim of increasing enzyme activity in organic solvents. Investigations that have been carried out to optimize encapsulation and to characterize the lipase-containing gels are reviewed. Possible applications in organic synthesis are indicated."
"Truth Cerium: Although cerium-based catalysts already find wide use in catalytic converters, because of their oxygen storage capacity, they are also being more frequently applied in organic chemistry and catalysis because of their exceptional redox and surface acid–base properties. This Review analyzes the main research directions explored during the last ten years according to the nature of the ceria sites: basic, acidic, redox, or a combination of these. Ceria has been the subject of thorough investigations, mainly because of its use as an active component of catalytic converters for the treatment of exhaust gases. However, ceria-based catalysts have also been developed for different applications in organic chemistry. The redox and acid–base properties of ceria, either alone or in the presence of transition metals, are important parameters that allow to activate complex organic molecules and to selectively orient their transformation. Pure ceria is used in several organic reactions, such as the dehydration of alcohols, the alkylation of aromatic compounds, ketone formation, and aldolization, and in redox reactions. Ceria-supported metal catalysts allow the hydrogenation of many unsaturated compounds. They can also be used for coupling or ring-opening reactions. Cerium atoms can be added as dopants to catalytic system or impregnated onto zeolites and mesoporous catalyst materials to improve their performances. This Review demonstrates that the exceptional surface (and sometimes bulk) properties of ceria make cerium-based catalysts very effective for a broad range of organic reactions.

"
"The following reactions are described in which the synthetic interest is predominantly determined by the polar characteristics of the radicals and substrates involved: 1. Homolytic Amination of Aromatic Compounds 1.1. Amination with N-Chloroalkylamines 1.2. Amination with Hydroxylamine and Hdroxylamie-O-sulfonic Acid 2. Homolytic Substitution of Protonated Heteroaromatic Bases 2.1. Homolytic Alkylation 2.2. Homolytic Acylation 2.3. Homolytic α-Oxyalkylation 2.4. Homolytic Aminocarbonylation and α-Amidoalkylation 3. Selective Halogenation of Saturated Aliphatic Compounds 4. Alternating Addition of C-Radicals to Conjugated Olefins

"
"The scanning of objects with an intimacy not previously known to provide images of surfaces with atomic and molecular resolution is possible with scanning tunneling microscopy and atomic force microscopy. As a result these techniques are entering many fields of physics, chemistry, and biology. Apart from the observation and characterization of surfaces, designed modifications such as depicted on the right in an image of circular hydrocarbon domains on a fluorocarbon monolayer can be made—which opens up further applications. The enigmatic faces were created by localized manipulation with the AFM probe.
 When Kekulé awoke from dreams of snakes biting their own tails, he didn't have the benefit of a scanning tunneling microscope (STM) image to confirm that his vision of benzene as a cyclic molecule was accurate. References to STM in the chemical literature increase steadily, although the technique was perhaps oversold in its early days of the 1980s, with such promises as DNA sequencing and tailored bi-molecular chemical reactions (literally, two molecules). Publications alternate between attempting to explain the process by which images of traditional insulators are obtained and simply presenting the end images themselves as stunning views of atoms and molecules. While imaging mechanisms are still being debated, these instruments' ability to “see” single molecules has been established, albeit at the fringes of our expectations. For example, whereas STM studies at present might not be able to answer the question of why adsorption of CO doubles the density of platinum atoms on the surface of a single crystal of the metal, the images go far in illustrating that this is a process which platinum undergoes. As with any emerging analytical tool, these scanning, very localized microscopic methods are undergoing the growing pains of irreproducible results and mis-marketed artifacts. Nonetheless, we assemble here, primarily for the uninitiated, a collection of careful and credible studies to mark the progress of scanning tunneling microscopy and atomic force microscopy into chemistry, and to encourage a healthy blend of idealism and skepticism toward future work.


"
"This study focuses on the implementation of a peer-led team learning (PLTL) instructional approach for all students in an undergraduate organic chemistry course and the evaluation of student outcomes over 8 years. Students who experienced the student-centered instruction and worked in small groups facilitated by a peer leader (treatment) in 1996–1999 were compared with students who experienced the traditional recitation section (control) in 1992–1994. Quantitative and qualitative data show statistically significant improvements in student performance, retention, and attitudes about the course. These findings suggest that using undergraduate leaders to implement a peer-led team learning model that is built on a social constructivist foundation is a workable mechanism for effecting change in undergraduate science courses. © 2002 Wiley Periodicals, Inc. J Res Sci Teach 39: 606–632, 2002"
"This prospective study applied self-determination theory to investigate the effects of students' course-specific self-regulation and their perceptions of their instructors' autonomy support on adjustment and academic performance in a college-level organic chemistry course. The study revealed that: (1) students' reports of entering the course for relatively autonomous (vs. controlled) reasons predicted higher perceived competence and interest/enjoyment and lower anxiety and grade-focused performance goals during the course, and were related to whether or not the students dropped the course; and (2) students' perceptions of their instructors' autonomy support predicted increases in autonomous self-regulation, perceived competence, and interest/enjoyment, and decreases in anxiety over the semester. The change in autonomous self-regulation in turn predicted students' performance in the course. Further, instructor autonomy support also predicted course performance directly, although differences in the initial level of students' autonomous self-regulation moderated that effect, with autonomy support relating strongly to academic performance for students initially low in autonomous self-regulation but not for students initially high in autonomous self-regulation. © 2000 John Wiley & Sons, Inc. Sci Ed 84:740–756, 2000.

"
"Iodine and compounds of iodine in higher oxidation states have emerged as versatile and environmentally benign reagents for organic chemistry. One of the most impressive recent achievements in this area has been the discovery of catalytic activity of iodine in numerous oxidative transformations leading to the formation of new Csingle bondO, Csingle bondN, and Csingle bondC bonds in organic compounds. These catalytic transformations in many cases are very similar to the transition metal-catalyzed reactions, but have the advantage of environmental sustainability and efficient utilization of natural resources. Iodine is an environmentally friendly and a relatively inexpensive element, which is currently underutilized in industrial applications. One of the main goals of this review is presenting to industrial researchers the benefits of using catalytic iodine in chemical technology as an environmentally sustainable alternative to transition metals. The present review summarizes catalytic applications of iodine and compounds of iodine in organic synthesis. The material is organized according to the nature of active catalytic species (hypoiodite, trivalent, or pentavalent hypervalent iodine species) generated in these reactions from appropriate pre-catalysts. Numerous synthetic procedures based on iodine(III) or iodine(V) catalytic species in the presence of hydrogen peroxide, Oxone, peroxyacids or other stoichiometric oxidants are summarized. A detailed discussion of catalytic cycles involving hypervalent iodine, hypoiodites, and other active intermediates is presented."
"The rates and selectivities of the hydrogen-atom abstraction reactions of electrically-neutral free radicals are known to depend on polar effects which operate in the transition state. Thus, an electrophilic species such as an alkoxyl radical abstracts hydrogen much more readily from an electron-rich C–H bond than from an electron-deficient one of similar strength. The basis of polarity-reversal catalysis (PRC) is to replace a single-step abstraction, that is slow because of unfavourable polar effects, with a two-step process in which the radicals and substrates are polarity-matched. This review explores the concept of PRC and describes its application in a variety of situations relevant to mechanistic and synthetic organic chemistry.

"
"Biomass and waste exhibit great potential for replacing fossil resources in the production of chemicals. The search for alternative reaction media to replace petroleum-based solvents commonly used in chemical processes is an important objective of significant environmental consequence. Recently, bio-based derivatives have been either used entirely as green solvents or utilized as pivotal ingredients for the production of innovative solvents potentially less toxic and more bio-compatible. This review presents the background and classification of these new media and highlights recent advances in their use in various areas including organic synthesis, catalysis, biotransformation and separation. The greenness, advantages and limitations of these solvents are also discussed.

"
"Solvent effects on a number of different processes have been surveyed, and results of the application of multiple linear regression analysis are discussed. The processes examined include examples of solubility of gases or vapours, distribution coefficients of solutes between water and a series of solvents, and solvent effects on conformational equilibria, on keto–enol tautomerism, and on reaction rates. It is shown that two particular equations, that due to Koppel and Palm and extended by Makitra and Pirig, and that due to Abraham, Kamlet, and Taft, can cope quite satisfactorily with solvent effects on these various processes. It is pointed out that interpretation of parameters obtained from equations that involve macroscopic quantities such as ΔG≠ or ΔG0 is not necessarily straightforward, and that some model is needed in order to interpret these macroscopic quantities in terms of microscopic quantities that can characterise, for example, solute–solvent"
"2-Oxazolines have permeated numerous sub-disciplines in synthetic organic chemistry in the 100 years since their discovery. This versatile heterocycle has served as protecting group, coordinating ligand, and activating moiety, often exhibiting all of these characteristics in a single transformation. The well-defined reactivity of chiral oxazolines has given rise to numerous highly efficient strategies for asymmetric synthesis including their use as ligands in asymmetric catalysis. Various unique properties have dictated their use in a multitude of dissimilar applications: as monomers in polymer production, as moderators in analytical processes, and as conformationally rigid peptide mimics in medicinal chemistry. Even natural systems have chosen to incorporate oxazolines into their chemical arsenal, as evidenced by the rapidly growing number of identified natural products and their attendant pharmacological properties.

Future studies will undoubtedly uncover unexpected properties and applications. The number of important publications detailing oxazoline-related chemistry is growing, suggesting that research in this area, as in synthetic organic chemistry in general, has yet to mature."
"This review is concerned with the use of pillared, cation-exchanged and acid-treated montmorillonites as catalysts for reactions which can be carried out on a preparative laboratory or industrial scale. Most notably the clays display Brønsted and Lewis acid activities, but Diels-Alder reactions can also be effected using these materials. An introduction covers the basis of the various types of catalytic activity and gives some experimental details, while the main body of the text reviews reaction types, giving typical conditions and yields.

"
"The choice of a new generation: 2-Methyltetrahydrofuran (2-MeTHF) is a biomass-derived chemical that finds widespread use as alternative (co)solvent for organic reactions, both in industry and academia. 2-MeTHF has applications in organometallics, organocatalysis, biotransformations, and biomass processing. This Minireview describes current applications of 2-MeTHF, and gives a prognosis for future uses.

2-Methyl-tetrahydrofuran (2-MeTHF) can be derived from renewable resources (e.g., furfural or levulinic acid) and is a promising alternative solvent in the search for environmentally benign synthesis strategies. Its physical and chemical properties, such as its low miscibility with water, boiling point, remarkable stability compared to other cyclic-based solvents such as THF, and others make it appealing for applications in syntheses involving organometallics, organocatalysis, and biotransformations or for processing lignocellulosic materials. Interestingly, a significant number of industries have also started to assess 2-MeTHF in several synthetic procedures, often with excellent results and prospects. Likewise, preliminary toxicology assessments suggest that the use of 2-MeTHF might even be extended to more processes in pharmaceutical chemistry. This Minireview describes the properties of 2-MeTHF, the state-of-the-art of its use in synthesis, and covers several outstanding examples of its application from both industry and academia.

"
"This is the definitive text in a market consisting of senior and graduate environmental engineering students who are taking a chemistry course. The text is divided into a chemistry fundamentals section and an applications section. In this new edition, the authors have retained the thorough, yet concise, coverage of basic chemical principles from general, physical, equilibrium, organic, biochemistry, colloid, and nuclear chemistry. In addition, the authors have retained their classic two-fold approach of (1) focusing on the aspects of chemistry that are particularly valuable for solving environmental problems, and (2) laying the groundwork for understanding water and wastewater analysis-a fundamental basis of environmental engineering practice and research.' from publisher's description.PART ONE: Fundamentals of chemistry and environmental engineering and science -- Introduction -- Basic concepts from general chemistry -- Basic concepts from physical chemistry -- Basic concepts from equilibrium chemistry -- Basic concepts from organic chemistry -- Basic concepts from biochemistry -- Basic concepts from colloid chemistry -- Basic concepts from nuclear chemistry. PART TWO: Water and wastewater analysis -- Introduction -- Statistical analysis of analytical data -- Basic concepts from quantitative chemistry -- Instrumental methods of analysis -- Turbidity -- Color -- Standard solutions -- phD -- Acidity -- Alkalinity -- Hardness -- Residual chlorine and chlorine demand -- Chloride -- Dissolved oxygen -- Biochemical oxygen demand -- Chemical oxygen demand -- Nitrogen -- Solids -- Iron and manganese -- Fluoride -- Sulfate -- Phosphorus and phosphate -- Oil and grease -- Volatile acids -- Gas analysis -- Trace contaminants.

"
"Organofluorine compounds are widely used in many different applications, ranging from pharmaceuticals and agrochemicals to
advanced materials and polymers. It has been recognised for many years that fluorine substitution can confer useful molecular properties such as enhanced stability and hydrophobicity. Another impact of fluorine substitution is to influence the conformations of
organic molecules. The stereoselective introduction of fluorine atoms can therefore be exploited as a conformational tool for the
synthesis of shape-controlled functional molecules. This review will begin by describing some general aspects of the C–F bond and
the various conformational effects associated with C–F bonds (i.e. dipole–dipole interactions, charge–dipole interactions and hyperconjugation). Examples of functional molecules that exploit these conformational effects will then be presented, drawing from a
diverse range of molecules including pharmaceuticals, organocatalysts, liquid crystals and peptides.
"
"Palladium catalysed reactions serve as versatile tools in synthetic organic chemistry. By using these methodologies carbon monoxide can be introduced directly into a number of different sites in an organic molecule leading to the synthesis of carbonyl compounds and carboxylic acid derivatives.

The substrate is reacted with a nucleophile such as an alcohol (alkoxycarbonylation), a primary or secondary amine (aminocarbonylation) or water (hydroxycarbonylation) or an organometallic reagent (formylation, crosscoupling reactions) in the presence of carbon monoxide and a palladium complex. Cyclocarbonylation, leading to a variety of heterocyclic compounds, can be regarded as a special type of the former reactions. Double carbonylation usually takes place at elevated CO pressures and produces α-ketoamides or -esters. Cascade reactions may be defined as multireaction ‘one-pot’ sequences in which the first reaction creates the functionality to trigger the second one. The use of two-phase processes makes catalyst recovery and recirculation, one of the greatest drawbacks of homogeneous catalytic processes, possible. As palladium-catalysed carbonylations usually tolerate a great variety of functional groups, they are attractive methods for the selective synthesis of intermediates of natural or biologically active products.

The reactions mentioned above can often be achieved in good yield and with high selectivity usually under very mild conditions. Because of the vast number of publications in this field, this review is dealing only with the conversion of organic halides and its contents is limited to the description of the most recent developments published until the end of 2001."
"Fluorocarbons, organic molecules with carbon skeletons and fluorine “skins”, differ fundamentally from their hydrocarbon counterparts in interesting and useful ways. A selection of the myriad applications fluorocarbons and their derivatives have found in modern life is described and related to molecular properties. Salient aspects of the nature and reactivity of fluorocarbon compounds are highlighted by comparison with their more familiar hydrocarbon analogues."
"The fixation of ligands onto molecules, surfaces and materials by use of reactions using a simple and unified chemistry is among the everlasting desires of chemists. Besides the general insensitivity with respect to the chemical structures of the ligand, the completeness of the reaction as well as the insensitivity from external reaction parameters (i.e.: solvents, ambient temperature) is wished. The copper(I)-catalysed azide/alkyne “click”-reaction (also termed Sharpless “click”-reaction, a variation of the Huisgen 1,3-dipolar cycloaddition reaction between terminal acetylenes and azides) is a recent re-discovery of a reaction fulfilling these requirements. Extremely high yields (usually above 95%) are combined with a high tolerance of functional groups and reactions running at moderate temperatures (25°C - 70 °C). The present review assembles recent literature for applications of these reactions in the field of material science, in particular on surfaces, polymers, and for the ligation of ligands to larger biomolecules, including own publications in this field. Since this is an extremely fast developing area, this review offers important knowledge to the interested reader. A number of >64 references are included.
"
"Various commonly used organic solvents were dried with several different drying agents. A glovebox-bound coulometric Karl Fischer apparatus with a two-compartment measuring cell was used to determine the efficiency of the drying process. Recommendations are made relating to optimum drying agents/conditions that can be used to rapidly and reliably generate solvents with low residual water content by means of commonly available materials found in most synthesis laboratories. The practical method provides for safer handling and drying of solvents than methods calling for the use of reactive metals, metal hydrides, or solvent distillation."
"Both chemistry teachers and nonmajor students appear to agree that freshman chemistry may well be the most problematic traditional science discipline taught in the first year of college—as far as students' misunderstandings, learning difficulties, and misconceptions are concerned. The above is probably due to the many abstract, nonintuitive concepts, which are not directly interrelated. Consequently, in such cases, the powerful, general teaching strategy of “concept mapping” must be replaced by alternative, specific strategies. Selected illustrative examples of students' learning difficulties and misconceptions in freshman general and organic chemistry are presented in the students' terms, followed by the corresponding successfully applied, specific, concept-oriented, eclectic intervention strategies the author uses in order to overcome the difficulties. Based on longitudinal in-class observations, interpretive study, and analysis it is suggested that those students' misconceptions in freshman chemistry which are not interrelated logically and/or derived from one another are not prone to the general “concept mapping” approach and should be dealt with by using the appropriate, specific teaching strategy."
"A novel post-synthesis analysis tool is presented which evaluates quality of the organic preparation based on yield, cost, safety, conditions and ease of workup/purification. The proposed approach is based on assigning a range of penalty points to these parameters. This semi-quantitative analysis can easily be modified by other synthetic chemists who may feel that some parameters should be assigned different relative penalty points. It is a powerful tool to compare several preparations of the same product based on safety, economical and ecological features."
"Compact flow reactors have been constructed and optimized to perform continuous organic photochemistry on a large scale. The reactors were constructed from commercially available or customized immersion well equipment combined with UV-transparent, solvent-resistant fluoropolymer (FEP) tubing. The reactors were assessed using the [2 + 2] photocycloaddition of malemide 1 and 1-hexyne forming the cyclobutene product 2 and the intramolecular [5 + 2] photocycloaddition of 3,4-dimethyl-1-pent-4-enylpyrrole-2,5-dione 3 to form the bicyclic azepine 4. The reactors were shown to be capable of producing >500 g of 2 and 175 g of 4 in a continuous 24 h processing period. Due to the facile control of irradiation time, the continuous flow reactor was also shown to be superior to a batch reactor for performing a problematic photochemical reaction on a larger scale.

"
"A series of organic chromophores have been synthesized in order to approach optimal energy level composition in the TiO2−dye−iodide/triiodide system in the dye-sensitized solar cells. HOMO and LUMO energy level tuning is achieved by varying the conjugation between the triphenylamine donor and the cyanoacetic acid acceptor. This is supported by spectral and electrochemical experiments and TDDFT calculations. These results show that energetic tuning of the chromophores was successful and fulfilled the thermodynamic criteria for dye-sensitized solar cells, electrical losses depending on the size and orientation of the chromophores were observed."
"Preparative organic synthesis was investigated in aqueous media at temperatures up to 300 °C. Experiments were conducted with a recently disclosed pressurized microwave batch reactor (MBR) or in conventionally heated autoclaves. Thirty-six examples are presented. Among these, methods were developed for a Fischer synthesis, an intramolecular aldol condensation that was scaled up, decarboxylation of indole-2-carboxylic acid, Rupe rearrangement of 1-ethynyl-1-cyclohexanol, isomerization of carvone to carvacrol, and conversion of phenylacetylene to acetophenone. The applicability of high-temperature water was also demonstrated for biomimetic processes important in food, flavor, and aroma chemistry and for tandem reactions such as formation of 2-methyl-2,3-dihydrobenzofuran from allyl phenyl ether. When addition of acid or base was necessary, less agent was usually required for high-temperature processes than for those at and below boiling, and the reactions often proceeded more selectively. In some instances the requirement was orders of magnitude lower, with obvious consequences for safe, economic processing and for lowering costs of effluent disposal. The diversity of reactions indicates that high-temperature aqueous media could play an increasingly important role in the development of new preparative processes.

"
"Silica sulfuric acid and silica chloride, two silica based solid acids have been used for various organic functional group transformations either as reagent or as catalyst. All reactions have been carried out under mild and heterogeneous conditions. These reagents were used for C-C, C-N, and C-O bond formation and cleavage and also deprotection of different protecting groups. Silica sulfuric acid was recycled in many cases, and reused. Silica chloride has been used as a starting material for preparation of some new silica based reagents with special properties."
"The use of ionic liquids as aids for microwave heating of nonpolar solvents has been investigated. We show that hexane and toluene together with solvents such as THF and dioxane can be heated way above their boiling point in sealed vessels using a small quantity of an ionic liquid, thereby allowing them to be used as media for microwave-assisted chemistry. Using the appropriate ionic liquid, the heating can be performed with no contamination of the solvent. To show the applicability of the system, two test reactions have been successfully performed.

"
"Recent investigations have shown that cycloaddition reactions, widely used in organic chemistry to form ring compounds, can also be applied to link organic molecules to the (001) surfaces of crystalline silicon, germanium, and diamond. While these surfaces are comprised of SiSi, GeGe, and CC structural units that resemble the CC bonds of organic alkenes, the rates and mechanisms of the surface reactions show some distinct differences from those of their organic counterparts This article reviews recent studies of [2 + 2], [4 + 2] Diels−Alder, and other cycloaddition reactions of organic molecules with semiconductor surfaces and summarizes the current understanding of the reaction pathways.

"
"The potential of 4-thiazolidones (2,4-thiazolidinediones, 2-thioxo(imino)-4-thiazolidones) as drugs is under consideration by the pharmaceutical science since the beginning of the XX century. During recent years a new phase has been seen in this field. Centerarian history of synthetic research possibilities of these heterocycles lead to diversity in modelling biologically active compounds using 4-thiazolidone scaffolds. Modification of the 4- thiazolidone cycle on 2-, 3-, 4- or 5-position is successful to achieve synthetic products with a wide spectrum of pharmacological activity and has received considerable attention in this review. Currently 4-thiazolidones are considered as a new class of antidiabetic (insulin-sensitising) drugs and potent aldose reductase inhibitors, which possess potential for the treatment of diabetes complications (cataract, nephropathy, neuropathy). Novel 4- thiazolidones are undergoing different stages of clinical trials as potential thyromimetic, antimicrobial, antiviral, anti-ischaermic, cardiovascular, anticancer drugs.
"
"Sc(OTf)3 is a new type of a Lewis acid that is different from typical Lewis acids such as AlCl3, BF3, SnCl4, etc. While most Lewis acids are decomposed or deactivated in the presence of water, Sc(OTf)3 is stable and works as a Lewis acid in water solutions. Many nitrogen-containing compounds such as imines and hydrazones are also successfully activated by using a small amount of Sc(OTf)3 in both organic and aqueous solvents. In addition, Sc(OTf)3 can be recovered after reactions are completed and can be reused. While lanthanide triflates [Ln(OTf)3] have similar properties, the catalytic activity of Sc(OTf)3 is higher than that of Ln(OTf)3 in several cases.

"
"The synthesis of complex molecules requires control over both chemical reactivity and reaction conditions. While reactivity drives the majority of chemical discovery, advances in reaction condition control have accelerated method development/discovery. Recent tools include automated synthesizers and flow reactors. In this Synopsis, we describe how flow reactors have enabled chemical advances in our groups in the areas of single-stage reactions, materials synthesis, and multistep reactions. In each section, we detail the lessons learned and propose future directions.

"
"The use of water as solvent features many benefits such as improving reactivities and selectivities, simplifying the workup procedures, enabling the recycling of the catalyst and allowing mild reaction conditions and protecting-group free synthesis in addition to being benign itself. In addition, exploring organic chemistry in water can lead to uncommon reactivities and selectivities complementing the organic chemists' synthetic toolbox in organic solvents. Studying chemistry in water also allows insight to be gained into Nature's way of chemical synthesis. However, using water as solvent is not always green. This tutorial review briefly discusses organic synthesis in water with a Green Chemistry perspective.

"
"Catalytic C–H functionalizations are increasingly viable tools for sustainable syntheses. In recent years, inexpensive cobalt complexes were identified as powerful catalysts for C–H arylations with challenging organic electrophiles. In particular, cobalt complexes of N-heterocyclic carbenes enabled high catalytic efficacy under exceedingly mild reaction conditions. This strategy set the stage for challenging direct alkylations with primary and sterically hindered secondary alkyl halides. Herein, the recent rapid evolution of cobalt-catalyzed C–H transformations with organic electrophiles is reviewed until summer 2014.

"
"21st Century DESs: Worries about the sustainability of our civilization on Earth are forcing changes on all aspects of industrial production. In organic synthesis, solvents (including their production and degradation) are the main waste component in reactions. Deep eutectic solvents (and related mixtures) offer an irresistible opportunity to improve the sustainability of processes in this century. This microreview summarizes the use of deep eutectic solvents (DESs) and related melts in organic synthesis. Solvents of this type combine the great advantages of other proposed environmentally benign alternative solvents, such as low toxicity, high availability, low inflammability, high recyclability, low volatility, and low price, avoiding many disadvantages of the more modern media. The fact that many of the components of these mixtures come directly from nature assures their biodegradability and renewability. The classification and distribution of the reactions into different sections in this microreview, as well as the emphasis paid to their scope, easily allow a general reader to understand the actual state of the art and the great opportunities opened, not only for academic purposes but also for industry.

"
"Reactions triggered by light constitute a treasure trove of unique synthetic methods that are available to chemists. Photoinduced redox processes using visible light in conjunction with sensitizing dyes offer a great variety of catalytic transformations useful in the realm of organic synthesis. The recent literature amply shows that this preparative toolbox is expanding substantially. This review discusses historical and contemporary work in the area of photoredox catalysis with [Ru(bpy)3]2+. Elegant examples from the most recent literature document the importance of this fast developing area of research. The photoredox chemistry has also emerged as a promising bond-making and bond-breaking tool for chemical biology and materials chemistry. A review with 96 references.

"
"Computed enthalpies of formation for various Lewis acid complexes with representative unsaturated compounds (aldehydes, imines, alkynes, and alkenes) provide a means to evaluate the applicability of a particular catalyst in a catalytic reaction. As expected, main group Lewis acids such as BX3 show much stronger complexes with heteroatoms than with carbon−carbon multiple bonds (σ-electrophilic Lewis acids). Gold(I) and copper(I) salts with non-nucleophilic anions increase the relative strength of coordination to the carbon−carbon multiple bonds (π-electrophilic Lewis acids). As representative examples for the use of σ-electrophilic Lewis acids in organic synthesis, the Lewis acid mediated allylation reactions of aldehydes and imines with allylic organometallic reagents which give the corresponding homoallyl alcohols and amines, respectively, are mentioned. The allylation method is applied for the synthesis of polycyclic ether marine natural products, such as hemibrevetoxin B, gambierol, and brevetoxin B. As representative examples for the use of π-electrophilic Lewis acids in organic synthesis, the Zr-, Hf-, or Al-catalyzed trans-stereoselective hydro- and carbosilylation/stannylation of alkynes is mentioned. This method is extended to σ−π chelation controlled reduction and allylation of certain alkynylaldehydes. Gold- and copper-catalyzed benzannulation of ortho-alkynylaldehydes (and ketones) with alkynes (and alkenes) is discovered, which proceeds through the reverse electron demand Diels−Alder type [4 + 2] cycloaddition catalyzed by the π-electrophilic Lewis acids. This reaction is applied for the short synthesis of (+)-ochromycinone. Palladium and platinum catalysts act as a σ- and/or π-electrophilic catalyst depending on substrates and reaction conditions.

"
"Bioconjugation is a burgeoning field of research. Novel methods for the mild and site-specific derivatization of proteins, DNA, RNA, and carbohydrates have been developed for applications such as ligand discovery, disease diagnosis, and high-throughput screening. These powerful methods owe their existence to the discovery of chemoselective reactions that enable bioconjugation under physiological conditions-a tremendous achievement of modern organic chemistry. Here, we review recent advances in bioconjugation chemistry. Additionally, we discuss the stability of bioconjugation linkages-an important but often overlooked aspect of the field. We anticipate that this information will help investigators choose optimal linkages for their applications. Moreover, we hope that the noted limitations of existing bioconjugation methods will provide inspiration to modern organic chemists.
"
"Several important solvothermal (including hydrothermal) in situ metal/ligand reactions and their mechanisms, including dehydrogenative carbon−carbon coupling, hydroxylation of aromatic rings, cycloaddition of organic nitriles with azide and ammonia, transformation of inorganic and organic sulfur, as well as the CuII to CuI reduction, are outlined in this Account. The current progress clearly demonstrates the important potential of such reactions in the crystal engineering of functional coordination compounds and one-pot synthesis of some unusual organic ligands that are inaccessible or not easily obtainable via conventional methods, thereby substantiating our expectation that a new bridge has been created between coordination chemistry and synthetic organic chemistry.

"
